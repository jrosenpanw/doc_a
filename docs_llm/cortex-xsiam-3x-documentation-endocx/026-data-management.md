## Data management

### Broker VM

Set up and configure the Broker VM to create a secure connection for
routing endpoints, collecting logs, and forwarding logs and files for
analysis.  Learn how to manage the Broker VM, and implement it within a
high availability (HA) cluster setup.

#### What is the Broker VM?

The Palo Alto Networks Broker VM is a secured virtual machine,
integrated with Cortex XSIAM, that bridges your network and Cortex
XSIAM. By setting up the Broker VM, you establish a secure connection in
which you can route your endpoints, collect logs, and forward logs and
files for analysis.

Cortex XSIAM can leverage the Broker VM to run different services
separately using the same Palo Alto Networks authentication. After you
complete the initial setup, the Broker VM automatically receives updates
and enhancements from Cortex XSIAM, providing you with new capabilities
without having to install a new VM or manually update the existing VM.

![](media/rId4560.png){width="5.833333333333333in"
height="3.3541666666666665in"}

According to your Cortex XSIAM license, the following figure illustrates
the different Broker VM features that could be available on your
organization side:

![](media/rId4563.png){width="5.833333333333333in" height="2.49375in"}

#### Set up and configure Broker VM

You can set up a standalone Broker VM or add a Broker VM to a High
Availability (HA) cluster to prevent a single point of failure. For more
information, see [Broker VM High Availability
Cluster](#UUID23bf9c984626b51101b7e7fee66538b8).

**Setup**

To set up the Broker virtual machine (VM), you need to deploy an image
created by Palo Alto Networks on your network or supported cloud
infrastructure and activate the available applications. You can set up
several Broker VMs for the same tenant to support larger environments.
Ensure each environment matches the necessary requirements.

##### Requirements

Before you set up the Broker VM, verify you meet the following
requirements:

Hardware

For standard installation, use a minimum of a 4-core processor, 8 GB
RAM, and 512 GB disk.

- If you only intend to use the Broker VM for the agent proxy, you can
  use a 2-core processor.

- If you intend to use the Broker VM for the agent installer and content
  caching, you must use a minimum of an 8-core processor and increase
  the disk space allocated for data storage to 1024 GB. For more
  information, see [Increase Broker VM storage allocated for data
  caching](#UUID6d23c0dc17ebf6d797a12705e2bd1b79).

> **Note**
>
> The Broker VM comes with a 512 GB disk. Therefore, deploy the Broker
> VM with thin provisioning, meaning the hard disk can grow up to 512 GB
> but will do so only if needed.

Bandwidth

Bandwidth is higher than 10 mbit/s.

When the Broker VM is collecting data, the optimal outgoing bandwidth
into the Cortex XSIAM server should be about 25% of the incoming data
traffic into the Broker VM applets.

> **Important**
>
> There can be instances in which the Broker VM requires up to 50% of
> the incoming bandwidth as outgoing. Such instances can be, network
> instability between the Broker VM and Cortex XSIAM, or data that is
> being collected, but not well compressed.

Virtual machine compatibility

Ensure that your virtual machine (VM) is compatible with one of the
following options and install the applicable broker image according to
the installation steps provided:

+-----------------------+-----------------------+----------------------------------------------------+
| Infrastructure        | Image Type            | Broker Image Installation                          |
+=======================+=======================+====================================================+
| Alibaba Cloud         | QCOW2                 | [Set up Broker VM on Alibaba                       |
|                       |                       | Cloud](#UUID149a0c6f5e503dbca27403a1cf06f01d)      |
+-----------------------+-----------------------+----------------------------------------------------+
| Amazon Web Services   | VMDK                  | [Set up Broker VM on Amazon Web                    |
| (AWS)                 |                       | Services](#UUID388bfa322f5e2bbf6202e9e85b17e451)   |
+-----------------------+-----------------------+----------------------------------------------------+
| Google Cloud Platform | VMDK                  | [Set up Broker VM on Google Cloud Platform         |
|                       |                       | (GCP)](#UUID8e07af9f8f6d4468b5d647474bcba55f)      |
+-----------------------+-----------------------+----------------------------------------------------+
| KVM                   | QCOW2                 | [Set up Broker VM on KVM using                     |
|                       |                       | Ubuntu](#UUIDb179fbefe4e0383c1175ecc1742a08c1)     |
+-----------------------+-----------------------+----------------------------------------------------+
| Microsoft Azure       | VHD (Azure)           | [Set up Broker VM on Microsoft                     |
|                       |                       | Azure](#UUID0ff99738409e18d53f2b852dcad7d2dc)      |
+-----------------------+-----------------------+----------------------------------------------------+
| Microsoft Hyper-V     | VHD                   | Hyper-V 2012 or later                              |
| 2012                  |                       |                                                    |
|                       |                       | [Set up Broker VM on Microsoft                     |
|                       |                       | Hyper-V](#UUID02c961186a431f52918d63291ed0b2a8)    |
+-----------------------+-----------------------+----------------------------------------------------+
| Nutanix Hypervisor    | QCOW2                 | Nutanix AHV 2021                                   |
|                       |                       |                                                    |
|                       |                       | [Set up Broker VM on Nutanix                       |
|                       |                       | Hypervisor](#UUID4ca6b27901059cc28f4a81dd55c53d8e) |
+-----------------------+-----------------------+----------------------------------------------------+
| VMware ESXi           | OVA                   | VMware ESXi 6.5 or later                           |
|                       |                       |                                                    |
|                       |                       | [Set up Broker VM on VMware ESXi using vSphere     |
|                       |                       | Client](#UUIDd14485ff8489bcf0b2158473372d7063)     |
+-----------------------+-----------------------+----------------------------------------------------+

Communication between services and applications

Enable communication between the Broker Service, and other Palo Alto
Networks services and applications.

> **Important**
>
> The internal network for the Broker VM must be unique and reserved.
> Other devices should not use the same IP as the Broker VM internal
> network as it can lead to communication issues with the Broker VM.

+-----------------------------------------------------+-----------------------------------+
| FQDN, Protocol, and Port                            | Description                       |
+=====================================================+===================================+
| (*Default*)                                         | Broker\'s NTP server used for     |
|                                                     | broker registration and           |
| - `time.google.com`                                 | communication encryption. The     |
|                                                     | Broker VM provides default        |
| - `pool.ntp.org`                                    | servers you can use, or you can   |
|                                                     | define an NTP server of your      |
| UDP port 123                                        | choice.                           |
+-----------------------------------------------------+-----------------------------------+
| `br-<XDR tenant>.xdr.<region>.paloaltonetworks.com` | Broker Service server depending   |
|                                                     | on the region of your deployment, |
| HTTPS over TCP port 443                             | such as `us` or `eu`.             |
+-----------------------------------------------------+-----------------------------------+
| `distributions.traps.paloaltonetworks.com`          | Information needed to communicate |
|                                                     | with your Cortex XSIAM tenant.    |
| HTTPS over TCP port 443                             | Used by tenants deployed in all   |
|                                                     | regions.                          |
+-----------------------------------------------------+-----------------------------------+
| `br-<xdr-tenant>.xdr.federal.paloaltonetworks.com`  | Broker Service server for Federal |
|                                                     | (US Government) deployment.       |
| HTTPS over TCP port 443                             |                                   |
+-----------------------------------------------------+-----------------------------------+
| `distributions-prod-fed.traps.paloaltonetworks.com` | Used by tenants with Federal (US  |
|                                                     | Government) deployment            |
| HTTPS over TCP port 443                             |                                   |
+-----------------------------------------------------+-----------------------------------+
| From Broker VM version 19.x.x and later, you can    | Broker VM web console             |
| navigate to the following URL to open the Broker VM |                                   |
| web console: `https://<broker_vm_ip_address>.:4443` | > **Note**                        |
|                                                     | >                                 |
| HTTPS over TCP port 4443                            | > When DHCP is not enabled in     |
|                                                     | > your network and there isn\'t   |
|                                                     | > an IP address for your Broker   |
|                                                     | > VM, configure the Broker VM     |
|                                                     | > with a static IP using the      |
|                                                     | > serial console menu.            |
+-----------------------------------------------------+-----------------------------------+

Enable access to Cortex XSIAM

Enable access to Cortex XSIAM from the Broker VM to allow communication
between agents and collectors and Cortex XSIAM. The Broker VM
communicates with the Cortex XSIAM tenant with TLS 1.2 (or higher, if
that applies).

For more information on enabling access to Cortex XSIAM, see [Enable
access to required PANW
resources](#UUID24cb15454f44c259b484435238bb6a33).

> **Important**
>
> If you use SSL decryption in your firewalls and proxies, see the
> Understanding CA certificate functionality in Broker VM deployments
> section below. In addition, verify that the proxies used support
> HTTP/2, gRPC-specific headers, and HTTP/2 trailers, and the inspection
> policies support gRPC traffic. Any devices that you use with this
> configuration should also support these standards.
>
> When adding a CA certificate to the broker is not possible, ensure
> that you've added the Broker Service FQDNs to the SSL Decryption
> Exclusion list on your firewalls. For more information on adding a
> trusted self-signed certificate authority, see [Update the Trusted CA
> Certificate for the Broker
> VM](#X8ebd10e8892aaa9ac8269e947a8721c0e959746) in Task 1. Configure
> the Broker VM settings.

##### Understanding CA certificate functionality in Broker VM deployments

The Broker VM utilizes a CA certificate to establish trust with
intermediary network devices, such as firewalls performing SSL/TLS
decryption, positioned between the Broker VM and the tenant environment.
Failure of the Broker VM to validate the certificate presented by an
intermediate network component results in the termination of the SSL/TLS
connection.

This CA certificate is optional to configure depending on your system
configurations and helps provide more flexibility in securing
communications between the Broker VM and the tenant according to your
preferences and network topology. Specifically, it can help facilitate
all communication between the Broker VM and tenant, such as the
following:

- Broker VM configuration: Secure transmission of configuration
  parameters.

- Broker VM upgrades: Authenticated delivery and execution of upgrade
  packages.

- Metric Uploads: Encrypted and authenticated transfer of operational
  metrics to the tenant.

> **Note**

- > When configuring a Local Agent Settings applet with installer and
  > content caching, you need to configure an SSL certificate for the
  > Broker VM as explained in the task below. For more information on
  > specific requirements for the Local Agent Settings applet, see
  > [Activate Local Agent
  > Settings](#UUIDb8df6e3fcc33d9e9cf6530ce57a3119d).

- > Keep in mind that several Broker VM applets, such as the Syslog
  > Collector and Kafka Collector, have their own dedicated CA
  > certificate bundle.

##### Initial Setup

Perform the following procedures in the order listed below.

###### Task 1. Generate a token for your broker

1.  Select Settings \> Configurations \> Data Broker \> Broker VMs.

2.  Click Add Broker \> Generate Token, and copy to your clipboard. The
    token is valid for 24 hours. A new token is generated each time you
    select **Generate Token**.

- You\'ll paste this token after configuring settings and the Broker VM
  is registered in [Task 2. Register your Broker
  VM](#Xfe48604f4ad1c818d427bf5230d7c7751170d30).

###### Task 2. Open the Broker VM URL

Depending on the Broker VM version, navigate to either of the following
URLs:

- From Broker VM version 19.x.x and later:
  `https://<broker_vm_ip_address>.:4443`

- From Broker VM version 18.x.x and earlier:
  `https://<broker_vm_ip_address>/`

> **Note**
>
> When DHCP is not enabled in your network and there isn\'t an IP
> address for your Broker VM, configure the Broker VM with a static IP
> using the serial console menu.

###### Task 3. Log in and set a new password

Log in with the default password `!nitialPassw0rd`, and then define your
own unique password. The password must contain a minimum of eight
characters, contain letters and numbers, and at least one capital letter
and one special character.

##### How to configure Broker VM settings

Perform the following procedures in the order listed below.

###### Task 1. Configure the Broker VM settings

1.  Define the network interfaces settings.

- Review the pre-configured **Name**, **IP** address, and
  **MAC Address**, and select the **Address Allocation**: **DHCP**
  (default) or **Static**. If you choose **Static**, define the static
  **IP** address, **Netmask**, **Default Gateway**, and **DNS Server**
  settings, and then save your configurations.

  > **Important**

  > When configuring more than one network interface, ensure that only
  > one **Default Gateway** is defined. The rest must be set to
  > `0.0.0.0`, which configures them as undefined. In addition, we
  > recommend assigning each network interface to a different subnet, as
  > oppose to configuring two interfaces on the same subnet which can
  > potentially cause unexpected behavior.

  You can also specify which of the network interfaces is designated as
  the **Admin** and can be used to access the Broker VM web interface.
  Only one interface can be assigned for this purpose from all of the
  available network interfaces on the Broker VM, and the rest should be
  set to **Disable**.

2.  (Optional) Set the internal network settings (requires Broker VM
    14.0.42 and later).

- Specify a network subnet to avoid the Broker VM dockers colliding with
  your internal network. By default, the **Network Subnet** is set to
  `172.17.0.1/16`.

  > **Important**

  > Internal IP must be:

  - > Formatted as `prefix/mask`, for example `192.0.2.1/24`.

  - > Must be within `/8` to `/24` range.

  - > Cannot be configured to end with a zero.

  > For Broker VM version 9.0 and earlier, Cortex XSIAM will only accept
  > `172.17.0.0/16`.

3.  (Optional) Configure a proxy server address and other related
    details to route Broker VM communication.

    a.  Select the proxy **Type** as **HTTP**, **SOCKS4**, or
        **SOCKS5**.

    - For any proxy selected, you must ensure the proxy supports HTTP/2,
      gRPC-specific headers, and HTTP/2 trailers, and the inspection
      policies support gRPC traffic. Any devices that you use with this
      configuration should also support these standards.

      > **Note**

      > You can configure another Broker VM as a proxy server for this
      > Broker VM by selecting the **HTTP** type. When selecting
      > **HTTP** to route Broker VM communication, you need to add the
      > IP **Address** and **Port** number (set when activating the
      > Agent Proxy) for another Broker VM registered in your tenant.
      > This designates the other Broker VM as a proxy for this Broker
      > VM.

    b.  Specify the proxy **Address** (IP or FQDN), **Port**, and an
        optional **User** and **Password**. Select the pencil icon to
        specify the password. Avoid using special characters in the
        proxy username and password.

    c.  Save your configurations.

4.  (Optional) Configure your **NTP** servers (requires Broker VM 8.0
    and later).

- Specify the required server addresses using the FQDN or IP address of
  the server.

5.  (Optional) Allow SSH connections to the Broker VM (Requires Broker
    VM 8.0 and later).

- > **Important**

  - > We strongly recommend disabling SSH connectivity when it\'s not
    > being used. Therefore, activate SSH connectivity when it\'s needed
    > and disable it right afterwards.

  - > When generating a new SSH key ensure to avoid embedding the
    > domain-style username, by not using any backslashes (`\`) in the
    > comment field, to ensure the SSH key passes validation.

  Enable or disable SSH connections to the Broker VM. SSH access is
  authenticated using a public key, provided by the user. Using a public
  key grants remote access to colleagues and Cortex XSIAM support who
  need the private key. You must have Instance Administrator role
  permissions to configure SSH access.

  To enable connection, generate an RSA Key Pair, and enter the public
  key in the **SSH Public Key** section. Once one SSH public key is
  added, you can **Add Another**. When you are finished, **Save** your
  configuration.

  When using PuTTYgen to create your public and private key pairs, you
  need to copy the public key generated in the
  **Public key for pasting into OpenSSH authorized_keys file** box, and
  paste it in the Broker VM **SSH Public Key** section as explained
  above. This public key is only available when the PuTTYgen console is
  open after the public key is generated. If you close the PuTTYgen
  console before pasting the public key, you will need to generate a new
  public key.

  When you SSH the Broker VM using PuTTY or a command prompt, you need
  to use the `admin` username. For example:

      ssh -i [/path/to/private.key] admin@[broker_vm_address]

6.  (Optional) Update the SSL Server certificates for the Broker VM.

- Upload your signed server certificate and key to establish a validated
  secure SSL connection between your endpoints and the Broker VM. Ensure
  the Private Key is uploaded in an unencrypted format. When you
  configure the server certificate and the key files in the Broker VM,
  Cortex XSIAM automatically updates them in the tenant UI. Cortex XSIAM
  validates that the certificate and key match, but does not validate
  the Certificate Authority (CA).

  > **Note**

  > The Palo Alto Networks Broker VM supports only strong cipher
  > SHA256-based certificates. MD5/SHA1-based certificates are not
  > supported.

7.  Update the Trusted CA Certificate for the Broker VM.

- Upload your Certificate Authority (CA) bundle file associated with the
  public TLS certificates belonging to the applicable firewalls, and
  click **Save**. These applicable firewalls include SSL/TLS decryption.
  For example, when [configuring Palo Alto Networks NGFW to decrypt
  SSL](https://knowledgebase.paloaltonetworks.com/KCSArticleDetail?id=kA10g000000ClmyCAC)
  using a self-signed certificate, you need to ensure the Broker VM can
  validate a self-signed CA by uploading the `cert_ssl-decrypt.crt` file
  on the Broker VM.

  > **Note**

  > If adding a CA certificate to the Broker VM is not possible, ensure
  > that you've added the Broker Service FQDNs to the SSL Decryption
  > Exclusion list on your firewalls. See [Enable Access to Cortex
  > XDR](#UUID24cb15454f44c259b484435238bb6a33).

8.  (Optional) Collect and **Generate New Logs** (Requires Broker VM 8.0
    and later). Your Cortex XSIAM logs will download automatically after
    approximately 30 seconds.

###### Task 2. Register your Broker VM

Register and enter your unique **Token**, created in the **Broker VMs**
page. This can take up to 30 seconds.

After a successful registration, Cortex XSIAM displays a notification.

You are directed to Settings \> Configurations \> Data Broker \> Broker
VMs. The **Broker VMs** page displays your Broker VM details and allows
you to edit the defined configurations.

##### Broker VM image installations

Ensure that your virtual machine (VM) is compatible with one of the
following options and install the applicable broker image according to
the installation steps provided:

+-----------------------+-----------------------+----------------------------------------------------+
| Infrastructure        | Image Type            | Broker Image Installation                          |
+=======================+=======================+====================================================+
| Alibaba Cloud         | QCOW2                 | [Set up Broker VM on Alibaba                       |
|                       |                       | Cloud](#UUID149a0c6f5e503dbca27403a1cf06f01d)      |
+-----------------------+-----------------------+----------------------------------------------------+
| Amazon Web Services   | VMDK                  | [Set up Broker VM on Amazon Web                    |
| (AWS)                 |                       | Services](#UUID388bfa322f5e2bbf6202e9e85b17e451)   |
+-----------------------+-----------------------+----------------------------------------------------+
| Google Cloud Platform | VMDK                  | [Set up Broker VM on Google Cloud Platform         |
|                       |                       | (GCP)](#UUID8e07af9f8f6d4468b5d647474bcba55f)      |
+-----------------------+-----------------------+----------------------------------------------------+
| KVM                   | QCOW2                 | [Set up Broker VM on KVM using                     |
|                       |                       | Ubuntu](#UUIDb179fbefe4e0383c1175ecc1742a08c1)     |
+-----------------------+-----------------------+----------------------------------------------------+
| Microsoft Azure       | VHD (Azure)           | [Set up Broker VM on Microsoft                     |
|                       |                       | Azure](#UUID0ff99738409e18d53f2b852dcad7d2dc)      |
+-----------------------+-----------------------+----------------------------------------------------+
| Microsoft Hyper-V     | VHD                   | Hyper-V 2012 or later                              |
| 2012                  |                       |                                                    |
|                       |                       | [Set up Broker VM on Microsoft                     |
|                       |                       | Hyper-V](#UUID02c961186a431f52918d63291ed0b2a8)    |
+-----------------------+-----------------------+----------------------------------------------------+
| Nutanix Hypervisor    | QCOW2                 | Nutanix AHV 2021                                   |
|                       |                       |                                                    |
|                       |                       | [Set up Broker VM on Nutanix                       |
|                       |                       | Hypervisor](#UUID4ca6b27901059cc28f4a81dd55c53d8e) |
+-----------------------+-----------------------+----------------------------------------------------+
| VMware ESXi           | OVA                   | VMware ESXi 6.5 or later                           |
|                       |                       |                                                    |
|                       |                       | [Set up Broker VM on VMware ESXi using vSphere     |
|                       |                       | Client](#UUIDd14485ff8489bcf0b2158473372d7063)     |
+-----------------------+-----------------------+----------------------------------------------------+

###### Set up Broker VM on Alibaba Cloud

After you download your Cortex XSIAM Broker virtual machine (VM)
**QCOW2** image, you need to upload it to Alibaba Cloud. Since the image
file is larger than 5G, you need to download the `ossutil` utility file
provided by Alibaba Cloud to upload the image.

> **Prerequisite**
>
> Download a Cortex XSIAM Broker VM **QCOW2** image. For more
> information, see the virtual machine compatibility requirements in
> [Set up and configure Broker
> VM](#UUIDa2b1b832d74850d81f427e175514c501).

Perform the following procedures in the order listed below.

####### Task 1. Download the `ossutil` utility file provided by Alibaba Cloud

The download is dependent on the operating system and infrastructure you
are using.

- Alibaba Cloud supports using the following operating systems for the
  utility file: Windows, Linux, and macOS.

- Supported architectures: x86 (32-bit and 64-bit) and ARM (32-bit and
  64-bit)

For more information on downloading the utility, see the [Alibaba Cloud
documentation](https://www.alibabacloud.com/help/doc-detail/120075.htm?spm=a2c63.p38356.879954.4.4a3265d0RjYjwJ#concept-303829).

####### Task 2. Upload the image file to Alibaba Cloud using the utility file you downloaded

The command is dependent on the operating system and architecture you
are using. Below are a few examples of the commands to use based on the
different operating systems and architectures, which you may need to
modify based on your system requirements.

Linux (using CLI)

- Format

<!-- -->

- ./ossutil64 cp Downloads/<name of Broker VM QCOW2 image> oss://<directory name>/<file name for uploaded image>

<!-- -->

- Example

<!-- -->

- ./ossutil64 cp Downloads/QCOW2_broker-vm-14.0.1.qcow2 oss://kvm-images-qcow2/Cortex XSIAM
                                             -broker-vm-14.0.1.qcow2

macOS (using CLI)

- Format

<!-- -->

- ./ossutilmac64 cp Downloads/<name of Broker VM QCOW2 image oss://<directory name>/<file name for uploaded image>

<!-- -->

- Example

<!-- -->

- ./ossutilmac64 cp Downloads/QCOW2_broker-vm-14.0.1.qcow2 oss://kvm-images-qcow2/Cortex XSIAM
                                             -broker-vm-14.0.1.qcow2

Windows (using CMD)

- Format for 64-bit

<!-- -->

- D:\ossutil>ossutil64.exe cp Downloads\<name of Broker VM QCOW2 image> oss://<directory name>/<file name for uploaded image>

<!-- -->

- Example for 64-bit

<!-- -->

- D:\ossutil>ossutil64.exe cp Downloads\QCOW2_broker-vm-14.0.1.qcow2 oss://kvm-images-qcow2/Cortex XSIAM
                                             -broker-vm-14.0.1.qcow2

> **Note**
>
> For Linux and Windows uploads, you can use Alibaba Cloud's graphical
> management tool called
> [ossbrowser](https://partners-intl.aliyun.com/help/doc-detail/209974.htm?spm=a2c63.p38356.b99.270.7ae22454encexz).

####### Task 3. Create the image file in the Alibaba Cloud format

1.  Open the [Alibaba Cloud
    console](https://homenew-intl.console.aliyun.com/).

2.  Select Hamburger menu \> Object Storage Service \> \<directory
    name\>, where the **\<directory name\>** is the directory you
    configured when uploading the image. For example, in the step above
    the **\<directory name\>** used in the examples provided is
    **kvm-images-qcow2**.

- > **Note**

  > The **Object Storage Service** must be created in the same
  > **Region** as the image of the virtual machine.

3.  From the list of images displayed, find the row for the Broker VM
    QCOW2 image that you uploaded, and click **View Details**.

4.  In the **URL** field of the **View Details** right-pane displayed,
    copy the internal link for the image in Alibaba cloud. The URL that
    you copy ends with **.com** and you should not include any of the
    text displayed after this.

5.  Select Hamburger menu \> Elastic Compute Service \> Instances &
    Images \> Images.

6.  In the **Import Images** area on the **Images** page, click
    **Import Images**.

7.  In the **Import Images** window, set the following parameters:

    - **OSS Object Address**: This field is a combination of the
      internal link that you copied for the Broker VM image and the file
      name for the uploaded image, using this format
      **\<internal link\>/\<file name for uploaded image\>**. Paste the
      internal link for the Broker VM QCOW2 image in Alibaba Cloud that
      you copied, and add the following text after the **.com**:
      **/\<file name for uploaded image\>**.

    - **Image Name**: Specify a name for the image.

    - **Operating System/Platform**: Leave **Linux** configured and
      change **CentOS** to **Ubuntu**.

    - **System Architecture**: Leave the default **x86_64** selected.

    - Leave the rest of the fields as defined by the default or change
      them according to your system requirements.

8.  Click **OK**.

- A notification is displayed indicating that image was imported
  successfully. Once the **Status** for the imported image in the
  **Images** page changes to **Available**, you will know the process is
  complete. This can take a few minutes.

####### Task 4. Create a new VM in Alibaba Cloud

1.  Select Hamburger menu \> Elastic Compute Service \> Instances &
    Images \> Instances.

2.  **Create Instance** to open a wizard to define the VM machine.

3.  Define the **Basic Configurations** screen by setting these
    parameters:

    - **Billing Method**: Select the applicable billing method according
      to your system requirements.

    - **Region**: Ensure the **Region** selected is the same as the
      **OSS Object Address**.

    - **Instance Type**: Set these settings according to your system
      requirements.

    - **Selected Instance Type Quantity**: Set these settings according
      to your system requirements.

    - **Image**: Select **Custom Image**, and in the field select the
      image that you imported to Alibaba Cloud.

    - **Storage** *(Optional)*: Set these settings according to your
      system requirements.

    - **Snapshot** *(Optional)*: Set these settings according to your
      system requirements.

4.  Click **Next**.

5.  Define the **Networking** screen by setting these parameters:

    - **Network Type**: Select the applicable **Network Type** and
      update the field according to your system configuration.

    - **Public IP Address** *(Optional)*: Enable the instance to access
      the public network.

    - **Security Group**: You must select a **Security Group** for
      setting network access controls for the instance. Ensure that port
      22 and port 443 are allowed in the security group rules to access
      the Broker VM.

    - **Elastic Network Interface** *(Optional)*: Add an ENI according
      to you system requirements.

6.  Click **Next**.

7.  Define the **System Configurations** screen by setting these
    parameters:

    - **Logon Credentials**: Select **Inherit Password From Image**.

    - **Instance Name**: You can either leave the default instance name
      or specify a new name for the VM instance.

    - **Description** *(Optional)*: Specify a description for the VM
      instance.

    - The rest of the fields are optional to configure.

8.  Click **Next**.

9.  *(Optional)* Define the **Grouping** screen according to your system
    requirements.

10. Click **Next**.

11. Review the **Preview** screen settings, select
    **ECS Terms of Service and Product Terms of Service**, and click
    **Create Instance**.

- A dialog box is displayed indicating that the VM instance has been
  created. Click **Console** to bring you back to the **Instances**
  page, where you can see the **IP Address** listed to connect to the VM
  instance.

####### Task 5. Reboot the Broker VM

Reboot the Broker VM before logging in for the first time.

###### Set up Broker VM on Amazon Web Services

After you download your Cortex XSIAM Broker **VMDK** image, you can
convert the image to an Amazon Web Services (AWS) Amazon Machine Image
(AMI) using the AWS CLI. The task below explains how to do this on
Ubuntu Linux.

> **Prerequisite**

- > Download a Cortex XSIAM Broker VM **VMDK** image. For more
  > information, see the virtual machine compatibility requirements in
  > [Set up and configure Broker
  > VM](#UUIDa2b1b832d74850d81f427e175514c501).

- > You need to set up an AWS VM Import role (`vmimport`) before you
  > continue with the steps to convert the image as it is required for
  > the `import-image` CLI command. You can use a different role, if the
  > role `vmimport` doesn\'t exist or doesn\'t have the required
  > permissions. You\'ll need an Administrator role or the necessary
  > permissions to create these permissions. For more information on
  > setting up an AWS VM Import role and the permissions required, see
  > [Required service
  > role](https://docs.aws.amazon.com/vm-import/latest/userguide/vmie_prereqs.html#vmimport-role).

To convert the image to AWS, perform the following procedures in the
order listed below.

####### Task 1. Create an IAM User with Proper Permissions

You need to log in using an AWS Identity and Access Management (IAM)
user, where the permissions are defined in the IAM policy to use the
virtual machine Import and export.

1.  Log in to the [AWS IAM
    Console](https://console.aws.amazon.com/iam/home), and in the
    navigation pane, select Access Management \> Users \> Add Users.

2.  Select **Access key - Programmatic access** as the AWS credential
    type, and click **Next: Permissions**.

3.  Select Attach Existing Policies directly \> Create Policy,

4.  In the **JSON** tab, copy and paste the following syntax to define
    the policy:

- {
        "Version": "2012-10-17",
        "Statement": [
          {
            "Effect": "Allow",
            "Action": [
              "s3:GetBucketLocation",
              "s3:GetObject",
              "s3:PutObject"
            ],
            "Resource": ["arn:aws:s3:::mys3bucket","arn:aws:s3:::mys3bucket/*"]
          },
          {
            "Effect": "Allow",
            "Action": [
              "ec2:CancelConversionTask",
              "ec2:CancelExportTask",
              "ec2:CreateImage",
              "ec2:CreateInstanceExportTask",
              "ec2:CreateTags",
              "ec2:DescribeConversionTasks",
              "ec2:DescribeExportTasks",
              "ec2:DescribeExportImageTasks",
              "ec2:DescribeImages",
              "ec2:DescribeInstanceStatus",
              "ec2:DescribeInstances",
              "ec2:DescribeSnapshots",
              "ec2:DescribeTags",
              "ec2:ExportImage",
              "ec2:ImportInstance",
              "ec2:ImportVolume",
              "ec2:StartInstances",
              "ec2:StopInstances",
              "ec2:TerminateInstances",
              "ec2:ImportImage",
              "ec2:ImportSnapshot",
              "ec2:DescribeImportImageTasks",
              "ec2:DescribeImportSnapshotTasks",
              "ec2:CancelImportTask"
            ],
            "Resource": "*"
          }
        ]
      }

5.  Click **Next** until you can specify the **Policy name**, and then
    click **Create Policy**.

6.  Select the policy that you created above based on the syntax you
    added.

7.  Complete the user creation process.

8.  After confirmation that the user is created, record the following
    user information, which you will need later.

    - User name

    - Access key ID

    - Secret access key

####### Task 2. Setup AWS CLI in Ubuntu 18

Install the AWS CLI and configure it with the IAM user that you created.

1.  Login to the server with admin privilege and install the AWS CLI.

- # sudo bash
      # apt install awscli

2.  Run the following command to configure the AWS CLI:

- # aws configure

  You need to specify the proper configurations for the following:

  - **AWS Access Key ID**---The **Access key ID** for the IAM user you
    created.

  - **AWS Secret Access Key**---The **Secret access key** for the IAM
    user you created.

  - **Default region name**---The **Region** where you\'ve defined the
    IAM user you created.

  You are now ready to implement commands in the AWS CLI.

####### Task 3. Create an AMI Image

To create an AMI image, you need to download Broker VM VMDK file from
the Cortex XSIAM Web Console, import this file to your S3 bucket, and
then convert the VMDK file in the S3 bucket into an AMI Image.

1.  In the Cortex XSIAM Web Console , select Settings \> Configurations
    \> Data Broker \> Broker VMs \> Add Broker \> VMDK.

2.  Download the VMDK file, such as
    `broker-vm-<broker-vm-version>.vmdk`, to you ubuntu computer.

3.  Navigate and log in to your AWS account.

4.  In the AWS Console, navigate to Services \> Storage \> S3 \>
    Buckets.

5.  In the **S3 buckets** page, **+ Create bucket** to upload your
    Broker VM image to this bucket.

- Specify a unique name for the S3 bucket and use the default
  configurations.

6.  Upload the Broker VM VMDK you downloaded from Cortex XSIAM to the
    AWS S3 bucket.

- Run

  `# aws s3 cp ~/<path/to/broker-vm-version.vmdk> s3://<your_bucket/broker-vm-version.vmdk>`

7.  Prepare the following configurations files on your hard drive.

    - configuration.json

      1.  Run the following command in Ubuntu:

      - # vi configuration.json

      2.  Copy and paste the following syntax into the json file.

      - In **S3Bucket**, replace **\<your_bucket\>** with the Bucket
        Name and not its ARN Name. **S3Key** is the VMDK filename, which
        you should replace instead of **\<broker-vm-version.vmdk\>**.

            [
                {
                    "Description":"Cortex XSIAM Broker VM <version>",
                    "Format":"vmdk",
                    "UserBucket":{
                        "S3Bucket":"<your_bucket>",
                        "S3Key":"<broker-vm-version.vmdk>"
                    }
                }
            ]

    - trust-policy.json

      1.  Run the following command in ubuntu:

      - # vi trust-policy.json

      2.  Copy and paste the following syntax into the json file.

      - {
               "Version": "2012-10-17",
               "Statement": [
                  {
                     "Effect": "Allow",
                     "Principal": { "Service": "vmie.amazonaws.com" },
                     "Action": "sts:AssumeRole",
                     "Condition": {
                        "StringEquals":{
                           "sts:Externalid": "vmimport"
                        }
                     }
                  }
               ]
            }

    - role-policy.json

      1.  Run the following command in Ubuntu:

      - # vi role-policy.json

      2.  Copy and paste the following syntax into the json file.
          Replace the **\<disk-image-file-bucket\>** and
          **\<export-bucket\>** with the correct bucket name. You can
          specify \* to configure access to all your S3 buckets.

      - {
               "Version":"2012-10-17",
               "Statement":[
                  {
                     "Effect": "Allow",
                     "Action": [
                        "s3:GetBucketLocation",
                        "s3:GetObject",
                        "s3:ListBucket" 
                     ],
                     "Resource": [
                        "arn:aws:s3:::<disk-image-file-bucket>",
                        "arn:aws:s3:::<disk-image-file-bucket>/*"
                     ]
                  },
                  {
                     "Effect": "Allow",
                     "Action": [
                        "s3:GetBucketLocation",
                        "s3:GetObject",
                        "s3:ListBucket",
                        "s3:PutObject",
                        "s3:GetBucketAcl"
                     ],
                     "Resource": [
                        "arn:aws:s3:::<export-bucket>",
                        "arn:aws:s3:::<export-bucket>/*"
                     ]
                  },
                  {
                     "Effect": "Allow",
                     "Action": [
                        "ec2:ModifySnapshotAttribute",
                        "ec2:CopySnapshot",
                        "ec2:RegisterImage",
                        "ec2:Describe*"
                     ],
                     "Resource": "*"
                  }
               ]
            }

8.  Use the `create-role` command to create a role named `vmimport` and
    grant VM import and export access to the `trust-policy.json` file.

- `# aws iam create-role --role-name vmimport --assume-role-policy-document "file://trust-policy.json"`

9.  Use the `put-role-policy` command to attach the policy to the
    `vmimport` role created above.

- `# aws iam put-role-policy --role-name vmimport --policy-name vmimport --policy-document "file:// role-policy.json"`

10. Create an AMI image from the VMDK file.

- Run

  `# aws ec2 import-image --description "Cortex XSIAM Broker VM <Version>" --disk-containers "file://configuration.json"`

  > **Note**

  > Creating an AMI image can take up to 60 minutes to complete.

  To track the progress, use the `task id` value from the output and
  run:

  `# aws ec2 describe-import-image-tasks --import-task-ids import-ami-<task-id>`

  Completed status output example:

  `{ "ImportImageTasks":[ { "...", "SnapshotDetails":[ { "Description":"Broker VM version", "DeviceName":"/dev/<name>", "DiskImageSize":2976817664.0, "Format":"VMDK", "SnapshotId":"snap-1234567890", "Status":"completed", "UserBucket":{ "S3Bucket":"broker-vm", "S3Key":"broker-vm-<version>.vmdk" } } ], "Status":"completed", "..." } ]}`

  Once the task is complete, the AMI Image is ready for use.

11. (*Optional*) After the AMI image has been created, you can define a
    new name for the image.

- Select Services \> EC2 \> IMAGES \> AMIs and locate your AMI image
  using the task ID. Select the pencil icon to specify a new name.

####### Task 4. Launch a Broker VM Instance in AWS EC2

You can launch the a Broker VM instance in AWS EC2 using the AMI Image
created.

> **Important**
>
> A t2.medium (4GB RAM) is the lowest machine type that can be used as
> an instance type. Usually, the lowest machine type is sufficient with
> the Local Agent Settings applet. Yet, when enabling more applets, 8 GB
> is required.

1.  To view the AMI image that you added, select Services \> EC2 \>
    Images \> AMIs.

2.  Select EC2 \> Instances, and click **Launch instances** to create an
    instance of the AMI image.

3.  In the **Launch Instance Wizard** define the instance according to
    your company requirements and **Launch**.

4.  (*Optional*) In the **Instances** page, locate your instance and use
    the pencil icon to rename the instance **Name**.

5.  Define HTTPS and SSH access (*optional*) to your instance.

- Right-click your instance, and select Networking \> Change Security
  Groups.

  In the **Change Security Groups** pop-up, select HTTPS to be able to
  access the Broker VM Web UI, and SSH to allow for remote access when
  troubleshooting. Make sure to allow these connections to the Broker VM
  from secure networks only.

  > **Note**

  > Assigning security groups can take up to 15 minutes.

6.  Verify the Broker VM has started correctly.

- Locate your instance, right-click, and select Instance Settings \> Get
  Instance Screenshot.

  You are directed to your Broker VM console listing your Broker
  details.

####### Task 5. Register the Broker VM

Registration of the Broker VM to Cortex XSIAM is performed from the
Broker VM Web Console.

1.  Obtain a registration token from the Cortex XSIAM Web Console by
    selecting Settings \> Configurations \> Data Broker \> Broker VMs \>
    Add Broker \> Generate Token.

2.  Determine the IP Address of the EC2 instance and use it to open the
    Broker VM Web Console, such as `https://<ip_address>:4443`.

3.  Complete the registration process by entering the token information.

###### Set up Broker VM on Google Cloud Platform (GCP)

You can deploy the Broker VM on Google Cloud Platform. The Broker VM
allows communication with external services through the installation and
setup of applets such as the Syslog collector applet.

To set up the Broker VM on the Google Cloud Platform, install the VMDK
image provided in Cortex XSIAM.

> **Prerequisite**

- > Download a Cortex XSIAM Broker VM VMDK image. For more information,
  > see the virtual machine compatibility requirements in [Set up and
  > configure Broker VM](#UUIDa2b1b832d74850d81f427e175514c501).

- > To complete the set up, you must have G Cloud installed and have an
  > authenticated user account.

Perform the following procedures in the order listed below.

####### Task 1. Create a Google Cloud Storage bucket in G Cloud

From G Cloud, create a Google Cloud Storage bucket to store the Broker
VM image.

1.  [Create a project in
    GCP](https://cloud.google.com/resource-manager/docs/creating-managing-projects)
    and enable Google Cloud Storage, for example, brokers-project. Make
    sure you have defined a default network.

2.  [Create a
    bucket](https://cloud.google.com/storage/docs/creating-buckets) to
    store the image, such as broker-vms.

####### Task 2. Set up the GCP project

Open a command prompt and run the following:

    gcloud config set project <project-name>

####### Task 3. Upload the VMDK image to the Google Cloud Storage bucket

Upload the VMDK image to the bucket, run the following:

    gsutil cp </path/to/broker.vmdk> gs://<bucket-name>

####### Task 4. Import the GCP image

You can import the GCP image using either G Cloud CLI or Google Cloud
console.

> **Note**
>
> The import tool uses Cloud Build API, which must be enabled in your
> project. For the import to work, Cloud Build service account must have
> `compute.admin` and `iam.serviceAccountUser` roles. When using the
> Google Cloud console to import the image, you will be prompted to add
> these permissions automatically.

gcloud CLI

> **Warning**
>
> Before importing a GCP image using the gcloud CLI, ensure that you
> update the Google Cloud components to version 371.0.0 and above using
> the following command:

    gcloud components update

The following command uses the minimum required parameters. For more
information on permissions and available parameters, refer to the
[Google Cloud
SDK](https://cloud.google.com/sdk/gcloud/reference/beta/compute/images/import).

Open a command prompt and run the following:

    gcloud compute images import <VMDK image> --data-disk --source-file="gs://<image path>" --network=<network_name> --subnet=<subnet_name> --zone=<region> --async

####### Task 5. Create a new instance of the image

When the Google Compute completes the image creation, create a new
instance.

1.  From the Google Cloud Platform, select Compute Engine \> VM
    instances.

2.  Click **Create instance**.

3.  In the Boot disk option, choose **Custom images** and select the
    image you created.

4.  Set up the instance according to your needs.

- If you are using the Broker VM to facilitate only Agent Proxy, use
  **e2-startdard-2**. If you are using the Broker VM for multiple
  applets, use **e2-standard-4**.

####### Task 6. Allow the 4443 port in your firewall configuration by creating a firewall rule

1.  From the Google Cloud menu, select VPC network \> Firewall, and
    click **CREATE FIREWALL RULE**.

2.  Set the following parameters for the rule:

    - **Name**: Name of the rule.

    - **Network**: Select the applicable network where the Broker VM
      resides.

    - **Direction of traffic**: Select **Ingress** (default).

    - **Targets**: Select **All instances in the network**.

    - **Source IPv4 ranges**: Enter the IP network of computers that
      will be connecting to the Broker VM. To include all machines,
      enter `0.0.0.0/0`.

    - **TCP**: Enter port 4443.

3.  Click **CREATE**.

- The rule is listed under **VPC firewall rules**.

####### Task 7. Verify that the firewall rule is assigned to the Broker VM

1.  From the Google Cloud menu, select Compute Engine \> VM instances.

2.  For the specific Broker VM containing the rule, select the ellipse
    to display More actions, and select **View network details**.

3.  In the **Firewall and routes details** section, select the
    **FIREWALLS** tab.

4.  Verify that the firewall rule is listed.

You can now connect to the Broker VM web console using the Broker VM IP
address. Connect with https over port 4443 using the format
`https://<ip address>:4443`.

###### Set up Broker VM on KVM using Ubuntu

After you download your Cortex XSIAM Broker virtual machine (VM)
**QCOW2** image, you need to upload it to a kernel-based Virtual Machine
(KVM). The instructions below provide an example of doing this using
Ubuntu 18.04 LTS.

> **Prerequisite**
>
> Download a Cortex XSIAM Broker VM **QCOW2** image. For more
> information, see the virtual machine compatibility requirements in
> [Set up and configure Broker
> VM](#UUIDa2b1b832d74850d81f427e175514c501).

1.  Open your kernel-based Virtual Machine (KVM) on Ubuntu.

2.  Click the New VM icon
    (![](media/rId4623.png){width="0.2708333333333333in"
    height="0.20833333333333334in"}) to open the
    **Create a new virtual machine wizard**.

3.  In the **Step 1** screen of the wizard, select
    **Import existing disk image**, and click **Forward**.

4.  Define the **Step 2** screen of the wizard:

- Provide the existing storage path
  1.  **Browse** to the downloaded QCOW2 image file.

  2.  Click **Browse Local**, select the QCOW2 image file that you
      downloaded, and click **Open**.

  OS type
  Leave the **Generic** option selected.

  Version
  Leave the **Generic** option selected.

5.  Click **Forward**.

6.  Define the **Step 3** screen of the wizard:

- Memory (RAM)
  Specify **8192** (8 GB) of memory

  CPUs
  Specify **4** CPUs.

7.  Click **Forward**.

8.  In the **Step 4** screen of the wizard, set a **Name** for your new
    VM.

9.  Click **Finish**.

- You new VM is now listed and available to use.

###### Set up Broker VM on Microsoft Azure

After you download your Cortex XSIAM Broker **VHD (Azure)** image, you
need to upload it to Azure as a storage blob.

> **Prerequisite**
>
> Download a Cortex XSIAM Broker VM **VHD (Azure)** image. For more
> information, see the virtual machine compatibility requirements in
> [Set up and configure Broker
> VM](#UUIDa2b1b832d74850d81f427e175514c501).

Perform the following procedures in the order listed below.

####### Task 1. Extract the downloaded VHD (Azure) image

Make sure you extract the zipped hard disk file on a server that has
more then 512 GB of free space.

> **Note**
>
> Extraction can take up to a few hours.

####### Task 2. Create a new storage blob on your Azure account by uploading the VHD file

Upload from Microsoft Windows or Ubuntu.

Microsoft Windows

1.  Verify you have:

    - Windows PowerShell version 5.1 or later.

    - .NET Framework 4.7.2 or later.

2.  Open PowerShell and run `Set-ExecutionPolicy unrestricted`.

    - `[Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12`

    - `Install-PackageProvider -Name NuGet -MinimumVersion 2.8.5.201-Force`

3.  Install `azure cmdlets`.

- `Install-Module -Name Az -AllowClobber`

4.  Connect to your Azure account.

- `Connect-AzAccount`

5.  Start the upload.

    - For Azure PowerShell:

    <!-- -->

    - Set-AzStorageBlobContent -Container $containerName -File $localFilePath -Context $storageContext -BlobType Page

    <!-- -->

    - For Azure CLI:

    <!-- -->

    - az storage blob upload -f <vhd to upload> -n <vhd name> -c <container name> --account-name <account name>

- > **Note**

  > Upload can take up to a few hours.

Ubuntu 18.04

1.  Install Azure util.

- `curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash`

2.  Connect to Azure.

- `az login`

3.  Start the upload.

- `az storage blob upload -f <vhd to upload> -n <vhd name> -c <container name> --account-name <account name>`

####### Task 3. Add and configure a new disk in Azure

1.  In the Azure home page, navigate to Azure services \> Disks and
    **Add** a new disk.

2.  Navigate to the Create a managed disk \> Basics page, and define the
    following information:

+-----------------------------------+-----------------------------------+
| Heading                           | Parameter                         |
+===================================+===================================+
| Project details                   | **Resource group**: Select your   |
|                                   | resource group.                   |
+-----------------------------------+-----------------------------------+
| Disk details                      | **Disk name**: Enter a name for   |
|                                   | the disk object.                  |
+-----------------------------------+-----------------------------------+
| **Region**: Select your preferred |                                   |
| region.                           |                                   |
+-----------------------------------+-----------------------------------+
| **Source type**: Select           |                                   |
| `Storage Blob`.                   |                                   |
|                                   |                                   |
| Additional fields are displayed,  |                                   |
| which you can define as follows:  |                                   |
|                                   |                                   |
| - **Source blob**:                |                                   |
|                                   |                                   |
|   1.  Select **Browse**. You are  |                                   |
|       directed to the             |                                   |
|       **Storage accounts** page.  |                                   |
|                                   |                                   |
|   2.  From the navigation panel,  |                                   |
|       select the bucket and then  |                                   |
|       container to which you      |                                   |
|       uploaded the Cortex XSIAM   |                                   |
|       VHD image.                  |                                   |
|                                   |                                   |
|   3.  In the **Container** page,  |                                   |
|       **Select** your VHD image.  |                                   |
|                                   |                                   |
| - **OS type**: Select **Linux**   |                                   |
|                                   |                                   |
| - **VM generation**: Select       |                                   |
|   **Gen 1**                       |                                   |
+-----------------------------------+-----------------------------------+

3.  Check you settings by clicking **Review + create**.

####### Task 4. Create the Broker VM disk

1.  **Create** your Broker VM disk, and after deployment is complete,
    click **Go to resource**.

2.  In your created **Disks** page, click **Create VM**.

3.  In the **Create a virtual machine** page, define the following:

+-----------------------------------------------+-----------------------------------------+
| Heading                                       | Parameter                               |
+===============================================+=========================================+
| Instance details                              | (*Optional*)** Virtual machine name**:  |
|                                               | Enter the same name as the disk name    |
|                                               | you defined.                            |
+-----------------------------------------------+-----------------------------------------+
| **Size**: Select the size according to your   |                                         |
| company guidelines.                           |                                         |
|                                               |                                         |
| Select **Next** to navigate to the            |                                         |
| **Networking** tab.                           |                                         |
+-----------------------------------------------+-----------------------------------------+
| Network interface                             | **NIC network security group**---Select |
|                                               | **Advanced**.                           |
+-----------------------------------------------+-----------------------------------------+
| **Configure network security group**---Select |                                         |
| HTTPS to be able to access the Broker VM Web  |                                         |
| UI, and SSH to allow for remote access when   |                                         |
| troubleshooting. Make sure to allow these     |                                         |
| connection to the Broker VM from secure       |                                         |
| networks only.                                |                                         |
+-----------------------------------------------+-----------------------------------------+

4.  To check your settings, click **Review + create**.

5.  **Create** your VM.

- After deployment is complete, click **Go to resource**. You are
  directed to your VM page.

  > **Note**

  > Creating the VM can take up to 15 minutes. The Broker VM Web UI is
  > not accessible during this time.

6.  Ensure that the VM you created contains an **Outbound port rule**
    that allows the broker to reach the
    **Azure Instance Metadata Service** using the IP address
    `169.254.169.254` and port `80`. For more information about the
    Azure Instance Metadata Service, see the [Azure
    Documentation](https://learn.microsoft.com/en-us/azure/virtual-machines/instance-metadata-service?tabs=windows).

- To configure an outbound rule on your VM, select Networking \> Network
  settings, and under the Rules \> Outbound port rules section, you can
  either:

  > **Note**

  > For more information on creating a rule in an Azure VM, see [Create
  > a Security
  > Rule](https://learn.microsoft.com/en-us/azure/virtual-network/manage-network-security-group?tabs=network-security-group-portal#create-a-security-rule)
  > in the Azure Documentation.

  - Configure a new outbound port rule by selecting Create port rule \>
    Outbound port rule and setting the following settings in the
    **Add outbound security rule** dialog box:

    - **Destination**: Select **IP Addresses**.

    - **Destination IP addresses/CIDR ranges**: Enter the IP address as
      `169.254.169.254`.

    - **Destination port ranges**: Enter the port as `80`.

    - **Protocol**: Select **TCP**.

    - **Name**: Enter a unique name for this new outbound port rule,
      such as **AzureInstanceMetadataService**.

  <!-- -->

  - Click **Add** to create the new outbound port rule.

  <!-- -->

  - Edit an existing outbound port rule and ensure that the settings
    provided above for creating a new outbound port rule match what is
    already configured in the rule.

###### Set up Broker VM on Microsoft Hyper-V

To set up a Broker virtual machine (VM) image on Microsoft Hyper-V, you
need to download a Cortex XSIAM Broker VM **VHD** image, and then upload
it to your newly created Microsoft Hyper-V VM. Microsoft Hyper-V 2012 or
later is supported.

> **Prerequisite**
>
> Download a Cortex XSIAM Broker VM VHD image. For more information, see
> the virtual machine compatibility requirements in [Set up and
> configure Broker VM](#UUIDa2b1b832d74850d81f427e175514c501).

Perform the following procedures in the order listed below.

####### Task 1. Create a new VM in the Hyper-V Manager and upload the VHD image

1.  In the Hyper-V Manager, select New \> Virtual Machine to open the
    **New Virtual Machine Wizard**.

2.  In the **Specify Name and Location** screen, specify a **Name** for
    your VM, and click **Next**.

3.  In the **Specify Generation** screen, select **Generation 1**, and
    click **Next**.

4.  In the **Assign Memory** screen, set the **Startup memory** to
    **8192** MB, and click **Next**.

5.  In the **Configuring Networking** screen, select the network adapter
    for the **Connection**, and click **Next**.

6.  In the **Connect Virtual Hard Disk** screen, select
    **Use an existing virtual hard disk**, **Browse** to the downloaded
    VHD image file, and click **Next**.

7.  In the **Completing the New Virtual Machine Wizard** screen, click
    **Finish**.

####### Task 2. Start the VM that you created for Microsoft Hyper-V

1.  From the **Virtual Machines** list, right-click the VM that you
    created, and select **Start**.

2.  When the **State** of the VM updates to **Running**, right-click the
    VM, and select **Connect**.

- The Broker VM console now displays.

###### Set up Broker VM on Nutanix Hypervisor

After you download your Cortex XSIAM Broker virtual machine (VM)
**QCOW2** image, you need to upload it to a Nutanix hypervisor. The
Nutanix AOS 6.5 version is supported.

> **Prerequisite**
>
> Download a Cortex XSIAM Broker VM **QCOW2** image. For more
> information, see the virtual machine compatibility requirements in
> [Set up and configure Broker
> VM](#UUIDa2b1b832d74850d81f427e175514c501).

Perform the following procedures in the order listed below.

####### Task 1. Upload the downloaded QCOW2 image file to a Nutanix hypervisor

1.  Select Compute & Storage \> Images, and click **Add Image**.

2.  In the **Add Images** page, ensure the **Image Source** is set to
    **Image File**, and click **Add File**.

3.  Select the downloaded QCOW2 file and click **Open**. Additional
    fields related to the QCOW2 file are automatically displayed in the
    **Add Image** page, where the **Name** and **Type** of file are
    automatically populated.

4.  *(Optional)* Define the rest of the fields displayed for the QCOW2
    file.

5.  Click **Next**.

6.  Select the location by defining the **Placement Method** and
    **Select Clusters** settings.

7.  Click **Save**.

- The image is now listed in the list of images.

  > **Note**

  > Saving the image to Nutanix hypervisor can take time as it's a large
  > file.

####### Task 2. Create a new VM

1.  Select Hamburger menu \> Compute & Storage \> VMs, and click
    **Create VM**.

2.  In the **Create VM** screen, set the following **Configuration**
    fields:

    - **Name**: Specify a name for the new VM.

    - **Description** *(Optional)*: Specify a description to identify
      the VM.

    - **Number of VMs**: Select the number of VMs you want to create.
      The default is set to 1.

    - **VM Properties**

      - **CPU**: Select **4** CPUs.

      - **Cores per CPU**: Select the number of cores to create for each
        CPU. The default number is 1.

      - **Memory**: Select **8GB** as the allotted memory for the VM.

3.  Click **Next**.

4.  Set the **Resources** fields:

- Disks
  **Attach Disk** and set the following field settings:

  - **Type**: Leave the default **Disk** type.

  - **Operation**: Select **Clone from Image**.

  - **Image**: Select the QCOW2 image file that you uploaded.

  - **Capacity**: Specify the capacity of the image file as 512 GB.

  - **Bus Type**---Leave the default **SCUI** selected.

  When you finish, click **Save**.

  Networks
  **Attach to Subnet** and set the following field settings.

  - **Subnet**: Select the subnet from the list.

  - **Network Connection State**: Leave the default **Connected** option
    selected.

  When you finish, click **Save**.

  Boot Configuration
  Leave the default **Legacy BIOS Mode** selected.

5.  Click **Next**.

6.  Set the **Management** fields, where you can leave the default
    settings for the various fields.

7.  Click **Next**.

8.  Click **Create VM**.

- The VM is now listed in the list of VMs.

  > **Note**

  > Creating the VM can take up to 15 minutes. The Broker VM Web user
  > interface is not accessible during this time.

####### Task 3. Review the VM details for connecting to the VM

Select **Summary** and you can use the **IP Addresses** and **Host IP**
listed to connect to the VM.

###### Set up Broker VM on VMware ESXi using vSphere Client

To set up the Broker VM on VMware ESXi, you deploy the OVA image
provided in Cortex XSIAM. VMware ESXi 6.5 or later is supported. The
instructions below provide an example of doing this using vSphere Client
7.0.3.01400.

> **Prerequisite**

- > Ensure you have a virtualization platform installed that is
  > compatible with an OVA image, and have an authenticated user
  > account.

- > Download a Cortex XSIAM Broker VM **OVA** image. For more
  > information, see the virtual machine compatibility requirements in
  > [Set up and configure Broker
  > VM](#UUIDa2b1b832d74850d81f427e175514c501).

**Deploy the Broker VM OVA image on vSphere Client**

1.  From vSphere Client, right-click an inventory object for the virtual
    machine of your broker, and select **Deploy OVF Template**.

2.  In the **Select an OVF template** page of the wizard, select
    **Local file**, click **UPLOAD FILES** to select the OVA image file
    that you downloaded, and click **NEXT**.

3.  In the **Select a name and folder** page, enter a unique name for
    the virtual machine, select a deployment location, and
    click **NEXT**.

4.  In the **Select a compute resource** page, select a resource where
    to run the deployed VM template, and click **NEXT**.

5.  In the **Review details** page, verify the OVA template details, and
    click **NEXT**.

6.  In the **Select storage** page, define where and how to store the
    files for the deployed OVA template, and click **NEXT**. For more
    information on the options available, see the [VMware vSphere
    documentation](https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.vm_admin.doc/GUID-17BEDA21-43F6-41F4-8FB2-E01D275FE9B4.html).

7.  In the **Select networks** page, select a source network and map it
    to a destination network, and click **NEXT**.

- The **Source Network** column lists all networks that are defined in
  the OVA template.

8.  In the **Ready to complete** page, review the details and click
    **FINISH**.

- A new task for creating the virtual machine is displayed in the
  **Recent Tasks** pane. When the **Status** of the task reaches 100%,
  the task is complete, and the new virtual machine is created on the
  selected resource.

9.  Navigate to the resource where the new virual machine is created,
    right-click the resource, and select Power \> Power On.

##### Broker VM data collector applets

The Broker VM has a number of data collector applets that you can
configure to ingest different types of data. These data collector
applets are in addition to the others that are available in the Settings
\> Configurations \> Data Collection \> Data Sources page.

###### Activate Apache Kafka Collector

Apache Kafka is an open-source distributed event streaming platform for
high-performance data pipelines, streaming analytics and data
integration. Kafka records are organized into Topics. The partitions for
each Topic are spread across the bootstrap servers in the Kafka cluster.
The bootstrap servers are responsible for transferring data from
Producers to Consumer Groups, which enable the Kafka server to save
offsets of each partition in the Topic consumed by each group.

The Broker VM provides a Kafka Collector applet that enables you to
monitor and collect events from Topics on self-managed on-prem Kafka
clusters directly to your log repository for query and visualization
purposes. The applet supports Kafka setups with no authentication, with
SSL authentication, and SASL SSL authentication.

After you activate the Kafka Collector applet, you can collect events as
datasets (`<Vendor>_<Product>_raw`) by defining the following.

- Kafka connection details including the Bootstrap Server List and
  Authentication Method.

- Topics Collection configuration for the Kafka topics that you want to
  collect.

> **Prerequisite**

- > Apache Kafka version 2.5.1 and above.

- > Kafka cluster set up on premises, from which the data will be
  > ingested.

- > Privileges to manage Broker Service configuration, such as Instance
  > Administrator privileges.

- > Create a user in the Kafka cluster with the necessary permissions
  > and the following authentication details:

  - > Broker Certificate and Private Key for an SSL connection.

  - > Username and Password for an SASL SSL connection.

- > [Set up and configure Broker
  > VM](#UUIDa2b1b832d74850d81f427e175514c501)

####### How to activate the Kafka Collector

1.  Select Settings \> Configurations \> Data Broker \> Broker VMs.

2.  Do one of the following:

    - On the **Brokers** tab, find the Broker VM, and in the **APPS**
      column, left-click Add \> Kafka Collector.

    - On the **Clusters** tab, find the Broker VM, and in the **APPS**
      column, left-click Add \> Kafka Collector.

3.  Configure the **Kafka Connection**.

    a.  Specify the **Bootstrap Server List**, which is the
        `<hostname/ip>:<port>` of the bootstrap server (or servers). You
        can specify multiple servers, separated with a comma. For
        example, `hostname1:9092,1.1.1.1:9092`.

    b.  Select one of the **Authentication Methods**:

    - No Authentication
      Default connection method for a new Kafka setup, which doesn't
      require authentication. With a standard Kafka setup, any user or
      application can write messages to any topic, as well as read data
      from any topic.

      SSL Authentication
      Authenticate your connection to Kafka using an SSL certificate.
      Use this authentication method when the connection to the Kafka
      server is a secure TCP, and upload the following:

      - **Broker Certificate**: Signed certificate used for the applet
        to authenticate to the Kafka server.

      - **Private Key**: Private key for the applet used for decrypting
        the SSL messages coming from the Kafka server.

      - (Optional) **CA Certificate**: CA certificate that was used to
        sign the server and private certificates. This CA certificate is
        also used to authenticate the Kafka server identity.

      SASL SSL (SCRAM-SHA-256)
      Authenticate your connection to the Kafka server with your
      **Username**, **Password**, and optionally, your
      **CA Certificate**.

    c.  **Test Connection** to verify that you can connect to the Kafka
        server. An error message is displayed for each server connection
        test that fails.

4.  Configure the **Topics Collection** parameters.

- Topic Subscription Method
  Select the **Topic Subscription Method** for subscribing to Kafka
  topics. Use **List Topics** to specify a list of topics. Use
  **Regex Pattern Matching** to specify a regular expression to search
  available topics.

  Topic(s)
  Specify **Topic(s)** from the Kafka server. For the List Topics
  subscription method, use a comma separated list of topics to subscribe
  to. For the Regex Pattern Matching subscription method, use a regular
  expression to match the Topic(s) to subscribe to.

  (optional) Consumer Group
  Specify a **Consumer Group**, a unique string or label that identifies
  the consumer group this log source belongs to. Each record that is
  published to a Kafka topic is delivered to one consumer instance
  within each subscribing consumer group. Kafka uses these labels to
  load balance the records over all consumer instances in a group. When
  specified, the Kafka collector uses the given consumer group. When not
  specified, Cortex XSIAM assigns the Kafka applet collector to a new
  automatically generated consumer group which is automatically
  generated for this log source with the name
  `PAN-<Broker VM device name>-<topic name>`.

  Log Format
  Select the **Log Format** from the list as either **RAW** (default),
  **JSON**, **CEF**, **LEEF**, **CISCO**, or **CORELIGHT**. This setting
  defines the parser used to parse all the processed event types defined
  in the **Topics** field, regardless of the file names and extension.
  For example, if the **Topics** field is set to `*` and the
  **Log Format** is **JSON**, all files (even those named `file.log`) in
  the cluster are processed by the collector as JSON, and any entry that
  does not comply with the JSON format are dropped.

  Vendor and Product
  Specify the **Vendor** and **Product** which will be associated with
  each entry in the dataset. The vendor and product are used to define
  the name of your Cortex Query Language (XQL) dataset
  (`<Vendor>_<Product>_raw`).

  > **Note**

  > For CEF and LEEF logs, Cortex XSIAM takes the vendor and product
  > names from the log itself, regardless of what you configure on this
  > page.

  (optional) Add Query
  Click **Add Query** to create another Topic Collection. Each topic can
  be added for a server only once.

  (optional) Other available options for Topic Collection
  As needed, you can manage your Topic Collection settings. Here are the
  actions available to you.

  - Edit the Topics Collection details.

  - Disable/Enable a Topics Collection by hovering over the top area of
    the Topics Collection section, on the opposite side of the Topics
    Collection name, and selecting the applicable button.

  - Rename a Topics Collection by hovering over the top area of the
    Topics Collection section, on the opposite side of the Topics
    Collection name, and selecting the pen icon.

  - Delete a Topics Collection by hovering over the top area of the
    Topics Collection section, on the opposite side of the Topics
    Collection name, and selecting the delete icon.

5.  (*Optional*) Click **Add Connection** to create another Kafka
    Connection for collecting data.

6.  (*Optional*) Other available options for Connections.

- As needed, you can return to your Kafka Collector settings to manage
  your connections.

  Here are the actions available to you.

  - Edit the Connection details.

  - Rename a connection by hovering over the default Collection name,
    and selecting the edit icon to edit the text.

  - Delete a connection by hovering over the top area of the connection
    section, on the opposite side of the connection name, and selecting
    the delete icon. You can only delete a connection when you have more
    than one connection configured. Otherwise, this icon is not
    displayed.

7.  Activate the Kafka Collector applet. The **Activate** button is
    enabled when all the mandatory fields are filled in.

- After a successful activation, the **APPS** field displays **Kafka**
  with a green dot indicating a successful connection.

8.  (*Optional*) To view metrics about the Kafka Collector, in the
    **Broker VMs** page, left-click the **Kafka** connection displayed
    in the **APPS** field for your Broker VM.

- Cortex XSIAM displays **Resources**, including the amount of **CPU**,
  **Memory**, and **Disk** space the applet is using.

9.  Manage the Kafka Collector.

- After you activate the Kafka Collector, you can make additional
  changes as needed. To modify a configuration, left-click the **Kafka**
  connection in the **APPS** column to display the Kafka Collector
  settings, and select the following:

  - **Configure** to redefine the Kafka Collector configurations.

  - **Deactivate** to disable the Kafka Collector.

  Ensure that you **Save** your changes, which is enabled when all
  mandatory fields are filled in.

  You can also [Ingest Apache Kafka events as
  datasets](#UUIDf0aa33aeb07e07ff251eb0a030d7858f).

###### Activate CSV Collector

The Broker VM provides a CSV Collector applet that enables you to
monitor and collect CSV (comma-separated values) log files from a shared
Windows directory directly to your log repository for query and
visualization purposes. After you activate the CSV Collector applet on a
Broker VM in your network, you can ingest CSV files as datasets by
defining the list of folders mounted to the Broker VM and setting the
list of CSV files to monitor and upload to Cortex XSIAM using a username
and password.

> **Prerequisite**

- > [Set up and configure Broker
  > VM](#UUIDa2b1b832d74850d81f427e175514c501).

- > Ensure that you **share** the applicable CSV files.

- > Know the complete file path for the Windows directory.

####### How to activate the CSV Collector

1.  Select Settings \> Configurations \> Data Broker \> Broker VMs.

2.  Do one of the following:

    - On the **Brokers** tab, find the Broker VM, and in the **APPS**
      column, left-click Add \> CSV Collector.

    - On the **Clusters** tab, find the Broker VM, and in the **APPS**
      column, left-click Add \> CSV Collector.

3.  Configure your CSV Collector by defining the list of folders mounted
    to the Broker VM and specifying the list of CSV files to monitor and
    upload to Cortex XSIAM. You must also specify a username and
    password.

- Mounted Folders
  Define the folders mounted onto the Broker VM:

  -----------------------------------------------------------------------
  Field                               Description
  ----------------------------------- -----------------------------------
  Folder Path                         Specify the complete file path to
                                      the Windows directory containing
                                      the shared CSV files using the
                                      format: `//host/<folder_path>`. For
                                      example, `//testenv1pc10/CSVFiles`.

  Username                            Specify the username for accessing
                                      the Windows directory.

  Password                            Specify the password for accessing
                                      the Windows directory.
  -----------------------------------------------------------------------

- After you configure the mounted folder details, **Add**
  (![](media/rId4669.png){width="0.2604166666666667in"
  height="0.20833333333333334in"}) details to the applet.

  Mounted CSV Files

+-----------------------------------+------------------------------------------+
| Field                             | Description                              |
+===================================+==========================================+
| Folder Path + Name                | Select the monitored Windows directory   |
|                                   | and specify the name of the CSV file.    |
|                                   | Use a wildcard file search using these   |
|                                   | characters in the name of the directory, |
|                                   | CSV file name, and **Path Exclusion**.   |
|                                   |                                          |
|                                   | - `?`: Matches a single char, such as    |
|                                   |   `202?-report.csv`.                     |
|                                   |                                          |
|                                   | - `*`: Matches either multiple           |
|                                   |   characters, such as                    |
|                                   |   `2021-report*.csv`, or all CSV files   |
|                                   |   with `*.csv`.                          |
|                                   |                                          |
|                                   | - `**`: Searches all directories and     |
|                                   |   subdirectories. For example, if you    |
|                                   |   want to include all the CSV files in   |
|                                   |   the directory and any subdirectories,  |
|                                   |   use the syntax                         |
|                                   |   `//host/<folder_path>/**/*.csv`.       |
|                                   |                                          |
|                                   | > **Note**                               |
|                                   | >                                        |
|                                   | > When you implement a wildcard file     |
|                                   | > search, ensure that the CSV files      |
|                                   | > share the same columns and header rows |
|                                   | > as all other logs that are collected   |
|                                   | > from the CSV files to create a single  |
|                                   | > dataset.                               |
+-----------------------------------+------------------------------------------+
| Path Exclusion (Optional)         | Specify the complete file path for any   |
|                                   | files from the Windows directory that    |
|                                   | you do not want included. The same       |
|                                   | wildcard file search characters are      |
|                                   | allowed in this field as explained above |
|                                   | for the **FOLDER PATH +NAME** field. For |
|                                   | example, if you want to exclude any CSV  |
|                                   | file prefixed with \'`exclude_`\' in the |
|                                   | directory and subdirectories of          |
|                                   | `//host/<folder_path>`, use the syntax   |
|                                   | `//host/<folder_path>/**/exclude_*.csv`. |
+-----------------------------------+------------------------------------------+
| Tags (Optional)                   | To easily query the CSV data in the      |
|                                   | database, you can add a tag to the       |
|                                   | collected CSV data. This tag is appended |
|                                   | to the data using the format             |
|                                   | `<data>_<tag>`.                          |
+-----------------------------------+------------------------------------------+
| Target Dataset                    | Either select the target dataset for the |
|                                   | CSV data or create a new dataset by      |
|                                   | specifying the name for the new dataset. |
+-----------------------------------+------------------------------------------+

4.  Activate the CSV Collector applet.

- After a successful activation, the **APPS** field displays **CSV**
  with a green dot indicating a successful connection.

  > **Note**

  > The CSV Collector checks for new CSV files every 10 minutes.

5.  (*Optional*) To view metrics about the CSV Collector, left-click the
    **CSV** connection in the **APPS** field for your Broker VM.

- Cortex XSIAM displays **Resources**, including the amount of **CPU**,
  **Memory**, and **Disk** space the applet is using.

6.  Manage the CSV Collector.

- After you activate the CSV Collector, you can make additional changes
  as needed. To modify a configuration, left-click the **CSV**
  connection in the **APPS** column to display the CSV settings, and
  select:

  - **Configure** to redefine the CSV Collector configurations.

  - **Deactivate** to disable the CSV Collector.

###### Activate Cortex Network Scanner

The Cortex Network Scanner identifies and analyzes devices, services,
and vulnerabilities in your internal network. It discovers responsive
hosts within specified IP ranges, including on-premises and cloud
environments. The scanner supports both non-authenticated and
authenticated vulnerability scanning, with authenticated scans providing
deeper insights through credential-based access. Scan results are
seamlessly integrated into the inventory and vulnerability management
views in Cortex XSIAM, providing a centralized view of all discovered
assets, vulnerabilities, and issues.

Cortex Network Scanner is installed as an applet on a Broker VM.

> **Note**
>
> Requires the Exposure Management add-on.
>
> **Important**
>
> The Cortex Network Scanner applet is not supported for FedRAMP
> customers.
>
> **Prerequisites**

- > Review the Cortex Network Scanner [deployment
  > recommendations](#UUID0e3d8fa156b2bf50af7d5ae47716dfff) and complete
  > any prerequisites.

- > [Set up and configure Broker
  > VM](https://docs-cortex.paloaltonetworks.com/r/Cortex-XSIAM/Cortex-XSIAM-Premium-Documentation/Set-up-and-configure-Broker-VM)

**How to activate Cortex Network Scanner**

1.  Navigate to Settings \> Configurations \> Data Broker \> Broker VMs.

2.  Right click the Broker VM, and select Add App \> Network Scanner.

3.  After the applet has installed, the scanner should automatically
    connect to the tenant. If the connection is successful, you'll see a
    green dot next to **Network Scanner** in the Apps column of the
    Broker VMs table.

- A red dot indicates that an error occurred and the scanner is not
  connected.

  ![](media/rId4345.png){width="4.375in" height="0.9953116797900262in"}

4.  (Optional) Click on the network scanner in the table to display
    details about the scanner or to deactivate it.

5.  Validate the installation. Navigate to Settings \> Configurations \>
    Network Scanners \> Network Scanners and find your new scanner in
    the list.

- The **Network Scanners** page displays all your deployed and
  configured scanners, along with additional details about each of them.

  ![](media/rId4348.png){width="4.375in" height="1.00625in"}

After setting up a Broker VM and activating Cortex Network Scanner,
refer to [Get started with Cortex Network
Scanner](#UUID0e3d8fa156b2bf50af7d5ae47716dfff) for information about
adding networks, adding credentials for authenticated scans, and
configuring scans.

###### Activate Database Collector

The Broker VM provides a Database Collector applet that enables you to
collect data from a client relational database directly to your log
repository for query and visualization purposes. After you activate the
Database Collector applet on a Broker VM in your network, you can
collect records as datasets (`<Vendor>_<Product>_raw`) by defining the
following.

- Database connection details, where the connection type can be MySQL,
  PostgreSQL, MSSQL, and Oracle. Cortex XSIAM uses Open Database
  Connectivity (ODBC) to access the databases.

- Settings related to the query details for collecting the data from the
  database to monitor and upload to Cortex XSIAM .

> **Prerequisite**

- > [Set up and configure Broker
  > VM](#UUIDa2b1b832d74850d81f427e175514c501)

####### How to activate the Database Collector

1.  Select Settings \> Configurations \> Data Broker \> Broker VMs.

2.  Do one of the following:

    - On the **Brokers** tab, find the Broker VM, and in the **APPS**
      column, left-click Add \> DB Collector.

    - On the **Clusters** tab, find the Broker VM, and in the **APPS**
      column, left-click Add \> DB Collector.

3.  Configure your Database Collector settings.

- Database Connection

+-----------------------------------+-----------------------------------+
| Field                             | Description                       |
+===================================+===================================+
| Connection                        | Select the type of database       |
|                                   | connection as **MySQL**,          |
|                                   | **PostegreSQL**, **MSSQL**, or    |
|                                   | **Oracle**.                       |
+-----------------------------------+-----------------------------------+
| Host                              | Specify the hostname or IP        |
|                                   | address of the database.          |
+-----------------------------------+-----------------------------------+
| Port                              | Specify the port number of the    |
|                                   | database.                         |
+-----------------------------------+-----------------------------------+
| Database                          | Specify the database name for the |
|                                   | type of database configured. This |
|                                   | field is relevant when            |
|                                   | configuring a **Connection Type** |
|                                   | for **MySQL**, **PostegreSQL**,   |
|                                   | and **MSSQL**.                    |
|                                   |                                   |
|                                   | When configuring an **Oracle**    |
|                                   | connection, this field is called  |
|                                   | **Service Name**, so you can      |
|                                   | specify the name of the service.  |
+-----------------------------------+-----------------------------------+
| Enable SSL                        | Select whether to **Enable SSL**  |
|                                   | (default) to encrypt the data     |
|                                   | while in transit between the      |
|                                   | database and the Broker VM.       |
+-----------------------------------+-----------------------------------+
| Username                          | Enter the username to access the  |
|                                   | database. The username may only   |
|                                   | contain the following characters: |
|                                   |                                   |
|                                   | - Letters: `A-Z`                  |
|                                   |                                   |
|                                   | - Digits: `0-9`                   |
|                                   |                                   |
|                                   | - Underscore: `_`                 |
|                                   |                                   |
|                                   | - Dollar sign: `$`                |
|                                   |                                   |
|                                   | - Hash sign: `#`                  |
+-----------------------------------+-----------------------------------+
| Password                          | Enter the password to access the  |
|                                   | database.                         |
+-----------------------------------+-----------------------------------+
| Test Connection                   | Select to validate the database   |
|                                   | connection.                       |
+-----------------------------------+-----------------------------------+

- Database Query

+-----------------------------------+-----------------------------------+
| Field                             | Description                       |
+===================================+===================================+
| Rising Column                     | Specify a column for the Database |
|                                   | Collector applet to keep track of |
|                                   | new rows from one input execution |
|                                   | to the next. This column must be  |
|                                   | included in the query results.    |
+-----------------------------------+-----------------------------------+
| Retrieval Value                   | Specify a **Retrieval Value** for |
|                                   | the Database Collector applet to  |
|                                   | determine which rows are new from |
|                                   | one input execution to the next.  |
|                                   | Cortex XSIAM supports configuring |
|                                   | this value as an integer or a     |
|                                   | string that contains a timestamp. |
|                                   | The following string timestamp    |
|                                   | formats are supported: ISO 8601   |
|                                   | format, RFC 2822 format, date     |
|                                   | strings with month names spelled  |
|                                   | out, such as "January 1, 2022",   |
|                                   | date strings with abbreviated     |
|                                   | month names, such as "Jan 1,      |
|                                   | 2022\", and date strings with     |
|                                   | two-digit years- MM/DD/YY.        |
|                                   |                                   |
|                                   | The first time the input is run,  |
|                                   | the Database Collector applet     |
|                                   | only selects those rows that      |
|                                   | contain a value higher than the   |
|                                   | value you specified in this       |
|                                   | field. Each time the input        |
|                                   | finishes running, the Database    |
|                                   | Collector applet updates the      |
|                                   | input\'s **Retrieval Value** with |
|                                   | the value in the last row of the  |
|                                   | **Rising Column**.                |
+-----------------------------------+-----------------------------------+
| Unique IDs (Optional)             | Specify the column name(s) to     |
|                                   | match against when multiple       |
|                                   | records have the same value in    |
|                                   | the **Rising Column**. This       |
|                                   | column must be included in the    |
|                                   | query results. This is a comma    |
|                                   | separated field that supports     |
|                                   | multiple values. In addition,     |
|                                   | when specifying a **Unique IDs**, |
|                                   | the query should use the greater  |
|                                   | than equal to sign (`>=`) in      |
|                                   | relation to the                   |
|                                   | **Retrieval Value**. If the       |
|                                   | **Unique IDs** is left empty, the |
|                                   | user should use the greater than  |
|                                   | sign (`>`).                       |
+-----------------------------------+-----------------------------------+
| Collect Every                     | Specify the execution frequency   |
|                                   | of collection by designating a    |
|                                   | number and then selecting the     |
|                                   | unit as either **Seconds**,       |
|                                   | **Minutes**, **Hours**, or        |
|                                   | **Days**.                         |
+-----------------------------------+-----------------------------------+
| Vendor and Product                | Specify the **Vendor** and        |
|                                   | **Product** for the type of data  |
|                                   | being collected. The vendor and   |
|                                   | product are used to define the    |
|                                   | name of your Cortex Query         |
|                                   | Language (XQL) dataset            |
|                                   | (`<Vendor>_<Product>_raw`).       |
+-----------------------------------+-----------------------------------+
| SQL Query                         | Specify the **SQL Query** to run  |
|                                   | and collect data from the         |
|                                   | database by replacing the example |
|                                   | query provided in the editor box. |
|                                   | The question mark (`?`) in the    |
|                                   | query is a checkpoint placeholder |
|                                   | for the **Retrieval Value**.      |
|                                   | Every time the input is run, the  |
|                                   | Database Collector applet         |
|                                   | replaces the question mark with   |
|                                   | the latest checkpoint value (i.e. |
|                                   | start value) for the              |
|                                   | **Retrieval Value**.              |
+-----------------------------------+-----------------------------------+
| Generate Preview                  | Select **Generate Preview** to    |
|                                   | display up to 10 rows from the    |
|                                   | **SQL Query** and **Preview** the |
|                                   | results. The **Preview** works    |
|                                   | based on the Database Collector   |
|                                   | settings, which means that if     |
|                                   | after running the query no        |
|                                   | results are returned, then the    |
|                                   | **Preview** returns no records.   |
+-----------------------------------+-----------------------------------+
| Add Query (Optional)              | To define another **Query** for   |
|                                   | data collection on the configured |
|                                   | database connection, select       |
|                                   | **Add Query**. Another **Query**  |
|                                   | section is displayed for you to   |
|                                   | configure.                        |
+-----------------------------------+-----------------------------------+

4.  (*Optional*) Click **Add Connection** to define another database
    connection to collect data from another client relational database.

5.  (O*ptional*) Other available options.

- As needed, you can return to your Database Collector settings to
  manage your connections. Here are the actions available to you:

  - Edit the connection name by hovering over the default **Collection**
    name, and selecting the edit icon to edit the text.

  - Edit the query name by hovering over the default **Query** name, and
    selecting the edit icon to edit the text.

  - **Disable**/**Enable** a query by hovering over the top area of the
    query section, on the opposite side of the query name, and selecting
    the applicable button.

  - Delete a connection by hovering over the top area of the connection
    section, on the opposite side of the connection name, and selecting
    the delete icon. You can only delete a connection when you have more
    than one connection configured. Otherwise, this icon is not
    displayed.

  - Delete a query by hovering over the top area of the query section,
    on the opposite side of the query name, and selecting the delete
    icon. You can only delete a query when you have more than one query
    configured. Otherwise, this icon is not displayed.

6.  Activate the Database Collector applet.

- After a successful activation, the **APPS** field displays **DB** with
  a green dot indicating a successful connection.

7.  (*Optional*) To view metrics about the Database Collector,
    left-click the **DB** connection in the **APPS** field for your
    Broker VM.

- Cortex XSIAM displays **Resources**, including the amount of **CPU**,
  **Memory**, and **Disk** space the applet is using.

8.  Manage the Database Collector.

- After you activate the Database Collector, you can make additional
  changes as needed. To modify a configuration, left-click the **DB**
  connection in the **APPS** column to display the Database Collector
  settings, and select:

  - **Configure** to redefine the Database Collector configurations.

  - **Deactivate** to disable the Database Collector.

###### Activate Files and Folders Collector

The Broker VM provides a Files and Folders Collector applet that enables
you to monitor and collect logs from files and folders in a network
share for a Windows or Linux directory, directly to your log repository
for query and visualization purposes. The Files and Folders collector
applet only starts to collect files that are more than 256 bytes and is
only supported with a Network File System version 4 (NFSv4). After you
activate the Files and Folders Collector applet, you can collect files
as datasets (`<Vendor>_<Product>_raw`) by defining the following.

- Details of the folder path on the network share containing the files
  that you want to monitor and upload to Cortex XSIAM.

- Settings related to the list of files to monitor and upload to Cortex
  XSIAM, where the log format is either Raw (default), JSON, CSV, TSV,
  PSV, CEF, LEEF, Corelight, or Cisco.

> **Note**
>
> Cortex XSIAM only supports ingestion of files encoded in UTF-8 format.
>
> **Prerequisite**

- > [Set up and configure Broker
  > VM](#UUIDa2b1b832d74850d81f427e175514c501).

- > Know the complete path to the files and folders that you want Cortex
  > XSIAM to monitor.

- > Ensure that the user permissions for the network share include the
  > ability to rename and delete files in the folder that you want to
  > configure collection.

####### How to activate the Files and Folders Collector

1.  Select Settings \> Configurations \> Data Broker \> Broker VMs.

2.  Do one of the following:

    - On the **Brokers** tab, find the Broker VM, and in the **APPS**
      column, left-click Add \> Files and Folder Collector.

    - On the **Clusters** tab, find the Broker VM, and in the **APPS**
      column, left-click Add \> Files and Folder Collector.

3.  Configure the Files and Folder Collector settings.

- Shared Folder Connection

+-----------------------------------+--------------------------------------+
| Field                             | Description                          |
+===================================+======================================+
| Folder Path                       | Specify the path to the files and    |
|                                   | folders that you want Cortex XSIAM   |
|                                   | to monitor continuously to collect   |
|                                   | the files. The following formats are |
|                                   | available based on the type of       |
|                                   | machine you are using:               |
|                                   |                                      |
|                                   | - **Windows**:                       |
|                                   |   `\\<hostname>\<shared_folder>` or  |
|                                   |   `smb://<hostname>/<shared_folder>` |
|                                   |                                      |
|                                   | - **Linux**:                         |
|                                   |   `/<srv>/<shared_folder>` or        |
|                                   |   `nfs://<srv>/<shared_folder>`      |
|                                   |                                      |
|                                   | <!-- -->                             |
|                                   |                                      |
|                                   | - > **Note**                         |
|                                   |                                      |
|                                   |   > When using the Linux file share, |
|                                   |   > including the Linux share with   |
|                                   |   > nfs, a **Username** and          |
|                                   |   > **Password** is not required, so |
|                                   |   > these fields are grayed out in   |
|                                   |   > the screen.                      |
+-----------------------------------+--------------------------------------+
| Recursive                         | Select this checkbox to configure    |
|                                   | the Files and Folders Collector      |
|                                   | applet to recursively examine any    |
|                                   | subfolders for new files as long as  |
|                                   | the folders are readable. This is    |
|                                   | not configured by default.           |
+-----------------------------------+--------------------------------------+
| Username                          | Specify the username to access the   |
|                                   | shared resource using a User         |
|                                   | Principal Name (UPN) format.         |
+-----------------------------------+--------------------------------------+
| Password                          | Specify the password to access the   |
|                                   | shared resource.                     |
+-----------------------------------+--------------------------------------+
| Test Connection                   | Select to validate the connection    |
|                                   | and permissions.                     |
+-----------------------------------+--------------------------------------+

- File and Folder Settings

+-----------------------------------+-----------------------------------+
| Field                             | Description                       |
+===================================+===================================+
| Mode                              | Select the mode to use for        |
|                                   | collecting data. The settings     |
|                                   | displayed change depending on     |
|                                   | your selection.                   |
|                                   |                                   |
|                                   | - **Tail**: Continuously monitors |
|                                   |   the files for new data          |
|                                   |   (default). The collector adds   |
|                                   |   the new data from the files to  |
|                                   |   the dataset.                    |
|                                   |                                   |
|                                   | - **Batch**: Reads the files      |
|                                   |   automatically at user           |
|                                   |   determined intervals, updates   |
|                                   |   the lookup datasets, and then   |
|                                   |   renames or deletes the uploaded |
|                                   |   source files. Renaming or       |
|                                   |   deleting the read source files  |
|                                   |   ensures that the collector      |
|                                   |   always reads the most           |
|                                   |   up-to-date file. Depending on   |
|                                   |   the **Storage Method**, the     |
|                                   |   collector can **Append** the    |
|                                   |   new data from the files to the  |
|                                   |   dataset or completely           |
|                                   |   **Replace** the data in the     |
|                                   |   dataset.                        |
|                                   |                                   |
|                                   | <!-- -->                          |
|                                   |                                   |
|                                   | - > **Note**                      |
|                                   |                                   |
|                                   |   > In Batch mode, the Files and  |
|                                   |   > Folders Collector supports    |
|                                   |   > collecting logs from a        |
|                                   |   > network share for a maximum   |
|                                   |   > file size of 500 MB.          |
+-----------------------------------+-----------------------------------+
| Collect Every                     | This option is only displayed in  |
|                                   | **Batch Mode**. Specify the       |
|                                   | execution frequency of collection |
|                                   | by designating a number and then  |
|                                   | selecting the unit as either      |
|                                   | **Minutes**, **Hours**, or        |
|                                   | **Days**.                         |
+-----------------------------------+-----------------------------------+
| After Files Uploaded              | This option is only displayed in  |
|                                   | **Batch Mode**. Select what to do |
|                                   | with the files after they are     |
|                                   | uploaded to the Cortex XSIAM      |
|                                   | server. You can                   |
|                                   | **Rename files with a suffix**    |
|                                   | (default) or you can              |
|                                   | **Delete files**. When renaming,  |
|                                   | the suffix is added to the end of |
|                                   | the original file name using the  |
|                                   | format `<file name>.<suffix>`,    |
|                                   | which becomes the new name of the |
|                                   | file.                             |
+-----------------------------------+-----------------------------------+
| Include                           | Specify the files and folders     |
|                                   | that must match to be monitored   |
|                                   | by Cortex XSIAM. Multiple values  |
|                                   | are allowed with commas           |
|                                   | separating the values and are     |
|                                   | case-sensitive.                   |
|                                   |                                   |
|                                   | Allowed wildcard:                 |
|                                   |                                   |
|                                   | - \'?\' matches a single alphabet |
|                                   |   character in a specific         |
|                                   |   position.                       |
|                                   |                                   |
|                                   | - \'\*\' matches any character or |
|                                   |   set of characters, including no |
|                                   |   character.                      |
|                                   |                                   |
|                                   | `log*.jsonlog*.json` includes any |
|                                   | JSON file starting with \'log\'.  |
+-----------------------------------+-----------------------------------+
| Exclude (Optional)                | Specify the files and folders     |
|                                   | that must match to not be         |
|                                   | monitored by Cortex XSIAM .       |
|                                   | Multiple values are allowed with  |
|                                   | commas separating the values.     |
|                                   |                                   |
|                                   | Allowed wildcard:                 |
|                                   |                                   |
|                                   | - \'?\' matches a single alphabet |
|                                   |   character in a specific         |
|                                   |   position.                       |
|                                   |                                   |
|                                   | - \'\*\' matches any character or |
|                                   |   set of characters, including no |
|                                   |   character.                      |
|                                   |                                   |
|                                   | `*.backup` excludes any file      |
|                                   | ending with \'.backup\'.          |
+-----------------------------------+-----------------------------------+
| Log Format                        | Select the **Log Format** from    |
|                                   | the list as either **Raw**        |
|                                   | (default), **JSON**, **CSV**,     |
|                                   | **TSV**, **PSV**, **CEF**,        |
|                                   | **LEEF**, **Corelight**, or       |
|                                   | **Cisco**. This setting defines   |
|                                   | the parser used to parse all the  |
|                                   | processed files as defined in the |
|                                   | **Include** and **Exclude**       |
|                                   | fields, regardless of the file    |
|                                   | names and extension. For example, |
|                                   | if the **Include** field is set   |
|                                   | `*` and the **Log Format** is     |
|                                   | **JSON**, all files (even those   |
|                                   | named `file.log`) in the          |
|                                   | specified folder are processed by |
|                                   | the Files and Folders Collector   |
|                                   | as JSON, and any entry that does  |
|                                   | not comply with the JSON format   |
|                                   | are dropped.                      |
|                                   |                                   |
|                                   | > **Note**                        |
|                                   | >                                 |
|                                   | > When uploading **JSON** files,  |
|                                   | > Cortex XSIAM only parses the    |
|                                   | > first level of nesting and only |
|                                   | > supports single line JSON       |
|                                   | > format, such that every new     |
|                                   | > line means a separate entry.    |
+-----------------------------------+-----------------------------------+
| \# of Lines to Skip (Optional)    | Specify the number of lines to    |
|                                   | skip at the beginning of the      |
|                                   | file. This is set to 0 by         |
|                                   | default.                          |
|                                   |                                   |
|                                   | > **Note**                        |
|                                   | >                                 |
|                                   | > Use this option only in cases   |
|                                   | > where your files contain some   |
|                                   | > sort of \"header\" lines, such  |
|                                   | > as a general description, an    |
|                                   | > introduction, a disclaimer, or  |
|                                   | > similar, and you want to skip   |
|                                   | > ingesting them. The Lines to    |
|                                   | > Skip are not part of the file   |
|                                   | > format. For example, in CSV     |
|                                   | > files, there is no need to skip |
|                                   | > lines.                          |
+-----------------------------------+-----------------------------------+

- Data Source Mapping

+-----------------------------------+----------------------------------------+
| Field                             | Description                            |
+===================================+========================================+
| Storage Method                    | This option is only displayed in       |
|                                   | **Batch Mode**. Specify whether to     |
|                                   | **Append** the read data to the        |
|                                   | dataset, or to **Replace** all the     |
|                                   | data in the dataset with the newly     |
|                                   | read data.                             |
|                                   |                                        |
|                                   | - **Append**: This mode is useful for  |
|                                   |   log files where you want to keep all |
|                                   |   the log info from before.            |
|                                   |                                        |
|                                   | - **Replace**: This mode is useful for |
|                                   |   adding inventory data from CSV and   |
|                                   |   JSON files which include properties, |
|                                   |   for example, a list of machines, a   |
|                                   |   list of users, or a mapping of       |
|                                   |   endpoints to users to create a       |
|                                   |   lookup dataset. In each data         |
|                                   |   collection cycle, the new data       |
|                                   |   completely replaces the existing     |
|                                   |   data in the dataset. You can use the |
|                                   |   records from the lookup datasets for |
|                                   |   correlation and enrichment through   |
|                                   |   parsing rules, correlation rules,    |
|                                   |   and queries.                         |
|                                   |                                        |
|                                   | <!-- -->                               |
|                                   |                                        |
|                                   | - > **Note**                           |
|                                   |                                        |
|                                   |   - > When the storing method is       |
|                                   |     > **Replace**, the maximum size    |
|                                   |     > for the total data to be         |
|                                   |     > imported into a lookup dataset   |
|                                   |     > is 30 MB each time the data is   |
|                                   |     > fetched.                         |
|                                   |                                        |
|                                   |   - > The inventory data ingested      |
|                                   |     > using the Files and Folders      |
|                                   |     > collector is counted towards     |
|                                   |     > license utilization.             |
|                                   |                                        |
|                                   |   - > When you use a JOINT function    |
|                                   |     > with a lookup table in a query   |
|                                   |     > or correlation rule, make sure   |
|                                   |     > you configure the conflict       |
|                                   |     > strategy to point to the raw     |
|                                   |     > dataset. This ensures that the   |
|                                   |     > system fields are taken from the |
|                                   |     > raw dataset and not from the     |
|                                   |     > lookup table.                    |
+-----------------------------------+----------------------------------------+
| Target Dataset                    | This option is only displayed in       |
|                                   | **Batch Mode** when the storing method |
|                                   | is **Replace**. Select the name of an  |
|                                   | existing Lookup dataset or create a    |
|                                   | new Lookup dataset by specifying the   |
|                                   | name.                                  |
|                                   |                                        |
|                                   | When you create a new target dataset   |
|                                   | name, specify a name that will be more |
|                                   | meaningful for your users when they    |
|                                   | query the dataset. For example, if the |
|                                   | original file name is `accssusr.csv`,  |
|                                   | you can save the dataset as            |
|                                   | `access_per_users`.                    |
|                                   |                                        |
|                                   | Dataset names can contain special      |
|                                   | characters from different languages,   |
|                                   | numbers (`0-9`) and underscores (`_`). |
|                                   | You can create dataset names using     |
|                                   | uppercase characters, but in queries,  |
|                                   | dataset names are always treated as if |
|                                   | they are lowercase.                    |
|                                   |                                        |
|                                   | > **Note**                             |
|                                   |                                        |
|                                   | - > You can\'t specify a file name     |
|                                   |   > that\'s the same as a system file  |
|                                   |   > name.                              |
|                                   |                                        |
|                                   | - > The name of a dataset created from |
|                                   |   > a *tsv* file must always include   |
|                                   |   > the extension. If the original     |
|                                   |   > file name is mrkdptusrsnov23.tsv,  |
|                                   |   > you can name save the dataset with |
|                                   |   > the name                           |
|                                   |   > marketing_dept_users_Nov_2023.tsv. |
+-----------------------------------+----------------------------------------+
| Vendor and Product                | Specify the **Vendor** and **Product** |
|                                   | for the type of data being collected.  |
|                                   | The vendor and product are used to     |
|                                   | define the name of your Cortex Query   |
|                                   | Language (XQL) dataset                 |
|                                   | (`<Vendor>_<Product>_raw`).            |
|                                   |                                        |
|                                   | > **Note**                             |
|                                   | >                                      |
|                                   | > The **Vendor** and **Product**       |
|                                   | > defaults to **Auto-Detect** when the |
|                                   | > **Log Format** is set to **CEF** or  |
|                                   | > **LEEF**.                            |
+-----------------------------------+----------------------------------------+

- Generate Preview
  Select **Generate Preview** to display up to 10 rows from the first
  file and **Preview** the results. The **Preview** works based on the
  Files and Folders Collector settings, which means that if all the
  files that were configured to be monitored were already processed,
  then the **Preview** returns no records.

4.  (*Optional*) Click **Add Connection** to define another Files and
    Folders connection for collecting logs from files and folders in a
    shared resource.

5.  (*Optional*) Other available options.

- As needed, you can return to your Files and Folders Collector settings
  to manage your connections. Here are the actions available to you:

  - Edit the connection name by hovering over the default **Collection**
    name, and selecting the edit icon to edit the text.

  - Disable/Enable a connection by hovering over the top area of the
    connection section, on the opposite side of the connection name, and
    selecting the applicable button.

  - Delete a connection by hovering over the top area of the connection
    section, on the opposite side of the connection name, and selecting
    the delete icon. You can only delete a connection when you have more
    than one connection configured. Otherwise, this icon is not
    displayed.

6.  Activate the Files and Folders Collector applet.

- After a successful activation, the **APPS** field displays **File**
  with a green dot indicating a successful connection.

7.  (*Optional*) To view metrics about the Files and Folders, left-click
    the **File** connection in the **APPS** field for your Broker VM.

- Cortex XSIAM displays **Resources**, including the amount of **CPU**,
  **Memory**, and **Disk** space the applet is using.

8.  Manage the Files and Folders Collector.

- After you activate the Files and Folders Collector, you can make
  additional changes as needed. To modify a configuration, left-click
  the **File** connection in the **APPS** column to display the Files
  and Folder Collector settings, and select:

  - **Configure** to redefine the Files and Folders Collector
    configurations.

  - **Deactivate** to disable the Files and Folders Collector.

###### Activate FTP Collector

The Broker VM provides a FTP Collector applet that enables you to
monitor and collect logs from files and folders via FTP, FTPS, and SFTP
directly to your log repository for query and visualization purposes. A
maximum file size of 500 MB, and a minimum file size of 256 B are
supported. After you activate the FTP Collector applet on a Broker VM in
your network, you can collect files as datasets
(`<Vendor>_<Product>_raw`) by defining the following.

- FTP, FTPS, or SFTP (default) connection details with the path to the
  folder containing the files that you want to monitor and upload to
  Cortex XSIAM .

- Settings related to the list of files to monitor and upload to Cortex
  XSIAM , where the log format is either Raw (default), JSON, CSV, TSV,
  PSV, CEF, LEEF, Corelight, or Cisco. Once the files are uploaded to
  Cortex XSIAM , you can define whether in the source directory the
  files are renamed or deleted.

> **Prerequisite**

- > [Set up and configure Broker
  > VM](#UUIDa2b1b832d74850d81f427e175514c501).

- > Ensure that the user permissions for the FTP, SFTP, or FTPS include
  > the ability to rename and delete files in the folder that you want
  > to configure collection.

- > When setting up an FTPS Collector with a server using a Self-signed
  > certificate, you must upload the certificate first to the Broker VM
  > as a Trusted CA certificate.

####### How to activate the FTP Collector

1.  Select Settings \> Configurations \> Data Broker \> Broker VMs.

2.  Do one of the following:

    - On the **Brokers** tab, find the Broker VM, and in the **APPS**
      column, left-click Add \> FTP Collector.

    - On the **Clusters** tab, find the Broker VM, and in the **APPS**
      column, left-click Add \> FTP Collector.

3.  Configure the **FTP Collector** settings.

- FTP Connection

+-----------------------------------+---------------------------------------------------------+
| Field                             | Description                                             |
+===================================+=========================================================+
| Type                              | Select the type of FTP connection as **FTP**, **SFTP**, |
|                                   | or **FTPS**.                                            |
+-----------------------------------+---------------------------------------------------------+
| Host                              | Enter the hostname, IP address, or FQDN of the FTP      |
|                                   | server. When configuring a **FTPS** Collector, you must |
|                                   | specify the FQDN.                                       |
+-----------------------------------+---------------------------------------------------------+
| Port                              | Enter the FTP port number.                              |
+-----------------------------------+---------------------------------------------------------+
| Username                          | Enter the username to login to the FTP server.          |
+-----------------------------------+---------------------------------------------------------+
| Password                          | Enter the password to login to the FTP server.          |
+-----------------------------------+---------------------------------------------------------+
| SSH Key-Based Authentication      | This checkbox is only displayed when setting a **SFTP** |
|                                   | Collector, which works with both **Username** and       |
|                                   | **Password** authentication or                          |
|                                   | **SSH Key-Based Authentication**. You can either leave  |
|                                   | this checkbox clear and set a **Username** and          |
|                                   | **Password** (default) or select                        |
|                                   | **SSH Key-Based Authentication** to **Browse** to a     |
|                                   | **Private Key**. When this connection is established    |
|                                   | with a server using a Self-signed certificate, you must |
|                                   | upload it first to the Broker VM as a Trusted CA        |
|                                   | Certificate.                                            |
|                                   |                                                         |
|                                   | > **Note**                                              |
|                                   | >                                                       |
|                                   | > When configuring an SFTP connection, Cortex XSIAM     |
|                                   | > expects the private key to be in the RSA format that  |
|                                   | > is included in the `-----BEGIN RSA PRIVATE KEY-----`  |
|                                   | > tag. Cortex XSIAM does not support providing the      |
|                                   | > private key in the OpenSSH format from the            |
|                                   | > `-----BEGIN OPENSSH PRIVATE KEY-----` tag.            |
|                                   | >                                                       |
|                                   | > When using `ssh-keygen` using a Mac, you get the      |
|                                   | > OpenSSH format by default. The command for getting    |
|                                   | > the RSA format is:                                    |
|                                   |                                                         |
|                                   |     ssh-keygen -t rsa -b 4096 -C <email address> -m PEM |
+-----------------------------------+---------------------------------------------------------+
| Folder Path                       | Specify the path to the folder on the FTP site where    |
|                                   | the files are located that you want to collect.         |
+-----------------------------------+---------------------------------------------------------+
| Recursive                         | Select this checkbox to configure the FTP Collector     |
|                                   | applet to recursively examine any subfolders for new    |
|                                   | files as long as the folders are readable. This is not  |
|                                   | configured by default.                                  |
+-----------------------------------+---------------------------------------------------------+
| Test Connection                   | Select to validate the FTP connection.                  |
+-----------------------------------+---------------------------------------------------------+

- FTP Settings

+-----------------------------------+-----------------------------------+
| Field                             | Description                       |
+===================================+===================================+
| Collect Every                     | Specify the execution frequency   |
|                                   | of collection by designating a    |
|                                   | number and then selecting the     |
|                                   | unit as either **Minutes**,       |
|                                   | **Hours**, or **Days**.           |
+-----------------------------------+-----------------------------------+
| After Files Uploaded              | Select what to do with the files  |
|                                   | after they are uploaded to the    |
|                                   | Cortex XSIAM server. You can      |
|                                   | either select                     |
|                                   | **Rename files with a suffix**    |
|                                   | (default) and then you must       |
|                                   | specify the **Suffix** or         |
|                                   | **Delete files**. When adding a   |
|                                   | suffix, the suffix is added at    |
|                                   | the end of the original file name |
|                                   | using the format                  |
|                                   | `<file name>.<suffix>`, which     |
|                                   | becomes the new name of the file. |
+-----------------------------------+-----------------------------------+
| Include                           | Specify the files and folders     |
|                                   | that must match to be monitored   |
|                                   | by Cortex XSIAM . Multiple values |
|                                   | are allowed with commas           |
|                                   | separating the values.            |
|                                   |                                   |
|                                   | Allowed wildcard:                 |
|                                   |                                   |
|                                   | - \'?\' matches a single alphabet |
|                                   |   character in a specific         |
|                                   |   position.                       |
|                                   |                                   |
|                                   | - \'\*\' matches any character or |
|                                   |   set of characters, including no |
|                                   |   character.                      |
|                                   |                                   |
|                                   | `log*.json` includes any JSON     |
|                                   | file starting with \'log\'.       |
+-----------------------------------+-----------------------------------+
| Exclude (Optional)                | Specify the files and folders     |
|                                   | that must match to not be         |
|                                   | monitored by Cortex XSIAM .       |
|                                   | Multiple values are allowed with  |
|                                   | commas separating the values.     |
|                                   |                                   |
|                                   | Allowed wildcard:                 |
|                                   |                                   |
|                                   | - \'?\' matches a single alphabet |
|                                   |   character in a specific         |
|                                   |   position.                       |
|                                   |                                   |
|                                   | - \'\*\' matches any character or |
|                                   |   set of characters, including no |
|                                   |   character.                      |
|                                   |                                   |
|                                   | `*.backup` excludes any file      |
|                                   | ending with \'.backup\'.          |
+-----------------------------------+-----------------------------------+
| Log Format                        | Select the **Log Format** from    |
|                                   | the list as either **Raw**        |
|                                   | (default), **JSON**, **CSV**,     |
|                                   | **TSV**, **PSV**, **CEF**,        |
|                                   | **LEEF**, **Corelight**, or       |
|                                   | **Cisco**, which indicates to     |
|                                   | Cortex XSIAM how to parse the     |
|                                   | data in the file. This setting    |
|                                   | defines the parser used to parse  |
|                                   | all the processed files as        |
|                                   | defined in the **Include** and    |
|                                   | **Exclude** fields, regardless of |
|                                   | the file names and extension. For |
|                                   | example, if the **Include** field |
|                                   | is set `*` and the **Log Format** |
|                                   | is **JSON**, all files (even      |
|                                   | those named `file.log`) in the    |
|                                   | specified folder are processed by |
|                                   | the FTP Collector as JSON, and    |
|                                   | any entry that does not comply    |
|                                   | with the JSON format are dropped. |
|                                   |                                   |
|                                   | > **Note**                        |
|                                   | >                                 |
|                                   | > When uploading **JSON** files,  |
|                                   | > Cortex XSIAM only parses the    |
|                                   | > first level of nesting and only |
|                                   | > supports single line JSON       |
|                                   | > format, such that every new     |
|                                   | > line means a separate entry.    |
+-----------------------------------+-----------------------------------+
| \# of Lines to Skip (Optional)    | Enter the number of lines to skip |
|                                   | at the beginning of the file.     |
|                                   | This is set to 0 by default.      |
|                                   |                                   |
|                                   | > **Note**                        |
|                                   | >                                 |
|                                   | > Use this option only in cases   |
|                                   | > where your files contain some   |
|                                   | > sort of \"header\" lines, such  |
|                                   | > as a general description, an    |
|                                   | > introduction, a disclaimer, or  |
|                                   | > similar, and you want to skip   |
|                                   | > ingesting them. The Lines to    |
|                                   | > Skip are not part of the file   |
|                                   | > format. For example, in CSV     |
|                                   | > files, there is no need to skip |
|                                   | > lines.                          |
+-----------------------------------+-----------------------------------+

- Data Source Mapping
  Specify the **Vendor** and **Product** for the type of data being
  collected. The vendor and product are used to define the name of your
  Cortex Query Language (XQL) dataset (`<Vendor>_<Product>_raw`).

  > **Note**

  - > The **Vendor** and **Product** defaults to **Auto-Detect** when
    > the **Log Format** is set to **CEF** or **LEEF**.

  Preview
  Select **Generate Preview** to display up to 10 rows from the first
  file and **Preview** the results. The **Preview** works based on the
  FTP Collector settings, which means that if all the files that were
  configured to be monitored were already processed, then the
  **Preview** returns no records.

4.  (*Optional*) Click **Add Connection** to define another FTP
    connection for collecting logs from files and folders via FTP, FTPS,
    or SFTP.

5.  (*Optional*) Other available options.

- As needed, you can return to your FTP Collector settings to manage
  your connections. Here are the actions available to you:

  - Edit the connection name by hovering over the default **Collection**
    name, and selecting the edit icon to edit the text.

  - Disable/Enable a connection by hovering over the top area of the
    connection section, on the opposite side of the connection name, and
    selecting the applicable button.

  - Delete a connection by hovering over the top area of the connection
    section, on the opposite side of the connection name, and selecting
    the delete icon. You can only delete a connection when you have more
    than one connection configured. Otherwise, this icon is not
    displayed.

6.  Activate the FTP Collector applet.

- After a successful activation, the **APPS** field displays **FTP**
  with a green dot indicating a successful connection.

7.  (*Optional*) To view metrics about the FTP Collector, left-click the
    **FTP** connection in the **APPS** field for your Broker VM.

- Cortex XSIAM displays **Resources**, including the amount of **CPU**,
  **Memory**, and **Disk** space the applet is using.

8.  Manage the FTP Collector.

- After you activate the FTP Collector, you can make additional changes
  as needed. To modify a configuration, left-click the **FTP**
  connection in the **APPS** column to display the FTP Collector
  settings, and select:

  - **Configure** to redefine the FTP Collector configurations.

  - **Deactivate** to disable the FTP Collector.

###### Activate Local Agent Settings

The Local Agent Settings applet on the Palo Alto Networks Broker VM
enables you to:

Deploy the Broker VM proxy

To deploy Cortex XSIAM in restricted networks where endpoints do not
have a direct connection to the internet, setup the Broker VM to act as
a proxy that routes all the traffic between the Cortex XSIAM management
server and XDR agents/XDR Collectors via a centralized and controlled
access point. This enables your agents and XDR Collectors to receive
security policy updates, upgrades, and send logs and files to Cortex
XSIAM without a direct internet connection. The Broker VM acts like a
transparent proxy and doesn't decrypt the secure connection between the
server and the XDR agent/XDR Collectors, and hides the XDR agent's/XDR
Collector\'s original IP addresses. If your network topology includes
SSL decryption in an upstream proxy/firewall, the Broker VM does not
participate in the trust relationship as it is not initiating the
connection to the server to be fully transparent.

Enable broker caching

To reduce your external network bandwidth loads, you can cache XDR agent
installations, upgrades, and content updates on your Cortex XSIAM Broker
VM. Every 15 minutes, the Broker VM retrieves the latest installers and
content files from Cortex XSIAM. The Broker VM stores this content for 7
days and agent installers for up to 30 days from the agent\'s last
request. If the files were not available on the Broker VM at the time of
the ask, the agent proceeds to download the files directly from the
XSIAM server.

####### Requirements

Before you activate the Local Agent Settings applet, verify the
following prerequisites and limitations listed by the main features.

General

The Local Agent Settings applet on the Broker VM is capable of
supporting:

- Up to 50,000 agents for Agent Proxy.

- Up to 10,000 agents for Content Caching.

> **Note**
>
> This is assuming a standard hardware setup with 2vCPU 8 GB memory.

Agent Proxy

- Supported with Traps agent version 5.0.9 and Traps agent version 6.1.2
  and later releases.

- Broker VM supports forwarding the XDR Collectors request URLs on all
  Broker VM versions.

- Supported with all XDR Collector versions.

<!-- -->

- > **Note**

  > Broker VMs can act as as a proxy for routing XDR Collector traffic
  > to the Cortex XSIAM tenant. The Broker VM does not cache XDR
  > Collector installers.

<!-- -->

- The Agent Proxy can also act as a proxy for other brokers. It supports
  all the data that brokers send to the server, including the logs they
  collect, using the Cortex Broker VM applets.

Agent Installer and Content Caching

- Supported with XDR agent version 7.4 and later releases and Broker VM
  12.0 and later.

- Requires a Broker VM with a minimum of an 8-core processor and
  increase the disk space allocated for data storage to 1024 GB to
  support caching for 10,000 agents. For more information, see [Increase
  Broker VM storage allocated for data
  caching](#UUID6d23c0dc17ebf6d797a12705e2bd1b79).

- For the agent installer and content caching to work properly, you must
  configure different settings where the instructions differ depending
  on whether you are configuring a standalone Broker VM or High
  Availability (HA) cluster:

<!-- -->

- Standalone broker
  - FQDN: A FQDN must be configured for the standalone broker as
    configured in your local DNS server. This is to ensure that XDR
    agents know who to access to receive agent installer and content
    caching data.

  - SSL certificates: Ensure you upload strong cipher SHA256-based SSL
    certificates when you setup the Broker VM. For more information, see
    [Set up and configure Broker
    VM](#UUIDa2b1b832d74850d81f427e175514c501).

  - Download source: Requires adding the Broker VM as a download source
    in your Agent Settings Profile.

  HA cluster
  - FQDN: A FQDN must be configured in the cluster settings as
    configured in your local DNS server, which points to a Load
    Balancer. This ensures that the XDR agents turn to the load balancer
    to route the requests for the agent installer and content caching
    data to the correct broker. For more information on configuring the
    Load Balancer FQDN in a HA cluster, see [Configure High Availability
    Cluster](#UUIDcdafbc5b0f072deba4bdea1f522d867d).

  - SSL certificates: In each broker in the cluster, ensure you upload
    strong cipher SHA256-based SSL certificates when you setup the
    Broker VM. For more information, see [Set up and configure Broker
    VM](#UUIDa2b1b832d74850d81f427e175514c501).

  - Download source: Requires adding the cluster as a download source in
    your Agent Settings Profile.

Agent communication with Broker VM

Agents communicate with the Broker VM using Hypertext Transfer Protocol
Secure (https) over port 443. You must ensure this port is open so that
the Broker VM is accessible to all agents that are configured to use its
cache.

Broker communication with cloud manager

The broker needs to communicate with the same URLs that the agents
communicate with to avoid receiving any inaccessible URLs errors. For a
complete list of the URLs that you need to allow access, see [Enable
access to required PANW
resources](#UUID24cb15454f44c259b484435238bb6a33).

####### How to activate the Local Agent Settings applet

After you configure and register your Palo Alto Networks Broker VM,
proceed to set up your Local Agent Settings applet.

1.  Select Settings \> Configurations \> Data Broker \> Broker VMs.

2.  In either the **Brokers** tab or the **Clusters** tab, locate your
    Broker VM.

3.  (*Optional*) To set up the Agent Proxy:

    a.  Right-click the Broker VM, select **Configure**.

    - Ensure your proxy server is configured. If not, proceed to add it
      as described in [Set up and configure Broker
      VM](#UUIDa2b1b832d74850d81f427e175514c501).

    b.  In the **APPS** column, left-click Add \> Local Agent Settings.

    c.  In the **Activate Local Agent** configuration, enable
        **Agent Proxy** by setting the **Proxy** to **Enabled**, and
        specify the **Port**. You can also configure the
        **Listening Interface**, where the default is set to **All**.

    - > **Note**

      > When you install your XDR agents, you need to configure the IP
      > address of the Broker VM and a port number during the
      > installation. You can use the default 8888 port or set a custom
      > port. You are not permitted to configure port numbers between
      > 0-1024 and 63000-65000, or port numbers 4369, 5671, 5672, 5986,
      > 6379, 8000, 9100, 15672, 25672. Additionally, you are not
      > permitted to reuse port numbers you already assigned to the
      > Syslog Collector applet.

4.  (*Optional*) To setup up Agent Installer and Content Caching:

    a.  Ensure you uploaded your SHA256-based certificates.

    - If not, upload them as described in [Set up and configure Broker
      VM](#UUIDa2b1b832d74850d81f427e175514c501) and **Save**.

    b.  Specify the Broker VM FQDN.

    - Right-click the Broker VM, select **Configure**. Under
      **Device Name**, enter your Broker VM **FQDN**. This FQDN record
      must be configured in your local DNS server.

      > **Important**

      > A FQDN must be configured for WEC and Agent Installer and
      > Content Caching to function properly.

    c.  Activate the Local Agent Settings applet on the Broker VM.

    - You can either right-click the Broker VM and select Add App \>
      Local Agent Settings, or in the **APPS** column, select Add \>
      Local Agent Settings.

    d.  Activate installer and content caching.

    - In the **Activate Local Agent** configuration, enable
      **Agent Installer and Content Caching** by setting **Caching** to
      **Enabled**.

      > **Important**

      > You can only enable Agent Installer and Content Caching, when in
      > the Broker VM Configuration, you\'ve uploaded your signed SSL
      > Server Certificate and key and set the FQDN. For more
      > information, see the Agent Installer and Content Caching
      > requirements explained above.

    e.  To enable agents to start using Broker VM caching, you must add
        the Broker VM as a download source in your Agent Settings
        profile and select which Broker VMs to use. Then, ensure the
        profile is associated with a policy for your target agents.

5.  After a successful activation, the **APPS** field displays
    **Local Agent Settings** with a green dot indicating a successful
    connection. Left-click the **Local Agent Settings** connection to
    view the applet status and resource usage.

- To help you easily troubleshoot connectivity issues for a Local Agent
  Settings applet on the Palo Alto Networks Broker VM, Cortex XSIAM
  displays a list of **Denied URLs**. These URLs are displayed when you
  left-click the **Local Agent Settings** applet to view the
  **Connectivity Status**. As a result, in a situation where the Local
  Agent Settings applet is reported as activated with a failed
  connection, you can easily determine the URLs that need to be allowed
  in your network environment.

6.  Manage the local agent settings. After the local agent settings have
    been activated, left-click the **Local Agent Settings** connection
    in the **APPS** column to display the settings, and select:

    - **Configure** to change your settings.

    - **Deactivate** to disable the local agent settings altogether.

###### Activate NetFlow Collector

To receive NetFlow flow records from an external source, you must first
set up the NetFlow Collector applet on a Broker VM within your network.
NetFlow versions 5, 9, and IPFIX are supported.

To increase the log ingestion rate, you can add additional CPUs to the
Broker VM. The NetFlow Collector listens for flow records on specific
ports either from any, or from specific IP addresses.

After the NetFlow Collector is activated, the NetFlow Exporter sends
flow records to the NetFlow Collector, which receives, stores, and
pre-processes that data for later analysis.

Performance Requirements

The following setups are required to meet your performance needs:

- 4 CPUs for up to 50K flows per second (FPS).

- 8 CPUs for up to 100K FPS.

> **Note**
>
> Since multiple network devices can send data to a single NetFlow
> Collector, we recommend that you configure a maximum of 50 NetFlow
> Collectors per Broker VM applet, with a maximum aggregated rate of
> approximately 50K flows per second (FPS) to maintain system
> performance.
>
> **Prerequisite**
>
> [Set up and configure Broker
> VM](#UUIDa2b1b832d74850d81f427e175514c501)

####### How to activate the NetFlow Collector

1.  Select Settings \> Configurations \> Data Broker \> Broker VMs.

2.  Do one of the following:

    - On the **Brokers** tab, find the Broker VM, and in the **APPS**
      column, left-click Add \> NetFlow Collector.

    - On the **Clusters** tab, find the Broker VM, and in the **APPS**
      column, left-click Add \> NetFlow Collector.

3.  Click **+Add New**.

4.  Configure your NetFlow Collector.

- General Settings
  Specify the number of the **UDP Port** on which the NetFlow Collector
  listens for flow records (default **2055**).

  This port number must match the UDP port number in the NetFlow
  exporter device. The rules for each port are evaluated, line by line,
  on a first match basis. Cortex XSIAM discards logs for non-configured
  flow records without an "Any" rule.

  > **Note**

  > Since Cortex XSIAM reserves some port numbers, it is best to select
  > a port number that is not in the range of 0-1024 (except for 514),
  > in the range of 63000-65000 or has one of the following values:
  > 4369, 5671, 5672, 5986, 6379, 8000, 8888, 9100, 15672, or 28672.

  Custom Settings

+-----------------------------------+------------------------------------------------------+
| Field                             | Description                                          |
+===================================+======================================================+
| Source Network                    | Specify the IP address or a Classless Inter-Domain   |
|                                   | Routing (CIDR) of the source network device that     |
|                                   | sends the flow records to Cortex XSIAM . Leave the   |
|                                   | field empty to receive data from any device on the   |
|                                   | specified port (default). If you do not specify an   |
|                                   | IP address or a CIDR, Cortex XSIAM can receive data  |
|                                   | from any source IP address or CIDR that transmits    |
|                                   | via the specified port. If IP addresses overlap in   |
|                                   | multiple rows in the **Source Network** field, such  |
|                                   | as 10.0.0.10 in the first row and 10.0.0.0/24 in the |
|                                   | second row, the NetFlow Collector captures the IP    |
|                                   | address in the first row.                            |
+-----------------------------------+------------------------------------------------------+
| Vendor and Product                | Specify a particular vendor and product to be        |
|                                   | associated with each dataset entry or leave the      |
|                                   | default **IP Flow** setting.                         |
|                                   |                                                      |
|                                   | The **Vendor** and **Product** values are used to    |
|                                   | define the name of your Cortex Query Language (XQL)  |
|                                   | dataset `<Vendor>_<Product>_raw`. If you do not      |
|                                   | define a vendor or product, Cortex XSIAM uses the    |
|                                   | default values with the resulting dataset name       |
|                                   | `ip_flow_ip_flow_raw`. Consider changing the default |
|                                   | values in order to uniquely identify the source      |
|                                   | network device.                                      |
|                                   |                                                      |
|                                   | After each configuration, select                     |
|                                   | ![](media/rId2150.png){width="0.14583333333333334in" |
|                                   | height="0.20833333333333334in"} to save your changes |
|                                   | and then select **Done** to update the NetFlow       |
|                                   | Collector with your settings.                        |
+-----------------------------------+------------------------------------------------------+

5.  (*Optional*) Make additional changes to the NetFlow Collector data
    sources.

    - You can make additional changes to the **Port** by right-clicking
      the applicable UDP port and selecting the following:

      - **Edit**: To change the **UDP Port**, **Source Network**,
        **Vendor**, or **Product** defined.

      - **Remove**: To delete a **Port**.

    - You can make additional changes to the **Source Network** by
      right-clicking on the **Source Network** value.

    <!-- -->

    - > **Note**

      > The options available change, according to the set
      > **Source Network** value.

  -----------------------------------------------------------------------
  Option                              Description
  ----------------------------------- -----------------------------------
  Edit                                To change the **UDP Port**,
                                      **Source Network**, **Vendor**, or
                                      **Product** defined.

  Remove                              To delete a **Port**.

  Copy entire row                     To copy the **Source Network**,
                                      **Product**, and **Vendor**
                                      information.

  Open IP View                        To view network operations and to
                                      view any open cases on this IP
                                      within a defined period. This
                                      option is only available when the
                                      **Source Network ** value is a
                                      specific IP address or CIDR.

  Open in Quick Launcher              To search for information using the
                                      Quick Launcher shortcut . This
                                      option is only available when the
                                      **Source Network ** value is a
                                      specific IP address or CIDR.
  -----------------------------------------------------------------------

- To prioritize the order of the NetFlow formats listed for the
  configured data source, drag and drop the rows to change their order.

6.  **Activate** the NetFlow collector applet.

- After successful activation, the **APPS** field displays **NetFlow**
  with a green dot indicating a successful connection.

7.  (*Optional*) To view NetFlow Collector metrics, left-click the
    **NetFlow** connection in the **APPS** field for your Broker VM.

- Cortex XSIAM displays the following information:

  -----------------------------------------------------------------------
  Option                              Description
  ----------------------------------- -----------------------------------
  Connectivity Status                 Whether the applet is connected to
                                      Cortex XSIAM.

  Logs Received and Logs Sent         Number of logs that the applet
                                      received and sent per second over
                                      the last 24 hours. If there are
                                      more logs received than sent, this
                                      can indicate a connectivity issue.

  Resources                           Displays the amount of **CPU**,
                                      **Memory**, and **Disk** space the
                                      applet uses.
  -----------------------------------------------------------------------

8.  Manage the NetFlow Collector.

- After you activate the NetFlow Collector, you can make additional
  changes. To modify a configuration, left-click the **NetFlow**
  connection in the **APPS** column to display the NetFlow Collector
  settings, and select:

  - **Configure** to redefine the NetFlow Collector configurations.

  - **Deactivate** to disable the NetFlow Collector.

  You can also [Ingest NetFlow flow records as
  datasets](#UUIDaa3914c7f3c2f723764f47f5ff2e0a77).

###### Activate Network Mapper

> **Prerequisite**
>
> After you have configured and registered your Broker VM, you can
> choose to activate the Network Mapper application.

The Network Mapper allows you to scan your network to detect and
identify unmanaged hosts in your environment according to defined IP
address ranges. The Network Mapper configurations are used to locate
unmanaged assets that appear in the Assets table. For more information,
see
[/document/preview/1159222#UUID-177e5820-a47a-3a78-eeed-063897c9712c](/document/preview/1159222#UUID-177e5820-a47a-3a78-eeed-063897c9712c).

1.  Select Settings \> Configurations \> Data Broker \> Broker VMs.

2.  Do one of the following:

    - On the **Brokers** tab, find the Broker VM, and in the **APPS**
      column, left-click Add \> Network Mapper.

    - On the **Clusters** tab, find the Broker VM, and in the **APPS**
      column, left-click Add \> Network Mapper.

3.  In the **Activate Network Mapper** window, define the following
    parameters:

+-----------------------------------+------------------------------------------------------+
| Field                             | Description                                          |
+===================================+======================================================+
| Scan Method                       | Select the either **ICMP echo** or **TCP SYN** scan  |
|                                   | method to identify your network hosts. When          |
|                                   | selecting TCP SYN you can enter single ports and     |
|                                   | ranges together, for example `80-83, 443`.           |
+-----------------------------------+------------------------------------------------------+
| Scan Requests per Second          | Define the maximum number of scan requests you want  |
|                                   | to send on your network per second. By default, the  |
|                                   | number of scan requests are defined as 1000.         |
|                                   |                                                      |
|                                   | > **Note**                                           |
|                                   | >                                                    |
|                                   | > Each IP address range can receive multiple scan    |
|                                   | > requests based on it\'s availability.              |
+-----------------------------------+------------------------------------------------------+
| Scanning Scheduler                | Define when you want to run the network mapper scan. |
|                                   | You can select either daily, weekly, or monthly at a |
|                                   | specific time.                                       |
+-----------------------------------+------------------------------------------------------+
| Scanned Ranges                    | Select from the list of exiting IP address ranges to |
|                                   | scan. Make sure to                                   |
|                                   | ![](media/rId3274.png){width="0.14583333333333334in" |
|                                   | height="0.20833333333333334in"} after each           |
|                                   | selection.                                           |
|                                   |                                                      |
|                                   | > **Note**                                           |
|                                   | >                                                    |
|                                   | > IP address ranges are displayed according to what  |
|                                   | > you defined as your Network Parameters.            |
+-----------------------------------+------------------------------------------------------+

4.  **Activate** the applet.

- After a successful activation, the **APPS** field displays
  **Network Mapper** with a green dot indicating a successful
  connection.

5.  In the **APPS** field, left-click the **Network Mapper** connection
    to view the following scan and applet metrics:

- Scan Details

  -----------------------------------------------------------------------
  Field                               Description
  ----------------------------------- -----------------------------------
  Connectivity Status                 Whether the applet is connected to
                                      Cortex XSIAM .

  Scan Status                         State of the scan.

  Scan Start Time                     Timestamp of when the scan started.

  Scan Duration                       Period of time in minutes and
                                      seconds the scan is running.

  Scan Progress                       How much of the scan has been
                                      completed in percentage and IP
                                      address ratio.

  Detected Hosts                      Number of hosts identified from
                                      within the IP address ranges.

  Scan Rate                           Number of IP addresses scanned per
                                      second.
  -----------------------------------------------------------------------

- Applet Metrics
  **Resources**: Displays the amount of **CPU**, **Memory**, and
  **Disk** space the applet is using.

6.  Manage the Network Mapper.

- After the network mapper has been activated, left-click the
  **Network Mapper** connection in the **APPS** column to display the
  Network Mapper settings, and select:

  - **Configure** to redefine the network mapper configurations.

  - **Scan Now** to initiate a scan.

  - **Deactivate** to disable the network mapper.

###### Activate Pathfinder

> **Important**
>
> The Broker VM Pathfinder applet is now deprecated.

- > In tenants where the applet was implemented, it will remain
  > operational until January 25, 2026.

- > In all other tenants, the applet is unavailable and cannot be used
  > as of July 20, 2025. To ensure complete coverage and protection, we
  > recommend deploying XDR Agents on all endpoints by this date.

- > Migration guidance and deployment resources for XDR Agents are
  > available. See [Install Cortex XDR
  > agents](#UUID50efdc5d1ba2e5663253d4a8edbeb278)p.

- > For questions or transition support, contact your [Customer Support
  > team](https://support.paloaltonetworks.com/Support/Index).

> **Important**
>
> The Pathfinder applet isn\'t supported when configuring Broker VMs in
> high availability (HA) clusters.

Pathfinder is a highly recommended, but optional component integrated
with the Broker VM that deploys a non-persistent data collector on
network hosts, servers, and workstations that are not managed by a
Cortex XDR agent. The collector is automatically triggered by
analytics-type issues with a severity of high and medium and provides
insights into assets that you couldn\'t scan previously. For more
information about analytics issues, see [Cortex XDR Analytics Alert
Reference](https://docs-cortex.paloaltonetworks.com/r/Cortex-XDR/Cortex-XDR-Analytics-Alert-Reference-by-Alert-name).

When an issue is triggered, the data collector can run for up to two
weeks gathering EDR data from unmanaged hosts. You can track and manage
the collector directly from Cortex XSIAM, and investigate the EDR data
by running a query from the Query Center.

> **Prerequisite**

- > Configure and register a Broker VM.

- > Except for Vanilla Windows 7, Cortex XSIAM supports activating
  > Pathfinder on Windows operating systems with PowerShell version 3
  > and later. Verify these requirements wherever you want to activate
  > Pathfinder.

- > The Pathfinder configuration must contain at least one IP address
  > range to run. Make sure that your internal IP address ranges are
  > defined on your network. To avoid a collision, IP address ranges can
  > only be associated with one Pathfinder applet. For more information,
  > see [Configure Cortex XSIAM network
  > parameters](#UUID79661fb4f0136c198d25dbc9c7d9642e).

- > When using Kerberos as the authentication method for the Pathfinder
  > credentials, confirm that you have a reverse DNS zone and reverse
  > DNS records on your DNS server. The Broker VM has access to domain
  > controllers over port 88 and is able to acquire the authentication
  > ticket. It is recommended to use Kerberos for better security.

- > Verify connectivity between all your networks.

- > The Broker VM requires a Service Account (SA) that has administrator
  > privileges on all Windows workstations and servers in your
  > environment. Cortex XSIAM recommends that you limit the number of
  > users granted access to the SA account as it poses a credential
  > compromise security threat.

**How to activate Pathfinder**

1.  Select Settings \> Configurations \> Data Broker \> Broker VMs.

2.  Do one of the following:

    - On the **Brokers** tab, find the Broker VM, and in the **APPS**
      column, left-click Add \> Pathfinder.

    - On the **Clusters** tab, find the Broker VM, and in the **APPS**
      column, left-click Add \> Pathfinder.

- > **Note**

  > Pathfinder isn\'t supported when configuring Broker VMs in high
  > availability (HA) clusters.

3.  Do one of the following to define the Pathfinder credentials:

    - Define the domain access credentials. Make sure to enter the user
      name and password using the Service Account with Local Admin
      privileges on the remote endpoint.

    - (Broker VM version 9.0 and later) Define Pathfinder to access
      target hosts using credentials stored in your CyberArk vault.
      Credentials are not stored on the Broker VM; Pathfinder queries
      CyberArk each time according to the defined parameters.

4.  Click **Test** to run a test on the credentials and Pathfinder
    permissions. Testing may take a few minutes to complete but ensures
    that Pathfinder can deploy a data collector.

5.  Click **Next**, and define the data collector settings.

- By default the proxy settings are disabled, and data collected is sent
  directly to the cloud. For **Agent Proxy Settings**, collected data is
  routed using the settings provided in the Local Agent Settings applet,
  which must be enabled for these settings to work.

6.  Click **Next**, and select the IP address ranges to scan from your
    defined network configurations.

- By default, every IP address range will use the Pathfinder credentials
  and settings you defined in the **Credentials** section and is labeled
  as an **Applet Configuration**.

  If you want to configure other credentials for a specific range,
  override the settings in the right pane. IP address ranges you edit
  are labeled as  **Custom Configuration**. Make sure to test the
  credentials for this specific range.

7.  Activate Pathfinder. After the activation is complete, Pathfinder is
    displayed in the **APPS** column with a green dot indicating a
    successful connection.

- Hovering over the Pathfinder connection shows details such as the
  connectivity status, handled and failed tasks, and the resources the
  applet is using.

###### Activate Registry Scanner

The Broker VM provides a Registry Scanner applet that scans and secures
your container image registries. It supports Docker V2 or JFrog
self-hosted registries located on-premises or in private cloud networks.

> **Note**

- > FedRAMP is currently not supported for this applet.

- > You cannot activate the Registry Scanner directly on a new or
  > existing Broker VM. You can only activate or deactivate existing
  > Registry Scanner applets. To activate or deactivate existing
  > applets, see **Step 4** under Verify Registry Scanner connection
  > section.

####### Verify Registry Scanner connection

After the registry scanner is initialized, perform the following steps
to verify that the **Registry Scanner** applet is connected to the
**Broker VM**:

> **Prerequisite:**

- > To initialize registry scanning on your Broker VM, you must first
  > add the necessary data connectors. For details, see [Connect Docker
  > V2-compliant container
  > registry](#UUID6eaac2b964fdffd94a949aaaa5e994fe) and [Connect JFrog
  > container registry](#UUID681088abe7ed7ca3aad6629a03a1098a)

- > When sizing your Broker VM, consider the following recommendations:

  - > **Disk Size:** Calculate the required disk space by multiplying
    > the average container image size in your environment by 10. This
    > factor accounts for simultaneous operations with a buffer.

  <!-- -->

  - > For example, If your average image size is 500 MB, allocate at
    > least 5 GB of disk space (500 MB \* 10 = 5000 MB = 5 GB).

  <!-- -->

  - > **CPU:** Allocate a minimum of 8 CPU cores.

  - > **Memory:** Allocate a minimum of 16 GB of RAM.

1.  Go to Settings \> Configurations \> Data Broker \> Broker VMs.

2.  On either the **Brokers** or **Clusters tab**, find the Broker VM.

3.  In the **APPS** column for the **Broker VM**, verify that the
    **Registry Scanner** app appears.

4.  Select the **Registry Scanner** app to open a window displaying the
    following information:

- ![](media/rId4729.png){width="5.833333333333333in"
  height="0.9552077865266841in"}

  - **Connection**: Shows the app\'s current connection status. You can
    also **Deactivate** the app.

  <!-- -->

  - To reactivate the **Registry Scanner** app, do one of the following:

    - On the **Brokers** tab, locate the Broker VM, select **+Add** in
      the **APPS** column, and then choose **Registry Scanner**.

    - On the **Clusters** tab, locate the Broker VM, select **+Add** in
      the **APPS** column, and then choose **Registry Scanner**.

    If the **Registry Scanner** app is not listed in the drop-down menu
    when you click **+Add**, it means that the registry scanning was not
    configured for that **Broker VM**. You must first add the data
    connectors. For details, see [Connect Docker V2-compliant container
    registry](#UUID6eaac2b964fdffd94a949aaaa5e994fe) and [Connect JFrog
    container registry](#UUID681088abe7ed7ca3aad6629a03a1098a).

  <!-- -->

  - **Resources**: Shows the percentage of **CPU**, **Memory**, and
    **Disk** resources used by the app.

5.  To manage the Registry Scanner applet, see [Manage a Docker V2
    connector](#UUIDbe1d787057c9d5a295dc0d7ce4a81921) and [Manage a
    JFrog connector](#UUID8435241904f6d82477f0dc1e586e166a).

###### Activate Syslog Collector

To receive Syslog data from an external source, you must first set up
the Syslog Collector applet on a Broker VM within your network. The
Syslog Collector supports a log ingestion rate of 90,000 logs per second
(lps) with the recommended Broker VM setup.

The Syslog collector supports TCP/Secure TCP/UDP. The RFC 6587 standard,
which specifies the transmission of syslog messages over TCP, is
supported by the Syslog collector. When syslog messages are transmitted
over TCP, there are two options:

- Octet Framing

- Non-Transparent-Framing

<!-- -->

- This is the most commonly used option. The Syslog collector supports
  the newline character `\n` (Hex 0x0A) as the end-of-line delimiter for
  syslog messages.

To increase the log ingestion rate, you can add additional CPUs to the
Broker VM. The Syslog Collector listens for logs on specific ports and
from any or specific IP addresses. A Syslog Collector configuration
supports up to 100 ports.

> **Prerequisite**
>
> [Set up and configure Broker
> VM](#UUIDa2b1b832d74850d81f427e175514c501)

Perform the following procedures in the order listed below.

####### Task 1. Add a Syslog Collector

1.  Select Settings \> Configurations \> Data Broker \> Broker VMs.

2.  Do one of the following:

    - On the **Brokers** tab, find the Broker VM, and in the **APPS**
      column, left-click Add \> Syslog Collector.

    - On the **Clusters** tab, find the Broker VM, and in the **APPS**
      column, left-click Add \> Syslog Collector.

####### Task 2. Configure the Syslog Collector

Cortex XSIAM supports multiple sources over a single port on a single
Syslog Collector. The following options are available:

- Edit the **Optional Settings** of the default **PORT/PROTOCOL**:
  **514/UDP**. See **Task 3**.

<!-- -->

- > **Note**

  > Once configured, you cannot change the **Port/PROTOCOL**. If you
  > don't want to use a data source, ensure to remove the data source
  > from the list as explained in **Task 5**.

<!-- -->

- Add a new Syslog Collector data source. See **Task 4**.

####### Task 3. Edit the default 514/UDP Syslog Collector data source

1.  Right-click the **514/UDP** PORT/PROTOCOL, and select **Edit**.

2.  Configure these **Optional Settings**:

+-----------------------------------+-----------------------------------+
| Field                             | Description                       |
+===================================+===================================+
| Format                            | Select the Syslog format you want |
|                                   | to send to the UDP 514 protocol   |
|                                   | and port on the Syslog Collector: |
|                                   | **Auto-Detect** (default),        |
|                                   | **CEF**, **LEEF**, **CISCO**,     |
|                                   | **CORELIGHT**, or **RAW**.        |
|                                   |                                   |
|                                   | > **Note**                        |
|                                   |                                   |
|                                   | - > The **Vendor** and            |
|                                   |   > **Product** defaults to       |
|                                   |   > **Auto-Detect** when the      |
|                                   |   > **Log Format** is set to      |
|                                   |   > **CEF** or **LEEF**.          |
|                                   |                                   |
|                                   | - > For a **Log Format** set to   |
|                                   |   > **CEF** or **LEEF**, Cortex   |
|                                   |   > XSIAM reads events row by row |
|                                   |   > to look for the **Vendor**    |
|                                   |   > and **Product** configured in |
|                                   |   > the logs. When the values are |
|                                   |   > populated in the event log    |
|                                   |   > row, Cortex XSIAM uses these  |
|                                   |   > values even if you specified  |
|                                   |   > a value in the **Vendor** and |
|                                   |   > **Product** fields in the     |
|                                   |   > Syslog Collector settings.    |
|                                   |   > Yet, when the values are      |
|                                   |   > blank in the event log row,   |
|                                   |   > Cortex XSIAM uses the         |
|                                   |   > **Vendor** and **Product**    |
|                                   |   > that you specified in the     |
|                                   |   > Syslog Collector settings. If |
|                                   |   > you did not specify a         |
|                                   |   > **Vendor** or **Product** in  |
|                                   |   > the Syslog Collector settings |
|                                   |   > and the values are blank in   |
|                                   |   > the event log row, the values |
|                                   |   > for both fields are set to    |
|                                   |   > **unknown**.                  |
+-----------------------------------+-----------------------------------+
| Vendor and Product                | Specify a particular vendor and   |
|                                   | product for the Syslog format     |
|                                   | defined or leave the default      |
|                                   | **Auto-Detect** setting.          |
+-----------------------------------+-----------------------------------+
| Source Network                    | Specify the IP address or         |
|                                   | Classless Inter-Domain Routing    |
|                                   | (CIDR). If you leave this blank,  |
|                                   | Cortex XSIAM will allow receipt   |
|                                   | of logs from any source IP        |
|                                   | address or CIDR that transmits    |
|                                   | over the specified protocol and   |
|                                   | port. When you specify            |
|                                   | overlapping addresses in the      |
|                                   | **Source Network** field in       |
|                                   | multiple rows, such as 10.0.0.10  |
|                                   | in the first row and 10.0.0.0/24  |
|                                   | in the second row, the order of   |
|                                   | the addresses matter. In this     |
|                                   | example, the IP address 10.0.0.10 |
|                                   | is only captured from the first   |
|                                   | row definition. For more          |
|                                   | information on prioritizing the   |
|                                   | order of the syslog formats, see  |
|                                   | **Task 5**.                       |
+-----------------------------------+-----------------------------------+

- After each configuration, select
  ![](media/rId2150.png){width="0.14583333333333334in"
  height="0.20833333333333334in"} to save the changes and then **Done**
  to update the Syslog Collector with your settings.

####### Task 4. Add a new Syslog Collector data source

1.  Select **Add New**.

2.  Configure these mandatory **General settings**:

- Protocol
  Choose a protocol over which the Syslog will be sent: **UDP**,
  **TCP**, or **Secure TCP**.

  When configuring the **Protocol** as **Secure TCP**, these additional
  **General Settings** are available:

  - **Server Certificate**: Browse to your server certificate to
    configure server authentication.

  - **Private Key**: Browse to your private key for the server
    certificate.

  - **Optional CA Certificate**: (*Optional*) Browse to your CA
    certificate for mutual authentication.

  <!-- -->

  - The log forwarder (for example, a firewall) authenticates the Broker
    VM by default. The Broker VM does not authenticate the log forwarder
    by default, but you can use this option to set set up such
    authentication. If you use this option, ensure that you have a
    client certificate on the log forwarding side that matches the CA
    certificate on the Broker VM side.

  <!-- -->

  - **Minimal TLS Version**: Select either **1.0** or **1.2** (default)
    as the minimum TLS version allowed.

  > **Note**

  - > The server certificate and private key pair is expected in a PEM
    > format.

  - > Cortex XSIAM will notify you when your certificates are about to
    > expire.

  Port
  Choose a port on which the Syslog Collector will listen for logs. A
  Syslog Collector configuration supports up to 100 ports.

  > **Note**

  > Because some port numbers are reserved by Cortex XSIAM , you must
  > choose a port number that is not:

  - > In the range of 0-1024 (except for 514)

  - > In the range of 63000-65000

  - > Values of 4369, 5671, 5672, 5986, 6379, 8000, 8888, 9100, 15672,
    > or 28672

3.  Configure these **Optional Settings**:

  -----------------------------------------------------------------------
  Field                               Description
  ----------------------------------- -----------------------------------
  Format                              Select the Syslog format you want
                                      to send to the UDP/514 protocol and
                                      port on the Syslog Collector:
                                      **Auto-Detect** (default), **CEF**,
                                      **LEEF**, **CISCO**, **CORELIGHT**,
                                      or **RAW**.

  Vendor and Product                  Enter a particular vendor and
                                      product for the Syslog format
                                      defined or leave the default
                                      **Auto-Detect** setting.

  Source Network                      Specify the IP address or Classless
                                      Inter-Domain Routing (CIDR). If you
                                      leave this blank, Cortex XSIAM will
                                      allow receipt of logs from any
                                      source IP address or CIDR that
                                      transmits over the specified
                                      protocol and port. When you specify
                                      overlapping addresses in the
                                      **Source Network** field in
                                      multiple rows, such as 10.0.0.10 in
                                      the first row and 10.0.0.0/24 in
                                      the second row, the order of the
                                      addresses matter. In this example,
                                      the IP address 10.0.0.10 is only
                                      captured from the first row
                                      definition. For more information on
                                      prioritizing the order of the
                                      syslog formats, see **Task 5**.
  -----------------------------------------------------------------------

- After each configuration, select
  ![](media/rId2150.png){width="0.14583333333333334in"
  height="0.20833333333333334in"} to save the changes and then **Done**
  to update the Syslog Collector with your settings.

####### Task 5. Make additional changes to the Syslog Collector data sources configured

- To remove a Syslog Collector data source, right-click the row after
  the **Port/Protocol** entry, and select **Remove**.

- To prioritize the order of the Syslog formats listed for the protocols
  and ports configured, drag and drop the rows to the order you require.

####### Task 6. Save the Syslog Collector settings

Click **Save**. After a successful activation, the **APPS** field
displays **Syslog** with a green dot indicating a successful connection.

####### Task 7. (optional) View metrics about the Syslog Collector

To view metrics about the Syslog Collector, left-click the **Syslog**
connection in the **APPS** field for your Broker VM. Cortex XSIAM
displays the following information:

  -----------------------------------------------------------------------
  Metric                              Description
  ----------------------------------- -----------------------------------
  Connectivity Status                 Whether the applet is connected to
                                      Cortex XSIAM.

  Logs Received and Logs Sent         Number of logs received and sent by
                                      the applet per second over the last
                                      24 hours. If the number of incoming
                                      logs received is larger than the
                                      number of logs sent, it could
                                      indicate a connectivity issue.

  Resources                           Displays the amount of **CPU**,
                                      **Memory**, and **Disk** space the
                                      applet is using.
  -----------------------------------------------------------------------

####### Step 8. Manage the Syslog Collector

After the Syslog Collector has been activated, you can make additional
changes to your configuration if needed. To modify a configuration,
left-click the **Syslog** connection in the **APPS** column to display
the Syslog Collector settings, and select:

- **Configure** to redefine the Syslog configurations.

- **Deactivate** to disable the Syslog Collector.

###### Activate Transporter

The Transporter over Broker VM enables secure communication between your
self-hosted Version Control Systems (VCS) and Cortex XSIAM. This
solution addresses the need for secure code scanning without exposing
your internal network to the cloud.

> **Prerequisites**

- > **Permissions**: To configure and manage Transporter applet
  > settings, you must have permissions to manage **Broker Service**
  > configurations (such as an **Instance Administrator**)

- > [/document/preview/859888#UUID-ed18ada1-4744-8347-ae52-62afb6818c9c](/document/preview/859888#UUID-ed18ada1-4744-8347-ae52-62afb6818c9c)

- > Confirm that your Broker is v 28 or above

- > Whitelist IP addresses to enable access to Cortex XSIAM resources.
  > The IP addresses for the Transporter are in the Broker VM Resources
  > section of the [Enable access to required PANW
  > resources](#UUID24cb15454f44c259b484435238bb6a33) document

- > Open port `4052`, which is required for the Transporter\'s IP
  > address communication

- > Open Port `443` (outbound), which is required for the Broker VM to
  > pull data from the your version control system (VCS)

####### License

To gain access to the Transporter applet, your Cortex XSIAM license must
be **C1** (Posture management) or above.

> **Warning**
>
> The Transporter applet is not supported for FedRAMP customers.

####### How to activate the Transporter applet

1.  Select Settings \> Configurations \> Broker VMs (under Data Broker.

2.  Select the Brokers tab \> locate your Broker VM \> hover and click +
    Add under the Apps column \> AppSec Transporter.

3.  Configure the Transporter connection in the provided fields:

    - **Transporter Name** (required). Requires a unique name as you can
      integrate multiple applets for different integrations

    - **Provider Self Signed CA Certificate Path**: Specify the file
      path for a custom Certificate Authority (CA) certificate used by
      the Transporter to securely communicate with services

4.  Click Save.

5.  Verify connectivity: Navigate to the **Apps** column and verify that
    your **AppSec Transporter** applet has been added and displays a
    connected status.

6.  **Next step**: After activating the Transporter, proceed to
    configure the Transporter applet on your self-managed VCS data
    source instance.

- For more information, refer to [Setup a Transporter on your
  VCS](#UUID556ddcc5b44b5994ed4bab2b840d8a28).

####### Manage Transporter applets

To manage Transporter applet configurations, disable connections, or
deactivate an applet, navigate to the Broker VMs page. From there,
select your **Appsec Transporter** under the App column.

- **Edit applet configurations**: Select the Appsec Transporter under
  the App column \> Configure. You are redirected to the Transporter
  applet settings to manage its configurations

- **Disable applet connection for a single integration**:

  1.  Select the Appsec Transporter under the App column \> Configure.

  2.  On the Transporter applet configurations page, click on the
      specific Transporter applet \> Disable.

  - This disables the specific integration, but it can be re-enabled.

- **Deactivate an applet** (all connections): Select the Appsec
  Transporter under the App column \> Deactivate \> Confirm when
  prompted

<!-- -->

- All existing connections are deleted but their configurations are
  saved in the database. When adding a new connection, you\'ll be
  prompted if you want to reuse previous configurations.

###### Activate Windows Event Collector

After you have configured and registered your Broker VM, activate your
Windows Event Collector application.

The Windows Event Collector (WEC) runs on the Broker VM collecting event
logs from Windows Servers, including Domain Controllers (DCs). The
Windows Event Collector can be deployed in multiple setups, and can be
connected directly to multiple event generators (DCs or Windows Servers)
or routed using one or more Windows Event Collectors. Behind each
Windows event collector there may be multiple generating sources.

To enable the collection of the event logs, you need to configure and
establish trust between the Windows Event Forwarding (WEF) collectors
and the WEC. Establishing trust between the WEFs and the WEC is achieved
by mutual authentication over TLS using server and client certificates.
The WEF, a WinRM plugin, runs under the Network Service account.
Therefore, you need to provide the WEFs with the relevant certificates
and grant the account access permissions to the private key used for
client authentication, for example, authenticate with WEC.

> **Note**
>
> You can also activate the Windows Event Collector on Windows Core. For
> more information, see [Activate Windows Event Collector on Windows
> Core](#UUIDd314401aabf2f328d6c8623636b31d1a).
>
> **Prerequisite**

- > [Set up and configure Broker
  > VM](#UUIDa2b1b832d74850d81f427e175514c501)

- > Broker VM version 8.0 and later

- > You have knowledge of Windows Active Directory and Domain
  > Controllers.

- > You must configure different settings related to the FQDN where the
  > instructions differ depending on whether you are configuring a
  > standalone Broker VM or High Availability (HA) cluster.

<!-- -->

- Standalone broker
  > A FQDN must be configured for the standalone broker as configured in
  > your local DNS server. Therefore, the Broker VM is registered in the
  > DNS, its FQDN is resolvable from the events forwarder (Windows
  > server), and the Broker VM FQDN is configured. For more information,
  > see [Configure High Availability
  > Cluster](#UUIDcdafbc5b0f072deba4bdea1f522d867d).

  HA cluster
  > A FQDN must be configured in the cluster settings as configured in
  > your local DNS server, which points to a Load Balancer. For more
  > information, see [Configure High Availability
  > Cluster](#UUIDcdafbc5b0f072deba4bdea1f522d867d).

<!-- -->

- > Windows Server 2012 r2 or later.

After ingestion, Cortex XSIAM normalizes and saves the Windows event
logs in the dataset `xdr_data`. The normalized logs are also saved in a
unified format in `microsoft_windows_raw`. This enables you to search
the data using Cortex Query Language (XQL) queries, build correlation
rules, and generate dashboards based on the data.

Perform the following procedures in the order listed below.

####### Task 1. Add, configure, and activate a Windows Event Collector

1.  Select Settings \> Configurations \> Data Broker \> Broker VMs.

2.  Do one of the following:

    - On the **Brokers** tab, find the Broker VM, and in the **APPS**
      column, left-click Add \> Windows Event Collector.

    - On the **Clusters** tab, find the Broker VM, and in the **APPS**
      column, left-click Add \> Windows Event Collector.

3.  In the **Activate Windows Event Collector** window, define the
    **Collected Events** to configure the events collected by the
    applet. This lists event sources from which you want to collect
    events.

+-----------------------------------+-----------------------------------------------------------------------------------------------------+
| Field                             | Description                                                                                         |
+===================================+=====================================================================================================+
| Source                            | Select from the pre-populated list with the most common event sources on Windows Servers. The event |
|                                   | source is the name of the software that logs the events.                                            |
|                                   |                                                                                                     |
|                                   | A source provider can only appear once in your list. When selecting event sources, depending on the |
|                                   | type event you want to forward, ensure the event source is enabled, for example [auditing security  |
|                                   | events](https://docs.microsoft.com/en-us/defender-for-identity/configure-windows-event-collection). |
|                                   | If the source is not enabled, the source configuration in the given row will fail.                  |
+-----------------------------------+-----------------------------------------------------------------------------------------------------+
| Min. Event Level                  | Minimum severity level of events that are collected.                                                |
+-----------------------------------+-----------------------------------------------------------------------------------------------------+
| Event IDs Group                   | Whether to **Include**, **Exclude**, or collect **All** event ID groups.                            |
+-----------------------------------+-----------------------------------------------------------------------------------------------------+
| Minimal TLS Version               | Select either **1.0** or **1.2** (default) as the minimum TLS version allowed. Ensure that you      |
|                                   | verify that all Windows event forwarders are supporting the minimal defined TLS version.            |
+-----------------------------------+-----------------------------------------------------------------------------------------------------+

- To forward all the Windows Event Collector events to the Broker VM,
  define as follows:

  - **Source**: `ForwardedEvents`

  - **Min. Event Level**: `Verbose`

  - **Event IDs Group**: `All`

  > **Note**

  > By default, Cortex XSIAM collects Palo Alto Networks predefined
  > **Security** events that are used by the Cortex XSIAM detectors.
  > Removing the Security collector interferes with the Cortex XSIAM
  > detection functionality. **Restore to Default** to reinstate the
  > Security event collection.

4.  Click **Activate**. After a successful activation, the **APPS**
    field displays **WEC** with a green dot indicating a successful
    connection.

####### Task 2. Configure the Windows Event Collector settings

1.  In the **APPS** column, left-click the **WEC** connection to display
    the Windows Event Collector settings, and select **Configure**.

2.  In the **Windows Event Forwarder Configuration** window, perform the
    following tasks:

    a.  In the **Subscription Manager URL** field, click
        ![](media/rId4759.png){width="0.14583333333333334in"
        height="0.20833333333333334in"} (copy) . This will be used when
        you configure the subscription manager in the GPO (Global Policy
        Object) on your domain controller.

    b.  Enter a password in the
        **Define Client Certificate Export Password** field to be used
        to secure the downloaded WEF certificate that establishes the
        connection between your DC/WEF and the WEC. You will need this
        password when the certificate is imported to the events
        forwarder.

    c.  Download the WEF certificate in a PFX format to your local
        machine.

    - To view your Windows Event Forwarding configuration details at any
      time, select your Broker VM, right-click and navigate to Windows
      Event Collector \> Configure.

- Cortex XSIAM monitors the certificate and triggers a Certificate
  Expiration notification 30 days prior to the expiration date. The
  notification is sent daily specifying the number of days left on the
  certificate, or if the certificate has already expired.

####### Task 3. Install your WEF Certificate on the WEF to establish connection

> **Note**
>
> You must install the WEF certificate on every Windows Server, whether
> DC or not, for the WEFs that are supposed to forward logs to the
> Windows Event Collector applet on the Broker VM.

1.  Locate the PFX file you downloaded from the Cortex XSIAM console and
    double-click to open the **Certificate Import Wizard**.

2.  In the **Certificate Import Wizard**:

    a.  Select **Local Machine**, and then click **Next**.

    b.  Verify the **File name** field displays the PFX certificate file
        you downloaded and click **Next**.

    c.  In the **Passwords** field, specify the [Client Certificate
        Export Password](#id1e9264170ab848c380af70b06994d1c5) you
        defined in the Cortex XSIAM console followed by **Next**.

    d.  Select
        **Automatically select the certificate store based on the type of certificate, and then click Next**
        and **Finish**.

3.  From a command prompt, run `certlm.msc`.

4.  In the file explorer, navigate to **Certificates** and verify the
    following for each of the folders:

    - In the Personal \> Certificates folder, ensure the certificate
      `forwarder.wec.paloaltonetworks.com` is displayed.

    - In the Trusted Root Certification Authorities \> Certificates
      folder, ensure the CA `ca.wec.paloaltonetworks.com` is displayed.

5.  Navigate to Certificates Personal Certificates.

6.  Right-click the certificate and navigate to All tasks \> Manage
    Private Keys.

7.  In the **Permissions** window, select **Add** and in the
    **Enter the object name** section, enter `NETWORK SERVICE`, and then
    click **Check Names** to verify the object name. The object name is
    displayed with an underline when valid. and then click **OK**.

- ![](media/rId4763.png){width="5.833333333333333in"
  height="3.055207786526684in"}

8.  Click **OK**, verify the **Group or user names** that are displayed,
    and then click **Apply Permissions for private keys**.

- ![](media/rId4766.png){width="5.833333333333333in"
  height="2.9166666666666665in"}

####### Task 4. Add the Network Service account to the domain controller Event Log Readers group.

> **Note**
>
> You must install the WEF certificate on every Windows Server, whether
> DC or not, for the WEFs that are supposed to forward logs to the
> Windows Event Collector applet on the Broker VM.

1.  To enable events forwarders to forward events, the Network Service
    account must be a member of the Active Directory Event Log Readers
    group. In PowerShell, execute the following command on the domain
    controller that is acting as the event forwarder:

- PS C:\> net localgroup "Event Log Readers" "NT Authority\Network Service" /add

  Make sure you see `The command completed successfully` message.

2.  Grant access to view the security event logs.

    a.  Run `wevtutil gl security` and take note of your `channelAccess`
        value.

    - `PS C:\Users\Administrator> wevtutil gl security
          name: security
          enabled: true
          type: Admin
          owningPublisher:
          isolation: Custom
          channelAccess: O:BAG:SYD:(A;;0xf0005;;;SY)(A;;0x5;;;BA)(A;;0x1;;;S-1-5-32-573)
          logging:
            logFileName: %SystemRoot%\System32\Winevt\Logs\security.evtx
            retention: false
            autoBackup: false
            maxSize: 134217728
          publishing:
            fileMax: 1

      Take note of value:
      `channelAccess: O:BAG:SYD:(A;;0xf0005;;;SY)(A;;0x5;;;BA)(A;;0x1;;;S-1-5-32-573)`

    b.  Run
        `wevtutil sl security "/ca:<channelAccess value>(A;;0x1;;;S-1-5-20)"`

    - PS C:\Users\Administrator> wevtutil sl security "/ca:O:BAG:SYD:(A;;0xf0005;;;SY)(A;;0x5;;;BA)(A;;0x1;;;S-1-5-32-573)(A;;0x1;;;S-1-5-20)"

- Make sure you grant access on each of your domain controller hosts.

####### Task 5. Create a WEF Group Policy that applies to every Windows server you want to configure as a WEF

1.  In a command prompt, open `gpmc.msc`.

2.  In the **Group Policy Management** window, navigate to Domains \>
    your domain name \> Group Policy Object, right-click and select
    **New**.

3.  In the **New GPO** window, enter your group policy **Name:** as
    **Windows Event Forwarding**, and click **OK**.

4.  Navigate to Domains \> your domain name \> Group Policy Objects \>
    Windows Event Forwarding, right-click and select **Edit**.

- ![](media/rId4773.png){width="5.833333333333333in" height="2.45in"}

5.  In the **Group Policy Management Editor**:

    - Set the Windows Remote Management Service for automatic startup.

      1.  Select Computer Configuration \> Policies \> Windows Settings
          \> Security Settings \> System Services, and in the view panel
          locate and double-click
          **Windows Remote Management (WS-Management)**.

      2.  Mark the **Define this policy setting** checkbox, select
          **Automatic**, and then click **Apply** and **OK**.

    - At a minimum for your WEC configuration, you must enable logging
      of the same events that you have configured to be collected in
      your WEC configuration on your domain controller. Otherwise, you
      will not be able to view these events as the WEC only controls
      querying not logging. For example, if you have configured
      authentication events to be collected by your WEC using an
      authentication protocol, such as Kerberos, you should ensure all
      relevant audit events for authentication are configured on your
      domain controller. In addition, you should ensure that all
      relevant audit events that you want collected, such as the success
      and failure of account logins for Windows Event ID 4625, are
      properly configured, particularly for those that you want Cortex
      XSIAM to apply grouping and analytics inspection.

    <!-- -->

    - > **Note**

      > This step overrides any local policy settings.

      Here is an example of how to configure the WEC to collect
      authentication events using Kerberos as the authentication
      protocol to enable the collection of Broker VM supported Kerberos
      events, Kerberos pre-authentication, authentication, request, and
      renewal tickets.

      1.  Select Computer Configuration \> Policies \> Windows Settings
          \> Security Settings \> Advanced Audit Policy Configuration \>
          Audit Policies \> Account Logon.

      2.  In the view pane, right-click
          **Audit Kerberos Authentication Service** and select
          **Properties**. In the
          **Audit Kerberos Authentication Service** window, mark
          **Configure the following audit events:**, and click
          **Success** and **Failure** followed by **Apply** and **OK**.

      - Repeat for **Audit Kerberos Service Ticket Operations**.

6.  Configure the subscription manager.

- Navigate to Computer Configuration \> Policies \> Administrative
  Templates: Policy definitions \> Windows Components \> Event
  Forwarding, right-click **Configure target Subscription Manager** and
  select **Edit**.

+-------------------------------------------------------+---------------------------------------------+
| - ![](media/rId4777.png){width="2.9166666666666665in" | - In the                                    |
|   height="1.581424978127734in"}                       |   **Configure target Subscription Manager** |
|                                                       |   window, perform the following:            |
+-------------------------------------------------------+---------------------------------------------+

a.  Mark **Configure target Subscription Manager** as **Enabled**.

b.  In the **Options** section, select **Show** and in the
    **Show Contents** window, paste the [Subscription Manage
    URL](#X9844aa9104306c781f63aa6cc78c14c0cb55e97) you copied from the
    Cortex XSIAM console, and then click **OK**.

c.  Click **Apply** and **OK** to save your changes.

<!-- -->

7.  Add Network Service to Event Log Readers group.

- Select Computer Configuration \> Preferences \> Control Panel Settings
  \> Local Users and Groups, right-click and select New \> Local Group.

  ![](media/rId4780.png){width="5.736111111111111in"
  height="6.402777777777778in"}

  In the **New Local Group Properties** window:

  a.  In the **Group name** field, select
      **Event Log Readers (built-in)**.

  b.  In the **Members** section, click **Add** and enter in the
      **Name** filed `Network Service` followed by **OK**.

  - > **Note**

    > You must type out the name, do not select the name from the browse
    > button.

  c.  Click **Apply** and **OK** to save your changes, and close the
      **Group Policy Management Editor** window.

8.  Configure the Windows Firewall.

- > **Note**

  > If Windows Firewall is enabled on your event forwarders, you will
  > have to define an outbound rule to enable the WEF to reach port 5986
  > on the WEC.

  In the **Group Policy Management** window, select Computer
  Configuration \> Policies \> Windows Settings \> Security Settings \>
  Windows Firewall with Advanced Security \> Outbound Rules, right-click
  and select **New Rule**.

  In the **New Outbound Rule Wizard** define the following **Steps**:

  a.  **Rule Type**: Select **Port** followed by **Next**.

  b.  **Protocols and Ports**: Select **TCP** and in the
      **Specific Remote Ports** field enter `5986` followed by **Next**.

  c.  **Action**: Select **Allow the connection** followed by **Next**.

  d.  **Profile**: Select **Domain** and disable **Private** and
      **Public** followed by **Next**.

  e.  **Name**: Specify `Windows Event Forwarding`.

  f.  To save your changes, click **Finish**.

####### Task 6. Apply the WEF Group Policy

Link the policy to the OU or the group of Windows servers you would like
to configure as event forwarders. In the following flow, the domain
controllers are configured as an event forwarder.

1.  Select Group Policy Management \> \<your domain name\> \> Domain
    Controllers, right-click and select **Link an existing GPO\...**.

2.  In the **Select GPO** window, click **Windows Event Forwarding**
    followed by **OK**.

3.  In an administrative PowerShell console, execute the following
    commands:

    a.  PS C:\Users\Administrator> gpupdate /force

    - Verify that the
      `Computer Policy update has completed successfully. User Policy update has completed successfully.`
      confirmation message is displayed.

    b.  PS C:\Users\Administrator> Restart-Service WinRM

####### Task 7. Verify Windows Event Forwarding

1.  In an administrative PowerShell console, run the following command:

- PS C:\Users\Administrator> Get-WinEvent Microsoft-windows-WinRM/operational -MaxEvents 10

2.  Look for `WSMan operation EventDelivery completed successfully`
    confirmation messages. These indicate events forwarded successfully.

####### Task 8. Manage the Window Event Collector (Optional)

After the Windows Event Collector has been activated in the Cortex XSIAM
Management Console, left-click the **WEC** connection in the **APPS**
column to display the Windows Event Collector settings, and select:

- **Configure** to define the event configuration information.

- **Collection Configuration** to view or edit existing or add new
  events to collect.

- **Deactivate** to disable the Windows Event Collector.

####### Task 9. View Windows Event Collector metrics (Optional)

To view metrics about the Windows Event Collector, left-click the
**WEC** connection in the **APPS** field for your Broker VM, and you\'ll
see the following metrics:

- **Connectivity Status**: Whether the applet is connected to Cortex
  XSIAM.

- **Logs Received** and **Logs Sent**: Number of logs received and sent
  by the applet per second over the last 24 hours. If the number of
  incoming logs received is larger than the number of logs sent, it
  could indicate a connectivity issue.

- **Resources**: Displays the amount of **CPU**, **Memory**, and
  **Disk** space the applet is using.

####### Activate Windows Event Collector on Windows Core

After you have configured and registered your Broker VM, you can
activate your Windows Event Collector application on Windows Core OS
(WCOS). WCOS is a stripped-down, lightweight version of Windows that can
be adapted to run on a wide variety of devices with minimal work
compared to the previous way explained in [Activate Windows Event
Collector](#UUIDc6ef2ebad58d3dba56c7dddddee05aca).

The Windows Event Collector (WEC) runs on the Broker VM collecting event
logs from Windows Servers, including Domain Controllers (DCs). The
Windows Event Collector can be deployed in multiple setups, and can be
connected directly to multiple event generators (DCs or Windows Servers)
or routed using one or more Windows Event Collectors. Behind each
Windows event collector there may be multiple generating sources.

To enable the collection of the event logs, you are configuring and
establishing trust between the Windows Event Forwarding (WEF) collectors
and the WEC. Establishing trust between the WEFs and the WEC is achieved
by mutual authentication over TLS using server and client certificates.
The WEF, a WinRM plugin, runs under the Network Service account.
Therefore, you need to provide the WEFs with the relevant certificates
and grant the account access permissions to the private key used for
client authentication, for example, authenticate with WEC.

> **Prerequisite**

- > [Set up and configure Broker
  > VM](#UUIDa2b1b832d74850d81f427e175514c501)

- > Broker VM version 8.0 and later

- > You have knowledge of Windows Active Directory and Domain
  > Controllers.

- > You must configure different settings related to the FQDN where the
  > instructions differ depending on whether you are configuring a
  > standalone Broker VM or High Availability (HA) cluster.

<!-- -->

- Standalone broker
  > A FQDN must be configured for the standalone broker as configured in
  > your local DNS server. Therefore, the Broker VM is registered in the
  > DNS, its FQDN is resolvable from the events forwarder (Windows
  > server), and the Broker VM FQDN is configured. For more information,
  > see [Edit Broker VM
  > Configuration](#UUID2be2001b0995833c7552aafcffdbfac7).

  HA cluster
  > A FQDN must be configured in the cluster settings as configured in
  > your local DNS server, which points to a Load Balancer. For more
  > information, see [Configure High Availability
  > Cluster](#UUIDcdafbc5b0f072deba4bdea1f522d867d).

<!-- -->

- > Windows Server 2012 r2 or later.

After ingestion, Cortex XSIAM normalizes and saves the Windows event
logs in the dataset `xdr_data`. The normalized logs are also saved in a
unified format in `microsoft_windows_raw`. This enables you to search
the data using XQL queries, build correlation rules, and generate
dashboards based on the data.

Perform the following procedures in the order listed below.

######## Task 1. Add, configure, and activate a Windows Event Collector

1.  Select Settings \> Configurations \> Data Broker \> Broker VMs.

2.  Do one of the following:

    - On the **Brokers** tab, find the Broker VM, and in the **APPS**
      column, left-click Add \> Windows Event Collector.

    - On the **Clusters** tab, find the Broker VM, and in the **APPS**
      column, left-click Add \> Windows Event Collector.

3.  In the **Activate Windows Event Collector** window, define the
    **Collected Events** to configure the events collected by the
    applet. This lists event sources from which you want to collect
    events.

+-----------------------------------+-----------------------------------------------------------------------------------------------------+
| Field                             | Description                                                                                         |
+===================================+=====================================================================================================+
| Source                            | Select from the pre-populated list with the most common event sources on Windows Servers. The event |
|                                   | source is the name of the software that logs the events.                                            |
|                                   |                                                                                                     |
|                                   | A source provider can only appear once in your list. When selecting event sources, depending on the |
|                                   | type event you want to forward, ensure the event source is enabled, for example [auditing security  |
|                                   | events](https://docs.microsoft.com/en-us/defender-for-identity/configure-windows-event-collection). |
|                                   | If the source is not enabled, the source configuration in the given row will fail.                  |
+-----------------------------------+-----------------------------------------------------------------------------------------------------+
| Min. Event Level                  | Minimum severity level of events that are collected.                                                |
+-----------------------------------+-----------------------------------------------------------------------------------------------------+
| Event IDs Group                   | Whether to **Include**, **Exclude**, or collect **All** event ID groups.                            |
+-----------------------------------+-----------------------------------------------------------------------------------------------------+
| Minimal TLS Version               | Select either **1.0** or **1.2** (default) as the minimum TLS version allowed. Ensure that you      |
|                                   | verify that all Windows event forwarders are supporting the minimal defined TLS version.            |
+-----------------------------------+-----------------------------------------------------------------------------------------------------+

- To forward all the Windows Event Collector events to the Broker VM,
  define as follows:

  - **Source**: `ForwardedEvents`

  - **Min. Event Level**: `Verbose`

  - **Event IDs Group**: `All`

  > **Note**

  > By default, Cortex XSIAM collects Palo Alto Networks predefined
  > **Security** events that are used by the Cortex XSIAM detectors.
  > Removing the Security collector interferes with the Cortex XSIAM
  > detection functionality. **Restore to Default** to reinstate the
  > Security event collection.

4.  Click **Activate**. After a successful activation, the **APPS**
    field displays **WEC** with a green dot indicating a successful
    connection.

######## Task 2. Configure the Windows Event Collector settings

1.  In the **APPS** column, left-click the **WEC** connection to display
    the Windows Event Collector settings, and select **Configure**.

2.  In the **Windows Event Forwarder Configuration** window, perform the
    following tasks.:

    a.  In the **Subscription Manager URL** field, click
        ![](media/rId4759.png){width="0.14583333333333334in"
        height="0.20833333333333334in"} (copy) . This will be used when
        you configure the subscription manager in the GPO (Global Policy
        Object) on your domain controller.

    b.  Enter a password in the
        **Define Client Certificate Export Password** field to be used
        to secure the downloaded WEF certificate that establishes the
        connection between your DC/WEF and the WEC. You will need this
        password when the certificate is imported to the events
        forwarder.

    c.  Download the WEF certificate in a PFX format to your local
        machine.

    - To view your Windows Event Forwarding configuration details at any
      time, select your Broker VM, right-click and navigate to Windows
      Event Collector \> Configure.

- Cortex XSIAM monitors the certificate and triggers a Certificate
  Expiration notification 30 days prior to the expiration date. The
  notification is sent daily specifying the number of days left on the
  certificate, or if the certificate has already expired.

######## Task 3. Install your WEF Certificate on the WEF to establish connection

1.  Start PowerShell with elevated privileges.

    a.  Run PowerShell with the following command:

    - PowerShell

    b.  From inside a `PowerShell` command run the following command:

    - Start-Process -Verb RunAs PowerShell

2.  Copy the PFX file that you downloaded to the local Core machine in
    one of the following ways:

    - (Recommended) If you\'re able to RDP to your server, open Notepad,
      and select File \> Open to copy and paste files from your local
      machine directly to the server. If you have any local drives
      mapped through the RDP options, the local drives are also
      displayed. We recommend this method as it\'s the simplest.

    - If you have enabled `WinRM` for remote `PowerShell` execution, you
      can copy over PowerShell using this command:

    <!-- -->

    - $session = New-PSSession –ComputerName <computer name>

          Copy-Item –Path <path to PFX certificate file> –Destination '<temporary file path>' –ToSession $session

          $session = New-PSSession –ComputerName SERVER1

          Copy-Item –Path C:\Downloads\forwarder.wec.paloaltonetworks.com.pfx –Destination 'C:\temp\forwarder.wec.paloaltonetworks.com.pfx' –ToSession $session

      To enable `WinRM`, use this command:

          Execute "Start-Service winRM"

          Execute "WinRM quickconfig"

    <!-- -->

    - Use SSH on server core. This includes enabling SSH on server core
      and using `winscp` to drag and drop the PFX file.

    - Use SMB to open the file share `c$` on the `\\server1\c$` server.
      You can only use this option if you are an administrator and the
      firewall on your network isn\'t set to block file sharing.

    <!-- -->

    - You can also launch PowerShell and run the following command to
      tell the remote server to copy a file from your local computer
      using SMB:

          Copy-Item –Path <path to PFX certificate file> –Destination '\\<computer name>\c$\<path to PFX file>

          Copy-Item –Path C:\Downloads\forwarder.wec.paloaltonetworks.com.pfx –Destination '\\windows-core-server\c$\forwarder.wec.paloaltonetworks.com.pfx

3.  Import the PFX file from PowerShell.

- Use the following command to import the PFX file:

      certutil -f -importpfx '<path to PFX file from Destination>'

      certutil -f -importpfx '.\forwarder.wec.paloaltonetworks.com.pfx'

  You will need to enter the Client Certificate Export Password you
  defined in the Cortex XSIAM console.

  When the import is complete, the following message is displayed:

      CertUtil: -importPFX command completed successfully. 

4.  Verify that the certificates are in the correct locations.

    - Ensure the client certificate appears in \"My\" (Personal) store
      by running the following command:

    <!-- -->

    - certutil -store My

    <!-- -->

    - Ensure the CA appears in Trusted Root Certification Authorities by
      running the following command:

    <!-- -->

    - certutil -store root

5.  Manage the private key of the
    `forwarder.wec.paloaltonetworks.com.pfx` certificate.

- This entails applying permissions for the `NETWORK SERVICE` user.

  a.  Retrieve the Thumbprint of the
      `forwarder.wec.paloaltonetworks.com.pfx` certificate by running
      the following script:

  - $store = New-Object System.Security.Cryptography.X509Certificates.X509Store("My","LocalMachine")
        $store.Open("ReadWrite")
        echo $store.Certificates

    After the script runs, copy the relevant thumbprint.

  b.  Grant `NT AUTHORITY\NETWORK SERVICE` with read permissions by
      running the following script with the `$thumbprint` set to the
      value you copied in the previous step by replacing
      `<Thumbprint retrieved value>`.

  - $thumbprint = '<Thumbprint retrieved value>'
        $account = 'NT AUTHORITY\NETWORK SERVICE'
        #Open Certificate store and locate certificate based on provided thumbprint
        $store = New-Object System.Security.Cryptography.X509Certificates.X509Store("My","LocalMachine")
        $store.Open("ReadWrite")
        $cert = $store.Certificates | where {$_.Thumbprint -eq $thumbprint}
         
        #Create new CSP object based on existing certificate provider and key name
        #Note: Ensure this command is pasted to the same row and doesn’t break to multiple rows. 
        #Otherwise, the command will fail with errors.
        $csp = New-Object System.Security.Cryptography.CspParameters($cert.PrivateKey.CspKeyContainerInfo.ProviderType, $cert.PrivateKey.CspKeyContainerInfo.ProviderName,
        $cert.PrivateKey.CspKeyContainerInfo.KeyContainerName)
         
        # Set flags and key security based on existing cert
        $csp.Flags = "UseExistingKey","UseMachineKeyStore"
        $csp.CryptoKeySecurity = $cert.PrivateKey.CspKeyContainerInfo.CryptoKeySecurity
        $csp.KeyNumber = $cert.PrivateKey.CspKeyContainerInfo.KeyNumber
         
        # Create new access rule - could use parameters for permissions, but I only needed GenericRead
        $access = New-Object System.Security.AccessControl.CryptoKeyAccessRule($account,"GenericRead","Allow")
        # Add access rule to CSP object

        $csp.CryptoKeySecurity.AddAccessRule($access)
         
        #Create new CryptoServiceProvider object which updates Key with CSP information created/modified above
        $rsa2 = New-Object System.Security.Cryptography.RSACryptoServiceProvider($csp)
         
        #Close certificate store
        $store.Close()
        echo $csp.CryptoKeySecurity

  c.  After the script runs, validate the permissions are now set
      correctly.

  - ![](media/rId4795.png){width="5.833333333333333in"
    height="0.5766054243219597in"}

######## Task 4. Add the Network Service account to the domain controller Event Log Readers group.

> **Note**
>
> You must install the WEF certificate on every Windows Server, whether
> DC or not, for the WEFs that are supposed to forward logs to the
> Windows Event Collector applet on the Broker VM.

1.  To enable events forwarders to forward events, the Network Service
    account must be a member of the Active Directory Event Log Readers
    group. In PowerShell, execute the following command on the domain
    controller that is acting as the event forwarder:

- PS C:\> net localgroup "Event Log Readers" "NT Authority\Network Service" /add

  Make sure you see `The command completed successfully` message.

2.  Grant access to view the security event logs.

    a.  Run `wevtutil gl security` and take note of your `channelAccess`
        value.

    - `PS C:\Users\Administrator> wevtutil gl security
          name: security
          enabled: true
          type: Admin
          owningPublisher:
          isolation: Custom
          channelAccess: O:BAG:SYD:(A;;0xf0005;;;SY)(A;;0x5;;;BA)(A;;0x1;;;S-1-5-32-573)
          logging:
            logFileName: %SystemRoot%\System32\Winevt\Logs\security.evtx
            retention: false
            autoBackup: false
            maxSize: 134217728
          publishing:
            fileMax: 1

      Take note of value:
      `channelAccess: O:BAG:SYD:(A;;0xf0005;;;SY)(A;;0x5;;;BA)(A;;0x1;;;S-1-5-32-573)`

    b.  Run
        `wevtutil sl security "/ca:<channelAccess value>(A;;0x1;;;S-1-5-20)"`

    - PS C:\Users\Administrator> wevtutil sl security "/ca:O:BAG:SYD:(A;;0xf0005;;;SY)(A;;0x5;;;BA)(A;;0x1;;;S-1-5-32-573)(A;;0x1;;;S-1-5-20)"

- Make sure you grant access on each of your domain controller hosts.

######## Task 5. Create a WEF Group Policy that applies to every Windows server you want to configure as a WEF

As a Group Policy Management Console is not available on Core servers,
it's not possible to fully edit a Group Policy Object (GPO) either with
PowerShell or using a web solution. As a result, follow this alternative
method, which is based on configuring a group policy from another
Windows DC by remotely configuring the group policy.

1.  Use any DC that has the Group Policy Management Console available in
    the same domain as the Core server, and verify the connection
    between the servers with a simple ping.

2.  Run `cmd` as an administrator.

3.  Run the following command:

- gpmc.msc /gpcomputer: <computer name.Domain>

      gpmc.msc /gpcomputer: WIN-SI2SVDOKIMV.ENV21.LOCAL

4.  In the **Group Policy Management** window, navigate to Domains \>
    your domain name \> Group Policy Object, right-click and select
    **New**.

5.  In the **New GPO** window, enter your group policy **Name:** as
    **Windows Event Forwarding**, and click **OK**.

6.  Navigate to Domains \> your domain name \> Group Policy Objects \>
    Windows Event Forwarding, right-click and select **Edit**.

- ![](media/rId4773.png){width="5.833333333333333in" height="2.45in"}

7.  In the **Group Policy Management Editor**:

    - Set the Windows Remote Management Service for automatic startup.

      1.  Select Computer Configuration \> Policies \> Windows Settings
          \> Security Settings \> System Services, and in the view panel
          locate and double-click
          **Windows Remote Management (WS-Management)**.

      2.  Mark the **Define this policy setting** checkbox, select
          **Automatic**, and then click **Apply** and **OK**.

    - At a minimum for your WEC configuration, you must enable logging
      of the same events that you have configured to be collected in
      your WEC configuration on your domain controller. Otherwise, you
      will not be able to view these events as the WEC only controls
      querying not logging. For example, if you have configured
      authentication events to be collected by your WEC using an
      authentication protocol, such as Kerberos, you should ensure all
      relevant audit events for authentication are configured on your
      domain controller. In addition, you should ensure that all
      relevant audit events that you want collected, such as the success
      and failure of account logins for Windows Event ID 4625, are
      properly configured, particularly for those that you want Cortex
      XSIAM to apply grouping and analytics inspection.

    <!-- -->

    - > **Note**

      > This step overrides any local policy settings.

      Here is an example of how to configure the WEC to collect
      authentication events using Kerberos as the authentication
      protocol to enable the collection of Broker VM supported Kerberos
      events, Kerberos pre-authentication, authentication, request, and
      renewal tickets.

      1.  Select Computer Configuration \> Policies \> Windows Settings
          \> Security Settings \> Advanced Audit Policy Configuration \>
          Audit Policies \> Account Logon.

      2.  In the view pane, right-click
          **Audit Kerberos Authentication Service** and select
          **Properties**. In the
          **Audit Kerberos Authentication Service** window, mark
          **Configure the following audit events:**, and click
          **Success** and **Failure** followed by **Apply** and **OK**.

      - Repeat for **Audit Kerberos Service Ticket Operations**.

8.  Configure the subscription manager.

- Navigate to Computer Configuration \> Policies \> Administrative
  Templates: Policy definitions \> Windows Components \> Event
  Forwarding, right-click **Configure target Subscription Manager** and
  select **Edit**.

+-------------------------------------------------------+---------------------------------------------+
| - ![](media/rId4777.png){width="2.9166666666666665in" | - In the                                    |
|   height="1.581424978127734in"}                       |   **Configure target Subscription Manager** |
|                                                       |   window:                                   |
+-------------------------------------------------------+---------------------------------------------+

a.  Mark **Configure target Subscription Manager** as **Enabled**.

b.  In the **Options** section, select **Show** and in the
    **Show Contents** window, paste the [Subscription Manage
    URL](#X6fda4e5e0df49ffd1f3914c4225e960419deeba) you copied from the
    Cortex XSIAM console, and then click **OK**.

c.  Click **Apply** and **OK** to save your changes.

<!-- -->

9.  Add Network Service to Event Log Readers group.

- Select Computer Configuration \> Preferences \> Control Panel Settings
  \> Local Users and Groups, right-click and select New \> Local Group.

  ![](media/rId4780.png){width="5.736111111111111in"
  height="6.402777777777778in"}

  In the **New Local Group Properties** window:

  a.  In the **Group name** field, select
      **Event Log Readers (built-in)**.

  b.  In the **Members** section, click **Add** and enter in the
      **Name** filed `Network Service` followed by **OK**.

  - > **Note**

    > You must type out the name, do not select the name from the browse
    > button.

  c.  Click **Apply** and **OK** to save your changes, and close the
      **Group Policy Management Editor** window.

10. Configure the Windows Firewall.

- > **Note**

  > If Windows Firewall is enabled on your event forwarders, you will
  > have to define an outbound rule to enable the WEF to reach port 5986
  > on the WEC.

  In the **Group Policy Management** window, select Computer
  Configuration \> Policies \> Windows Settings \> Security Settings \>
  Windows Firewall with Advanced Security \> Outbound Rules, right-click
  and select **New Rule**.

  In the **New Outbound Rule Wizard** define the following **Steps**:

  a.  **Rule Type**: Select **Port** followed by **Next**.

  b.  **Protocols and Ports**: Select **TCP** and in the
      **Specific Remote Ports** field enter `5986` followed by **Next**.

  c.  **Action**: Select **Allow the connection** followed by **Next**.

  d.  **Profile**: Select **Domain** and disable **Private** and
      **Public** followed by **Next**.

  e.  **Name**: Specify `Windows Event Forwarding`.

  f.  To save your changes, click **Finish**.

######## Task 6. Apply the WEF Group Policy

Link the policy to the OU or the group of Windows servers you would like
to configure as event forwarders. In the following flow, the domain
controllers are configured as an event forwarder.

1.  Select Group Policy Management \> \<your domain name\> \> Domain
    Controllers, right-click and select **Link an existing GPO\...**.

2.  In the **Select GPO** window, click **Windows Event Forwarding**
    followed by **OK**.

3.  In an administrative PowerShell console, execute the following
    commands:

    a.  PS C:\Users\Administrator> gpupdate /force

    - Verify that the
      `Computer Policy update has completed successfully. User Policy update has completed successfully.`
      confirmation message is displayed.

    b.  PS C:\Users\Administrator> Restart-Service WinRM

######## Task 7. Verify Windows Event Forwarding

1.  In an administrative PowerShell console, run the following command:

- PS C:\Users\Administrator> Get-WinEvent Microsoft-windows-WinRM/operational -MaxEvents 10

2.  Look for `WSMan operation EventDelivery completed successfully`
    confirmation messages. These indicate events forwarded successfully.

######## Task 8. Manage the Window Event Collector (Optional)

After the Windows Event Collector has been activated in the Cortex XSIAM
Management Console, left-click the **WEC** connection in the **APPS**
column to display the Windows Event Collector settings, and select:

- **Configure** to define the event configuration information.

- **Collection Configuration** to view or edit existing or add new
  events to collect.

- **Deactivate** to disable the Windows Event Collector.

######## Task 9. View Windows Event Collector metrics (Optional)

To view metrics about the Windows Event Collector, left-click the
**WEC** connection in the **APPS** field for your Broker VM, and you\'ll
see the following metrics:

- **Connectivity Status**: Whether the applet is connected to Cortex
  XSIAM.

- **Logs Received** and **Logs Sent**: Number of logs received and sent
  by the applet per second over the last 24 hours. If the number of
  incoming logs received is larger than the number of logs sent, it
  could indicate a connectivity issue.

- **Resources**: Displays the amount of **CPU**, **Memory**, and
  **Disk** space the applet is using.

####### Renew WEC certificates

Renewing your WEC certificates in Cortex XSIAM includes renewing your
Windows Event Forwarding (WEF) client certificate and your WEC server
certificate. You must install the WEF certificate on every Windows
server, whether a Domain Controller (DC) or not, for the WEFs that are
supposed to forward logs to the Windows Event Collector applet on the
Broker VM.

> **Important**
>
> After you receive a notification for renewing your WEC CA certificate,
> we recommend that you do not add any new WEF clients until the WEC
> certification renewal process is complete. Events from these WEF
> clients that are added afterwards will not be collected by the server
> until the WEC certificates are renewed.

In addition, Cortex XSIAM manages the renewal of your WEC certificates
by implementing the following time limits:

- The WEC CA certificate is increased for an extended period of time for
  a maximum of 20 years.

- The Broker VM applet includes an automatic renewal mechanism for a WEC
  server certificate, which has a lifespan of 12 months.

- The WEC client certificate after the renewal is issued with a lifespan
  of 5 years.

Perform the following procedures in the order listed below.

######## Task 1. Renew your WEF client certificate in Cortex XSIAM

1.  Select Settings \> Configurations \> Data Broker \> Broker VMs.

2.  Do one of the following:

    - On the **Brokers** tab, find the Broker VM, and in the **APPS**
      column, left-click the **WEC** connection to display the Windows
      Event Collector settings, and select **Configure**.

    - On the **Clusters** tab, find the Broker VM, and in the **APPS**
      column, left-click the **WEC** connection to display the Windows
      Event Collector settings, and select **Configure**.

3.  In the **Windows Event Forwarder Configuration** window, perform the
    following tasks:

    a.  In the **Subscription Manager URL** field, click
        ![](media/rId4759.png){width="0.14583333333333334in"
        height="0.20833333333333334in"} (copy) . This will be used when
        you configure the subscription manager in the GPO (Global Policy
        Object) on your domain controller.

    b.  Enter a password in the
        **Define Client Certificate Export Password** field to be used
        to secure the downloaded WEF certificate that establishes the
        connection between your DC/WEF and the WEC. You will need this
        password when the certificate is imported to the events
        forwarder.

    c.  Download the WEF certificate in a PFX format to your local
        machine.

4.  Install your WEF Certificate on the WEF to establish connection.

- > **Note**

  > You must install the WEF certificate on every Windows Server,
  > whether DC or not, for the WEFs that are supposed to forward logs to
  > the Windows Event Collector applet on the Broker VM.

  a.  Locate the PFX file you downloaded from the Cortex XSIAM console
      and double-click to open the **Certificate Import Wizard**.

  b.  In the **Certificate Import Wizard**:

      1.  Select **Local Machine**, and then click **Next**.

      2.  Verify the **File name** field displays the PFX certificate
          file you downloaded and click **Next**.

      3.  In the **Passwords** field, enter the Client Certificate
          Export Password you defined in the Cortex XSIAM console
          followed by **Next**.

      4.  Select
          **Automatically select the certificate store based on the type of certificate, and then click Next**
          and **Finish**.

  c.  From a command prompt, run `certlm.msc`.

  d.  In the file explorer, navigate to **Certificates** and verify the
      following for each of the folders:

      - In the Personal \> Certificates folder, ensure the certificate
        `forwarder.wec.paloaltonetworks.com` is displayed.

      - In the Trusted Root Certification Authorities \> Certificates
        folder, ensure the CA `ca.wec.paloaltonetworks.com` is
        displayed.

  - > **Note**

    > You can see more than one `ca.wec.paloaltonetworks.com` and
    > `forwarder.wec.paloaltonetworks.com` file from a previous
    > installation in the directory, so select the file with the most
    > extended **Expiration Date**. You can verify that you are using
    > the correct certificate:

    - > To verify the client certificate in the Personal \> Certificates
      > folder is related to the CA, you can select your
      > `forwarder.wec.paloaltonetworks.com` file and from the
      > **Certification Path** tab, double-click
      > **ca.wec.paloaltonetworks.com**. In the **Details** tab,
      > **Show: Properties only**, and verify the **Thumbprint** matches
      > the `ca.wec.paloaltonetworks.com` file **Thumbprint**.

    - > For the Trusted Root Certificate (i.e. CA certificate), you can
      > verify the **Thumbprint** of your `ca.wec.paloaltonetworks.com`
      > file matches the [Subscription Manager
      > URL](#SubscriptionManageURL) by double-clicking the file and
      > from the **Details** tab verifying the **Thumbprint**.

  e.  Navigate to Certificates Personal Certificates.

  f.  Right-click the certificate and navigate to All tasks \> Manage
      Private Keys.

  g.  In the **Permissions** window, select **Add** and in the
      **Enter the object name** section, enter `NETWORK SERVICE`, and
      then click **Check Names** to verify the object name. The object
      name is displayed with an underline when valid. and then click
      **OK**.

  - ![](media/rId4763.png){width="5.833333333333333in"
    height="3.055207786526684in"}

  h.  Click **OK**, verify the **Group or user names** that are
      displayed, and then click **Apply Permissions for private keys**.

  - ![](media/rId4766.png){width="5.833333333333333in"
    height="2.9166666666666665in"}

5.  Configure the subscription manager.

    a.  Navigate to Computer Configuration \> Policies \> Administrative
        Templates: Policy definitions \> Windows Components \> Event
        Forwarding, right-click
        **Configure target Subscription Manager** and select **Edit**.

    - ![](media/rId4777.png){width="5.833333333333333in"
      height="3.1628510498687663in"}

    b.  In the **Configure target Subscription Manager** window, perform
        the following:

        1.  Mark **Configure target Subscription Manager** as
            **Enabled**.

        2.  In the **Options** section, select **Show** and in the
            **Show Contents** window, paste the [Subscription Manage
            URL](#SubscriptionManageURL) you copied from the Cortex
            XSIAM console, and then click **OK**.

        3.  Click **Apply** and **OK** to save your changes.

6.  Complete the WEF Client certificate renewal.

- On every WEF DC, perform the following from a command prompt:

  a.  Run `gpupdate /force` to update the group policy.

  b.  To apply the configurations, `Restart-Service WinRM`.

######## Task 2. Renew your WEC server certificate in Cortex XSIAM

> **Note**
>
> Only perform this step under the following conditions:

- > You have completed the WEF certification renewal process for ALL
  > clients in your environment. Otherwise, events from the WEFs that
  > you did not install the new client certificate will not be collected
  > by the WEC.

- > You are approaching the WEC server CA certificate expiration date,
  > which is 2 years after the **Windows Event Collector** applet
  > activation, and receive a notification in the Cortex XSIAM console.

1.  Select Settings \> Configurations \> Data Broker \> Broker VMs.

2.  Do one of the following:

    - On the **Brokers** tab, find the Broker VM, and in the **APPS**
      column, left-click the **WEC** connection to display the Windows
      Event Collector settings, and select
      **Renew WEC Server Certificate**.

    - On the **Clusters** tab, find the Broker VM, and in the **APPS**
      column, left-click the **WEC** connection to display the Windows
      Event Collector settings, and select
      **Renew WEC Server Certificate**.

3.  Click **Renew**.

- Once Cortex XSIAM renews the WEC server certificate, the status of the
  **WEC** in the **APPS** field on the **Broker VMs** machine is
  **Connected** indicating the applet is running. In addition, the
  health status of the Windows Event Collector applet is now green
  instead of yellow and the warning message that appeared when you
  hovered over the health status no longer appears. Your WEC server
  certificate is issued with a lifespan of 12 months.

  We also suggest that you run the following XQL query to verify that
  your event logs are being captured:

      dataset = xdr_data 
      | filter _product = "Windows" 
      | fields _vendor,_product,action_evtlog_level,action_evtlog_event_id 
      | sort desc _time 
      | limit 20

  > **Note**

  > If this query does not display results with a timestamp from after
  > the renewal process, it could indicate that the renewal process is
  > not complete, so wait a few minutes before running another query. If
  > you are still having a problem, contact Technical Support.

#### Manage Broker VM

After you configure the Broker VMs, you can manage these brokers from
the Cortex XSIAM management console in the **Broker VMs** page.

When managing a Broker VM, the options differ for a standalone Broker VM
versus a Broker VM node that is added to a high availability (HA)
cluster. Certain configuration options that are only relevant for a
Broker VM cluster node, such as **Remove from Cluster**, are only
displayed when the Broker VM is a cluster peer.

Select Settings \> Configurations \> Data Broker \> Broker VMs to view
detailed information regarding your registered Broker VMs in the
**Brokers** tab.

##### Understanding the Broker VM table

The **Broker VMs** table enables you to monitor and mange your Broker VM
and applet connectivity status, version management, device details, and
usage metrics. A status icon is displayed in the following columns,
where the colors can indicate different statuses:

- **Device Name**: Indicates whether the Broker machine is registered
  and connected to Cortex XSIAM.

  - Black: Disconnected to Cortex XSIAM

  - Red: Disconnected from Cortex XSIAM

  - Green: Connected

- **Version**: Indicates whether the Broker VM is running the latest
  version.

  - Orange: Past Version

  - Green: Latest Version

- **Apps**: Indicates whether the available Broker VM data collector
  applets are connected to Cortex XSIAM.

  - Green (Connected): Indicates the applet has no issues.

  - Orange (Warning): Indicates the applet has minor issues.

  - Red (Error): Indicates the applet has errors.

<!-- -->

- > **Note**

  > For more information on troubleshooting errors and warnings for
  > these broker applets, see [Troubleshoot Broker VM applet
  > errors](#UUIDf260379f4b4fd8505baff065946fe056).

##### Broker VM table field descriptions

The following table describes common fields that you can add to the
Brokers table using the column manager and lists the fields in
alphabetical order.

> **Note**
>
> Certain fields are also exposed in the **Clusters** tab, when a Broker
> VM node is added to a High Availability (HA) cluster, and each cluster
> node is expanded to view the Broker VM nodes table. An asterisk (\*)
> is beside every field that is also included in the Broker VM nodes
> table for each HA cluster.

+-----------------------------------+------------------------------------------------------+
| Field                             | Description                                          |
+===================================+======================================================+
| ALL interfaces                    | All IP addresses of the different interfaces on the  |
|                                   | device.                                              |
+-----------------------------------+------------------------------------------------------+
| APPS\*                            | List of active or inactive applets and the           |
|                                   | connectivity status for each.                        |
+-----------------------------------+------------------------------------------------------+
| CLUSTER NAME\*                    | Indicates the name of the HA cluster that the Broker |
|                                   | VM has been added to. For a standalone Broker VM,    |
|                                   | which isn\'t added to any HA cluster, this field is  |
|                                   | empty.                                               |
+-----------------------------------+------------------------------------------------------+
| CPU USAGE\*                       | CPU usage percentage of the Broker VM device that is |
|                                   | synced every 5 minutes.                              |
+-----------------------------------+------------------------------------------------------+
| CONFIGURATION STATUS\*            | Broker VM configuration status. Status is defined by |
|                                   | the following according to changes made to any of    |
|                                   | the Broker VM configurations:                        |
|                                   |                                                      |
|                                   | - up to date: Broker VM configuration changes made   |
|                                   |   through the Cortex XSIAM console have been         |
|                                   |   applied.                                           |
|                                   |                                                      |
|                                   | - in progress: Broker VM configuration changes made  |
|                                   |   through the Cortex XSIAM console are being         |
|                                   |   applied.                                           |
|                                   |                                                      |
|                                   | - submitted: Broker VM configuration changes made    |
|                                   |   through the Cortex XSIAM console have reached the  |
|                                   |   Broker VM and awaiting implementation.             |
|                                   |                                                      |
|                                   | - failed: Broker VM configuration changes made       |
|                                   |   through the Cortex XSIAM console have failed. Need |
|                                   |   to open a Palo Alto Networks support ticket.       |
+-----------------------------------+------------------------------------------------------+
| DEVICE ID                         | Device ID allocated to the Broker VM by Cortex XSIAM |
|                                   | after registration.                                  |
+-----------------------------------+------------------------------------------------------+
| DEVICE NAME\*                     | Same as the Device ID.                               |
|                                   |                                                      |
|                                   | A                                                    |
|                                   | ![](media/rId4831.png){width="0.14583333333333334in" |
|                                   | height="0.20833333333333334in"}icon notifies of an   |
|                                   | expired Broker VM. To reconnect, generate a new      |
|                                   | token and re-register your Broker VM as described in |
|                                   | steps 1 through 7 of [Configure the Broker           |
|                                   | VM](urn:resource:component:924525). Once registered, |
|                                   | all previous Broker VM configurations are            |
|                                   | reinstated.                                          |
+-----------------------------------+------------------------------------------------------+
| DISK USAGE\*                      | Disk usage percentage from the total allocated for   |
|                                   | data caching in the Broker VM. Inside the brackets   |
|                                   | is displayed how much this is in GB from the total   |
|                                   | disk size in GB.                                     |
|                                   |                                                      |
|                                   | A notification is added to the Notification Center   |
|                                   | whenever the disk space is low disk and whenever the |
|                                   | disk size is increased.                              |
+-----------------------------------+------------------------------------------------------+
| EXTERNAL INTERFACE                | The IP interface the Broker VM is using to           |
|                                   | communicate with the server.                         |
|                                   |                                                      |
|                                   | For AWS and Azure cloud environments, the field      |
|                                   | displays the **Internal IP** value.                  |
+-----------------------------------+------------------------------------------------------+
| LAST SEEN                         | Indicates when the Broker VM was last seen on the    |
|                                   | network.                                             |
+-----------------------------------+------------------------------------------------------+
| MEMORY USAGE\*                    | Memory usage percentage of the Broker VM that is     |
|                                   | synced every 5 minutes.                              |
+-----------------------------------+------------------------------------------------------+
| STATUS\*                          | Connection status of the Broker VM. Status is        |
|                                   | defined by either **Connected** or **Disconnected**. |
|                                   |                                                      |
|                                   | Disconnected Broker VMs do not display               |
|                                   | **CPU Usage**, **Memory Usage**, and **Disk Usage**  |
|                                   | information.                                         |
|                                   |                                                      |
|                                   | Notifications about the Broker VM losing             |
|                                   | connectivity to Cortex XSIAM appear in the           |
|                                   | Notification Center.                                 |
+-----------------------------------+------------------------------------------------------+
| UPGRADE TIME                      | Timestamp of when the Broker VM was upgraded.        |
+-----------------------------------+------------------------------------------------------+
| VERSION\*                         | Version number of the Broker VM. If the status       |
|                                   | indicator is not green, then the Broker VM is not    |
|                                   | running the latest version.                          |
|                                   |                                                      |
|                                   | Notifications about the available new Broker VM      |
|                                   | version appear in the Notification Center.           |
+-----------------------------------+------------------------------------------------------+

##### Maintenance releases

Cortex XSIAM updates and enhances the Broker VM automatically through
maintenance releases. The Broker VM version release process uses several
security measures and tools to ensure that every released version is
highly secure. These include the following.

- CIS Server Level 1 and 2 benchmarks (using a 3rd party product)

- Vulnerability scanning for containers running on the Broker VM

- Vulnerability scanning for the host kernel

- Periodic 3rd party penetration testing

##### Edit Broker VM Configuration

After configuring and registering your Broker VM, you can edit existing
configurations and define additional settings in the **Broker VMs** page
in the **Brokers** tab. When you have a high availability (HA) cluster
configured, you can also edit any Broker VM nodes configurations in the
**Clusters** tab from the Broker VMs table under the Cluster.

Perform the following procedures in the order listed below.

###### Task 1. Open the Configurations page for the Broker VM

1.  Select Settings \> Configurations \> Data Broker \> Broker VMs.

2.  In the **Broker VMs** table, locate your Broker VM, right-click, and
    select **Configure**.

- If the Broker VM is disconnected, you can only **View** the
  configurations.

  > **Note**

  > For all Broker VM nodes added to a HA cluster, you can also
  > **Configure** the Broker VM nodes from the **Clusters** tab.

###### Task 2. Define the settings in the Configurations page

Network Interfaces, Proxy Server, NTP Server, and SSH Access

Edit the existing **Network Interfaces**, **Proxy Server**,
**NTP Server**, and **SSH Access** configurations.

Device Name (Requires Broker VM 8.0 and later)

Device Name

Change the name of your Broker VM device name by selecting the pencil
icon. The new name will appear in the Brokers table.

FQDN

Set your **Broker VM FQDN** as it will be defined in your Domain Name
System (DNS). This enables connection between the WEF and WEC, acting as
the subscription manager. The **Broker VM FQDN** settings affect the WEC
and Agent Installer and Content Caching.

(Optional) Internal Network (Requires Broker VM 8.0 and later)

Specify a network subnet to avoid the Broker VM dockers colliding with
your internal network. By default, the **Network Subnet** is set to
`172.17.0.1/16`.

> **Note**
>
> Internal IP must be:

- > Formatted as `prefix/mask`, for example `192.0.2.1/24`.

- > Must be within `/8` to `/24` range.

- > Cannot be configured to end with a zero.

> For Broker VM version 9.0 and lower, Cortex XSIAM accepts only
> `172.17.0.0/16`.

Auto Upgrade

**Enable** or **Disable** automatic upgrade of the Broker VM. By
default, auto upgrade is enabled at **Any** time for all **7 days** of
the week, but you can also set the **Days in Week** and **Specific**
time for the automatic upgrades. If you disable auto-upgrade, new
features and improvements will require manual upgrade.

Monitoring

**Enable** or **Disable** of local monitoring of the Broker VM usage
statistics in Prometheus metrics format, allowing you to tap in and
export data by navigating to `http://<broker_vm_address>:9100/metrics/`.
By default, monitoring your Broker VM is disabled. For more information
with an example of how to set up Prometheus and Grafana to monitor the
Broker VM, see [Monitor Broker VM using
Prometheus](#UUID78139f8ef333cb59392a15441d5ba448).

(Optional) SSH Access

Broker VM 7.4.5 and earlier

**Enable/Disable ssh** Palo Alto Networks support team SSH access by
using a Cortex XSIAM token.

Enabling allows Palo Alto Networks support team to connect to the Broker
VM remotely, not the customer, with the generated password. If you use
SSL decryption in your firewalls, you need to add a trusted self-signed
CA certificate on the Broker VM to prevent any difficulties with SSL
decryption. For example, when [configuring Palo Alto Networks NGFW to
decrypt
SSL](https://knowledgebase.paloaltonetworks.com/KCSArticleDetail?id=kA10g000000ClmyCAC)
using a self-signed certificate, you need to ensure the Broker VM can
validate a self-signed CA by uploading the `cert_ssl-decrypt.crt` file
on the Broker VM.

> **Note**
>
> Make sure you save the password before closing the window. The only
> way to re-generate a password is to disable ssh and re-enable.

Broker VM 14.0.42 and later

Customize the login banner displayed, when logging into SSH sessions on
the Broker VM in the **Welcome Message** field by overwriting the
default welcome message with a new one added in the field. When the
field is empty, the default message is used.

Broker UI Password

Reset your current Broker VM Web UI password. **Define** and **Confirm**
your new password. Password must be at least 8 characters.

(*Optional*) SSL Server Certificate section (Requires Broker VM 10.1.9
and later)

Upload your signed server certificate and key to establish a validated
secure SSL connection between your endpoints and the Broker VM. When you
configure the server certificate and the key files in the tenant UI,
Cortex XSIAM automatically updates them in the Broker VM UI, even when
the Broker VM UI is disabled.

Cortex XSIAM validates that the certificate and key match, but does not
validate the Certificate Authority (CA).

When you are done, **Save** your changes.

##### Increase Broker VM storage allocated for data caching

The storage allocated for data caching in the Broker VM is fixed at
around 346.4 GB using a Logical Volume Manager (LVM). You can increase
the disk space allocated to attain better resilience during network and
connectivity issues by adding a new disk. The disk needs to be added
manually to an applicable hypervisor that your broker supports, so that
the Broker VM automatically detects the physical disk and allows you to
connect to it. Extending the existing disk is not supported.

When allocating storage for data caching, ensure you are aware of the
following:

- You must allocate the entire disk as opposed to portions of the disk.

- You can connect multiple disks to increase the data caching space
  according to your requirements.

- Once a disk is connected, It\'s not possible to dismiss a disk that
  has already been allocated, or to reduce the disk space of the data
  caching.

- Adding a disk requires formatting and deleting all its contents.

> **Warning**
>
> This operation is irreversible, and will make the disk become an
> integral part of the broker, where disconnecting the disk will result
> in errors and data loss.

**How to increase the Broker VM disk size**

1.  Gracefully shutdown the applicable Broker VM in the hypervisor to
    manually add a disk.

2.  Add a disk manually through the hypervisor portal. This step
    involves accessing the portal and attaching a new disk to the VM.

- > **Note**

  > Follow your hypervisor documentation to understand how to add a
  > persistent disk storage to your VM.

3.  Select Settings \> Configurations \> Data Broker \> Broker VMs.

4.  In the **Broker VMs** table, locate your Broker VM, and wait for a
    few minutes until the status of the Broker VM is **Connected**,
    right-click, and select **Configure**.

5.  Scroll down to the **Storage** section, verify that your disk is
    detected with a new line that reads **New disk detected** with the
    correct disk name and disk size, and click
    **Add to data caching space**.

- > **Note**

  > If your disk is not listed and you didn\'t shutdown your Broker VM
  > in your hypervisor before manually adding a disk to the VM, you\'ll
  > need to reboot the Broker VM before the disk details are detected by
  > the Broker VM. This can be performed either in the hypervisor or
  > directly in the **Broker VMs** page.

6.  In the **ARE YOU SURE?** dialog box that is displayed, confirm that
    you want to add the new disk to the broker\'s data caching space and
    are aware of all the ramifications by clicking **Yes, add**.

7.  To apply your changes, click **Save**.

- Once completed, a notification is added to the Notification Center
  indicated whether the disk size was increased successfully. If not,
  the notification includes the errors encountered during the process.

  In addition, when the disk is added successfully, the total size of
  the disk space available is updated in the **DISK USAGE** column on
  the **Broker VMs** page.

##### Monitor Broker VM using Prometheus

You can enable local monitoring of the Broker VM to provide usage
statistics in a Prometheus metrics format. You can tap in and export
data by navigating to `http://<broker_vm_address>:9100/metrics/`. By
default, monitoring is disabled.

###### Prerequisite

To monitor the Broker VM using Prometheus, ensure that you enable
monitoring on the Broker VM. This is performed after configuring and
registering your Broker VM, when you can edit existing configurations
and define additional settings in the **Broker VMs** page.

1.  Select Settings \> Configurations \> Data Broker \> Broker VMs.

2.  In the **Broker VMs** table, locate your Broker VM, right-click, and
    select **Configure**.

- > **Note**

  > For all Broker VM nodes added to a HA cluster, you can also
  > **Configure** the Broker VM nodes from the **Clusters** tab.

3.  In the Broker VM **Configurations** page, select **Monitoring** from
    the left pane.

4.  Clear the **Use Default (Disabled)** checkbox.

5.  In the **Montoring** menu, select **Enabled**.

6.  Click **Save**.

###### How to set up Prometheus and Grafana to monitor the Broker VM

Below is an example of how to set up Prometheus and Grafana to monitor
the Broker VM. This is set up using a docker compose on an Ubuntu
machine to monitor the CPU usage.

Perform the following procedures in the order listed below.

####### Task 1. Install Docker and Docker Compose

1.  Update your Ubuntu system:

- sudo apt update

2.  Install Docker:

- > **Note**

  > For more information on Docker, see the [Docker
  > website](https://www.docker.com/).

      sudo apt install docker.io

3.  Start the Docker service:

- sudo systemctl start docker

4.  Enable Docker to start on boot:

- sudo systemctl enable docker

5.  Install Docker Compose:

- sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose

      sudo chmod +x /usr/local/bin/docker-compose

####### Task 2. Create a Docker Compose file

This task includes setting up Prometheus and Grafana.

1.  Create a file named `docker-compose.yml`, and open it for editing:

- vim docker-compose.yml

2.  Add the following content to the file:

- version: '3.8'
      services:
        prometheus:
          image: prom/prometheus:latest
          container_name: prometheus
          restart: unless-stopped
          volumes:
           - ./prometheus.yml:/etc/prometheus/prometheus.yml
           - prometheus_data:/prometheus
         command:
           - '--config.file=/etc/prometheus/prometheus.yml'
           - '--storage.tsdb.path=/prometheus'
           - '--web.console.libraries=/etc/prometheus/console_libraries'
           - '--web.console.templates=/etc/prometheus/consoles'
           - '--web.enable-lifecycle'
           - '--log.level=debug'
         ports:
           - '9090:9090'
       grafana:
         image: grafana/grafana-enterprise
         container_name: grafana
         restart: unless-stopped
         ports:
          - '3000:3000'
         volumes:
           - grafana_data:/var/lib/grafana
      volumes:
        grafana_data: {}
        prometheus_data: {}

3.  Save and close the file.

####### Task 3. Create a Prometheus configuration file

You need to configure Prometheus to scrape the Broker VM metrics by
creating a Prometheus configuration file.

1.  Create a Prometheus configuration file named `prometheus.yml` in the
    same directory as the `docker-compose.yml` file that you created
    above.

2.  Open the `prometheus.yml` file for editing:

- vim prometheus.yml

3.  Add the following content to the file:

- global:
        scrape_interval: 15s
        scrape_timeout: 10s
      scrape_configs:
        - job_name: 'prometheus'
          static_configs:
            - targets: ['<your server IP address>:9090']
        - job_name: 'node'
          static_configs:
            - targets: ['<Broker VM IP address>:9100']

4.  Save and close the file.

####### Task 4. Run Docker Compose

1.  In the terminal, run the following command from the project
    directory:

- docker-compose up -d

2.  Verify that Prometheus is running correctly:

- docker-compose logs -f prometheus

####### Task 5. Access Grafana and Set Up Prometheus as a Data Source

1.  Open a web browser and go to `http://<your server>:3000`.

2.  Log in to Grafana using the default credentials.

    - Username: `admin`

    - Password: `admin`

3.  Set up Prometheus as a data source:

    a.  In the left pane, select Administation \> Data sources.

    b.  Click **Add data source**, and select **Prometheus**.

    c.  Under **HTTP**, set the **URL**
        to `http://<your server IP address>:9090`.

    d.  To verify the connection, click **Save & Test**.

####### Task 6. Create Dashboards in Grafana

You can now create dashboards in Grafana to visualize the data from
Prometheus.

1.  In Grafana, on the left pane, click **Dashboards**.

2.  Select **New** and create a new dashboard.

3.  Add a panel to the dashboard and configure the dashboard to display
    the Prometheus metrics that you want.

4.  To monitor CPU usage, use the following metric:

- 100 - (avg by (instance) (rate(node_cpu_seconds_total{job="node",mode="idle"}[1m])) * 100)

##### Collect Broker VM Logs

Cortex XSIAM enables you to collect your Broker VM logs directly from
the Cortex XSIAM management console.

You can collect logs by either regenerating the most up-to-date logs and
downloading them once they are ready, or downloading the current logs
from the last creation date reflected in the TIMESTAMP.

1.  Select Settings \> Configurations \> Data Broker \> Broker VMs to
    view the **Broker VMs** table in the **Brokers** tab.

2.  Locate your Broker VM, right-click and select either
    **Generate New Logs** or **Download Logs (\<TIMESTAMP\>)**.

- > **Note**

  > The **Download Logs (\<TIMESTAMP\>)** is only displayed when you've
  > downloaded your logs previously using Generate New Logs.

  Logs are generated automatically, but can take up to a few minutes
  depending on the size of the logs.

##### Upgrade Broker VM

For all brokers that were deployed with a Broker VM image, downloaded
prior to July 9th, 2023 (installed with Ubuntu 18.04 or earlier), the
Broker VM must be reinstalled with a new image (installed with Ubuntu
20.04 or later) before upgrading to the latest version. For more
information on upgrading to a new Broker VM image, see [Migrating to a
New Broker VM
Image](https://docs-cortex.paloaltonetworks.com/r/Cortex-XSIAM/Cortex-XSIAM-Migration-Process-for-New-Broker-VM-Image).

1.  Select Settings \> Configurations \> Data Broker \> Broker VMs.

2.  In either the **Brokers** or **Clusters** tab, locate your Broker
    VM, right-click, and select **Upgrade Broker version**.

- Upgrading your Broker VM takes approximately 5 minutes.

  > **Important**

  > After a Broker VM upgrade, your broker may require a reboot to
  > finish installing important updates. A notification about this will
  > be sent to your Cortex XSIAM console Notification Center.

##### Import Broker VM Configuration

> **Important**
>
> This option can only be used on Broker VMs with version 20.0 and
> later, and is only suitable for importing a configuration of brokers
> in the same version, or from a broker in an older version to a broker
> in a newer version.

Importing Broker VM configurations allows you to copy, including applet
settings, the configuration of one Broker VM to another. The import
overrides the Broker VM and applet settings in the target Broker VM.

1.  To replace the Broker VM configuration, right-click the Broker VM
    and select **Import Configuration**.

2.  Select the Broker VM that has the configuration that you want to
    import.

3.  (Optional) After the import is complete and the new configurations
    are applied to the target Broker VM, you can choose to shutdown the
    original Broker VM (default configuration). This step ensures that
    there are no conflicts in data collection and applets operation.

4.  Select the confirmation checkbox.

5.  Click **Import**.

- After a successful import, the new configurations are immediately
  applied to the target Broker VM.

  > **Important**

  > If your source Broker VM configuration includes a WEC applet,
  > you\'ll need to ensure that you update the DNS record of this Broker
  > VM\'s FQDN to point to the target Broker VM IP address.

##### Open Live Terminal

Cortex XSIAM enables you to connect remotely to a Broker VM directly
from Cortex XSIAM.

1.  In Cortex XSIAM, select Settings \> Configurations \> Data Broker \>
    Broker VMs table.

2.  Locate the Broker VM you want to connect to, right-click and select
    **Open Live Terminal**.

- Cortex XSIAM opens a CLI window where you can perform the following
  commands:

  Logs
  Broker VM logs are located in `/data/logs/folder` and contain the
  applet name in the file name.

  Folder `/data/logs/[applet name]`, containing
  `container_ctrl_[applet name].log`

  Ubuntu commands
  The Broker VM allows commands which do not require Sudo.

  `route` or `ifconfig -a`

  Sudo commands
  Broker VM supports the commands listed in the following table. All the
  commands are located in the `/home/admin/sbin` folder.

  Cortex XSIAM requires you use the following values when running
  commands:

  Applet Names
  - CSV Collector: `file_collector`

  - Database Collector: `db_collector`

  - Files and Folders Collector: `log_collector`

  - FTP Collector: `ftp_collector`

  - Kafka Collector: `kafka_collector`

  - Local Agent Settings: `tms_proxy`

  - NetFlow Collector: `netflow_collector`

  - Network Mapper: `network_mapper`

  - Pathfinder: `odysseus`

  - Syslog Collector: `anubis`

  - Windows Event Collector: `wec`

  Services
  - Upgrade: `zenith_upgrade`

  - Frontend service: `webui`

  - Sync with Cortex XSIAM: `cloud_sync`

  - Internal messaging service (RabbitMQ): `rabbitmq-server`

  - Upload metrics to Cortex XSIAM: `metrics_uploader`

  - Prometheus node exporter: `node_exporter`

  - Backend service: `backend`

  The following table displays the available commands in alphabetical
  order:

+-----------------------+------------------------+---------------------------------------------------+
| Command               | Description            | Example                                           |
+=======================+========================+===================================================+
| `applets_restart`     | Restarts one or more   | `sudo ./applets_restart wec`                      |
|                       | applets.               |                                                   |
+-----------------------+------------------------+---------------------------------------------------+
| `applets_start`       | Start one or more      | `sudo ./applets_start wec`                        |
|                       | applets.               |                                                   |
+-----------------------+------------------------+---------------------------------------------------+
| `applets_status`      | Check the status of    | `sudo ./applets_status wec`                       |
|                       | one or more applets.   |                                                   |
+-----------------------+------------------------+---------------------------------------------------+
| `applets_stop`        | Stop one or more       | `sudo ./applets_stop wec`                         |
|                       | applets.               |                                                   |
+-----------------------+------------------------+---------------------------------------------------+
| `hostnamectl`         | Check and update the   | `sudo ./hostnamectl set-hostname <new_host_name>` |
|                       | machine hostname on a  |                                                   |
|                       | Linux operating        | Restart machine after running command.            |
|                       | system.                |                                                   |
+-----------------------+------------------------+---------------------------------------------------+
| `kill`                | Linux kill command.    | `sudo ./kill [some pid]`                          |
+-----------------------+------------------------+---------------------------------------------------+
| `restart_routes`      | Invoke a restart of    | `sudo ./restart_routes`                           |
|                       | the routing service    |                                                   |
|                       | after updating your    | > **Note**                                        |
|                       | static network route   | >                                                 |
|                       | configuration file,    | > You can either `restart_routes` or reboot the   |
|                       | `/etc/network/routes`. | > Broker VM for the changes in the                |
|                       |                        | > `/etc/network/routes` file to take affect.      |
|                       | The                    |                                                   |
|                       | `/etc/network/routes`  |                                                   |
|                       | configuration file is  |                                                   |
|                       | a standard Ubuntu      |                                                   |
|                       | routes configuration   |                                                   |
|                       | file and can be edited |                                                   |
|                       | directly. The admin    |                                                   |
|                       | user that you logged   |                                                   |
|                       | in with, when using    |                                                   |
|                       | the remote terminal or |                                                   |
|                       | via SSH, has           |                                                   |
|                       | read/write permissions |                                                   |
|                       | to this file.          |                                                   |
+-----------------------+------------------------+---------------------------------------------------+
| `route`               | Modify your IP address | `sudo ./route`                                    |
|                       | routing.               |                                                   |
+-----------------------+------------------------+---------------------------------------------------+
| `services_restart`    | Restarts one or more   | `sudo ./services_restart cloud_sync`              |
|                       | services. OS services  |                                                   |
|                       | are not supported.     |                                                   |
+-----------------------+------------------------+---------------------------------------------------+
| `services_start`      | Start one or more      | `sudo ./services_start cloud_sync`                |
|                       | services.              |                                                   |
+-----------------------+------------------------+---------------------------------------------------+
| `services_status`     | Check the status of    | `sudo ./services_status cloud_sync`               |
|                       | one or more services.  |                                                   |
+-----------------------+------------------------+---------------------------------------------------+
| `services_stop`       | Stop one or more       | `sudo ./services_restart cloud_sync`              |
|                       | services.              |                                                   |
+-----------------------+------------------------+---------------------------------------------------+
| `set_ui_password.sh`  | Change the password of | `sudo ./set_ui_password.sh`                       |
|                       | the Broker VM Web UI.  |                                                   |
|                       |                        |                                                   |
|                       | Run the command, enter |                                                   |
|                       | the new password       |                                                   |
|                       | followed by Ctrl+D.    |                                                   |
+-----------------------+------------------------+---------------------------------------------------+
| `squid_tail`          | Display the Proxy      | `sudo ./squid_tail`                               |
|                       | applet Squid log file  |                                                   |
|                       | in real-time.          |                                                   |
+-----------------------+------------------------+---------------------------------------------------+

##### Add Broker VM to cluster

You can add standalone Broker VMs to a high availability (HA) cluster
from either the **Brokers** tab or **Clusters** tab.

You can only add a Broker VM to a cluster, when the Broker VM version is
19.0 and later, the **STATUS** is **Connected**, and the Broker VM
version isn\'t older than the cluster version.

Once you add a Broker VM to a cluster, the Broker VM becomes a cluster
node and is added to the cluster folder in the **Clusters** tab. If it
is the only peer Broker VM in the cluster, it is designated as the
Primary node; otherwise, it is designated as a standby node.

1.  Select Settings \> Configurations \> Data Broker \> Broker VMs.

2.  Add a Broker VM in one of the following tabs:

- Brokers tab
  1.  Right-click a standalone Broker VM, and select
      **Add Broker to Cluster**.

  2.  In the **Select Cluster** field, choose the cluster that you want
      this Broker VM to be added to.

  Clusters tab
  1.  Right-click a cluster node, and select **Add Broker to Cluster**.

  2.  In the **Select broker** field, choose the standalone Broker VM
      that you want to add to this cluster.

3.  Click **Add Broker**.

- Adding a Broker VM to a cluster overrides all previous Broker VM
  settings and disables all active applets on this Broker VM. When the
  Broker VM is added to a cluster, the cluster configuration and cluster
  applet settings propagate to the Broker VM. The state of the applets
  on the Broker VM is dependent on the [applet
  mode](#UUID23bf9c984626b51101b7e7fee66538b8) and Broker VM node role
  in the cluster. When the operation completes, a notification is added
  to the Notification Center.

##### Switchover Primary Node in Cluster

You can manually change the role of the current Primary node in a high
availability (HA) cluster from both the **Brokers** tab and **Clusters**
tab of the **Broker VMs** page.

There are various reasons for changing the role of the current Primary
node to another node in the HA cluster, for example, to perform
maintenance, by initiating a manual switchover.

The option is only available for a Primary node, and only if there is
another available standby node that is connected in the cluster.

1.  Select Settings \> Configurations \> Data Broker \> Broker VMs.

2.  In either the **Brokers** tab or **Clusters** tab, right-click a
    Primary Broker VM node, and select **Switchover**.

3.  If multiple standby nodes are connected in the cluster, select the
    node that you want to change to Primary in the **Select broker**
    menu. When only one standby node is configured, skip this step.

4.  Click **Switchover**.

- When the switchover is completed, the roles of the node are switched.
  The new node is designated as **Primary** and the old node becomes a
  standby node. In addition, a notification is added to the Notification
  Center.

##### Remove from Cluster

You can remove a Broker VM node from a high availability (HA) cluster in
either the **Brokers** tab or **Clusters** tab of the **Broker VMs**
page. This option is only available if the Broker VM is currently a
member of a cluster.

When a Broker VM node is removed from a HA cluster, it becomes a
standalone Broker VM. All its configuration settings, including applet
settings, are reset to default like a newly created Broker VM. If you
remove a Primary node, an automatic failover occurs.

You can remove a Broker VM node from a cluster if the current node
**STATUS** is **Connected**.

1.  Select Settings \> Configurations \> Data Broker \> Broker VMs.

2.  In either the **Brokers** tab or **Clusters** tab, right-click a
    Broker VM node, and select **Remove from Cluster**.

3.  Follow the instructions in the dialog box, and click **Remove**.

- When removing the last node in the cluster, all applets in this
  cluster become **Inactive**, and the cluster becomes **Unavailable**.

  When the Broker VM receives the new configuration, the Broker VM
  becomes a standalone Broker VM with settings reset to default.

  > **Note**

  > If you\'ve enabled a **Load Balancer Health-Check** on the cluster,
  > you need to exclude this Broker VM from your Load Balancer settings.

#### Manage Broker VM data collector applets

After you activate a Broker VM data collector applet, you can make
additional changes as needed to the specific applet configured on the
Broker VM or cluster. Select Settings \> Configurations \> Data Broker
\> Broker VMs to view detailed information regarding your registered
Broker VMs in either the **Brokers** or **Clusters** tab. To modify a
configuration, left-click the Broker VM applet in the **APPS** column to
display the data collector applet settings and view detailed information
regarding your applet.

> **Note**
>
> For more information on the Broker VM applet connectivity status, see
> [Manage Broker VM](#UUID8b52177a86be69212a0bc06f47f5e1cb).

Configuration options available for all Broker VM data collector applets

The following options are available to select for all data collector
applets:

- **Configure**: Enables you to redefine the Broker VM data collector
  configurations.

- **Deactivate**: Disables the Broker VM data collector.

<!-- -->

- Cortex XSIAM provides the ability to maintain the Broker VM applet
  configurations whenever an applet is deactivated. This ensures that
  whenever the applet is reactivated the saved configuration is
  restored. When the dialog box is displayed to confirm deactivating the
  Broker VM data collector applet, leave the
  **Save applet configuration** checkbox selected (default) to maintain
  the applet configuration; otherwise, if this checkbox is unmarked, the
  applet configuration is deleted.

Configuration options available to specific Broker VM data collector
applets

The following additional options are only available to specific Broker
VM data collector applets:

- Network Mapper:

  - **Scan Now**: Initiates a scan.

- Windows Event Collector:

  - **Collection Configuration**: Enables you to view or edit existing
    events or add new events to the collect.

#### Broker VM High Availability Cluster

 

High availability (HA) is a deployment in which at least two Broker VMs
are placed in a Broker VM cluster and their configuration is
synchronized to prevent a single point of failure on your network at the
hardware and application level. A heartbeat connection between the
Broker VM nodes and the Cortex XSIAM Server ensures seamless failover if
a node fails. Setting up a HA cluster provides redundancy and enables
data collection continuity.

##### Cluster Architecture

The **Clusters** tab on the `Broker VMs` page enables you to view your
cluster configurations, which displays the associated nodes, node
statuses, applets configured, and applet statuses. You can add as many
clusters as you want in a tenant. Each Cortex XSIAM cluster can include
as many nodes as you need. The cluster operation is fully managed from
the tenant, and there is no need to install additional components. There
is no need for cluster nodes to communicate with one another on the
network. In each cluster, one Broker VM is designated as the Primary
cluster node and the rest of the nodes are designated as standby nodes.
The cluster architecture is dependent on the type of applets configured
in the cluster. Applets on cluster nodes run either in the active/active
mode or in the active/passive mode and exhibit different behaviors as
detailed in the table below.

Applet mode table

+-----------------------+-----------------------+-----------------------+
| Applet Mode           | Applet Behavior       | Applets               |
+=======================+=======================+=======================+
| active/active         | The applets that      | The active/active     |
|                       | operate in the        | applets are:          |
|                       | active/active mode    |                       |
|                       | listen simultaneously | - Syslog Collector    |
|                       | on all the nodes in   |                       |
|                       | the cluster to        | - Netflow Collector   |
|                       | achieve High          |                       |
|                       | Availability and Load | - Windows Event       |
|                       | Balancing. Failure of |   Collector           |
|                       | an applet on a        |                       |
|                       | particular node       | - Local Agent         |
|                       | causes all traffic to |   Settings            |
|                       | be redistributed to   |                       |
|                       | the remaining nodes   |                       |
|                       | in the HA cluster.    |                       |
|                       | Any applet that is a  |                       |
|                       | listener is           |                       |
|                       | active/active to      |                       |
|                       | ensure the source can |                       |
|                       | send data and anyone  |                       |
|                       | can pick it up based  |                       |
|                       | on availability.      |                       |
|                       |                       |                       |
|                       | > **Note**            |                       |
|                       | >                     |                       |
|                       | > For Load Balancing, |                       |
|                       | > you must install a  |                       |
|                       | > Load Balancer in    |                       |
|                       | > your network which  |                       |
|                       | > will distribute the |                       |
|                       | > incoming data       |                       |
|                       | > between the nodes.  |                       |
+-----------------------+-----------------------+-----------------------+
| active/passive        | The applets that      | The active/passive    |
|                       | operate in the        | applets are:          |
|                       | active/passive mode   |                       |
|                       | retrieve data from    | - Kafka Collector     |
|                       | the source, and run   |                       |
|                       | only on the Primary   | - Network Mapper      |
|                       | Node designated in    |                       |
|                       | the cluster. The      | - CSV Collector       |
|                       | other nodes are       |                       |
|                       | synchronized and      | - FTP Collector       |
|                       | ready to transition   |                       |
|                       | from standby to the   | - Files and Folders   |
|                       | active Primary Node   |   Collector           |
|                       | should there be a     |                       |
|                       | failover. In this     | - DB Collector        |
|                       | mode, all nodes share |                       |
|                       | the same              |                       |
|                       | configuration         |                       |
|                       | settings, while only  |                       |
|                       | one operates at a     |                       |
|                       | given time. Any       |                       |
|                       | applet that is going  |                       |
|                       | outbound and pulling  |                       |
|                       | data is               |                       |
|                       | active/passive as the |                       |
|                       | applet should only    |                       |
|                       | have only one active  |                       |
|                       | Primary Node at a     |                       |
|                       | point of time and the |                       |
|                       | rest of the nodes     |                       |
|                       | should be passive.    |                       |
+-----------------------+-----------------------+-----------------------+

> **Note**
>
> The Pathfinder applet isn\'t supported when configuring Broker VMs in
> HA clusters.

##### Automatic Failover

In each cluster, whenever there\'s a failure on the Primary node, Cortex
XSIAM automatically switches to one of the standby nodes, initiates the
applets on the new Primary node, and continues data collection on that
node. Any successful or unsuccessful failover attempt displays an issue
in the notification area and is logged in the **Management Audit Logs**
table.

The following conditions can trigger a failover for the Primary node:

- Connectivity issues between a Primary node and the Cortex XSIAM server

- Application failure, such as failing to start an applet or an applet
  crashes

- Any failure of one of the internal components, such as MariaDB, Redis,
  RabbitMQ, or Docker engine

- Hardware failure, including:

  - Running out of disk space

  - CPU usage of more than 95% for more than 10 minutes

  - Memory usage of more than 95% for more than 10 minutes

##### Manual Switchover

At any time, you can change the role of the current Primary node in the
cluster to another node in the HA cluster, for example, to perform
maintenance, by initiating a manual switchover.

##### Automatic Upgrades

You can configure automatic upgrades within Broker VM HA cluster nodes
to update cluster nodes without noticeable downtime or other disruption
of the HA cluster service by implementing the rolling upgrade mechanism.
An automatic upgrade is performed in the following order:

1.  Standby nodes are upgraded one by one.

2.  The Primary node is switched over to one of the upgraded standby
    nodes.

3.  The previous Primary node, now a standby node, is upgraded.

##### Configure High Availability Cluster

 

You can create a High Availability (HA) cluster by either creating a new
cluster from scratch and then adding applets and Broker VM nodes to the
cluster, or by creating a new cluster from an existing standalone Broker
VM. There is no limit to the number of clusters and nodes that you can
add.

There are a number of different ways that you can configure the HA
cluster to acheive fault tolerance depending on your system
requirements. For example, once a cluster is created from scratch, you
can start by configuring the applets that you want the cluster to
maintain and then adding the Broker VM nodes that will be managed by the
cluster to maintain this configuration, or vise versa. When you create a
new cluster from an existing Broker VM, the cluster inherits the applets
already configured, which can help save time with your cluster
configuration.

**Guidelines**

Note the following guidelines:

- For the cluster to start working and provide services, you need at
  least one operational node. Until this node is added, the cluster is
  unavailable. Once a node is added, the cluster begins operating, but
  it\'s not considered healthy. 

- For the cluster to be healthy and maintain HA and redundancy, you need
  at least two working nodes in the cluster.

- For active/active applets that require load balancing, you must
  install a Load Balancer in your network to distribute the incoming
  data between the nodes.

> **Prerequisite**
>
> Be sure you do the following tasks before creating a cluster from an
> existing Broker VM:

- > Since the Pathfinder applet isn\'t supported when configuring HA
  > clusters, you must ensure Pathfinder is deactivated on the Broker
  > VM.

- > If the Broker VM is explicitly specified in some Agent Settings
  > profile, which mean Cortex XSIAM agents retrieve release upgrades
  > and content updates from this Broker VM, you must change the Broker
  > VM\'s current designated role. To do this, you need to modify the
  > Agent Settings profile by removing the specific selection of this
  > broker as a Download Source for XDR agents (Endpoints \> Policy
  > Management \> Prevention \> Profiles \> Edit Profile \> Download
  > Source \> Broker Selection). After you create the cluster for this
  > broker, you can go back the Agent Settings profile and select the
  > cluster that you created from this broker to be used as a Download
  > Source for XDR agents.

Perform the following procedures in the order listed below.

###### Task 1. Open the Broker VMs page in Cortex XSIAM

Select Settings \> Configurations \> Data Broker \> Broker VMs.

###### Task 2: Determine how you want to create an HA cluster.

- To create a cluster and then add Broker VMs to the cluster, click
  **Add Cluster**.

- To create a new cluster from an existing Broker VM in the **Brokers**
  tab, right-click a standalone Broker VM, and click
  **Create a Cluster from this Broker**.

<!-- -->

- > **Important**

  - > You can only create a new cluster from an existing Broker VM, when
    > the Broker VM version is 19.0 and later, and the **STATUS** is
    > **Connected**.

  - > The **Create a Cluster from this Broker** option is only listed if
    > the Broker VM is not already added to a cluster.

###### Task 3. Set the applicable parameters

Define the following parameters:

Load Balancer FQDN

Specify the domain name of your Load Balancer FQDN as configured in your
local DNS server. The Load Balancer FQDN settings affect the Windows
Event Collector and Local Agent Settings applets.

When creating a cluster from an existing Broker VM and either a WEC or
Local Agent Settings applet are enabled in the Broker VM, the
**Load Balancer FQDN** is mandatory to configure, and is automatically
populated based on the Broker VM settings.

Load Balancer Health Check options

Implementing a Load Balancer requires exposing a health check API that
is called by the Load Balancer at regular intervals. You can access the
health check page by sending an HTTP request to
`http[s]://<Broker VM IP>:<port>/health/`. A successful HTTP response of
`200 OK` as the status code indicates the Broker VM's readiness to
receive logs.

Disabled/Enabled toggle

When **Disabled** the Load Balancer Health Check listening port is
blocked. When **Enabled** (default), the listening port is opened, and
you must define the Port number (default **8088**) and **Protocol**
(default `HTTP`).

> **Important**
>
> When the **Protocol** is set to **HTTPS**, you may need to perform a
> few follow-up steps to establish a validated secure SSL connection
> with the Broker VM.

- > If you\'re using your own Certificate Authority (CA) to sign the
  > certificates, you\'ll need to place the CA in the client, such as
  > the Load Balancer, and upload the certificates to the Broker VM.

- > If you\'re using a Trusted CA Signed SSL Certificate, you\'ll only
  > need to upload it to the Broker VM.

- > If the **SSL Server Certificates** of the Broker VM are self-signed
  > certificates, no further steps are necessary.

Auto Upgrade options

You can configure automatic upgrades within Broker VM HA cluster nodes
to update cluster nodes without noticeable down-time or other disruption
of the HA cluster service by implementing the rolling upgrade mechanism.
Setting automatic upgrades includes these parameters:

Auto Upgrade

In a HA cluster configuration, the rolling upgrades process is
automatically performed by default whenever a new version of the Broker
VM is available.

If you want to upgrade the Broker VM nodes manually, clear the
**Use Default (Enabled)** checkbox, and set **Auto Upgrade** to
**Disabled**. You can manually upgrade the Broker VM nodes individually
by right-clicking the Broker VM and selecting
**Upgrade Broker version**.

Days In Week

You can configure the days in the week that the rolling upgrades are
performed. By default, the upgrades are configured to run every day.

Schedule

You can configure whether the rolling upgrades are performed at any time
during the day or at a specific time by setting a time range of at least
4 hours.

Once configured, the rolling upgrades are only performed when the
cluster **STATUS** is **Healthy**. An automatic upgrade is performed in
the following order:

1.  Standby nodes are upgraded one by one.

2.  The Primary node is switched over to one of the upgraded standby
    nodes.

3.  The previous Primary node, now a standby node, is upgraded.

###### Task 4. Save your changes

Click **Save**.

The cluster is now listed in the **Clusters** tab of the **Broker VMs**
page, whose output differs depending on how the cluster was created:

New cluster added

When the cluster is added from scratch, the cluster is listed as an
empty folder, and you can start to add Broker VM nodes and applets to
this cluster. While the cluster doesn't have any peer nodes, the
**STATUS** is **Unavailable**.

Cluster added from an existing Broker VM

When the cluster is added from an existing Broker VM, the cluster
inherits all applet settings from the Broker VM. You can leave the
configuration as is or add/remove additional applets as desired. This
node automatically becomes the first node (Primary) in the cluster. You
can now add other Broker VM nodes to this HA cluster. While the cluster
contains only one Broker VM node, the **STATUS** is **Warning**.

###### Task 5. Add Broker VMs to your cluster as you require to achieve fault tolerance and high availability

For the cluster to be healthy and maintain HA and redundancy, you need
at least two working nodes in the cluster.

- To add Broker VM, see [Add Broker VM to
  cluster](#UUID13338c25905c5190a7b4e921d31b8c80).

- To add applets, see [Add applet to
  cluster](#UUIDfa3db3b357c8e9b52f6b637fb9c1e3a6).

##### Manage Broker VM clusters

After you\'ve configured a cluster, you can manage all your Broker VM
clusters from the **Clusters** tab on the `Broker VMs` page (Settings \>
Configurations \> Data Broker \> Broker VMs \> Clusters).

The **Clusters** tab displays in a heirarchical view the clusters with
their nodes, performance stats, applets configured, and the state of
each applet. You can right-click any cluster to open a menu listing the
tasks available management options.

###### View cluster details

The **Clusters** tab of the **Broker VMs** page (Settings \>
Configurations \> Data Broker \> Broker VMs) enables you to view
detailed information regarding your High Availability (HA) cluster.

The **Clusters** table enables you to monitor and mange your cluster
nodes and applets, and view stats.

In addition, when each cluster is expanded, a table is displayed, which
enables you to view detailed information regarding the various Broker VM
nodes that are currently added to your cluster. If you haven\'t added
any Broker VM nodes to a particular cluster, the table is empty.

####### Clusters Table

The following table describes all the fields that are available in the
Clusters table. You can hide any field column using the column manager.

+-----------------------------------+---------------------------------------------------+
| Fields                            | Description                                       |
+===================================+===================================================+
| CLUSTER NAME                      | Beside the full name of each cluster, a status    |
|                                   | indicator is displayed with one of the following  |
|                                   | colors:                                           |
|                                   |                                                   |
|                                   | - Green: **Healthy**, the Primary node and all    |
|                                   |   available standby nodes are connected and       |
|                                   |   operating with no warnings, and all activated   |
|                                   |   applets are running without a problem.          |
|                                   |                                                   |
|                                   | - Orange: **Warning** as the system has detected  |
|                                   |   errors in the cluster, but the applets can      |
|                                   |   still be running. For example, all applets are  |
|                                   |   running normally in the Primary Broker VM, but  |
|                                   |   no available standby nodes are detected, or the |
|                                   |   Primary node is operating fine, but there is an |
|                                   |   applet that failed to start in one of the       |
|                                   |   standby nodes. The errors must be addressed as  |
|                                   |   soon as possible.                               |
|                                   |                                                   |
|                                   | - Red: **Critical** as the system has detected    |
|                                   |   one or more critical errors in the cluster, and |
|                                   |   nodes are not able to run some applets. For     |
|                                   |   example, an error was detected in some Primary  |
|                                   |   applet and no standby node is available for     |
|                                   |   failover. All errors must be addressed as soon  |
|                                   |   as possible.                                    |
|                                   |                                                   |
|                                   | - Black: **Unavailable** as the cluster doesn't   |
|                                   |   have any peer nodes configured.                 |
+-----------------------------------+---------------------------------------------------+
| STATUS                            | Connection status of the cluster according to the |
|                                   | statuses and colors explained in the              |
|                                   | **CLUSTER NAME** field above.                     |
|                                   |                                                   |
|                                   | Unavailable clusters do not display               |
|                                   | **CPU USAGE**, **MEMORY USAGE**, and              |
|                                   | **DISK USAGE** information.                       |
|                                   |                                                   |
|                                   | Notifications about the cluster losing connection |
|                                   | between applets and Broker VM nodes appear in the |
|                                   | Notification Center.                              |
+-----------------------------------+---------------------------------------------------+
| CPU USAGE                         | Average CPU utilization between all nodes in the  |
|                                   | cluster as a percentage.                          |
+-----------------------------------+---------------------------------------------------+
| MEMORY USAGE                      | Sum of all memory in use out of the sum of the    |
|                                   | total memory on all nodes in the cluster as a     |
|                                   | percentage.                                       |
+-----------------------------------+---------------------------------------------------+
| DISK USAGE                        | Sum of all disk space in use out of the sum of    |
|                                   | the total disk space on all nodes in the cluster  |
|                                   | as a percentage.                                  |
+-----------------------------------+---------------------------------------------------+
| APPS                              | List of active applets and the connectivity       |
|                                   | status for each.                                  |
|                                   |                                                   |
|                                   | Colors depict the following statuses:             |
|                                   |                                                   |
|                                   | - Green (Connected): Indicates the applet has no  |
|                                   |   issues.                                         |
|                                   |                                                   |
|                                   | - Orange (Warning): Indicates the applet has      |
|                                   |   minor issues.                                   |
|                                   |                                                   |
|                                   | - Red (Error): Indicates the applet has errors.   |
|                                   |                                                   |
|                                   | - White (Inactive): Indicates the applet is       |
|                                   |   inactive.                                       |
|                                   |                                                   |
|                                   | > **Note**                                        |
|                                   | >                                                 |
|                                   | > For more information on troubleshooting errors  |
|                                   | > and warnings for these applets, see             |
|                                   | > [Troubleshoot Broker VM applet                  |
|                                   | > errors](#UUIDf260379f4b4fd8505baff065946fe056). |
+-----------------------------------+---------------------------------------------------+

####### Cluster Broker VM Nodes Table

The fields that are available in the Broker VM nodes table for each
cluster are similar to many of the fields that are displayed in the
table for the Broker VMs in the **Brokers** tab. For more information on
these fields, see [Manage Broker
VM](#UUID8b52177a86be69212a0bc06f47f5e1cb).

###### Edit cluster

After configuring a high availability (HA) cluster, you can always edit
the cluster configurations from the **Clusters** tab of the
**Broker VMs** page.

An HA cluster is always configurable no matter what the status of the
cluster or whether it has any Broker VM nodes added.

1.  Select Settings \> Configurations \> Data Broker \> Broker VMs, and
    select the **Clusters** tab.

2.  In the Clusters table, locate the cluster, right-click, and select
    **Configure**.

3.  In the **Cluster Configurations** window, you can edit the
    parameters based on your previous settings. For more information on
    each of these settings, see [Configure High Availability
    Cluster](#UUIDcdafbc5b0f072deba4bdea1f522d867d).

4.  Update the cluster with your changes.

###### Add applet to cluster

You can add an applet to a high availability (HA) cluster from the
**Clusters** tab of the **Brokers VM** page.

You can always add an applet to a cluster, even if the cluster status is
**Unavailable** or **Error**. When an applet is added to a cluster
without any Broker VM nodes, the cluster status is **Unavailable** and
the cluster **APPS** status displays as Inactive.

1.  Select Settings \> Configurations \> Data Broker \> Broker VMs, and
    select the **Clusters** tab.

2.  In the Clusters table, locate the cluster that you want to add an
    applet.

3.  You can either right-click the cluster, and select Add App \> \<name
    of applet\>, or in the **APPS** column, left-click Add \> \<name of
    applet\>.

- The applet is only available for you to add to the cluster if it
  hasn\'t already been added.

  > **Note**

  > With Cortex XDR Prevent, it\'s only relevant to configure a HA
  > cluster with a Local Agent Settings applet as this is the only
  > applet supported for this product license. The other applets are
  > collector applets, which are only available in Cortex XDR Pro or
  > Cortex XSIAM.

4.  Configure your applet.

- The various applets that you can configure are the same as when
  configuring a standalone Broker VM. For more information on a
  particular applet configuration, locate the applet in the Set up
  Broker VM section in the Cortex XSIAM Admin Guide.

  The applet is listed with a status indicator in the **APPS** column,
  where the colors depict the following statuses:

  - Green (Connected): Indicates the applet has no issues.

  - Orange (Warning): Indicates the applet has minor issues.

  - Red (Error): Indicates the applet has errors.

  - White (Inactive): Indicates the applet is inactive.

  > **Note**

  > For more information on troubleshooting errors and warnings for
  > these applets, see [Troubleshoot Broker VM applet
  > errors](#UUIDf260379f4b4fd8505baff065946fe056).

  Once the applet configuration is changed in a cluster, the changes are
  automatically applied to the cluster nodes depending on the applet and
  cluster node role. For example, if you add the Kafka Collector, which
  is an \"active/passive\" applet, the applet is automatically initiated
  and enters an active state on the Primary node and is on standby on
  the standby nodes. While if you add the Syslog Collector
  \"active/active\" applet, the changes automatically propagate so that
  the applet is active on all cluster nodes, including Primary and
  standby.

###### Add Broker VM to cluster

You can add standalone Broker VMs to a high availability (HA) cluster
from either the **Brokers** tab or **Clusters** tab.

You can only add a Broker VM to a cluster, when the Broker VM version is
19.0 and later, the **STATUS** is **Connected**, and the Broker VM
version isn\'t older than the cluster version.

Once you add a Broker VM to a cluster, the Broker VM becomes a cluster
node and is added to the cluster folder in the **Clusters** tab. If it
is the only peer Broker VM in the cluster, it is designated as the
Primary node; otherwise, it is designated as a standby node.

1.  Select Settings \> Configurations \> Data Broker \> Broker VMs.

2.  Add a Broker VM in one of the following tabs:

- Brokers tab
  1.  Right-click a standalone Broker VM, and select
      **Add Broker to Cluster**.

  2.  In the **Select Cluster** field, choose the cluster that you want
      this Broker VM to be added to.

  Clusters tab
  1.  Right-click a cluster node, and select **Add Broker to Cluster**.

  2.  In the **Select broker** field, choose the standalone Broker VM
      that you want to add to this cluster.

3.  Click **Add Broker**.

- Adding a Broker VM to a cluster overrides all previous Broker VM
  settings and disables all active applets on this Broker VM. When the
  Broker VM is added to a cluster, the cluster configuration and cluster
  applet settings propagate to the Broker VM. The state of the applets
  on the Broker VM is dependent on the [applet
  mode](#UUID23bf9c984626b51101b7e7fee66538b8) and Broker VM node role
  in the cluster. When the operation completes, a notification is added
  to the Notification Center.

###### Remove cluster

You can remove a high availability (HA) cluster in the **Clusters** tab
of the **Broker VMs** page.

When removing a cluster, the cluster is disassembled and the cluster
object is deleted. All nodes in the cluster are reverted back to
standalone Broker VMs with their settings reset to default as a newly
created Broker VMs.

If you\'ve configured load balancing for any \"active/active\" applets
configured, you need to update your Load Balancer configuration settings
to stop sending logs to these Broker VM nodes.

You cannot remove a cluster that is used as a download source from which
the Cortex XSIAM agents retrieve release upgrades and content updates.
You\'ll need to change the cluster\'s current designated role before
removing a cluster.

1.  Select Settings \> Configurations \> Data Broker \> Broker VMs.

2.  In the **Clusters** tab, right-click a cluster, and select
    **Remove Cluster**.

3.  Follow the instructions in the **REMOVE CLUSTER** window, whose
    instructions differ depending on the type of cluster you are trying
    to remove, and **Remove** the cluster.

#### Broker VM notifications

To help you monitor your Broker VM version, connectivity, and high
availability clusters, Cortex XSIAM sends notifications to your Cortex
XSIAM console Notification Center.

Cortex XSIAM sends the following notifications:

Add Cluster

Notifies when a cluster was added.

Applet Activated

Notifies when an applet is activated on a cluster.

Applet configuration

Notifies when an applet on a cluster configuration was updated.

Applet Deactivated

Notifies when an applet is deactivates on a cluster.

Broker VM Connectivity

Notifies when the Broker VM has lost connectivity to Cortex XSIAM .

Broker VM Disk Usage

Notifies when the Broker VM is utilizing over 90% of the allocated disk
space.

Cluster Configuration

- Notifies when a Broker VM node was added to a cluster.

- Notifies when a Broker VM node was removed from a cluster.

- Notifies when the configuration for the cluster needs to be set.

Cluster failover

- Notifies when a failover is initiated in the cluster from one Broker
  VM node to another.

- Notifies when a failover completed successfully. The Broker VM is now
  Primary in the cluster.

- Notifies when a failover in the cluster completed with errors and
  error message.

- Notifies when couldn\'t perform a failover in the cluster as there is
  no available standby node with sufficient redundancy.

Cluster health declined

- Notifies when failed to detect an available standby Broker VM node in
  the cluster.

- Notifies when critical errors detected in the cluster and there is no
  available standby Broker VM node for failover.

Cluster health recovered

Notifies when detected an available standby Broker VM node in the
cluster.

Disk space allocation on \<name of broker\> broker

Notifies whether the disk space allocated for data caching in the Broker
VM has been increased successfully. If not, the notification includes
the errors encountered during the process. For more information on
allocating disk space to the Broker VM, see [Increase Broker VM storage
allocated for data caching](#UUID6d23c0dc17ebf6d797a12705e2bd1b79).

\<Device ID\> Broker VM requires a reboot

Notifies after a Broker VM update whether a broker needs a reboot to
finish installing important updates.

New Broker VM Version

Notifies when a new Broker VM version has been released.

- If the Broker VM Auto Upgrade is disabled, the notification includes a
  link to the latest release information. It is recommend you upgrade to
  the latest version.

- If the Broker VM Auto Upgrade is enabled, 12 hours after the release
  you are notified of the latest upgrade, or you are notified that the
  upgrade failed. In such a case, open a Palo Alto Networks Support
  Ticket.

Reinstall Broker VM \<Broker VM name\> with a new image

For all brokers that were deployed with an old Broker VM image,
downloaded prior to July 9th, 2023 (installed with Ubuntu 18.04 or
earlier), the Broker VM must be reinstalled with a new image (installed
with Ubuntu 20.04 or later) before upgrading to the latest version. The
name of the Broker VM to upgrade is indicated with a link to the
instructions.

> **Note**
>
> For more information on upgrading to a new Broker VM image, see
> [Migrating to a New Broker VM
> Image](https://docs-cortex.paloaltonetworks.com/r/Cortex-XSIAM/Cortex-XSIAM-Migration-Process-for-New-Broker-VM-Image).

Remove Cluster

Notifies when a cluster was removed.

To ensure you stay informed about Broker VM activity, you can also
configure notification forwarding to forward your Broker audit logs to
an email distribution list or Syslog server. For more information about
the Broker VM audit logs, see Broker VM Activity in the Cortex XSIAM
Administrator Guide.

#### Monitor Broker VM activity

Cortex XSIAM logs entries for events related to the Broker VM monitored
activities. Cortex XSIAM stores the logs for 365 days. To view the
Broker VM audit logs, select Settings \> Management Audit Logs.

To ensure you and your colleagues stay informed about Broker VM
activity, you can [Configure notification
forwarding](#UUID3738ce324545c768e170afa247d5abc2) to forward your
Broker VM audit logs to an email distribution list or Syslog server.

You can customize your view of the logs by adding or removing filters to
the **Management Audit Logs** table. You can also filter the page result
to narrow down your search. The following table describes the default
and optional fields that you can view in the Cortex XSIAM
**Management Audit Logs** table:

> **Note**
>
> Certain fields are exposed and hidden by default. An asterisk (\*) is
> beside every field that is exposed by default.

+-----------------------------------+-----------------------------------+
| Field                             | Description                       |
+===================================+===================================+
| Description\*                     | Log message that describes the    |
|                                   | action.                           |
+-----------------------------------+-----------------------------------+
| Email                             | Email of the user who performed   |
|                                   | the action.                       |
+-----------------------------------+-----------------------------------+
| Host Name\*                       | Name of any relevant affected     |
|                                   | hosts.                            |
+-----------------------------------+-----------------------------------+
| ID                                | Unique ID of the action.          |
+-----------------------------------+-----------------------------------+
| Reason                            | This field is not applicable for  |
|                                   | Broker VM logs.                   |
+-----------------------------------+-----------------------------------+
| Result\*                          | The result of the action (        |
|                                   | `Success`, `Fail`, or `N/A`)      |
+-----------------------------------+-----------------------------------+
| Severity\*                        | Severity associated with the log: |
|                                   |                                   |
|                                   | - `Critical`                      |
|                                   |                                   |
|                                   | - `High`                          |
|                                   |                                   |
|                                   | - `Medium`                        |
|                                   |                                   |
|                                   | - `Low`                           |
|                                   |                                   |
|                                   | - `Informational`                 |
+-----------------------------------+-----------------------------------+
| Timestamp\*                       | Date and time when the action     |
|                                   | occurred.                         |
+-----------------------------------+-----------------------------------+
| Type\* and Sub-Type\*             | Additional classifications of     |
|                                   | Broker VM logs (Type and          |
|                                   | Sub-Type):                        |
|                                   |                                   |
|                                   | - **Broker VMs**:                 |
|                                   |                                   |
|                                   |   - Action on device              |
|                                   |                                   |
|                                   |   - Add Cluster                   |
|                                   |                                   |
|                                   |   - Applet Activated              |
|                                   |                                   |
|                                   |   - Applet Configuration          |
|                                   |                                   |
|                                   |   - Applet connection_test Action |
|                                   |                                   |
|                                   |   - Applet Deactivated            |
|                                   |                                   |
|                                   |   - Applet License Expired        |
|                                   |                                   |
|                                   |   - Applet Mount Share Action     |
|                                   |                                   |
|                                   |   - Applet Mount Share Test       |
|                                   |     Action                        |
|                                   |                                   |
|                                   |   - Applet preview Action         |
|                                   |                                   |
|                                   |   - Applet Scan Now Action        |
|                                   |                                   |
|                                   |   - Applet Set Configuration      |
|                                   |                                   |
|                                   |   - Applet Unmount All Shares     |
|                                   |     Action                        |
|                                   |                                   |
|                                   |   - Authentication succeeded      |
|                                   |                                   |
|                                   |   - Broker Log                    |
|                                   |                                   |
|                                   |   - Cluster Configuration         |
|                                   |                                   |
|                                   |   - Cluster Failover              |
|                                   |                                   |
|                                   |   - Cluster health declined       |
|                                   |                                   |
|                                   |   - Cluster health recovered      |
|                                   |                                   |
|                                   |   - Cluster Switchover            |
|                                   |                                   |
|                                   |   - Device configuration          |
|                                   |                                   |
|                                   |   - Disconnect                    |
|                                   |                                   |
|                                   |   - Register                      |
|                                   |                                   |
|                                   |   - Remove Cluster                |
|                                   |                                   |
|                                   |   - Remove Device                 |
|                                   |                                   |
|                                   |   - Rolling Upgrades              |
|                                   |                                   |
|                                   |   - Subscription Created          |
|                                   |                                   |
|                                   |   - Subscription Deleted          |
|                                   |                                   |
|                                   |   - Subscription Edited           |
|                                   |                                   |
|                                   | - **Broker API**:                 |
|                                   |                                   |
|                                   |   - Authentication failed         |
+-----------------------------------+-----------------------------------+
| User Name\*                       | Name of the user who performed    |
|                                   | the action.                       |
+-----------------------------------+-----------------------------------+

#### Troubleshoot Broker VM applet errors

You can monitor the Broker VM applet status from the **Broker VM** page
in the **Apps** column. For all Broker VM applets in both the
**Brokers** and **Clusters** tabs, a status indicator icon indicates
whether the applet is connected or has an error. Certain Broker VM
applets provide more information to help you troubleshoot by providing
specific errors and warnings. For each of these errors and warnings, a
different root cause is provided with a recommended action so that you
can easily resolve the problem. In addition, you can always monitor your
Broker VM application, connectivity, and processing errors for supported
applets using the Cortex Query Language (XQL) and the
`collection_auditing` dataset, and by creating correlation rules to
trigger collection health issues.

Where can I see if I have an issue with a Broker VM applet?

On the **Broker VM** page, Broker VM applets with an error status
display an error icon in the **Apps** column in both the **Brokers** and
**Clusters** tabs. In distinct supported cases, detailed error
descriptions are provided for some applets:

- Green (Connected): Indicates the applet has no issues.

- Orange (Warning): Indicates the applet has minor issues, such as
  processing errors.

- Red (Error): Indicates the applet has application or connectivity
  errors.

In addition, you can also left-click the applet, to get a brief summary
of the applet error or warning.

Where can I trace the status changes of a Broker VM applet?

Each status change of a supported Broker VM applet is logged in the
`collection_auditing` dataset. Querying this dataset can help you see
all the connectivity changes of an instance over time, the escalation or
recovery of the connectivity status, and the error, warning, and
informational messages related to status changes.

You can use the `collection_auditing` dataset to monitor your supported
Broker VMs applets. For example, by creating correlation rules to
trigger collection health alerts, you can ensure that you are notified
whenever an applet receives an error or warning. You can then query the
`collection_auditing` dataset to understand what error was thrown, and
then use the troubleshooting table provided in
*Understand how to troubleshoot* to resolve the problem. Once the
problem is resolved, you can ensure the Broker VM applet is active again
by querying the `collection_auditing` dataset, or by observing the
Connected green status of the applet in the **Broker VM** page in the
**Apps** column. The example below explains the different status changes
on the Local Agent Setting that can be used to help you troubleshoot the
applet\'s connectivity issues.

> **Note**
>
> For more information on creating correlation rules to trigger
> collection health issues, see [XSIAMHow can I set up correlation rules
> to trigger collection health
> issues?](#Xf67c0113faa2f81b71c37ed952fa0398539b347).

This example searches for status changes of the Local Agent Settings
applet, where the `_broker_device_id` is `LVVTKM6S`:

    dataset = collection_auditing 
    |filter  collector_type = "Local Agent Settings" and _broker_device_id = "LVVTKM6S"

Output results:

The results indicate that the Local Agent Settings applet was active on
Jan 10th 2025 16:25:21. On Jan 18th 2025 11:15:26, this applet had a
connectivity error relating to an inaccessible URL, and this issue was
resolved on Jan 18th 2025 16:27:26 when the applet was back to an active
status.

  ----------------------------------------------------------------------------------------------------------------------------------------
  COLLECTOR_TYPE   INSTANCE   CLASSIFICATION   DESCRIPTION    \_BROKER_DEVICE_ID   \_BROKER_DEVICE_NAME   \_BROKER_IP_ADDRESS   \_TIME
  ---------------- ---------- ---------------- -------------- -------------------- ---------------------- --------------------- ----------
  Local Agent      LVVTKM6S   INFORMATION      The applet is  LVVTKM6S             LVVTKM6S               155.11.23.22          Jan 18th
  Settings                                     back to an                                                                       2025
                                               active status                                                                    16:27:26

  Local Agent      LVVTKM6S   ERROR            There is at    LVVTKM6S             LVVTKM6S               155.11.23.22          Jan 18th
  Settings                                     least one                                                                        2025
                                               inaccessible                                                                     11:15:26
                                               url. The                                                                         
                                               broker VM                                                                        
                                               needs to                                                                         
                                               communicate                                                                      
                                               with the same                                                                    
                                               URLs that the                                                                    
                                               agents                                                                           
                                               communicate                                                                      
                                               with.                                                                            

  Local Agent      LVVTKM6S   INFORMATION      The applet is  LVVTKM6S             LVVTKM6S               155.11.23.22          Jan 10th
  Settings                                     active                                                                           2025
                                                                                                                                16:25:21
  ----------------------------------------------------------------------------------------------------------------------------------------

How can I set up correlation rules to trigger collection health issues?

You can create correlation rules that are based on the fields in the
`collection_auditing` dataset, so you are notified whenever a Broker VM
applet status changes to an error and warning.

Example: Trigger collection issues for error statuses on the Local Agent
Settings applet

In this example, a correlation rule triggers an issue if the Local Agent
Settings collector changes to an error status.

Example XQL:

    dataset = collection_auditing 
    |filter classification = "Error" and collector_type = "Local Agent Settings" and _broker_device_name = "LVVTKM6S"

Additional fields to specify in the correlation rule:

  --------------------------------------------------------------------------
  Field                               Value
  ----------------------------------- --------------------------------------
  Time Schedule                       Hourly

  Query time frame                    1 Hour

  Issue Suppression                   Select **Enable issue suppression**.

  Action                              Select **Generate issue**.

  Issue Domain                        Health

  Severity                            Medium

  Type                                Collection
  --------------------------------------------------------------------------

Understand how to troubleshoot

To help you troubleshoot your supported Broker VM applets, the table
below lists the different possible warning and error event types,
including the applicable error or warning that is displayed when you
left-click the applet on the Broker VM page, the description displayed
in the `collection_auditing` dataset, the root cause of the problem, and
the recommended action to resolve the problem. We recommend that you use
this table as a first resource to troubleshoot your application,
connectivity, and processing errors.

+-----------+-----------+---------------------+---------------------+----------------------------------------------------+----------------------------------------------------+
| Applet    | Event     | Broker VM page      | Description in the  | Root Cause                                         | Recommended Action                                 |
|           | Type      | message             | collection_auditing |                                                    |                                                    |
|           |           |                     | dataset             |                                                    |                                                    |
+===========+===========+=====================+=====================+====================================================+====================================================+
| Database  | Error     | Could not connect   | Could not connect   | An issue occurred when the collector tried to      | 1.  Check database connectivity and credentials.   |
| Collector |           | to the database.    | to the database.    | establish an initial connection to the specified   |                                                    |
|           |           | driver: \<driver\>, |                     | database.                                          | 2.  Check database for refused connections and     |
|           |           | server: \<server\>, |                     |                                                    |     reasons.                                       |
|           |           | port: \<port\>,     |                     |                                                    |                                                    |
|           |           | database:           |                     |                                                    | 3.  If the problem persists, contact support for   |
|           |           | \<database\>.       |                     |                                                    |     further assistance.                            |
+-----------+-----------+---------------------+---------------------+----------------------------------------------------+----------------------------------------------------+
| Database  | Error     | Could not run the   | Could not run one   | An issue occurred while executing one of the       | 1.  Check the query syntax and database            |
| Collector |           | query. driver:      | of the configured   | configured queries, possibly due to syntax or      |     permissions.                                   |
|           |           | \<driver\>, server: | queries.            | permissions.                                       |                                                    |
|           |           | \<server\>, port:   |                     |                                                    | 2.  Ensure all fields are properly defined.        |
|           |           | \<port\>, database: |                     |                                                    |                                                    |
|           |           | \<database\>, query |                     |                                                    | 3.  Try running the query directly on the          |
|           |           | title: \<query      |                     |                                                    |     database.                                      |
|           |           | title\>.            |                     |                                                    |                                                    |
|           |           |                     |                     |                                                    | 4.  If the problem persists, contact support for   |
|           |           |                     |                     |                                                    |     further assistance.                            |
+-----------+-----------+---------------------+---------------------+----------------------------------------------------+----------------------------------------------------+
| Database  | Warning   | The query has been  | One of the queries  | A long-running query was detected. This may        | 1.  Consider optimizing the query.                 |
| Collector |           | running for over    | has been running    | indicate inefficiencies in the query logic or      |                                                    |
|           |           | \<X\> minutes.      | for too long.       | large data retrieval.                              | 2.  Check database performance.                    |
|           |           | driver: \<driver\>, |                     |                                                    |                                                    |
|           |           | server: \<server\>, |                     |                                                    | 3.  Try running the query directly on the          |
|           |           | port: \<port\>,     |                     |                                                    |     database. Check runtime and verify the issue   |
|           |           | database:           |                     |                                                    |     is the query and not the collector.            |
|           |           | \<database\>, query |                     |                                                    |                                                    |
|           |           | title: \<query      |                     |                                                    | 4.  If the problem persists, contact support for   |
|           |           | title\>.            |                     |                                                    |     further assistance.                            |
+-----------+-----------+---------------------+---------------------+----------------------------------------------------+----------------------------------------------------+
| Database  | Warning   | Could not parse the | Could not parse the | The rising column initial value failed to parse    | 1.  Validate the initial value has a valid format. |
| Collector |           | initial value.      | initial value for a | when initializing the query parameters from the    |                                                    |
|           |           | driver: \<driver\>, | query.              | configuration.                                     | 2.  If the problem persists, contact support for   |
|           |           | server: \<server\>, |                     |                                                    |     further assistance.                            |
|           |           | port: \<port\>,     |                     |                                                    |                                                    |
|           |           | database:           |                     |                                                    |                                                    |
|           |           | \<database\>, query |                     |                                                    |                                                    |
|           |           | title: \<query      |                     |                                                    |                                                    |
|           |           | title\>, initial    |                     |                                                    |                                                    |
|           |           | value:              |                     |                                                    |                                                    |
|           |           | \<initial_value\>.  |                     |                                                    |                                                    |
+-----------+-----------+---------------------+---------------------+----------------------------------------------------+----------------------------------------------------+
| Database  | Error     | Could not write the | Could not write     | The collector encountered an error while trying to | This issue can occur intermittently. If the        |
| Collector |           | state to server     | data to the server  | write data to the server\'s cache.                 | problem persists, contact support for further      |
|           |           | cache.              | cache.              |                                                    | assistance.                                        |
+-----------+-----------+---------------------+---------------------+----------------------------------------------------+----------------------------------------------------+
| Database  | Error     | Could not stream    | Could not stream    | An issue occurred where results could not be       | This issue can occur intermittently. If the        |
| Collector |           | the collector       | results to the      | streamed to the tenant due to an internal          | problem persists, contact support for further      |
|           |           | results to the      | server.             | component failure.                                 | assistance.                                        |
|           |           | server.             |                     |                                                    |                                                    |
+-----------+-----------+---------------------+---------------------+----------------------------------------------------+----------------------------------------------------+
| Files and | Warning   | The total file size | The total folder    | An issue that occurs when the total folder size    | This issue occurs when the total folder size       |
| Folders   |           | in the folder       | size exceeded the   | exceeds the allowed 30MB limit in **Replace** mode | exceeds the allowed 30MB limit in **Replace** mode |
| Collector |           | \<folder_path\> for | allowed 30MB limit  | for the **Storage Method** as defined in the       | for the **Storage Method** as defined in the       |
|           |           | file type           | in replace mode.    | **Data Source Mapping**. For more information, see | **Data Source Mapping**. For more information, see |
|           |           | \<file_type\> has   |                     | [Activate Files and Folders                        | [Activate Files and Folders                        |
|           |           | exceeded the        |                     | Collector](#UUIDddd09123bfe9bf7a4a72aec8fb890d9e). | Collector](#UUIDddd09123bfe9bf7a4a72aec8fb890d9e). |
|           |           | allowed 30 MB limit |                     |                                                    |                                                    |
|           |           | in replace mode     |                     |                                                    |                                                    |
+-----------+-----------+---------------------+---------------------+----------------------------------------------------+----------------------------------------------------+
| Files and | Error     | Failed to stream    | Could not write     | An issue occurred where the collector failed to    | This issue can occur intermittently. If the        |
| Folders   |           | collector results   | data to server      | stream data to an internal component, preventing   | problem persists, contact support for further      |
| Collector |           | to server           | cache.              | it from reaching the tenant.                       | assistance.                                        |
+-----------+-----------+---------------------+---------------------+----------------------------------------------------+----------------------------------------------------+
| Files and | Error     | Invalid mount       | Invalid mount point | An issue that occurs when an invalid mount point   | 1.  Make sure the path configured is the correct   |
| Folders   |           | detected at path    | detected.           | is detected.                                       |     path and there are no permissions issues.      |
| Collector |           | \<folder_path\>     |                     |                                                    |                                                    |
|           |           |                     |                     |                                                    | 2.  If the problem persists, contact support for   |
|           |           |                     |                     |                                                    |     further assistance.                            |
+-----------+-----------+---------------------+---------------------+----------------------------------------------------+----------------------------------------------------+
| Local     | Warning   | Failed to download  | Failed to download  | An issue that occurs while downloading the         | If the problem persists, contact support for       |
| Agent     |           | and store           | and store           | content/installer and extracting it.               | further assistance.                                |
| Settings  |           | \<package_type\>    | content/installer   |                                                    |                                                    |
|           |           | (version:           | for agent package   |                                                    |                                                    |
|           |           | \<version\>, os:    | caching.            |                                                    |                                                    |
|           |           | \<os\>) inside the  |                     |                                                    |                                                    |
|           |           | Broker VM for agent |                     |                                                    |                                                    |
|           |           | package caching     |                     |                                                    |                                                    |
+-----------+-----------+---------------------+---------------------+----------------------------------------------------+----------------------------------------------------+
| Local     | Warning   | The Broker VM       | Failed to process   | Broker VM failed to get the list of                | If the problem persists, contact support for       |
| Agent     |           | failed to process   | request of agent    | content/installer from the tenant.                 | further assistance.                                |
| Settings  |           | request of agent    | package cache       |                                                    |                                                    |
|           |           | package cache       | update.             |                                                    |                                                    |
|           |           | update              |                     |                                                    |                                                    |
|           |           | \<package_type\>.   |                     |                                                    |                                                    |
+-----------+-----------+---------------------+---------------------+----------------------------------------------------+----------------------------------------------------+
| Local     | Error     | Inaccessible        | There is at least   | An issue with the accessibility to a URL as the    | 1.  Make sure the URL is not blocked by the        |
| Agent     |           | url:\<url\> The     | one inaccessible    | Broker VM needs to communicate with the same URLs  |     firewall.                                      |
| Settings  |           | Broker VM needs to  | URL. The Broker VM  | as the Agents.                                     |                                                    |
|           |           | communicate with    | needs to            |                                                    | 2.  Try running a curl command on the URL marked   |
|           |           | the same URLs as    | communicate with    |                                                    |     as inaccessible inside the VM: `curl -I <URL>` |
|           |           | the Agents.         | the same URLs that  |                                                    |                                                    |
|           |           |                     | the agents          |                                                    | 3.  Contact support for further assistance         |
|           |           |                     | communicate with.   |                                                    |                                                    |
+-----------+-----------+---------------------+---------------------+----------------------------------------------------+----------------------------------------------------+
| Local     | Warning   | The disk space      | The disk space      | Agent installer and content caching exceeds 90% of | If you intend to use the Broker VM for agent       |
| Agent     |           | allocated for agent | allocated for agent | the allocated disk space for.                      | installer and content caching, you must use extra  |
| Settings  |           | installer and       | installer and       |                                                    | disk space. For more information, [Increase Broker |
|           |           | content caching     | content caching     |                                                    | VM storage allocated for data                      |
|           |           | exceeds 90%         | exceeds 90%.        |                                                    | caching](#UUID6d23c0dc17ebf6d797a12705e2bd1b79).   |
|           |           | (\<number\> GB).    |                     |                                                    |                                                    |
+-----------+-----------+---------------------+---------------------+----------------------------------------------------+----------------------------------------------------+
| Local     | Error     | No more disk space  | No more disk space  | Agent installer and content caching exceeds 100%   | If you intend to use the Broker VM for agent       |
| Agent     |           | available for agent | available for agent | of the allocated disk space for.                   | installer and content caching, you must use extra  |
| Settings  |           | installer and       | installer and       |                                                    | disk space. For more information, [Increase Broker |
|           |           | content caching.    | content caching.    |                                                    | VM storage allocated for data                      |
|           |           |                     |                     |                                                    | caching](#UUID6d23c0dc17ebf6d797a12705e2bd1b79).   |
+-----------+-----------+---------------------+---------------------+----------------------------------------------------+----------------------------------------------------+
| Syslog    | Warning   | First log over the  | The initial         | The Syslog collector applet was configured to      | Check the logs transmitted over the connection and |
| Collector |           | connection doesn\'t | received log over   | receive logs of a specific type, and the logs that | ensure they match the log type specified in the    |
|           |           | have the expected   | the connection does | were received through the configured port are      | configuration.                                     |
|           |           | type. Log format:   | not match the       | either of a different type or contain errors.      |                                                    |
|           |           | \<received          | expected log        |                                                    |                                                    |
|           |           | format\>, expected  | format.             |                                                    |                                                    |
|           |           | format: \<expected  |                     |                                                    |                                                    |
|           |           | format\>. The       |                     |                                                    |                                                    |
|           |           | connection has been |                     |                                                    |                                                    |
|           |           | closed by the       |                     |                                                    |                                                    |
|           |           | collector.          |                     |                                                    |                                                    |
|           |           |                     |                     |                                                    |                                                    |
|           |           | vendor: \<vendor\>, |                     |                                                    |                                                    |
|           |           | product:            |                     |                                                    |                                                    |
|           |           | \<product\>,        |                     |                                                    |                                                    |
|           |           | source: \<source    |                     |                                                    |                                                    |
|           |           | ip\>, port:         |                     |                                                    |                                                    |
|           |           | \<destination       |                     |                                                    |                                                    |
|           |           | port\>.             |                     |                                                    |                                                    |
+-----------+-----------+---------------------+---------------------+----------------------------------------------------+----------------------------------------------------+
| Syslog    | Warning   | Log size exceeded   | A log entry         | The issue is caused by either a log exceeding 65   | Ensure the logs are within the size limit and      |
| Collector |           | 65 KB for \<source  | exceeded the        | KB or logs missing an end-of-line delimiter, which | include an end-of-line delimiter.                  |
|           |           | ip\>, on port:      | maximum allowed     | causes them to aggregate as \'partial logs\' and   |                                                    |
|           |           | \<dest port\>       | size.               | hit the 65 KB limit.                               |                                                    |
|           |           | (vendor:            |                     |                                                    |                                                    |
|           |           | \<vendor\>,         |                     |                                                    |                                                    |
|           |           | product:            |                     |                                                    |                                                    |
|           |           | \<product\>). This  |                     |                                                    |                                                    |
|           |           | may happen if a     |                     |                                                    |                                                    |
|           |           | single log entry is |                     |                                                    |                                                    |
|           |           | too large or if     |                     |                                                    |                                                    |
|           |           | logs lack           |                     |                                                    |                                                    |
|           |           | end-of-line         |                     |                                                    |                                                    |
|           |           | delimiters, causing |                     |                                                    |                                                    |
|           |           | them to merge and   |                     |                                                    |                                                    |
|           |           | exceed the size     |                     |                                                    |                                                    |
|           |           | limit.              |                     |                                                    |                                                    |
+-----------+-----------+---------------------+---------------------+----------------------------------------------------+----------------------------------------------------+
| Syslog    | Warning   | Connection rejected | Connection rejected | The configuration includes specific networks/IPs   | This is an informational event and does not impact |
| Collector |           | from \<source ip\>. | due to a source IP  | to send logs to a specific port of the Syslog      | the data collector applet\'s operation, except for |
|           |           | Port \<dest port\>  | restriction.        | collector applet. The Syslog collector applet      | rejecting the SRC IP connection. Examine the       |
|           |           | is restricted to    |                     | detected a `src IP` that tried to send logs        | `srcIP` detected by the data collector applet and  |
|           |           | specific source     |                     | through the port, which isn\'t in the allowed      | verify its origin.                                 |
|           |           | IPs, and the        |                     | list.                                              |                                                    |
|           |           | provided IP is not  |                     |                                                    |                                                    |
|           |           | on the allowed      |                     |                                                    |                                                    |
|           |           | list.               |                     |                                                    |                                                    |
+-----------+-----------+---------------------+---------------------+----------------------------------------------------+----------------------------------------------------+
| Syslog    | Warning   | Failed to parse log | Log parsing failed  | The port was configured to receive logs in a CEF   | Ensure that the logs are in the correct CEF        |
| Collector |           | from \<source ip\>  | due to an           | format, and the logs that were sent from the log   | format. If it does not match the format type,      |
|           |           | on port \<dest      | unexpected format.  | exporter were either of a different type or        | consider using \'RAW\' type logs in the            |
|           |           | port\> (vendor:     |                     | contain errors.                                    | configuration.                                     |
|           |           | \<vendor\>,         |                     |                                                    |                                                    |
|           |           | product:            |                     |                                                    |                                                    |
|           |           | \<product\>). The   |                     |                                                    |                                                    |
|           |           | log does not match  |                     |                                                    |                                                    |
|           |           | the expected format |                     |                                                    |                                                    |
|           |           | \'CEF\', as         |                     |                                                    |                                                    |
|           |           | configured for this |                     |                                                    |                                                    |
|           |           | port. Consider      |                     |                                                    |                                                    |
|           |           | using the \'RAW\'   |                     |                                                    |                                                    |
|           |           | type to ingest logs |                     |                                                    |                                                    |
|           |           | in their current    |                     |                                                    |                                                    |
|           |           | format.             |                     |                                                    |                                                    |
+-----------+-----------+---------------------+---------------------+----------------------------------------------------+----------------------------------------------------+
| Syslog    | Warning   | Failed to parse log | Log parsing failed  | The port was configured to receive logs in a LEEF  | Ensure that the logs are in the correct LEEF       |
| Collector |           | from \<source ip\>  | due to an           | format, and the logs that were sent from the log   | format. If it does not match the type, consider    |
|           |           | on port \<dest      | unexpected format.  | exporter were either of a different type or        | using \'RAW\' type logs in the configuration.      |
|           |           | port\> (vendor:     |                     | contain errors.                                    |                                                    |
|           |           | \<vendor\>,         |                     |                                                    |                                                    |
|           |           | product:            |                     |                                                    |                                                    |
|           |           | \<product\>). The   |                     |                                                    |                                                    |
|           |           | log does not match  |                     |                                                    |                                                    |
|           |           | the expected format |                     |                                                    |                                                    |
|           |           | \'LEEF\', as        |                     |                                                    |                                                    |
|           |           | configured for this |                     |                                                    |                                                    |
|           |           | port. Consider      |                     |                                                    |                                                    |
|           |           | using the \'RAW\'   |                     |                                                    |                                                    |
|           |           | type to ingest logs |                     |                                                    |                                                    |
|           |           | in their current    |                     |                                                    |                                                    |
|           |           | format.             |                     |                                                    |                                                    |
+-----------+-----------+---------------------+---------------------+----------------------------------------------------+----------------------------------------------------+
| Syslog    | Warning   | Unable to           | Unable to           | The configuration is set to \'auto-detect\' for    | Ensure that the logs being sent have one of the    |
| Collector |           | automatically       | automatically       | the log type, and the Syslog collector applet      | four supported log types: CEF, LEEF, Cisco, or     |
|           |           | detect the initial  | detect the initial  | can\'t detect the first log over the connection    | Corelight. If it does not match any of the types,  |
|           |           | log over the        | log over the        | from the four known log types: CEF, LEEF,          | consider using \'RAW\' type logs in the            |
|           |           | connection for      | connection, the log | Corelight, and Cisco.                              | configuration.                                     |
|           |           | vendor: \<vendor\>, | will be discarded.  |                                                    |                                                    |
|           |           | product:            |                     | The Syslog collector applet does attempt a second  |                                                    |
|           |           | \<product\>,        |                     | try on the second log over the connection. If this |                                                    |
|           |           | source: \<source    |                     | attempt fails as well, then the Syslog collector   |                                                    |
|           |           | ip\>, port: \<dest  |                     | uses the \'RAW\' type log for all the logs over    |                                                    |
|           |           | port\>. the log     |                     | this specific connection.                          |                                                    |
|           |           | will be discarded.  |                     |                                                    |                                                    |
+-----------+-----------+---------------------+---------------------+----------------------------------------------------+----------------------------------------------------+
| Syslog    | Warning   | Failed to parse     | Failed to process   | The log exporter is sending the logs using a       | Ensure that the logs are being sent correctly      |
| Collector |           | octet-framing       | the packet due to a | octet-framing method, and the Syslog collector is  | using the octet-framing method.                    |
|           |           | packet from         | missing length      | experiencing problems with the structure/format    |                                                    |
|           |           | \<source ip\> on    | header. The packet  | that is being sent.                                |                                                    |
|           |           | port \<port\>       | does not conform to |                                                    |                                                    |
|           |           | (vendor:            | the expected        |                                                    |                                                    |
|           |           | \<vendor\>,         | octet-framing       |                                                    |                                                    |
|           |           | product:            | format.             |                                                    |                                                    |
|           |           | \<product\>).       |                     |                                                    |                                                    |
|           |           | Please verify the   |                     |                                                    |                                                    |
|           |           | packet structure    |                     |                                                    |                                                    |
|           |           | and format.         |                     |                                                    |                                                    |
+-----------+-----------+---------------------+---------------------+----------------------------------------------------+----------------------------------------------------+
| Syslog    | Error     | Failed loading      | Failed to load a    | An issue with the certificates added by the user   | Verify the settings for the certificates and keys. |
| Collector |           | client key from the | client key from the | to the Syslog collector\'s configuration.          |                                                    |
|           |           | configuration on    | configuration.      |                                                    |                                                    |
|           |           | port \<dest port\>. |                     |                                                    |                                                    |
|           |           | Please verify the   |                     |                                                    |                                                    |
|           |           | certificate and key |                     |                                                    |                                                    |
|           |           | settings.           |                     |                                                    |                                                    |
+-----------+-----------+---------------------+---------------------+----------------------------------------------------+----------------------------------------------------+
| Syslog    | Error     | Failed to process   | Failed to parse the | An issue with the certificates added by the user   | Verify the settings for the certificates and keys. |
| Collector |           | CA certificate from | CA certificate from | to the Collector\'s configuration.                 |                                                    |
|           |           | the `.pem` file     | the provided        |                                                    |                                                    |
|           |           | configured on port  | configuration.      |                                                    |                                                    |
|           |           | \<dest port\>.      |                     |                                                    |                                                    |
|           |           | Please verify the   |                     |                                                    |                                                    |
|           |           | certificate and key |                     |                                                    |                                                    |
|           |           | settings.           |                     |                                                    |                                                    |
+-----------+-----------+---------------------+---------------------+----------------------------------------------------+----------------------------------------------------+
| WEC       | Error     | Failed starting WEC | Failed starting WEC | A rare error that can occur due to an internal     | Deactivate the WEC, and then reactivate it. The    |
|           |           | applet due to an    | applet due to an    | program issue.                                     | WEC configuration will be saved, and the collector |
|           |           | internal error. The | internal error.     |                                                    | will restart. If the error persists, contact       |
|           |           | applet would        |                     |                                                    | support for further assistance.                    |
|           |           | require a restart   |                     |                                                    |                                                    |
|           |           | to restore          |                     |                                                    |                                                    |
|           |           | functionality.      |                     |                                                    |                                                    |
+-----------+-----------+---------------------+---------------------+----------------------------------------------------+----------------------------------------------------+
| WEC /     | Error     | The collector could | Failed to establish | The persistence service for the data collector     | If the problem persists, contact support for       |
| Syslog    |           | not connect to the  | an initial          | applet, is not functioning properly. As a result,  | further assistance.                                |
| Collector |           | third-party         | connection to the   | the applet is unable to operate since this service |                                                    |
|           |           | persistence         | persistence         | is essential for its functionality.                |                                                    |
|           |           | service. This issue | service.            |                                                    |                                                    |
|           |           | must be resolved to |                     |                                                    |                                                    |
|           |           | restore             |                     |                                                    |                                                    |
|           |           | functionality.      |                     |                                                    |                                                    |
+-----------+-----------+---------------------+---------------------+----------------------------------------------------+----------------------------------------------------+
| WEC /     | Error     | The collector is    | Unable to read from | The persistence service for the applets, is not    | The data collector applet is unable to read from   |
| Syslog    |           | unable to read from | the persistence     | functioning properly. As a result, the applet is   | the persistence service. This issue must be        |
| Collector |           | the persistence     | service.            | unable to operate since this service is essential  | resolved to restore functionality.                 |
|           |           | service. This issue |                     | for its functionality.                             |                                                    |
|           |           | must be resolved to |                     |                                                    |                                                    |
|           |           | restore             |                     |                                                    |                                                    |
|           |           | functionality.      |                     |                                                    |                                                    |
+-----------+-----------+---------------------+---------------------+----------------------------------------------------+----------------------------------------------------+
| WEC /     | Error     | The collector could | Unable to connect   | The applet can\'t send logs to the tenant due to   | Review the network setup and check for any         |
| Syslog    |           | not reach the       | to the receptor for | some network problem.                              | firewalls, proxies, or other network components    |
| Collector |           | receptor (Tenant)   | data transmission.  |                                                    | that could potentially cause interruptions.        |
|           |           | to transmit data.   |                     |                                                    |                                                    |
|           |           | This issue must be  |                     |                                                    |                                                    |
|           |           | resolved to restore |                     |                                                    |                                                    |
|           |           | functionality.      |                     |                                                    |                                                    |
+-----------+-----------+---------------------+---------------------+----------------------------------------------------+----------------------------------------------------+

### XDR Collectors

> **Note**
>
> Ingestion of log events larger than 5 MB is not supported.

Cortex XSIAM provides an XDR Collectors (XDRC) configuration that is
dedicated for on-premise data collection on Windows and Linux machines.
The XDRC includes a dedicated installer, a collector upgrade
configuration, content updates, and policy management. The XDRC is a
data collector that gathers and processes logs and events from multiple
sources. It leverages Elasticsearch Filebeat, a lightweight log shipper,
to collect log data from various systems and applications. Additionally,
Winlogbeat gathers Windows event logs, ensuring comprehensive visibility
into Windows environments. These components facilitate centralized
analysis, threat detection, and investigation across the Cortex XSIAM
ecosystem.

#### XDR Collector audit logs

Cortex XSIAM logs entries for events related to the XDR Collector
monitored activities. Cortex XSIAM stores the logs for 365 days. To view
the XDR Collector audit logs, select Settings \> XDR Collector Audit
Logs.

#### XDR Collector machine requirements and supported operating systems

You can configure XDR Collectors that are dedicated for on-premise data
collection on Windows and Linux machines. The following hardware and
software specifications are required for the collector machines.

+-----------------------+-----------------------+--------------------------------------------------+
| Machine operating     | Requirement           | Specifications                                   |
| system                |                       |                                                  |
+=======================+=======================+==================================================+
| *Linux*               | Processor             | 2.3 GHz dual-core                                |
+-----------------------+-----------------------+--------------------------------------------------+
|                       | RAM                   | 4GB; 8GB recommended                             |
+-----------------------+-----------------------+--------------------------------------------------+
|                       | Hard disk space       | 10GB                                             |
+-----------------------+-----------------------+--------------------------------------------------+
|                       | Architecture          | x86 64-bit                                       |
+-----------------------+-----------------------+--------------------------------------------------+
|                       | Kernel version        | 2.6.32                                           |
+-----------------------+-----------------------+--------------------------------------------------+
|                       | Supported operating   | - Red Hat Enterprise Linux 6 (6.7 and later)     |
|                       | system versions       |                                                  |
|                       |                       | - Red Hat Enterprise Linux 7                     |
|                       |                       |                                                  |
|                       |                       | - Red Hat Enterprise Linux 8                     |
|                       |                       |                                                  |
|                       |                       | - Red Hat Enterprise Linux 9                     |
|                       |                       |                                                  |
|                       |                       | - SUSE Linux Enterprise Server 12                |
|                       |                       |                                                  |
|                       |                       | - SUSE Linux Enterprise Server 15 SP0            |
|                       |                       |                                                  |
|                       |                       | - SUSE Linux Enterprise Server 15 SP1            |
|                       |                       |                                                  |
|                       |                       | - SUSE Linux Enterprise Server 15 SP2            |
|                       |                       |                                                  |
|                       |                       | - SUSE Linux Enterprise Server 15 SP3            |
|                       |                       |                                                  |
|                       |                       | - SUSE Linux Enterprise Server 15 SP4            |
|                       |                       |                                                  |
|                       |                       | - SUSE Linux Enterprise Server 15 SP5            |
|                       |                       |                                                  |
|                       |                       | - SUSE Linux Enterprise Server 15 SP6            |
|                       |                       |                                                  |
|                       |                       | - Ubuntu Server 12                               |
|                       |                       |                                                  |
|                       |                       | - Ubuntu Server 14                               |
|                       |                       |                                                  |
|                       |                       | - Ubuntu Server 16                               |
|                       |                       |                                                  |
|                       |                       | - Ubuntu Server 18                               |
|                       |                       |                                                  |
|                       |                       | - Ubuntu Server 20                               |
|                       |                       |                                                  |
|                       |                       | - Ubuntu Server 22                               |
|                       |                       |                                                  |
|                       |                       | - Oracle Linux 6 (6.7 and later)                 |
|                       |                       |                                                  |
|                       |                       | - Oracle Linux 7                                 |
|                       |                       |                                                  |
|                       |                       | - Oracle Linux 8                                 |
|                       |                       |                                                  |
|                       |                       | - Oracle Linux 9                                 |
+-----------------------+-----------------------+--------------------------------------------------+
|                       | Software packages     | - Verify you have standard Unix programs         |
|                       |                       |   installed.                                     |
|                       |                       |                                                  |
|                       |                       | - ca-certificates                                |
|                       |                       |                                                  |
|                       |                       | - openssl 1.0.0 or a later release               |
|                       |                       |                                                  |
|                       |                       | - Distributions with SELinux in enforcing or     |
|                       |                       |   permissive mode:                               |
|                       |                       |                                                  |
|                       |                       |   - Red Hat Enterprise Linux 6 and Oracle Linux  |
|                       |                       |     6: policycoreutils-python                    |
|                       |                       |                                                  |
|                       |                       |   - Red Hat Enterprise Linux 7 and Oracle Linux  |
|                       |                       |     7: policycoreutils-python and                |
|                       |                       |     selinux-policy-devel                         |
|                       |                       |                                                  |
|                       |                       |   - SUSE: policycoreutils-python and             |
|                       |                       |     selinux-policy-devel                         |
|                       |                       |                                                  |
|                       |                       |   - Debian and Ubuntu: policycoreutils and       |
|                       |                       |     selinux-policy-dev                           |
+-----------------------+-----------------------+--------------------------------------------------+
|                       | Networking            | - Allow communication from the XDR Collector TCP |
|                       |                       |   port to the server (the default is port 443).  |
+-----------------------+-----------------------+--------------------------------------------------+
| *Windows*             | Processor             | - Intel Pentium 4 or later with SSE2 instruction |
|                       |                       |   set support                                    |
|                       |                       |                                                  |
|                       |                       | - AMD Opteron/Athlon 64 or later with SSE2       |
|                       |                       |   instruction set support                        |
|                       |                       |                                                  |
|                       |                       | - Dual core processor (minimum)                  |
+-----------------------+-----------------------+--------------------------------------------------+
|                       | RAM                   | 2GB minimum                                      |
+-----------------------+-----------------------+--------------------------------------------------+
|                       | Hard disk space       | 200MB minimum; 20GB recommended                  |
+-----------------------+-----------------------+--------------------------------------------------+
|                       | Supported operating   | **XDR Collector (XDRC) version 1.4.3 and later** |
|                       | system versions       |                                                  |
|                       |                       | - Windows 8                                      |
|                       |                       |                                                  |
|                       |                       |   - 8.1 (and with FIPS mode)                     |
|                       |                       |                                                  |
|                       |                       |   - Embedded 8.1 Professional (Supported until   |
|                       |                       |     January 2023)                                |
|                       |                       |                                                  |
|                       |                       | - Windows Server                                 |
|                       |                       |                                                  |
|                       |                       |   - 2012 R2 (Supported until January 2026), All  |
|                       |                       |     editions; FIPS mode                          |
|                       |                       |                                                  |
|                       |                       |   - Core option (Windows Server 2012 R2 only)    |
|                       |                       |                                                  |
|                       |                       | **XDR Collector (XDRC) version 1.5.0 and later** |
|                       |                       |                                                  |
|                       |                       | - Windows 10                                     |
|                       |                       |                                                  |
|                       |                       |   - Education                                    |
|                       |                       |                                                  |
|                       |                       |   - Pro (CB and CBB)                             |
|                       |                       |                                                  |
|                       |                       |   - Enterprise (CB, CBB, and LTSB)               |
|                       |                       |                                                  |
|                       |                       |   - Updates 21H2, 21H1, 20H2, 2004, 1709, 1909,  |
|                       |                       |     1903, 1809, 1803 (Enterprise and             |
|                       |                       |     Professional)                                |
|                       |                       |                                                  |
|                       |                       |   - Updates 22H2, 22H1                           |
|                       |                       |                                                  |
|                       |                       |   - Enterprise 2019 LTSC                         |
|                       |                       |                                                  |
|                       |                       |   - Windows 10 IoT Core                          |
|                       |                       |                                                  |
|                       |                       |   - Windows 10 IoT Enterprise                    |
|                       |                       |                                                  |
|                       |                       | - Windows 11                                     |
|                       |                       |                                                  |
|                       |                       |   - Windows 11                                   |
|                       |                       |                                                  |
|                       |                       |   - Updates 22H2, 22H1                           |
|                       |                       |                                                  |
|                       |                       |   - Pro/Pro Education/Pro Workstations           |
|                       |                       |                                                  |
|                       |                       |   - Enterprise                                   |
|                       |                       |                                                  |
|                       |                       |   - Education/Home                               |
|                       |                       |                                                  |
|                       |                       |   - IoT Enterprise                               |
|                       |                       |                                                  |
|                       |                       | - Windows Server                                 |
|                       |                       |                                                  |
|                       |                       |   - Datacenter                                   |
|                       |                       |                                                  |
|                       |                       |   - 2012 (Supported until October 2026), 2012 R2 |
|                       |                       |     (Supported until January 2026), All          |
|                       |                       |     editions; FIPS mode                          |
|                       |                       |                                                  |
|                       |                       |   - 2016 (Standard edition; Server with Desktop  |
|                       |                       |     experience, previously known as Server with  |
|                       |                       |     a GUI)                                       |
|                       |                       |                                                  |
|                       |                       |   - 2016 Datacenter edition                      |
|                       |                       |                                                  |
|                       |                       |   - 2019                                         |
|                       |                       |                                                  |
|                       |                       |   - Core option (Windows Server 2012, 2012 R2,   |
|                       |                       |     and 2016 only)                               |
|                       |                       |                                                  |
|                       |                       |   - 2019 Standard (Server Core)                  |
|                       |                       |                                                  |
|                       |                       |   - 2022                                         |
+-----------------------+-----------------------+--------------------------------------------------+
|                       | Networking            | - Allow communication from the XDR Collector TCP |
|                       |                       |   port to the server (the default is port 443).  |
+-----------------------+-----------------------+--------------------------------------------------+
|                       | Applications and      | - Windows Accessories (Notepad) to view logs     |
|                       | utilities             |                                                  |
+-----------------------+-----------------------+--------------------------------------------------+

#### Resources required to enable access to XDR Collectors

To enable access to XDR Collectors components, you must allow access to
various Palo Alto Networks resources. If you use the specific Palo Alto
Networks
[App-IDs](https://docs.paloaltonetworks.com/pan-os/8-1/pan-os-admin/app-id)
indicated in the table, you do not need to explicitly allow access to
the resource. A dash (-) indicates there is no App-ID coverage for a
resource.

> **Note**
>
> Some of the IP addresses required for access are registered in the
> United States. As a result, some GeoIP databases do not correctly
> pinpoint the location in which IP addresses are used. All customer
> data is stored in your deployment region, regardless of the IP address
> registration and restricts data transmission through any
> infrastructure to that region. For considerations, see
> [/document/preview/914342#UUID-5822a3de-172f-230e-83e3-642121116a63](/document/preview/914342#UUID-5822a3de-172f-230e-83e3-642121116a63).
>
> **Note**
>
> Throughout this topic, `<xsiam-tenant>` refers to the chosen subdomain
> of your Cortex XSIAM tenant and `<region>` is the region in which your
> Strata Logging Service is deployed.

Refer to the following tables for the FQDNs, IP addresses, ports, and
App-ID coverage for your deployment.

For IP address ranges in GCP, refer to the following tables for IP
address coverage for your deployment.

- <https://www.gstatic.com/ipranges/goog.json>: Refer to this list to
  look up and allow access to the IP address ranges subnets.

- <https://www.gstatic.com/ipranges/cloud.json>: Refer to this list to
  look up and allow access to the IP address ranges associated with your
  region.

The following table shows the required resources by region.

+---------------------------------------------------------+-----------------------+----------------------------+
| FQDN                                                    | IP addresses and port | App-ID coverage            |
+=========================================================+=======================+============================+
| `<xsiam-tenant>.xdr.<region>.paloaltonetworks.com`      | IP address by region: | `cortex-xdr`               |
|                                                         |                       |                            |
| Used to connect to the Cortex XSIAM management console. | - US (United States): |                            |
|                                                         |   35.244.250.18       |                            |
|                                                         |                       |                            |
|                                                         | - EU (Europe):        |                            |
|                                                         |   35.227.237.180      |                            |
|                                                         |                       |                            |
|                                                         | - CA (Canada):        |                            |
|                                                         |   34.120.31.199       |                            |
|                                                         |                       |                            |
|                                                         | - UK (United          |                            |
|                                                         |   Kingdom):           |                            |
|                                                         |   34.120.87.77        |                            |
|                                                         |                       |                            |
|                                                         | - JP (Japan):         |                            |
|                                                         |   35.241.28.254       |                            |
|                                                         |                       |                            |
|                                                         | - SG (Singapore):     |                            |
|                                                         |   34.117.211.129      |                            |
|                                                         |                       |                            |
|                                                         | - AU (Australia):     |                            |
|                                                         |   34.120.229.65       |                            |
|                                                         |                       |                            |
|                                                         | - DE (Germany):       |                            |
|                                                         |   34.98.68.183        |                            |
|                                                         |                       |                            |
|                                                         | - IN (India):         |                            |
|                                                         |   35.186.207.80       |                            |
|                                                         |                       |                            |
|                                                         | - CH (Switzerland):   |                            |
|                                                         |   34.111.6.153        |                            |
|                                                         |                       |                            |
|                                                         | - PL (Poland):        |                            |
|                                                         |   34.117.240.208      |                            |
|                                                         |                       |                            |
|                                                         | - TW (Taiwan):        |                            |
|                                                         |   34.160.28.41        |                            |
|                                                         |                       |                            |
|                                                         | - QT (Qatar):         |                            |
|                                                         |   35.190.0.180        |                            |
|                                                         |                       |                            |
|                                                         | - FA (France):        |                            |
|                                                         |   34.111.134.57       |                            |
|                                                         |                       |                            |
|                                                         | - IL (Israel):        |                            |
|                                                         |   34.111.129.144      |                            |
|                                                         |                       |                            |
|                                                         | - SA (Saudi Arabia):  |                            |
|                                                         |   35.244.157.127      |                            |
|                                                         |                       |                            |
|                                                         | - ID (Indonesia):     |                            |
|                                                         |   34.111.58.152       |                            |
|                                                         |                       |                            |
|                                                         | - ES (Spain):         |                            |
|                                                         |   34.111.188.248      |                            |
|                                                         |                       |                            |
|                                                         | - IT (Italy):         |                            |
|                                                         |   34.8.224.70         |                            |
|                                                         |                       |                            |
|                                                         | - KR (South Korea):   |                            |
|                                                         |   34.54.5.247         |                            |
|                                                         |                       |                            |
|                                                         | - ZA (South Africa):  |                            |
|                                                         |   34.149.165.12       |                            |
|                                                         |                       |                            |
|                                                         | Port: 443             |                            |
+---------------------------------------------------------+-----------------------+----------------------------+
| `distributions.traps.paloaltonetworks.com`              | - IP address:         | `traps-management-service` |
|                                                         |   35.223.6.69         |                            |
| Used for the first request in registration flow where   |                       |                            |
| the agent passes the distribution id and obtains the    | - Port: 443           |                            |
| `ch-<xsiam-tenant>.traps.paloaltonetworks.com` of its   |                       |                            |
| tenant.                                                 |                       |                            |
+---------------------------------------------------------+-----------------------+----------------------------+
| `panw-xdr-installers-prod-us.storage.googleapis.com`    | - IP ranges in GCP    | `cortex-xdr`               |
|                                                         |                       |                            |
| Used to download installers for upgrade actions from    | - Port: 443           |                            |
| the server.                                             |                       |                            |
|                                                         |                       |                            |
| *This storage bucket is used for all regions.*          |                       |                            |
+---------------------------------------------------------+-----------------------+----------------------------+
| `global-content-profiles-policy.storage.googleapis.com` | - IP ranges in GCP    | `cortex-xdr`               |
|                                                         |                       |                            |
| Used to download content updates.                       | - Port: 443           |                            |
+---------------------------------------------------------+-----------------------+----------------------------+
| `ch-<xsiam-tenant>.traps.paloaltonetworks.com`          | IP address by region: | `traps-management-service` |
|                                                         |                       |                            |
| Used for all other requests between the agent and its   | - US (United States): |                            |
| tenant server including heartbeat, uploads, action      |   34.98.77.231        |                            |
| results, and scan reports.                              |                       |                            |
|                                                         | - EU (Europe):        |                            |
|                                                         |   34.102.140.103      |                            |
|                                                         |                       |                            |
|                                                         | - CA (Canada):        |                            |
|                                                         |   34.96.120.25        |                            |
|                                                         |                       |                            |
|                                                         | - UK (United          |                            |
|                                                         |   Kingdom):           |                            |
|                                                         |   35.244.133.254      |                            |
|                                                         |                       |                            |
|                                                         | - JP (Japan):         |                            |
|                                                         |   34.95.66.187        |                            |
|                                                         |                       |                            |
|                                                         | - SG (Singapore):     |                            |
|                                                         |   34.120.142.18       |                            |
|                                                         |                       |                            |
|                                                         | - AU (Australia):     |                            |
|                                                         |   34.102.237.151      |                            |
|                                                         |                       |                            |
|                                                         | - DE (Germany):       |                            |
|                                                         |   34.107.161.143      |                            |
|                                                         |                       |                            |
|                                                         | - IN (India):         |                            |
|                                                         |   34.120.213.188      |                            |
|                                                         |                       |                            |
|                                                         | - CH (Switzerland):   |                            |
|                                                         |   34.149.180.250      |                            |
|                                                         |                       |                            |
|                                                         | - PL (Poland):        |                            |
|                                                         |   35.190.13.237       |                            |
|                                                         |                       |                            |
|                                                         | - TW (Taiwan):        |                            |
|                                                         |   34.149.248.76       |                            |
|                                                         |                       |                            |
|                                                         | - QT (Qatar):         |                            |
|                                                         |   34.107.129.254      |                            |
|                                                         |                       |                            |
|                                                         | - FA (France):        |                            |
|                                                         |   34.36.155.211       |                            |
|                                                         |                       |                            |
|                                                         | - IL (Israel):        |                            |
|                                                         |   34.128.157.130      |                            |
|                                                         |                       |                            |
|                                                         | - SA (Saudi Arabia):  |                            |
|                                                         |   34.107.213.85       |                            |
|                                                         |                       |                            |
|                                                         | - ID (Indonesia):     |                            |
|                                                         |   34.128.156.84       |                            |
|                                                         |                       |                            |
|                                                         | - ES (Spain):         |                            |
|                                                         |   34.120.102.147      |                            |
|                                                         |                       |                            |
|                                                         | - IT (Italy):         |                            |
|                                                         |   34.8.234.58         |                            |
|                                                         |                       |                            |
|                                                         | - KR (South Korea):   |                            |
|                                                         |   34.54.155.245       |                            |
|                                                         |                       |                            |
|                                                         | - ZA (South Africa):  |                            |
|                                                         |   35.190.79.68        |                            |
|                                                         |                       |                            |
|                                                         | Port: 443             |                            |
+---------------------------------------------------------+-----------------------+----------------------------+
| `api-<xsiam-tenant>.xdr.<region>.paloaltonetworks.com`  | IP address by region: | \-                         |
|                                                         |                       |                            |
| Used for API requests and responses.                    | - US (United States): |                            |
|                                                         |   35.222.81.194       |                            |
|                                                         |                       |                            |
|                                                         | - EU (Europe):        |                            |
|                                                         |   34.90.67.58         |                            |
|                                                         |                       |                            |
|                                                         | - CA (Canada):        |                            |
|                                                         |   35.203.82.121       |                            |
|                                                         |                       |                            |
|                                                         | - UK (United          |                            |
|                                                         |   Kingdom):           |                            |
|                                                         |   34.89.56.78         |                            |
|                                                         |                       |                            |
|                                                         | - JP (Japan):         |                            |
|                                                         |   34.84.125.129       |                            |
|                                                         |                       |                            |
|                                                         | - SG (Singapore):     |                            |
|                                                         |   34.87.83.144        |                            |
|                                                         |                       |                            |
|                                                         | - AU (Australia):     |                            |
|                                                         |   35.189.18.208       |                            |
|                                                         |                       |                            |
|                                                         | - DE (Germany):       |                            |
|                                                         |   34.107.57.23        |                            |
|                                                         |                       |                            |
|                                                         | - IN (India):         |                            |
|                                                         |   35.200.158.164      |                            |
|                                                         |                       |                            |
|                                                         | - CH (Switzerland):   |                            |
|                                                         |   34.65.248.119       |                            |
|                                                         |                       |                            |
|                                                         | - PL (Poland):        |                            |
|                                                         |   34.116.216.55       |                            |
|                                                         |                       |                            |
|                                                         | - TW (Taiwan):        |                            |
|                                                         |   35.234.8.249        |                            |
|                                                         |                       |                            |
|                                                         | - QT (Qatar):         |                            |
|                                                         |   34.18.46.240        |                            |
|                                                         |                       |                            |
|                                                         | - FA (France):        |                            |
|                                                         |   34.155.222.152      |                            |
|                                                         |                       |                            |
|                                                         | - IL (Israel):        |                            |
|                                                         |   34.165.156.139      |                            |
|                                                         |                       |                            |
|                                                         | - SA (Saudi Arabia):  |                            |
|                                                         |   34.166.58.79        |                            |
|                                                         |                       |                            |
|                                                         | - ID (Indonesia):     |                            |
|                                                         |   34.128.115.238      |                            |
|                                                         |                       |                            |
|                                                         | - ES (Spain):         |                            |
|                                                         |   34.175.30.176       |                            |
|                                                         |                       |                            |
|                                                         | - IT (Italy):         |                            |
|                                                         |   34.154.195.120      |                            |
|                                                         |                       |                            |
|                                                         | - KR (South Korea):   |                            |
|                                                         |   34.64.54.175        |                            |
|                                                         |                       |                            |
|                                                         | - ZA (South Africa):  |                            |
|                                                         |   34.35.64.191        |                            |
|                                                         |                       |                            |
|                                                         | Port: 443             |                            |
+---------------------------------------------------------+-----------------------+----------------------------+
| **Log forwarding to a syslog receiver**                 |                       |                            |
+---------------------------------------------------------+-----------------------+----------------------------+
| See [Integrate a syslog                                 |                       |                            |
| receiver](#UUID218d87cf0fa5bccd27b3bb119b0567ea) for    |                       |                            |
| information about log forwarding IP addresses per       |                       |                            |
| region for syslog receivers.                            |                       |                            |
+---------------------------------------------------------+-----------------------+----------------------------+

The following table lists the required resources for Federal (United
States - Government).

+-----------------------------------------------------------------+-------------------+----------------------------+----------------------------------------------------+
| FQDN                                                            | IP addresses and  | App-ID coverage            | Required for XDR Collectors                        |
|                                                                 | port              |                            |                                                    |
+=================================================================+===================+============================+====================================================+
| `distributions-prod-fed.traps.paloaltonetworks.com`             | - IP address:     | `traps-management-service` | ![](media/rId635.png){width="0.2777777777777778in" |
|                                                                 |   104.198.132.24  |                            | height="0.2777777777777778in"}                     |
| Used for the first request in registration flow where the agent |                   |                            |                                                    |
| passes the distribution ID and obtains the                      | - Port: 443       |                            |                                                    |
| `ch-<xsiam-tenant>.traps.paloaltonetworks.com` of its tenant.   |                   |                            |                                                    |
+-----------------------------------------------------------------+-------------------+----------------------------+----------------------------------------------------+
| `panw-xdr-installers-prod-fr.storage.googleapis.com`            | - IP ranges in    | `cortex-xdr`               | ![](media/rId635.png){width="0.2777777777777778in" |
|                                                                 |   GCP             |                            | height="0.2777777777777778in"}                     |
| Used to download installers for upgrade actions from the        |                   |                            |                                                    |
| server.                                                         | - Port: 443       |                            |                                                    |
+-----------------------------------------------------------------+-------------------+----------------------------+----------------------------------------------------+
| `global-content-profiles-policy-prod-fr.storage.googleapis.com` | - IP ranges in    | `cortex-xdr`               | ![](media/rId635.png){width="0.2777777777777778in" |
|                                                                 |   GCP             |                            | height="0.2777777777777778in"}                     |
| Used to download content updates.                               |                   |                            |                                                    |
|                                                                 | - Port: 443       |                            |                                                    |
+-----------------------------------------------------------------+-------------------+----------------------------+----------------------------------------------------+
| `ch-<xsiam-tenant>.traps.paloaltonetworks.com`                  | - IP address:     | `traps-management-service` | ![](media/rId635.png){width="0.2777777777777778in" |
|                                                                 |   130.211.195.231 |                            | height="0.2777777777777778in"}                     |
| Used for all other requests between the agent and its tenant    |                   |                            |                                                    |
| server including heartbeat, uploads, action results, and scan   | - Port: 443       |                            |                                                    |
| reports.                                                        |                   |                            |                                                    |
+-----------------------------------------------------------------+-------------------+----------------------------+----------------------------------------------------+
| `api-<xsiam-tenant>.xdr.federal.paloaltonetworks.com`           | - IP address:     | \-                         | ![](media/rId635.png){width="0.2777777777777778in" |
|                                                                 |   130.211.195.231 |                            | height="0.2777777777777778in"}                     |
| Used for API requests and responses.                            |                   |                            |                                                    |
|                                                                 | - Port: 443       |                            |                                                    |
+-----------------------------------------------------------------+-------------------+----------------------------+----------------------------------------------------+
| **Log forwarding to a syslog receiver**                         |                   |                            |                                                    |
+-----------------------------------------------------------------+-------------------+----------------------------+----------------------------------------------------+
| See [Integrate a syslog                                         |                   |                            |                                                    |
| receiver](#UUID218d87cf0fa5bccd27b3bb119b0567ea) for            |                   |                            |                                                    |
| information about log forwarding IP addresses per region for    |                   |                            |                                                    |
| syslog receivers.                                               |                   |                            |                                                    |
+-----------------------------------------------------------------+-------------------+----------------------------+----------------------------------------------------+

#### Manage XDR Collectors

On the **XDR Collectors Administration** page, you can view the list of
collectors and perform additional tasks such as changing the alias of
the collector, upgrading the collector version, and setting a proxy
address and port for the collector.

##### XDR Collectors installation resource for Windows and Linux

The following table provides important information about the XDR
Collectors installation for Windows and Linux.

+-----------------+------------------------------------------------------------+------------------+-----------------------------+
| Installation    | Default path                                               | Description      | Related files/Services      |
| component       |                                                            |                  |                             |
+=================+============================================================+==================+=============================+
| Installation    | - **Windows**:                                             | The default      | - **Windows**               |
| folder          |                                                            | installation     |                             |
|                 | <!-- -->                                                   | path for the XDR |   - Service name:           |
|                 |                                                            | Collector.       |     `XDR Collector`         |
|                 | - `%PROGRAMFILES%\Palo Alto Networks\XDR Collector`        | Contains all     |                             |
|                 |                                                            | Program Core     |   - Process name:           |
|                 | <!-- -->                                                   | files and        |     `xdrcollectorsvc.exe`   |
|                 |                                                            | executables.     |                             |
|                 | - **Linux**:                                               |                  | - **Linux**                 |
|                 |                                                            |                  |                             |
|                 | <!-- -->                                                   |                  |   - Service name: `xcd`     |
|                 |                                                            |                  |                             |
|                 | - `/opt/paloaltonetworks/xdr-collector`                    |                  |   - Process name:           |
|                 |                                                            |                  |     `xdr-collector.service` |
+-----------------+------------------------------------------------------------+------------------+-----------------------------+
| Logs            | - **Windows**:                                             | - **Windows**:   | - **Windows**               |
|                 |                                                            |   Contains the   |                             |
|                 | <!-- -->                                                   |   XDR Collector  |   - `scouter.log`           |
|                 |                                                            |   application    |                             |
|                 | - `%PROGRAMDATA%\XDR Collector\logs`                       |   Log, the       |   - `filebeat`              |
|                 |                                                            |   Filebeat       |                             |
|                 | <!-- -->                                                   |   application    |   - `winlogbeat`            |
|                 |                                                            |   log, and the   |                             |
|                 | - **Linux**:                                               |   Winlogbeat     | - **Linux**                 |
|                 |                                                            |   application    |                             |
|                 | <!-- -->                                                   |   log. Indicates |   - `scouter.log`           |
|                 |                                                            |   information,   |                             |
|                 | - `/opt/paloaltonetworks/xdr-collector/logs`               |   warnings, and  |   - `filebeat`              |
|                 |                                                            |   errors related |                             |
|                 |                                                            |   to the XDR     |                             |
|                 |                                                            |   Collector      |                             |
|                 |                                                            |   application.   |                             |
|                 |                                                            |                  |                             |
|                 |                                                            | - **Linux**:     |                             |
|                 |                                                            |   Contains the   |                             |
|                 |                                                            |   XDR Collector  |                             |
|                 |                                                            |   application    |                             |
|                 |                                                            |   Log as well as |                             |
|                 |                                                            |   the Filebeat   |                             |
|                 |                                                            |   application    |                             |
|                 |                                                            |   log. Indicates |                             |
|                 |                                                            |   information,   |                             |
|                 |                                                            |   warnings, and  |                             |
|                 |                                                            |   errors related |                             |
|                 |                                                            |   to the XDR     |                             |
|                 |                                                            |   Collector      |                             |
|                 |                                                            |   application.   |                             |
|                 |                                                            |                  |                             |
|                 |                                                            | Contains the XDR |                             |
|                 |                                                            | Collector        |                             |
|                 |                                                            | application Log  |                             |
|                 |                                                            | as well as the   |                             |
|                 |                                                            | Filebeat         |                             |
|                 |                                                            | application log. |                             |
|                 |                                                            | Indicates        |                             |
|                 |                                                            | information,     |                             |
|                 |                                                            | warnings, and    |                             |
|                 |                                                            | errors related   |                             |
|                 |                                                            | to the XDR       |                             |
|                 |                                                            | Collector        |                             |
|                 |                                                            | application.     |                             |
+-----------------+------------------------------------------------------------+------------------+-----------------------------+
| Configuration   | - **Windows**:                                             | Contains the XML | For both Windows and Linux, |
|                 |                                                            | configuration    | the file name is            |
|                 | <!-- -->                                                   | file of the XDR  | `XDR_Collector.xml`.        |
|                 |                                                            | Collector for    |                             |
|                 | - `%PROGRAMFILES%\Palo Alto Networks\XDR Collector\config` | both Windows and |                             |
|                 |                                                            | Linux.           |                             |
|                 | <!-- -->                                                   |                  |                             |
|                 |                                                            | Any change in    |                             |
|                 | - **Linux**:                                               | this XML         |                             |
|                 |                                                            | configuration    |                             |
|                 | <!-- -->                                                   | file is saved to |                             |
|                 |                                                            | the XDR          |                             |
|                 | - `/opt/paloaltonetworks/xdr-collector/config`             | Collector        |                             |
|                 |                                                            | database and the |                             |
|                 |                                                            | settings are     |                             |
|                 |                                                            | taken from this  |                             |
|                 |                                                            | file.            |                             |
|                 |                                                            |                  |                             |
|                 |                                                            | > **Note**       |                             |
|                 |                                                            | >                |                             |
|                 |                                                            | > In some        |                             |
|                 |                                                            | > circumstances, |                             |
|                 |                                                            | > such as after  |                             |
|                 |                                                            | > an XDR         |                             |
|                 |                                                            | > Collectors     |                             |
|                 |                                                            | > upgrade, the   |                             |
|                 |                                                            | > configured     |                             |
|                 |                                                            | > settings in    |                             |
|                 |                                                            | > the XML        |                             |
|                 |                                                            | > configuration  |                             |
|                 |                                                            | > file can be    |                             |
|                 |                                                            | > erased. Yet,   |                             |
|                 |                                                            | > this won\'t    |                             |
|                 |                                                            | > affect the     |                             |
|                 |                                                            | > saved settings |                             |
|                 |                                                            | > in the XDR     |                             |
|                 |                                                            | > Collectors     |                             |
|                 |                                                            | > database.      |                             |
+-----------------+------------------------------------------------------------+------------------+-----------------------------+
| Persistence     | - **Windows**:                                             | Contains the     | For both Windows and Linux, |
|                 |                                                            | Operating System | the file name is            |
|                 | <!-- -->                                                   | persistence file | `.scouter.json`.            |
|                 |                                                            | for the XDR      |                             |
|                 | - `%PROGRAMDATA%\XDR Collector\OSPersistence`              | Collector, which |                             |
|                 |                                                            | issued as part   |                             |
|                 | <!-- -->                                                   | of the           |                             |
|                 |                                                            | registration     |                             |
|                 | - **Linux**:                                               | process.         |                             |
|                 |                                                            |                  |                             |
|                 | <!-- -->                                                   |                  |                             |
|                 |                                                            |                  |                             |
|                 | - `/etc/panw/OSPersistence/`                               |                  |                             |
+-----------------+------------------------------------------------------------+------------------+-----------------------------+

##### Create an XDR Collector installation package

To install a Cortex XDR Collector for the first time, you must first
create an XDR Collector installation package. After you create and
download an installation package, you can then install it directly on
the collector machine or you can use a software deployment tool of your
choice to distribute the software to multiple collector machines.

To install the XDR Collector software, you must use a valid installation
package that exists in your XDR Collectors console. If you delete an
installation package, any XDR Collectors installed from this package are
not able to register to Cortex XSIAM .

> **Note**
>
> To move existing XDR Collectors between Cortex XSIAM managing servers,
> you need to first [Uninstall the XDR
> Collector](#UUIDd17442992e69bc19098584fc77565ad9) from the collector
> machine and then for the new XDR Collector create a new installation
> package.

To create a new installation package.

1.  In Cortex XSIAM, select Settings
    ![](media/rId3002.png){width="0.20833333333333334in"
    height="0.20833333333333334in"} \> Configurations \> XDR Collectors
    \> Installers.

- ![](media/rId4961.png){width="5.833333333333333in"
  height="1.698957786526684in"}

2.  Click **Create**.

- ![](media/rId4964.png){width="5.833333333333333in"
  height="3.7121205161854767in"}

3.  Enter a unique **Name** and an optional **Description** to identify
    the installation package.

- The package **Name** must be no more than 100 characters and can
  contain letters, numbers, hyphens, underscores, commas, and spaces.

4.  Select the **Platform** for which you want to create the
    installation package as either **Windows** or **Linux**.

5.  Select the **Version**.

6.  **Create** the installation package.

- Cortex XSIAM prepares your installation package and makes it available
  in the **XDR Collectors Installations** page.

7.  Download your installation package.

- When the status of the package displays `Completed`, right-click the
  **Collector Version** row, and click **Download**.

  - For a Windows installation, select **Download 64 bit installer**.

  - For a Linux installation, you can **Download Linux RPM installer**
    or **Download Linux DEB installer** (according to your Linux
    collector machine distribution), and deploy the installers on the
    on-premise collector machines using the Linux package manager.
    Alternatively, you can **Download Linux SH installer** and deploy it
    manually on the Linux collector machine.

  Once the applicable installation package is downloaded, you can
  install the package.

  - [Install the XDR Collector installation package for
    Windows](#UUIDd2c79567292021ce926be4f068b82fdb).

  - [Install the XDR Collector installation package for
    Linux](#UUID5303378a4a366a1ded777cb6da907ca3).

8.  Other available options.

- As needed, you can return to the **XDR Collectors Installations** page
  to manage your XDR Collectors installation packages. To manage a
  specific package, right click the **Collector Version**, and select
  the desired action:

  - **Edit** the package name or description.

  - **Delete** the installation package. Deleting an installation
    package does not uninstall the XDR Collector software from any
    on-premise collector machines.

  <!-- -->

  - > **Note**

    > Since Cortex XSIAM relies on the installation package ID to
    > approve XDR Collector registration during install, it is not
    > recommended to delete the installation package for any active
    > on-premise collector machines. Hiding the installation package
    > will remove it from the default list of available installation
    > packages, and can be useful to eliminate confusion in the XDR
    > Collectors console main view. These hidden installation can be
    > viewed by removing the default filter.

  <!-- -->

  - **Copy text to clipboard** to copy the text from a specific field in
    the row of an installation package.

  - **Hide** installation packages. Using the Hide option provides a
    quick method to filter out results based on a specific value in the
    table. You can also use the filters at the top of the page to build
    a filter from scratch. To create a persistent filter, save
    (![](media/rId1056.png){width="0.13541666666666666in"
    height="0.20833333333333334in"}) it.

##### Install the XDR Collector installation package for Windows

A standard XDR Collector installation for Windows is intended for
standard physical collector machines or persistent virtual collector
machines. You can perform the Windows installation for the XDR
Collectors using the [MSI](#UUIDb3f586e3eefc967e541ec630d6f3c2fc) or
[Msiexec](#UUID308ea46c7e13616734b46ce97db98dcd).

###### Install the XDR Collector on Windows using the MSI

Use the following workflow to install the XDR Collector using the MSI
file.

Before completing this task, ensure that you [create and download a
Cortex XDR Collector installation
package](#UUID68074145fa5033442bbd278489fbefde) in Cortex XSIAM.

To install an XDR Collector installation package on Windows using the
MSI file.

> **Note**
>
> When the package is executed using the MSI, an installation log is
> generated in `%TEMP%\MSI<Random characters>.log` by default.

1.  With Administrator level privileges, run the MSI file that you
    downloaded in Cortex XSIAM on the collector machine.

- The installer displays a welcome dialog.

2.  Click **Next**.

3.  Select **I accept the terms in the License Agreement** and click
    **Next**.

4.  **Install** the XDR Collector.

- The installer displays the **User Account Control** dialog box.

5.  Click **Yes**.

6.  After you complete the installation, verify that the Cortex XDR
    Collector can establish a connection with Cortex XSIAM.

- > **Note**

  > If the XDR Collector does not connect to Cortex XSIAM, verify your
  > internet connection on the collector machine. If the XDR Collector
  > still does not connect, verify that the installation package has not
  > been removed from the Cortex XSIAM tenant.

###### Install the XDR Collector on Windows using Msiexec

Msiexec provides full control over the installation process and allows
you to install, modify, and perform operations on a Windows Installer
from the command line interface (CLI). You can also use Msiexec to log
any issues encountered during installation.

You can also use Msiexec in conjunction with a System Center
Configuration Manager (SCCM), Altiris, Group Policy Object (GPO), or
other MSI deployment software to install the XDR Collector on multiple
collector machines for the first time.

When you install the XDR Collector with Msiexec, you must install the
XDR Collector per-machine and not per-user.

Although Msiexec supports additional options, the XDR Collectors
installers support only the options listed here. For example, with
Msiexec, the option to install the software in a non-standard directory
is not supported---you must use the default path.

The following parameters apply to the initial installation of the XDR
Collector on the collector machine.

- `/i <installer path>\<installer file name>.msi DATA_PATH=<Path> PROXY_LIST=<address or list> /quiet /l*v <installation log path>`:
  Installs a package quietly, changes data path, adds proxies, and
  creates an installation log.

<!-- -->

- For example,
  `msiexec /i c:\install\XDRCollector-Win_x64.msi DATA_PATH=c:\data PROXY_LIST=2.2.2.2:8888,1.1.1.1:8080 /quiet /l*v c:\installlog.txt`

  Where

  - `LOG_LEVEL`: Sets the level of logging for the XDR Collector log
    (`INFO`, `DEBUG`, `ERROR`, and `TRACE`).

  - `LOG_MAX_BYTES`: Sets the maximum log size in bytes.

  - `LOG_BACKUP_COUNT`: Number of cycling logs for the XDR Collector.

  - `PROXY_LIST`: Proxy address or name, where you can add a comma
    separated list, such as 2.2.2.2:8888,1.1.1.1:8080.

  - `LOG_PATH`: The path to save the XDR Collector, Filebeat, and
    Winlogbeat logs.

  - `DATA_PATH`: The path for persistence, content, Filebeat application
    data, Winlogbeat application data, and transaction data.

  - `PROVISIONING_SERVER`: Provisioning server address.

  - `DISTRIBUTION_ID`

  - `ELB_ADDRESS`: Load balancer for fresh XDR Collector installation.

Before completing this task, ensure that you [create and download a
Cortex XDR Collector installation
package](#UUID68074145fa5033442bbd278489fbefde) in Cortex XSIAM.

To install XDR Collectors using Msiexec:

1.  Use one of the following methods to open a command prompt as an
    administrator.

    - Select Start \> All Programs Accessories. Right-click **Command**
      prompt and **Run** as administrator.

    - Select **Start**. In the **Start Search** box, type `cmd`. Then,
      to open the command prompt as an administrator, press
      CTRL+SHIFT+ENTER keys.

2.  Run the `msiexec` command followed by one or more supported options
    and properties.

- For example:

  `msiexec /i XDRCollector-Win_x64.msi DATA_PATH=c:\data PROXY_LIST=2.2.2.2:8888,1.1.1.1:8080 /quiet /l*v c:\installlog.txt`

##### Install the XDR Collector installation package for Linux

You can install the XDR Collector using three available packages for a
Linux installation: Linux RPM, Linux DEB, and Linux SH. You can install
the XDR Collector package on any Linux server, including a physical or
virtual machine, and as temporary sessions.

You can install XDR Collectors in any Linux server period, whether its a
physical or virtual machine. Temporary sessions can be in either of
them.

> **Note**
>
> We recommend that you perform a Linux RPM or Linux DEB installation.

Before completing this task, ensure that you [create and download a
Cortex XDR Collector installation
package](#UUID68074145fa5033442bbd278489fbefde), and then upload these
installation files to your Linux environment.

To install the XDR Collectors installation package for Linux.

1.  Log on to the Linux server.

- For example:

      user@local ~
                              $
                              ssh root@ubuntu.example.com
                              Welcome to Ubuntu 16.04.3 LTS (GNU/Linux 4.4.0-1041-aws x86_64)

                              * Documentation:  https://help.ubuntu.com
                              * Management:     https://landscape.canonical.com
                              * Support:        https://ubuntu.com/advantage

                              Get cloud support with Ubuntu Advantage Cloud Guest:
                              http://www.ubuntu.com/business/services/cloud

                              0 packages can be updated.
                              0 updates are security updates.


                              Last login: Tue Aug 26 22:14:15 2021 from 192.168.1.100
                          

2.  Extract the installation files you uploaded using one of the
    following commands, which is dependent on the Linux package you
    downloaded:

  -------------------------------------------------------------------------------
  Linux Package                       Extract Command
  ----------------------------------- -------------------------------------------
  Linux RPM                           `tar xvf <installation_package_name>.rpm`

  Linux DEB                           `tar xvf <installation_package_name>.deb`

  Linux SH                            `tar xvf <installation_package_name>.sh`
  -------------------------------------------------------------------------------

3.  Create a directory and copy the `collector.conf` installation file
    to the `/etc/panw/` directory.

- sudo mkdir -p /etc/panw
      sudo cp ./collector.conf /etc/panw/

4.  Install the XDR Collectors software.

- You can install the XDR Collectors on the collector machine manually
  using the shell installer or using the Linux package manager for
  `.rpm` and `.deb` installers:

  > **Important**

  > When performing a XDR Collector installation or upgrade in Linux
  > using a shell installer, the  `/tmp` folder cannot be marked as
  > `noexec`. Otherwise, the installation or upgrade fails. As a
  > workaround, before the installation or upgrade, use the following
  > command:

      mount -o remount,exec /tmp

  To deploy using package manager:
  1.  Depending on your Linux distribution, install the XDR Collectors
      using one of the following commands, where the `<file name>` is
      taken from the files provided in the downloaded Linux installation
      package:

+-----------------------------------+---------------------------------------+
| Distribution                      | Install Command                       |
+===================================+=======================================+
| RHEL or Oracle                    | - `yum install ./<file_name>.rpm`     |
|                                   |                                       |
|                                   | - `rpm -i ./<file_name>.rpm`          |
+-----------------------------------+---------------------------------------+
| Ubuntu or Debian                  | - `apt-get install ./<file_name>.deb` |
|                                   |                                       |
|                                   | - `dpkg -i ./<file_name>.deb`         |
+-----------------------------------+---------------------------------------+
| SUSE                              | - `zypper install ./<file_name>.rpm`  |
|                                   |                                       |
|                                   | - `rpm -i ./<file_name>.rpm`          |
+-----------------------------------+---------------------------------------+

1.  Verify the XDR Collectors was installed on the collector machine.

- Enter the following command on the collector machine:

  `dpkg -l | grep xdr-collector` or `rpm -qa | grep xdr-collector`.

<!-- -->

- To deploy the shell installer:
  1.  Enable execution of the script using the `chmod +x <file_name>.sh`
      command, where the `<file name>` is taken from the file provided
      in the downloaded Linux installation package.

  2.  Run the install script as root or with root permissions.

  - For example:

        root@ubuntu:/home# chmod +x linux.sh                               
        root@ubuntu:/home# ./linux.sh
                                                                                        Verifying archive integrity... All good.
        Uncompressing XDR-Collector version 1.0.0.467 100%
        Systemd: starting xdr-collector service
        Synchronizing state of xdr-collector.service with SysV service script with /lib/systemd/systemd-sysv-install.
        Executing: /lib/systemd/systemd-sysv-install enable xdr-collector
        Created symlink /etc/systemd/system/multi-user.target.wants/xdr-collector.service→ /lib/systemd/system/xdr-collector.service.
                                

  > **Note**

  > If the XDR Collector does not connect to Cortex XSIAM, verify your
  > Internet connection on the collector machine. If the XDR Collector
  > still does not connect, verify the installation package has not been
  > removed from the Cortex XSIAM management console.

Additional options are available to help you customize your installation
if needed. The following table describes common options and parameters.

If you are using `rpm` or `deb` installers, you must also add these
parameters to the `/etc/panw/collector.conf` file prior to installation.

+---------------------------------------+----------------------------------------------------------+
| Option                                | Description                                              |
+=======================================+==========================================================+
| `--proxy-list "<proxyserver>:<port>"` | **Proxy communication**                                  |
|                                       |                                                          |
|                                       | Configure the XDR Collector to communicate through an    |
|                                       | intermediary such as a proxy.                            |
|                                       |                                                          |
|                                       | To enable the XDR Collector to direct communication to   |
|                                       | an intermediary, you use this installation option to     |
|                                       | assign the IP address and port number you want the XDR   |
|                                       | Collector to use. You can also configure the proxy by    |
|                                       | entering the FQDN and port number. When you enter the    |
|                                       | FQDN, you can use both lowercase and uppercase letters.  |
|                                       | Avoid using special characters or spaces.                |
|                                       |                                                          |
|                                       | Use double quotes (\" \") to enclose the IP address and  |
|                                       | port number. Use commas to separate multiple addresses.  |
|                                       | For example:                                             |
|                                       |                                                          |
|                                       | `--proxy-list "My.Network.Name:808, 10.196.20.244:8080"` |
|                                       |                                                          |
|                                       | After the initial installation, you can change the proxy |
|                                       | settings from using the configuration XML.               |
|                                       |                                                          |
|                                       | > **Note**                                               |
|                                       | >                                                        |
|                                       | > The XDR Collector does not support proxy communication |
|                                       | > in environments where proxy authentication is          |
|                                       | > required.                                              |
+---------------------------------------+----------------------------------------------------------+
| `--data-path <directory path>`        | **Directory path**                                       |
|                                       |                                                          |
|                                       | The path for persistence, content, Filebeat application  |
|                                       | data, and transaction data.                              |
|                                       |                                                          |
|                                       | `--data–path=/tmp/xdrLog`                                |
+---------------------------------------+----------------------------------------------------------+

##### Configure the XDR Collector upgrade scheduler

You can configure the Cortex XDR Collector upgrade scheduler and the
number of parallel upgrades. There can be a maximum of 500 parallel
upgrades scheduled in a week, which is the default configuration at any
time of day.

To define the XDR Collector upgrade scheduler and number of parallel
upgrades.

1.  In Cortex XSIAM, select Settings \> Configurations \> XDR Collectors
    \> Configuration.

2.  Set the XDR Collectors Configurations settings.

    - `Amount of Parallel Upgrades`: Specify the number of parallel
      upgrades, where the maximum number is 500 (default).

    - `Days in Week`: Select the specific days in the week that you want
      the upgrade to occur, where the default is configured as every day
      in the week.

    - `Schedule`: Select whether you want the upgrade to be at **Any**
      time (default) or at a **Specific** time. When setting a specific
      time, you can set the **From** and **To** times.

3.  Click **Save**.

##### Set an application proxy for XDR Collectors

In environments where Cortex XDR Collectors communicate with the Cortex
XSIAM server through a wide system proxy, you can set an
application-specific proxy for the XDR Collector without affecting the
communication of other applications on the collector machine. You can
set the proxy after installation from the
**XDR Collectors Administration** page in Cortex XSIAM as described in
this topic. You can assign up to ten different proxy servers per XDR
Collector. The proxy server that the agent uses is selected randomly and
with equal probability. If the communication between the XDR Collector
and the Cortex XSIAM server through the app-specific proxies fails, the
XDR Collector resumes communication through the system-wide proxy
defined on the collector machine. If that fails as well, the XDR
Collector resumes communication with Cortex XSIAM directly.

1.  In Cortex XSIAM, select Settings \> Configurations \> XDR Collectors
    \> Administration.

2.  If needed, filter the list of on-premise collector machines.

3.  Set an agent proxy.

    a.  Select the row of the on-premises collector machine that you
        want to set as a proxy.

    b.  Right-click the collector machine, and select
        **Set Collector proxy**.

    c.  You can assign up to ten different proxies per XDR Collector.
        For each proxy, specify the IP address and port number. After
        each **Proxy Address** and **Port** added, select
        ![](media/rId2150.png){width="0.14583333333333334in"
        height="0.20833333333333334in"} to add the values to a list
        underneath these fields. Broker VMs in the same tenant can also
        be configured to use as a proxy, by enabling Agent proxy in the
        Broker VMs.

    d.  Click **Set** when you're done.

    e.  If necessary later, you can disable the collector proxy by
        selecting **Disable Collector Proxy** from the right-click menu.

    - When you disable the proxy configuration, all proxies associated
      with that XDR Collector are removed. The XDR Collector resumes
      communication with the Cortex XSIAM server through the wide-system
      proxy if defined; otherwise, if a wide-system is not defined, the
      XDR Collector resumes communicating directly with the Cortex XSIAM
      server. If neither a wide-system proxy nor direct communication
      exist and you disable the proxy, the XDR Collector disconnects
      from Cortex XSIAM.

##### Set an alias for an XDR Collector machine

To identify one or more collector machines by a name that is different
from the collector machine hostname, you can configure an alias. You can
set an alias for a single collector machine or you can set an alias for
multiple collector machines in bulk. To quickly search for the collector
machines during investigation and when you need to take action, you can
use the either the collector machine hostname or the alias.

1.  Select Settings \> Configurations \> XDR Collectors \>
    Administration.

2.  Select one or more collector machines.

3.  Right-click anywhere in the collector machine rows, and select
    **Change Collector Alias**.

4.  Specify the alias name and **Update**.

5.  Use the Quick Launcher to search the collector machines by alias
    across the XDR Collectors console.

##### Upgrade XDR Collectors

After you install the Cortex XDR Collector and the XDR Collector
registers with Cortex XSIAM, you can upgrade the XDR Collector software
for on-premises Windows or Linux collector machine. You need to create a
new installation packages and push the XDR Collector package to up to
500 collector machines from Cortex XSIAM.

1.  Create an XDR Collector Installation Package for each operating
    system version where you want to upgrade the XDR Collector.

- Note the installation package names.

2.  Select Settings \> XDR Collectors \> Administration.

- If needed, filter the list of on-premises collector machines. To
  reduce the number of results, use the collector machine name search
  and filters at the top of the page.

3.  Select the collector machines that you want to upgrade.

- You can also select collector machines running different operating
  systems to upgrade the XDR Collectors at the same time.

4.  Right-click your selection, and select
    **Upgrade Collector version**.

- For each platform, select the name of the installation package you
  want to push to the selected on-premises collector machines.

  > **Note**

  > The XDR Collector keeps the name of the original installation
  > package after every upgrade.

5.  **Upgrade**.

- Cortex XSIAM distributes the installation package to the selected
  collector machine at the next heartbeat communication with the XDR
  Collector. To monitor the status of the upgrades, go to Investigation
  & Response \> Response \> Action Center. From the **Action Center**
  you can also view additional information about the upgrade
  (right-click the action and select **Additional data**) or cancel the
  upgrade (right-click the action and select
  **Cancel Collector Upgrade**).

##### Uninstall the XDR Collector

If you want to uninstall the XDR Collector from the on-premise collector
machine, you can do so from the XDR Collectors console at any time. You
can uninstall the XDR Collector from an unlimited number of collector
machines in a single bulk action. Uninstalling a collector machine
triggers the following lifespan flow:

- Once you uninstall the XDR Collector from the on-premise collector
  machine, Cortex XSIAM distributes the uninstall to the selected
  collector machine at the next heartbeat communication with the XDR
  Collector. All XDR Collector files are removed from the collector
  machine.

- The collector machine status changes to `Uninstalled`. After a
  retention period of 7 days, the XDR Collector is deleted from the
  database and is displayed in XDR as **Collector Machine Name** -
  `N/A (Uninstalled)`.

- Data associated with the deleted on-premise collector machine is
  displayed in the **Action Center** tables for the standard 90 days
  retention period.

The following workflow describes how to uninstall the XDR Collector from
one or more Windows or Linux on-premise collector machines.

1.  Select Settings \> Configurations \> XDR Collectors \>
    Administration.

2.  Select the collector machines you want to uninstall.

- You can also select collector machines running different operating
  systems to uninstall the XDR Collectors at the same time.

3.  Right-click your selection and select **Uninstall Collector**.

4.  To proceed, select **I agree** to confirm that you understand this
    action uninstalls the XDR Collector on all selected collector
    machines.

5.  Click **OK**.

- To monitor the status of the uninstall process, go to Investigation &
  Response \> Response \> Action Center.

#### Define XDR Collector machine groups

To easily apply policy rules and manage specific collector machines, you
can define a collector machine group. If you set up Directory Sync, you
can also leverage your Active Directory user, group, and computer
information in collector machine groups.

There are two methods you can use to define a collector machine group:

- Create a dynamic group by allowing Cortex XSIAM to populate your
  collector machine group dynamically using collector machine
  characteristics, such as a partial hostname or alias; full or partial
  domain name; IP address, range or subnet; XDR Collector version; or
  operating system version.

- Create a static group by selecting a list of specific collector
  machines.

After you define a collector machine group, you can then use it to
target policy and actions to specific recipients. The
**XDR Collectors Groups** page displays all collector machine groups
along with the number of collector machines and policy rules linked to
the collector machine group.

To define a collector machine static or dynamic group:

1.  In Cortex XSIAM , select Settings \> Configurations \> XDR
    Collectors \> Groups.

2.  Select **+Add Group** to create a new collector machine group.

3.  Specify a group name and optional description in the corresponding
    fields, to identify the collector machine group. The name that you
    assign to the group will be visible when you assign endpoint
    security profiles to endpoints.

4.  Determine the collector machine properties for creating a collector
    machine group:

    - **Dynamic**: Use the filters to define the criteria you want to
      use to dynamically populate a collector machine group. Dynamic
      groups support multiple criteria selections and can use **AND** or
      **OR** operators. For collector machine names and aliases, and
      domains, you can use `*` to match any string of characters. As you
      apply filters, Cortex XSIAM displays any registered collector
      machine matches to help you validate your filter criteria.

    <!-- -->

    - > **Note**

      > XDR Collectors only support IPv4 addresses.

    <!-- -->

    - **Static**: Select specific registered collector machines that you
      want to include in the collector machine group. Use the filters,
      as needed, to reduce the number of results.

    <!-- -->

    - When you create a static collector machine group from a file, the
      IP address, hostname, or alias of the collector machine must match
      an existing Cortex XSIAM that has registered with Cortex XSIAM .

      > **Note**

      > Disconnecting Directory Sync in your Cortex XSIAM deployment can
      > affect existing collector machine groups and policy rules based
      > on Active Directory properties.

5.  Create the collector machine group.

- After you save your collector machine group, it is ready for use to
  assign in policies for your collector machines and in other places
  where you can use collector machine groups.

6.  Manage a collector machine group, as needed.

- At any time, you can return to the **XDR Collectors Endpoints** page
  to view and manage your collector machine groups. To manage a group,
  right-click the group and select the desired action.

  - **Edit**: View the collector machines that match the group
    definition, and optionally refine the membership criteria using
    filters.

  - **Delete** the collector machine group.

  - **Save as new**: Duplicate the collector machine group and save it
    as a new group.

  - **View collectors**: Pivot from an collector machine group to a
    filtered list of collector machines on the **Administration** page
    where you can quickly view and initiate actions on the collector
    machines within the group.

  - **Copy text to clipboard** to copy the text from a specific field in
    the row of a group.

  - **Copy entire row** to copy the text from all the fields in a row of
    a group.

  - **Show rows with '\<Group name\>'** to filter the group list to only
    display the groups with a specific group name.

  - **Hide rows with '\<Group name\>'** to filter the group list to hide
    the groups for a specific group name.

#### About Cortex XDR Collector content updates

To quickly resolve any issues in policy, Palo Alto Networks can
seamlessly deliver software packages for Cortex XSIAM, called content
updates. Content updates for XDR Collectors contain changes or updates
to the Elasticsearch Filebeat infrastructure or the Elasticsearch\*
Winlogbeat infrastructure.

When a new update is available, Cortex XSIAM notifies the XDR
Collectors. The XDR Collectors then randomly choose a time within a
six-hour window during which they retrieve the content update from
Cortex XSIAM.

#### XDR Collector profiles

You can add XDR collector profiles that define the type of data that is
collected from Linux or Windows platforms.

##### Add an XDR Collector profile for Windows

> **Note**
>
> Ingestion of log events larger than 5 MB is not supported.

XDR Collector profiles define the data that is collected from a Windows
collector machine, and define automatic upgrade settings for the XDR
collector. For Windows, you can configure a Filebeat profile, a
Winlogbeat profile, and a Settings profile.

The Filebeat and Winlogbeat profiles use configuration files in YAML
format. To facilitate the configuration of the YAML file, you can use
out-of-the-box collection templates and templates added by the content
packs installed from the XSIAM Marketplace. Templates save you time, and
don\'t require previous knowledge of configuration file generation. You
can edit and combine the provided templates, and you can add your own
collection settings to the configuration file.

- Use an **XDR Collector Windows Filebeat profile** to collect file and
  log data using the Elasticsearch Filebeat default configuration file,
  called `filebeat.yml`.

<!-- -->

- Cortex XSIAM supports using Filebeat version 8.15 with the operating
  systems listed in the Elasticsearch support matrix that conform with
  the collector machine operating systems supported by Cortex XSIAM.
  Cortex XSIAM supports the input types and modules available in
  Elasticsearch Filebeat.

  > **Note**

  - > Fileset validation is enforced. You must enable at least one
    > fileset in the module, because filesets are disabled by default.

  - > Cortex XSIAM collects all logs in either an uncompressed JSON or
    > text format. Compressed files, such as the gzip format, are not
    > supported.

  - > Cortex XSIAM supports logs in single line format or multiline
    > format. For more information about handling messages that span
    > multiple lines of text in Elasticsearch Filebeat, see [Manage
    > Multiline
    > Messages](https://www.elastic.co/guide/en/beats/filebeat/current/multiline-examples.html).

  Related Information
  - [Elasticsearch Filebeat Overview
    Documentation](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-overview.html#filebeat-overview)

  - [Configure Filebeat Inputs in
    Elasticsearch](https://www.elastic.co/guide/en/beats/filebeat/current/configuration-filebeat-options.html)

  - [Configure Filebeat Modules in
    Elasticsearch](https://www.elastic.co/guide/en/beats/filebeat/current/configuration-filebeat-modules.html)

  - [Elasticsearch Support
    Matrix](https://www.elastic.co/support/matrix)

  - [/document/preview/1047108#UUID-ca91a864-7ee1-8be9-85ec-f71a3d073859](/document/preview/1047108#UUID-ca91a864-7ee1-8be9-85ec-f71a3d073859)

  - Collection of Windows DHCP logs and Windows DNS Debug logs:

    - [Windows DHCP
      logs](/document/preview/1048068#UUID-8510a8f0-62f4-3108-9c76-00c68d193482)

    - [Windows DNS Debug logs](#UUID2117962db7bcbbefd251b5cff563b8f0)

<!-- -->

- Use an **XDR Collector Windows Winlogbeat profile** to collect event
  log data, using the Elasticsearch Winlogbeat default configuration
  file, called `winlogbeat.yml`.

<!-- -->

- Cortex XSIAM supports using Winlogbeat version 8.15 with the Windows
  versions listed in the Elasticsearch support matrix that conform with
  the collector machine operating systems supported by Cortex XSIAM.
  Cortex XSIAM supports the modules available in Elasticsearch
  Winlogbeat.

  After ingestion, Cortex XSIAM normalizes and saves the Windows event
  logs collected by the Winlogbeat profile in the dataset `xdr_data`.
  The normalized logs are also saved in a unified format in
  `<vendor>_<product>_raw` if the product and vendor are defined, and
  otherwise, in `microsoft_windows_raw`. You can search the data using
  Cortex Query Language XQL queries, build correlation rules, and
  generate dashboards based on the data.

  Related information
  - [Elasticsearch Winlogbeat Overview
    Documentation](https://www.elastic.co/guide/en/beats/winlogbeat/current/_winlogbeat_overview.html)

  - [Winlogbeat Modules in
    ElasticSearch](https://www.elastic.co/guide/en/beats/winlogbeat/current/winlogbeat-modules.html)

  - [Elasticsearch Support
    Matrix](https://www.elastic.co/support/matrix)

  - Cortex XSIAM, see
    [/document/preview/1047108#UUID-ca91a864-7ee1-8be9-85ec-f71a3d073859](/document/preview/1047108#UUID-ca91a864-7ee1-8be9-85ec-f71a3d073859)

<!-- -->

- Use an **XDR Collector Settings profile** to configure automatic
  upgrade settings for XDR Collector releases.

To map your XDR Collector profile to a collector machine, you must use
an XDR Collector policy. After you have created your profile, map it to
a new or existing policy.

**How to configure XDR Collector profiles**

Filebeat profile

In the **Filebeat Configuration File** editor, you can define the data
collection for your Elasticsearch Filebeat configuration file called
`filebeat.yml`.

Cortex XSIAM provides YAML templates for DHCP, DNS, IIS, XDR Collector
Logs, NGINX, and any templates added by the content packs installed from
the XSIAM Marketplace.

1.  In Cortex XSIAM, select Settings \> Configurations \> XDR Collectors
    \> Profiles \> +Add Profile \> Windows.

2.  Select **Filebeat**, then click **Next**.

3.  Configure the **General Information** parameters.

    - **Profile Name**: Enter a unique name to identify the profile. The
      name can contain only letters, numbers, or spaces, and must be no
      more than 30 characters. The name that you enter here will be
      displayed in the list of profiles when you configure a policy.

    - (*Optional*) **Add description here**: To provide additional
      context for the purpose or business reason for your new profile,
      enter a profile description.

4.  In the **Filebeat Configuration File** editing box, type or paste
    the contents of your configuration file, or use a template. To add a
    template, select one from the list, and click **Add**.

5.  Cortex XSIAM supports all sections in the `filebeat.yml`
    configuration file, such as support for Filebeat fields and tags.
    You can use the \"Add fields\" processor to identify the
    product/vendor for the data collected by the XDR Collectors, so that
    the collected events go through the ingestion flow (Parsing Rules).
    To configure the product/vendor, ensure that you use the default
    `fields` attribute (do not use the **target** attribute), as shown
    in the following example:

- processors:
        - add_fields:
            fields:
              vendor: <Vendor>
              product: <Product>

  For more information about the \"Add fields\" processor, see
  [Add_fields](https://www.elastic.co/guide/en/beats/filebeat/current/add-fields.html).

6.  To finish creating your new profile, click **Create**.

- Your new profile will be listed under the applicable platform on the
  **XDR Collectors Profiles** page.

7.  Apply profiles to XDR Collector machine policies by performing one
    of the following:

    - Right-click a profile, and select
      **Create a new policy rule using this profile**.

    - Launch the new policy wizard from XDR Collectors \> Policies \>
      XDR Collectors Policies.

Winlogbeat profile

In the **Winlogbeat Configuration File** editor, you can define the data
collection for your Elasticsearch Winlogbeat configuration file called
`winlogbeat.yml`.

Cortex XSIAM provides YAML templates for Windows Security, and any
templates added by the content packs installed from the XSIAM
Marketplace. To add a template, select it and click **Add**.

1.  In Cortex XSIAM, select Settings \> Configurations \> XDR Collectors
    \> Profiles \> +Add Profile \> Windows.

2.  Select **Winlogbeat** profile, then click **Next**.

3.  Configure the **General Information** parameters.

    - **Profile Name**: Enter a unique name to identify the profile. The
      name can contain only letters, numbers, or spaces, and must be no
      more than 30 characters. The name that you enter here will be
      displayed in the list of profiles when you configure a policy.

    - (*Optional*) **Add description here**: To provide additional
      context for the purpose or business reason for your new profile,
      enter a profile description.

4.  In the **Winlogbeat Configuration File** editing box, type or paste
    the contents of your configuration file, or use the template. To add
    the template, click **Select template**, and then click
    **Windows Security**. Click **Add**.

5.  Cortex XSIAM supports all sections in the `winlogbeat.yml`
    configuration file, such as support for Winlogbeat fields and tags.
    You can use the \"Add fields\" processor to identify the
    product/vendor for the data collected by the XDR Collectors, so that
    the collected events go through the ingestion flow (Parsing Rules).
    To configure the product/vendor, ensure that you use the default
    `fields` attribute (do not use the `target` attribute), as shown in
    the following example:

- processors:
        - add_fields:
            fields:
              vendor: <Vendor>
              product: <Product>

  For more information about the \"Add fields\" processor, see
  [Add_fields](https://www.elastic.co/guide/en/beats/filebeat/current/add-fields.html).

6.  To finish creating your new profile, click **Create**.

- Your new profile will be listed under the applicable platform on the
  **XDR Collectors Profiles** page.

7.  Apply profiles to XDR Collector machine policies by performing one
    of the following:

    - Right-click a profile, and select
      **Create a new policy rule using this profile**.

    - Launch the new policy wizard from XDR Collectors \> Policies \>
      XDR Collectors Policies.

Settings profile

You can configure automatic upgrades for XDR Collector releases. By
default, this is disabled, and the **Use Default (Disabled)** option is
selected. To implement automatic upgrades, follow these steps:

1.  In Cortex XSIAM, select Settings \> Configurations \> XDR Collectors
    \> Profiles \> +Add Profile \> Windows.

2.  Select **Settings** profile, then click **Next**.

3.  Configure the **General Information** parameters.

    - **Profile Name**: Enter a unique name to identify the profile. The
      name can contain only letters, numbers, or spaces, and must be no
      more than 30 characters. The name that you enter here will be
      displayed in the list of profiles when you configure a policy.

    - (*Optional*) **Add description here**: To provide additional
      context for the purpose or business reason for your new profile,
      enter a profile description.

4.  Clear the **Use Default (Disabled)** checkbox.

5.  For **Collector Auto-Upgrade**, select **Enabled**.

- Additional fields are displayed for defining the scope of the
  automatic upgrade.

6.  Configure the scope of automatic upgrades:

    - To ensure the latest XDR Collector release is used, leave the
      **Use Default (Latest collector release)** checkbox selected.

    - To configure only a particular scope, perform the following steps:

      1.  Clear the **Use Default (Latest collector release)** checkbox.

      2.  For **Auto Upgrade Scope**, select one of the following
          options:

  -----------------------------------------------------------------------
  Option                              More details
  ----------------------------------- -----------------------------------
  Latest collector release            Configures the scope of the
                                      automatic upgrade to whenever a new
                                      XDR Collector release is available
                                      including maintenance releases and
                                      new features.

  Only maintenance release            Configures the scope of the
                                      automatic upgrade to whenever a new
                                      XDR Collector maintenance release
                                      is available.

  Only maintenance releases in a      Configures the scope of the
  specific version                    automatic upgrade to whenever a new
                                      XDR Collector maintenance release
                                      is available for a specific
                                      version. When this option is
                                      selected, you can select the
                                      specific **Release Version**.
  -----------------------------------------------------------------------

7.  To finish creating your new profile, click **Create**.

- Your new profile will be listed under the applicable platform on the
  **XDR Collectors Profiles** page.

8.  Apply profiles to XDR Collector machine policies by performing one
    of the following:

    - Right-click a profile, and select
      **Create a new policy rule using this profile**.

    - Launch the new policy wizard from XDR Collectors \> Policies \>
      XDR Collectors Policies.

**Additional XDR Collector profile management options**

As needed, you can return to the **XDR Collectors Profiles** page to
manage your XDR Collectors profiles. To manage a specific profile,
right-click anywhere in an XDR Collector profile row, and select the
desired action:

  -----------------------------------------------------------------------
  Option                              More details
  ----------------------------------- -----------------------------------
  Edit                                Lets you edit the XDR Collector
                                      profile

  Save As New                         Copies the existing profile with
                                      its current settings, so that you
                                      can make modifications, and save it
                                      as a new profile with a unique name

  Delete                              Deletes the XDR Collector profile

  View Collector Policies             Opens a new tab that displays the
                                      **XDR Collectors Policies** page,
                                      showing the policies that are
                                      currently associated with your XDR
                                      Collector profiles

  Copy text to clipboard              Copies the text from a specific
                                      field in the row of a XDR Collector
                                      profile

  Copy entire row                     Copies the text from the entire row
                                      of a XDR Collector profile
  -----------------------------------------------------------------------

###### Ingest logs from Windows DHCP using Elasticsearch Filebeat

You can extend visibility into logs from Windows DHCP, and enrich
network logs with Windows DHCP data by using one of the following data
collectors with Elasticsearch Filebeat :

- XDR Collector profile (recommended)

- Windows DHCP collector

When Cortex XSIAM begins receiving logs, it automatically creates a
Windows DHCP dataset (`microsoft_dhcp_raw`). Cortex XSIAM uses Windows
DHCP logs to enrich your network logs with hostnames and MAC addresses.
Using XQL Search, you will be able to search for these items in the
`microsoft_dhcp_raw` dataset.

> **Note**
>
> Although this enrichment is available when configuring a Windows DHCP
> collector for a cloud data collection integration, we recommend
> configuring Cortex XSIAM to receive Windows DHCP logs with an XDR
> Collector Windows Filebeat profile, because it is simpler to set up.

Related information

- For more information about configuring the `filebeat.yml` file, see
  [Elasticsearch Filebeat
  documentation](https://www.elastic.co/guide/en/beats/filebeat/current/configuring-howto-filebeat.html).

####### Ingest Windows DHCP Logs with an XDR Collector Profile

When you add an XDR Collector Windows Filebeat profile using the
Elasticsearch Filebeat default configuration file, called
`filebeat.yml`, you can define whether the collected data undergoes
follow-up processing in the backend for Windows DHCP data. You can
further enrich network logs with Windows DHCP data by setting `vendor`
to `“microsoft”`, and `product` to `“dhcp”` in the `filebeat.yml` file.

> **Note**
>
> Configuration activities include editing the `filebeat.yml` file. To
> avoid formatting issues in this file, use the template provided by
> Cortex XSIAM to make your customizations. We recommend that you edit
> the file inside the user interface, instead of copying it and editing
> it elsewhere. Validate the syntax of the YML file before you finish
> creating your profile.

**Configure Cortex XSIAM to receive logs from Windows DHCP using an XDR
Collector Windows Filebeat profile:**

1.  In Cortex XSIAM, select Settings \> Configurations \> XDR Collectors
    \> Profiles \> +Add Profile \> Windows.

2.  Select **Filebeat**, then click **Next**.

3.  Configure the **General Information** parameters:

    - **Profile Name**: Enter a unique name to identify the profile. The
      name can contain only letters, numbers, or spaces, and must be no
      more than 30 characters. The name that you enter here will be
      displayed in the list of profiles when you configure a policy.

    - (*Optional*) **Add description here**: To provide additional
      context for the purpose or business reason for your new profile,
      enter a profile description.

4.  In the **Filebeat Configuration File** editing box, select the
    **DHCP** template, and click **Add**.

- The template\'s content is displayed in the editing area.

5.  Edit the template text as necessary for your system.

6.  To finish creating your new profile, click **Create**.

- Your new profile will be listed under the applicable platform on the
  **XDR Collectors Profiles** page.

7.  Apply profiles to XDR Collector machine policies by performing one
    of the following:

    - Right-click a profile, and select
      **Create a new policy rule using this profile**.

    - Launch the new policy wizard from XDR Collectors \> Policies \>
      XDR Collectors Policies.

####### Ingest Windows DHCP Logs with the Windows DHCP Collector

To receive Windows DHCP logs with this collector, you must configure
data collection from Windows DHCP via Elasticsearch Filebeat. This is
configured by setting up a Windows DHCP Collector in Cortex XSIAM and
installing and configuring an Elasticsearch Filebeat agent on your
Windows DHCP Server. Cortex XSIAM supports using Filebeat up to version
8.0.1 with the Windows DHCP Collector.

Certain settings in the Elasticsearch Filebeat default configuration
file called `filebeat.yml` must be populated with values provided when
you configure the Data Sources settings in Cortex XSIAM for the Windows
DHCP Collector. To help you configure the `filebeat.yml` file correctly,
Cortex XSIAM provides an example file that you can download and
customize. After you set up collection integration, Cortex XSIAM begins
receiving new logs and data from the source.

Windows DHCP logs are stored as CSV (comma-separated values) log files.
The logs rotate by days (`DhcpSrvLog-<day>.log`), and each file contains
two sections: `Event ID Meaning`, and the events list.

> **Note**
>
> Configuration activities include editing the `filebeat.yml` file. To
> avoid formatting issues in this file, use the example file provided by
> Cortex XSIAM to make your customizations. Do not copy and paste the
> code syntax examples provided later in this procedure into your
> `filebeat.yml` file. Validate the syntax of the YML file before you
> finish creating your profile.

**Configure Cortex XSIAM to receive logs from Windows DHCP via
Elasticsearch Filebeat with the Windows DHCP collector:**

1.  In Cortex XSIAM, configure the Windows DHCP Collector.

    a.  Select Settings \> Data Sources.

    b.  Click **Add Instance** to begin a new configuration.

    c.  Search for `Windows DHCP`.

    d.  In the **Windows DHCP** collector box, click **Connect**.

    - The **Enable Windows DHCP Log Collection** dialog box is
      displayed.

    e.  (*Optional, but recommended*) Download the example
        `filebeat.yml` file.

    - To help you configure your `filebeat.yml` file correctly, Cortex
      XSIAM provides an example `filebeat.yml` file that you can
      download and customize. To download this file, click the
      **filebeat.yml** link provided in this dialog box.

    f.  In the **Name** field, specify a descriptive name for your log
        collection configuration.

    g.  Click **Save & Generate Token**. A key is displayed.

    - Click the copy icon next to the key, and save the copy somewhere
      safe. You will need to provide this key when you set the `api_key`
      value in the **Elasticsearch Output** section in the
      `filebeat.yml` file, as explained in [**Step
      #2**](#Xe6b5e6f9c7388998a36218719a01524ca83f424). If you forget to
      record the key and close the window, you will need to generate a
      new key and repeat this process.

    h.  Click **Done** to close the dialog box.

    i.  Expand the **Windows DHCP** collector that you just created.
        Click the **Copy api url** icon, and save the copy somewhere
        safe. You will need to provide this URL when you set the `hosts`
        value in the **Elasticsearch Output** section in the
        `filebeat.yml` file, as explained in [**Step
        #2**](#Xe6b5e6f9c7388998a36218719a01524ca83f424).

2.  On your Windows DHCP Server, configure an Elasticsearch Filebeat
    agent.

    a.  Navigate to the Elasticsearch Filebeat installation directory,
        and open the `filebeat.yml` file to configure data collection
        with Cortex XSIAM. We recommend that you use the download
        example file provided by Cortex XSIAM.

    b.  Update the following sections and tags in the `filebeat.yml`
        file. The following code examples detail the specific sections
        to make these changes in the file.

        - **Filebeat inputs**: Define the paths to crawl and fetch. The
          code in the example below shows how to configure the
          **Filebeat inputs** section in the `filebeat.yml` file with
          these paths configured.

        <!-- -->

        - Example
              # ============================== Filebeat inputs ===============================
              filebeat.inputs:
                # Each - is an input. Most options can be set at the input level, so
                # you can use different inputs for various configurations.
                # Below are the input specific configurations.
                - type: log  
                  # Change to true to enable this input configuration.  
                  enabled: true  
                  # Paths that should be crawled and fetched. Glob based paths.  
                  paths:       
                    - c:\Windows\System32\dhcp\DhcpSrvLog*.log    

        <!-- -->

        - **Elasticsearch Output**: Set the `hosts` and `api_key`, where
          both of these values were obtained when you configured the
          Windows DHCP Collector in Cortex XSIAM, as explained in
          **Step #1**. The following code example shows how to configure
          the **Elasticsearch Output** section in the `filebeat.yml`
          file, and indicates which settings need to be obtained from
          Cortex XSIAM.

        <!-- -->

        - Example
              # ---------------------------- Elasticsearch Output ----------------------------
              output.elasticsearch:  
                enabled: true  
                # Array of hosts to connect to.    
                hosts: ["OBTAIN THIS URL FROM CORTEX XDR"]  
                # Protocol - either `http` (default) or `https`.  
                protocol: "https"  
                compression_level: 5  
                # Authentication credentials - either API key or username/password. 
                api_key: "OBTAIN THIS KEY FROM CORTEX XDR"

        <!-- -->

        - **Processors**: Set the `tokenizer` and add a
          `drop_event processor` to drop all events that do not start
          with an event ID. The code in the example below shows how to
          configure the **Processors** section in the `filebeat.yml`
          file and indicates which settings need to be obtained from
          Cortex XSIAM.

        <!-- -->

        - > **Note**

          > The `tokenizer` definition is dependent on the Windows
          > server version that you are using, because the log format
          > differs.

          - > For platforms earlier than Windows Server 2008, use
            > `"%{id},%{date},%{time},%{description},%{ipAddress},%{hostName},%{macAddress}"`

          - > For Windows Server 2008 and 2008 R2, use
            > `"%{id},%{date},%{time},%{description},%{ipAddress},%{hostName},%{macAddress},%{userName},%{transactionID},%{qResult},%{probationTime},%{correlationID}"`

          - > For Windows Server 2012 and later, use
            > `"%{id},%{date},%{time},%{description},%{ipAddress},%{hostName},%{macAddress},%{userName},%{transactionID},%{qResult},%{probationTime},%{correlationID},%{dhcid},%{vendorClassHex},%{vendorClassASCII},%{userClassHex},%{userClassASCII},%{relayAgentInformation},%{dnsRegError}"`

          Example
              # ================================= Processors =================================
              processors:  
                - add_host_metadata:      
                  when.not.contains.tags: forwarded  
                - drop_event.when.not.regexp.message: "^[0-9]+,.*"  
                - dissect:       
                  tokenizer: "%{id},%{date},%{time},%{description},%{ipAddress},%{hostName},%{macAddress},%{userName},%{transactionID},%{qResult},%{probationTime},%{correlationID},%{dhcid},%{vendorClassHex},%{vendorClassASCII},%{userClassHex},%{userClassASCII},%{relayAgentInformation},%{dnsRegError}"  
                - drop_fields:       
                  fields: ["message"]  
                - add_locale: ~
                - rename:
                    fields:
                      - from: "event.timezone"
                        to: "dissect.timezone"
                    ignore_missing: true
                    fail_on_error: false
                - add_cloud_metadata: ~  
                - add_docker_metadata: ~  
                - add_kubernetes_metadata: ~

3.  Verify the status of the integration.

- Return to the integrations page in Cortex XSIAM, and view the
  statistics for the log collection configuration.

4.  After Cortex XSIAM begins receiving logs from Windows DHCP via
    Elasticsearch Filebeat, you can use XQL Search to search for logs in
    the new `microsoft_dhcp_raw` dataset.

###### Ingest Windows DNS debug logs using Elasticsearch Filebeat

Extend Cortex XSIAM visibility into Windows DNS Debug logs using an XDR
Collector Windows Filebeat profile.

During configuration of an XDR Collector Windows Filebeat profile, you
can configure the profile to enrich network logs with Windows DNS Debug
log data. You do this by editing the Elasticsearch Filebeat default
configuration file called `filebeat.yml`. In this file, you can define
whether the collected data undergoes follow-up processing in the backend
for Windows DNS Debug log data. Cortex XSIAM uses Windows DNS Debug logs
to enrich network logs. These logs can be searched, using XQL Search.
You can search the Windows DNS Debug Cortex Query Language dataset
(`microsoft_dns_raw`) for raw data, and the normalized stories using the
`xdr_data` dataset with the preset called `network_story`.

1.  Enable DNS debug logging in your Windows DNS server settings:

    a.  In Windows, open **DNS Manager**, right-click your Windows DNS
        Server, and select **Properties**.

    b.  Select Debug Logging \> Log packets for debugging, and keep the
        settings that are automatically configured for collecting
        regular Windows DNS logs in the **Packet direction** and
        **Packet contents** sections.

    c.  (Optional) To collect detailed Windows DNS logs, under the
        **Other options** section, select **Details**.

    - > **Note**

      > Detailed logs are significantly larger, because more information
      > is added to the logs.

    d.  In the **Log file** section, for **File path and name** , enter
        the file path and log name of your Windows DNS logs, such as
        `c:\Windows\System32\dns\DNS.log`. This path will also be
        configured in your `filebeat.yml` file, as explained in a later
        step (see [Example](#Xa97506822ea6916c319cc68a4e957ad86e1a269)).

    e.  Click **OK**.

2.  In Cortex XSIAM, go to Settings \> Configurations \> XDR Collectors
    \> Profiles \> +Add Profile \> Windows.

3.  Select **Filebeat**, then click **Next**.

4.  Configure the **General Information** parameters:

    - **Profile Name**: Enter a unique name to identify the profile. The
      name can contain only letters, numbers, or spaces, and must be no
      more than 30 characters. The name that you enter here will be
      displayed in the list of profiles when you configure a policy.

    - (*Optional*) **Add description here**: To provide additional
      context for the purpose or business reason for your new profile,
      enter a profile description.

5.  In the **Filebeat Configuration File** editing box, select the
    **DNS** template of your choice (**detailed**, or **non-detailed**).
    If you configured detailed collection in the [Windows DNS
    Manager](#X67317a15e01f768a0d51cd647856fd6f661f2d4), select the
    detailed DNS template here. Click **Add**.

- The template\'s content is displayed in the editing area.

6.  Configure the `filebeat.yml` file to collect Windows DNS Debug log
    data.

    a.  In the `filebeat.inputs:` section of the file, for `paths:`,
        configure the file path to your Windows DNS Debug logs. This
        file path must be the same as the one configured in your Windows
        DNS server settings, as explained in an earlier
        [step](#X426a6844957bddb875b9ec1714c7294424e9552).

    b.  Set `vendor` to `“microsoft”` and `product` to `“dns”`.

- The following examples show how to configure the `filebeat.yml` file
  to normalize Windows DNS Debug logs with an XDR Collector.

  > **Note**

  > To avoid formatting issues in your `filebeat.yml` file, we recommend
  > that you validate the syntax of the file.

  Example
  Example for non-detailed (regular) Windows DNS log collection:

      filebeat.inputs:
      - type: filestream
        enabled: true
        paths:
          -  c:\Windows\System32\dns\DNS.log
        processors:
          - add_fields:
              fields: 
                vendor: "microsoft"
                product: "dns"

  Example
  Example for detailed Windows DNS log collection:

      filebeat.inputs:
      - type: log
        enabled: true
        paths:
          -  c:\Windows\System32\dns\DNS.log
        multiline.type: pattern
        multiline.pattern: '^(?:\d{1,2}\/){2}\d{4}\s(?:\d{1,2}\:){2}\d\d\s(?:AM|PM)'
        multiline.negate: true
        multiline.match: after
        processors:
          - add_fields:
              fields: 
                vendor: "microsoft"
                product: "dns"

7.  To finish creating your new profile, click **Create**.

- Your new profile will be listed under the applicable platform on the
  **XDR Collectors Profiles** page.

8.  Apply profiles to XDR Collector machine policies by performing one
    of the following:

    - Right-click a profile, and select
      **Create a new policy rule using this profile**.

    - Launch the new policy wizard from XDR Collectors \> Policies \>
      XDR Collectors Policies.

##### Add an XDR Collector profile for Linux

> **Note**
>
> Ingestion of log events larger than 5 MB is not supported.

An XDR Collector Linux profile defines the data that is collected from a
Linux collector machine. For Linux, you can configure a Filebeat profile
and a Settings profile.

The Filebeat profile uses a configuration file in YAML format. To
facilitate the configuration of the YAML file, you can use
out-of-the-box collection templates, and templates added by the content
packs installed from the XSIAM Marketplace. Templates save you time, and
don\'t require previous knowledge of configuration file generation. You
can edit and combine the provided templates, and you can add your own
collection settings to the configuration file.

- Use an **XDR Collector Linux Filebeat profile** to collect file and
  log data using the Elasticsearch Filebeat default configuration file,
  called `filebeat.yml`.

<!-- -->

- Cortex XSIAM supports using Filebeat version 8.15 with the operating
  systems listed in the Elasticsearch Support Matrix that conform with
  the collector machine operating systems supported by Cortex XSIAM.
  Cortex XSIAM supports the input types and modules available in
  Elasticsearch Filebeat.

  > **Note**

  - > Fileset validation is enforced. You must enable at least one
    > fileset in the module, because filesets are disabled by default.

  - > Cortex XSIAM collects all logs in either an uncompressed JSON or
    > text format. Compressed files, such as the gzip format, are not
    > supported.

  - > Cortex XSIAM supports logs in single line format or multiline
    > format. For more information about handling messages that span
    > multiple lines of text in Elasticsearch Filebeat, see [Manage
    > Multiline
    > Messages](https://www.elastic.co/guide/en/beats/filebeat/current/multiline-examples.html).

  Related Information
  - [Elasticsearch Filebeat Overview
    Documentation](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-overview.html#filebeat-overview)

  - [Configure Filebeat Inputs in
    Elasticsearch](https://www.elastic.co/guide/en/beats/filebeat/current/configuration-filebeat-options.html)

  - [Configure Filebeat Modules in
    Elasticsearch](https://www.elastic.co/guide/en/beats/filebeat/current/configuration-filebeat-modules.html)

  - [Elasticsearch Support
    Matrix](https://www.elastic.co/support/matrix)

  - [/document/preview/1047108#UUID-ca91a864-7ee1-8be9-85ec-f71a3d073859](/document/preview/1047108#UUID-ca91a864-7ee1-8be9-85ec-f71a3d073859)

<!-- -->

- Use an **XDR Collector Settings profile** to configure automatic
  upgrade settings for XDR Collector releases.

To map your XDR Collector profile to a collector machine, you must use
an XDR Collector policy. After you have created your profile, map it to
a new or existing policy.

**How to configure XDR Collector profiles**

Filebeat profile

In the **Filebeat Configuration File** editor, you can define the data
collection for your Elasticsearch Filebeat configuration file called
`filebeat.yml`.

Cortex XSIAM provides YAML templates for XDR Collector Logs, Linux
(RHEL/CentOS), NGINX (Linux), Linux (Debian/Ubuntu), and any templates
added by the content packs installed from the XSIAM Marketplace.

1.  In Cortex XSIAM, select Settings \> Configurations \> XDR Collectors
    \> Profiles \> +Add Profile \> Linux.

2.  Select **Filebeat**, then click **Next**.

3.  Configure the **General Information** parameters.

    - **Profile Name**: Enter a unique name to identify the profile. The
      name can contain only letters, numbers, or spaces, and must be no
      more than 30 characters. The name that you enter here will be
      displayed in the list of profiles when you configure a policy.

    - (*Optional*) **Add description here**: To provide additional
      context for the purpose or business reason for your new profile,
      enter a profile description.

4.  In the **Filebeat Configuration File** editing box, type or paste
    the contents of your configuration file, or use a template. To add a
    template, select one from the list, and click **Add**.

5.  Cortex XSIAM supports all sections in the `filebeat.yml`
    configuration file, such as support for Filebeat fields and tags.
    You can use the \"Add fields\" processor to identify the
    product/vendor for the data collected by the XDR Collectors, so that
    the collected events go through the ingestion flow (Parsing Rules).
    To configure the product/vendor, ensure that you use the default
    `fields` attribute (do not use the **target** attribute), as shown
    in the following example:

- processors:
        - add_fields:
            fields:
              vendor: <Vendor>
              product: <Product>

  For more information about the \"Add fields\" processor, see
  [Add_fields](https://www.elastic.co/guide/en/beats/filebeat/current/add-fields.html).

6.  To finish creating your new profile, click **Create**.

- Your new profile will be listed under the applicable platform on the
  **XDR Collectors Profiles** page.

7.  Apply profiles to XDR Collector machine policies by performing one
    of the following:

    - Right-click a profile, and select
      **Create a new policy rule using this profile**.

    - Launch the new policy wizard from XDR Collectors \> Policies \>
      XDR Collectors Policies.

Settings profile

You can configure automatic upgrades for XDR Collector releases. By
default, this is disabled, and the **Use Default (Disabled)** option is
selected. To implement automatic upgrades, follow these steps:

1.  In Cortex XSIAM, select Settings \> Configurations \> XDR Collectors
    \> Profiles \> +Add Profile \> Linux.

2.  Select **Settings** profile, then click **Next**.

3.  Configure the **General Information** parameters.

    - **Profile Name**: Enter a unique name to identify the profile. The
      name can contain only letters, numbers, or spaces, and must be no
      more than 30 characters. The name that you enter here will be
      displayed in the list of profiles when you configure a policy.

    - (*Optional*) **Add description here**: To provide additional
      context for the purpose or business reason for your new profile,
      enter a profile description.

4.  Clear the **Use Default (Disabled)** checkbox.

5.  For **Collector Auto-Upgrade**, select **Enabled**.

- Additional fields are displayed for defining the scope of the
  automatic upgrade.

6.  Configure the scope of automatic upgrades:

    - To ensure the latest XDR Collector release is used, leave the
      **Use Default (Latest collector release)** checkbox selected.

    - To configure only a particular scope, perform the following steps:

      1.  Clear the **Use Default (Latest collector release)** checkbox.

      2.  For **Auto Upgrade Scope**, select one of the following
          options:

  -----------------------------------------------------------------------
  Option                              More details
  ----------------------------------- -----------------------------------
  Latest collector release            Configures the scope of the
                                      automatic upgrade to whenever a new
                                      XDR Collector release is available
                                      including maintenance releases and
                                      new features.

  Only maintenance release            Configures the scope of the
                                      automatic upgrade to whenever a new
                                      XDR Collector maintenance release
                                      is available.

  Only maintenance releases in a      Configures the scope of the
  specific version                    automatic upgrade to whenever a new
                                      XDR Collector maintenance release
                                      is available for a specific
                                      version. When this option is
                                      selected, you can select the
                                      specific **Release Version**.
  -----------------------------------------------------------------------

7.  To finish creating your new profile, click **Create**.

- Your new profile will be listed under the applicable platform on the
  **XDR Collectors Profiles** page.

8.  Apply profiles to XDR Collector machine policies by performing one
    of the following:

    - Right-click a profile, and select
      **Create a new policy rule using this profile**.

    - Launch the new policy wizard from XDR Collectors \> Policies \>
      XDR Collectors Policies.

**Additional XDR Collector profile management options**

As needed, you can return to the **XDR Collectors Profiles** page to
manage your XDR Collectors profiles. To manage a specific profile, right
click anywhere in an XDR Collector profile row, and select the desired
action:

  -----------------------------------------------------------------------
  Option                              More details
  ----------------------------------- -----------------------------------
  Edit                                Lets you edit the XDR Collector
                                      profile

  Save As New                         Copies the existing profile with
                                      its current settings, so that you
                                      can make modifications, and save it
                                      as a new profile with a unique name

  Delete                              Deletes the XDR Collector profile

  View Collector Policies             Opens a new tab that displays the
                                      **XDR Collectors Policies** page,
                                      showing the policies that are
                                      currently associated with your XDR
                                      Collector profiles

  Copy text to clipboard              Copies the text from a specific
                                      field in the row of a XDR Collector
                                      profile

  Copy entire row                     Copies the text from the entire row
                                      of a XDR Collector profile
  -----------------------------------------------------------------------

#### Apply profiles to collection machine policies

Enable a Cortex XDR Collector profile by mapping it to a policy. Each
policy that you create must apply to one or more collector machines or
collector machine groups.

1.  In Cortex XSIAM, do one of the following:

    - To create a policy from scratch on the **XDR Collectors Policies**
      page, select Settings \> Configurations \> XDR Collectors \>
      Policies \> +Add Policy.

    - To add a profile to an existing policy, select Settings \>
      Configurations \> XDR Collectors \> Policies, then right-click the
      policy that you want to edit, and select **Edit**.

    - To create a new policy from a profile on the
      **XDR Collectors Profiles** page, select Settings \>
      Configurations \> XDR Collectors \> Profiles, right-click the
      profile, and select
      **Create a new policy rule using this profile**.

2.  Configure the **General** settings for the policy:

    a.  **Policy Name**: Enter a unique name to identify the policy. The
        name can contain only letters, numbers, or spaces, and must be
        no more than 30 characters. The name that you enter here will be
        displayed when you view and configure policies.

    b.  (*Optional*) **Description**: To provide additional context for
        the purpose or business reason for your policy, enter a policy
        description.

    c.  **Platform**: Select the operating system of the XDR Collector
        machines that will use the policy.

    d.  Select the profiles that you want to map to the policy. If you
        do not specify a profile, the XDR Collector uses the **Default**
        profile.

    e.  Click **Next**.

3.  On the **XDR Collectors Endpoints** page, select the XDR Collectors
    (endpoints) or XDR Collector groups to which you want to map the
    policy. You can use the provided filters to find XDR Collectors
    listed on this page.

- Cortex XSIAM automatically applies a filter for the platform that you
  selected in the previous step. To change the platform, go **Back** to
  the general policy settings.

4.  Click **Next**.

5.  On the **Summary** page, review the settings that you configured for
    the new policy.

- If everything is correct, click **Done**. Otherwise, click **Back** to
  make changes.

6.  (*Optional*) If necessary, change a policy\'s position relative to
    other policies in the table on the **XDR Collectors Policies** page.

- The XDR Collector evaluates policies from top to bottom. When an XDR
  Collector finds the first match, it applies that policy as the active
  policy. To change the policy order, click and drag the arrows in the
  **Name** cell of a policy to the desired location in the policy
  hierarchy.

**Additional XDR Collector policy management options**

As needed, you can return to the **XDR Collectors Policies** page to
manage your XDR Collector policies. To manage a specific policy,
right-click anywhere in an XDR Collector policy row, and select the
desired action. You cannot delete or disable default policies.

  -----------------------------------------------------------------------
  Option                              More details
  ----------------------------------- -----------------------------------
  Disable                             Disables the selected XDR Collector
                                      policy

  Delete                              Deletes the selected XDR Collector
                                      policy

  View Policy Details                 Opens a new dialog box that
                                      displays details about the profiles
                                      mapped to the policy

  Save As New                         Copies the existing policy with its
                                      current settings, so that you can
                                      make modifications, and save it as
                                      a new policy with a different name

  Edit                                Lets you edit the XDR Collector
                                      policy

  Copy text to clipboard              Copies the text from a specific
                                      field in the row of a XDR Collector
                                      policy

  Copy entire row                     Copies the text from the entire row
                                      of a XDR Collector policy
  -----------------------------------------------------------------------

#### XDR Collector datasets

After Cortex XSIAM begins receiving data from your XDR Collectors
configuration that are dedicated for on-premises data collection on
Windows and Linux machines.

- For Filebeat, the app automatically creates an Cortex Query Language
  (XQL) dataset of event logs using the vendor name and the product name
  specified in the configuration file section of the Filebeat profile.
  The dataset name follows the format `<vendor>_<product>_raw`. If not
  specified, Cortex XSIAM automatically creates a new default dataset in
  the format `<module>_<module>_raw` or `<input>_<input>_raw`. For
  example, if you are using the NGINX module, the dataset is called
  `nginx_nginx_raw`.

- For Winlogbeat, the app automatically creates an XQL dataset of event
  logs using the vendor name and the product name specified in the
  configuration file section of the Winlogbeat profile. The dataset name
  follows the format `<vendor>_<product>_raw`. If not specified, Cortex
  XSIAM automatically creates a new default dataset,
  `microsoft_windows_raw`, for event log collection. Winlogbeat data is
  also normalized to `xdr_data` (and thus the `xdr_event_log` preset).

After Cortex XSIAM creates the dataset, you can search for your XDR
Collector data using XQL Search.

### Data Collection

Learn about the types of data that can be collected into your system,
vendor support, and how to configure collection.

#### Visibility of logs and issues from external sources

> **Note**
>
> Data collection may require an add-on.

The following table describes the visibility of each vendor and device
type, and where you can view information ingested from external sources,
depending on the data source.

A ![](media/rId635.png){width="0.20833333333333334in"
height="0.20833333333333334in"} indicates support and a dash (---)
indicates the feature is not supported.

##### Network connections

+---------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| Vendor and Device Type                                                    | Raw Data Visibility                                | Normalized Log Visibility                          | Cortex XSIAM Issue Visibility                      | Vendor Alert Visibility                            |
+===========================================================================+====================================================+====================================================+====================================================+====================================================+
| [Amazon S3 (flow logs)](#UUID5617c36b719ce5e3c8a354c63606f30a)            | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                |
|                                                                           | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     |                                                    |
|                                                                           |                                                    |                                                    |                                                    |                                                    |
|                                                                           | Raw data is searchable in XQL Search.              | Option to ingest network flow logs as Cortex       | Cortex XSIAM can generate Cortex XSIAM issues      |                                                    |
|                                                                           |                                                    | XSIAM network connection stories that are          | (Analytics, IOC, BIOC, and Correlation Rules) when |                                                    |
|                                                                           |                                                    | searchable in the Query Builder and in XQL Search. | relevant from logs.                                |                                                    |
|                                                                           |                                                    |                                                    |                                                    |                                                    |
|                                                                           |                                                    |                                                    | > **Note**                                         |                                                    |
|                                                                           |                                                    |                                                    | >                                                  |                                                    |
|                                                                           |                                                    |                                                    | > While Correlation Rules issues are generated on  |                                                    |
|                                                                           |                                                    |                                                    | > non-normalized and normalized logs, Analytics,   |                                                    |
|                                                                           |                                                    |                                                    | > IOC and BIOC issues are only generated on        |                                                    |
|                                                                           |                                                    |                                                    | > normalized logs.                                 |                                                    |
+---------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [Amazon S3 (Route 53 logs)](#UUIDca098ab3cc1668f15f34d2c25224b507)        | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                |
|                                                                           | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     |                                                    |
|                                                                           |                                                    |                                                    |                                                    |                                                    |
|                                                                           | Raw data is searchable in XQL Search.              | Option to ingest network Route 53 DNS logs         | Cortex XSIAM can generate Cortex XSIAM issues      |                                                    |
|                                                                           |                                                    | as Cortex XSIAM network connection stories that    | (Analytics, IOC, BIOC, and Correlation Rules) when |                                                    |
|                                                                           |                                                    | are searchable in the Query Builder and in XQL     | relevant from logs.                                |                                                    |
|                                                                           |                                                    | Search.                                            |                                                    |                                                    |
|                                                                           |                                                    |                                                    | > **Note**                                         |                                                    |
|                                                                           |                                                    |                                                    | >                                                  |                                                    |
|                                                                           |                                                    |                                                    | > While Correlation Rules issues are generated on  |                                                    |
|                                                                           |                                                    |                                                    | > non-normalized and normalized logs, Analytics,   |                                                    |
|                                                                           |                                                    |                                                    | > IOC and BIOC issues are only generated on        |                                                    |
|                                                                           |                                                    |                                                    | > normalized logs.                                 |                                                    |
+---------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [Azure Event                                                              | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                |
| Hub](/document/preview/1037046#UUID-002a2d13-12a4-36f9-d2e8-b330eb0f0a1d) | height="0.2777777777777778in"}                     |                                                    | height="0.2777777777777778in"}                     |                                                    |
|                                                                           |                                                    |                                                    |                                                    |                                                    |
|                                                                           | Raw data is searchable in XQL Search.              |                                                    | Cortex XSIAM can generate Cortex XSIAM issues      |                                                    |
|                                                                           |                                                    |                                                    | (Correlation Rules only) when relevant from flow   |                                                    |
|                                                                           |                                                    |                                                    | logs.                                              |                                                    |
+---------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [Azure Network Watcher (flow                                              | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                |
| logs)](#UUID1d467284cb48b500b897369b4830bfc3)                             | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     |                                                    |
|                                                                           |                                                    |                                                    |                                                    |                                                    |
|                                                                           | Raw data is searchable in XQL Search.              | Option to ingest network flow logs as Cortex       | Cortex XSIAM can generate Cortex XSIAM issues      |                                                    |
|                                                                           |                                                    | XSIAM network connection stories that are          | (Analytics, IOC, BIOC, and Correlation Rules) when |                                                    |
|                                                                           |                                                    | searchable in the Query Builder and in XQL Search. | relevant from flow logs.                           |                                                    |
|                                                                           |                                                    |                                                    |                                                    |                                                    |
|                                                                           |                                                    |                                                    | > **Note**                                         |                                                    |
|                                                                           |                                                    |                                                    | >                                                  |                                                    |
|                                                                           |                                                    |                                                    | > While Correlation Rules issues are generated on  |                                                    |
|                                                                           |                                                    |                                                    | > non-normalized and normalized logs, Analytics,   |                                                    |
|                                                                           |                                                    |                                                    | > IOC and BIOC issues are only generated on        |                                                    |
|                                                                           |                                                    |                                                    | > normalized logs.                                 |                                                    |
+---------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [Check Point FW1/VPN1](#UUID836518f132520c5006f82ed736c6eceb)             | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" |
|                                                                           | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     |
|                                                                           |                                                    |                                                    |                                                    |                                                    |
|                                                                           | Raw data is searchable in XQL Search.              | Network stories that include Check Point network   | Cortex XSIAM can generate Cortex XSIAM issues      | Alerts from Check Point firewalls are generated    |
|                                                                           |                                                    | connection logs are searchable in the Query        | (Analytics, IOC, BIOC, and Correlation Rules) when | throughout Cortex XSIAM when relevant.             |
|                                                                           | > **Note**                                         | Builder and in XQL Search.                         | relevant from logs.                                |                                                    |
|                                                                           | >                                                  |                                                    |                                                    |                                                    |
|                                                                           | > Logs with `sessionid = 0` are dropped.           | > **Note**                                         | > **Note**                                         |                                                    |
|                                                                           |                                                    | >                                                  | >                                                  |                                                    |
|                                                                           |                                                    | > Logs with `sessionid = 0` are dropped.           | > While Correlation Rules issues are generated on  |                                                    |
|                                                                           |                                                    |                                                    | > non-normalized and normalized logs, Analytics,   |                                                    |
|                                                                           |                                                    |                                                    | > IOC and BIOC issues are only generated on        |                                                    |
|                                                                           |                                                    |                                                    | > normalized logs.                                 |                                                    |
+---------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [Corelight Zeek](#UUID0740e5796b8c841be59b7b5a103cf12e)                   | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                |
|                                                                           | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     |                                                    |
|                                                                           |                                                    |                                                    |                                                    |                                                    |
|                                                                           | Raw data is searchable in XQL Search.              | Network stories that include Corelight Zeek        | Cortex XSIAM can generate Cortex XSIAM issues      |                                                    |
|                                                                           |                                                    | network connection logs are searchable in the      | (Analytics, IOC, BIOC, and Correlation Rules) when |                                                    |
|                                                                           |                                                    | Query Builder and in XQL Search.                   | relevant from logs.                                |                                                    |
|                                                                           |                                                    |                                                    |                                                    |                                                    |
|                                                                           |                                                    |                                                    | > **Note**                                         |                                                    |
|                                                                           |                                                    |                                                    | >                                                  |                                                    |
|                                                                           |                                                    |                                                    | > While Correlation Rules issues are generated on  |                                                    |
|                                                                           |                                                    |                                                    | > non-normalized and normalized logs, Analytics,   |                                                    |
|                                                                           |                                                    |                                                    | > IOC and BIOC issues are only generated on        |                                                    |
|                                                                           |                                                    |                                                    | > normalized logs.                                 |                                                    |
+---------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [Cisco ASA and Cisco AnyConnect                                           | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                |
| VPN](#UUID67771daf4b0571e9089b936cb4416b17)                               | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     |                                                    |
|                                                                           |                                                    |                                                    |                                                    |                                                    |
|                                                                           | Raw data is searchable in XQL Search.              | Network stories that include Cisco network         | Cortex XSIAM can generate Cortex XSIAM issues      |                                                    |
|                                                                           |                                                    | connection logs are searchable in the Query        | (Analytics, IOC, BIOC, and Correlation Rules) when |                                                    |
|                                                                           |                                                    | Builder and in XQL Search.                         | relevant from logs.                                |                                                    |
|                                                                           |                                                    |                                                    |                                                    |                                                    |
|                                                                           |                                                    |                                                    | > **Note**                                         |                                                    |
|                                                                           |                                                    |                                                    | >                                                  |                                                    |
|                                                                           |                                                    |                                                    | > While Correlation Rules issues are generated on  |                                                    |
|                                                                           |                                                    |                                                    | > non-normalized and normalized logs, Analytics,   |                                                    |
|                                                                           |                                                    |                                                    | > IOC and BIOC issues are only generated on        |                                                    |
|                                                                           |                                                    |                                                    | > normalized logs.                                 |                                                    |
+---------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [Fortinet Fortigate](#UUIDb5cc710d2c80ee7476ecc7ffce22338a)               | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" |
|                                                                           | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     |
|                                                                           |                                                    |                                                    |                                                    |                                                    |
|                                                                           | Raw data is searchable in XQL Search.              | Network stories that include Fortinet network      | Cortex XSIAM can generate Cortex XSIAM issues      | Alerts from Fortinet firewalls are generated       |
|                                                                           |                                                    | connection logs are searchable in the Query        | (Analytics, IOC, BIOC, and Correlation Rules) when | throughout Cortex XSIAM when relevant.             |
|                                                                           |                                                    | Builder and in XQL Search.                         | relevant from logs.                                |                                                    |
|                                                                           |                                                    |                                                    |                                                    |                                                    |
|                                                                           |                                                    |                                                    | > **Note**                                         |                                                    |
|                                                                           |                                                    |                                                    | >                                                  |                                                    |
|                                                                           |                                                    |                                                    | > While Correlation Rules issues are generated on  |                                                    |
|                                                                           |                                                    |                                                    | > non-normalized and normalized logs, Analytics,   |                                                    |
|                                                                           |                                                    |                                                    | > IOC and BIOC issues are only generated on        |                                                    |
|                                                                           |                                                    |                                                    | > normalized logs.                                 |                                                    |
+---------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [Google Cloud Platform (flow logs, DNS                                    | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                |
| logs)](#UUIDb95ef00075ad79e026c18a1695611928)                             | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     |                                                    |
|                                                                           |                                                    |                                                    |                                                    |                                                    |
|                                                                           | Raw data is searchable in XQL Search.              | Option to ingest network flow logs as Cortex       | Cortex XSIAM can generate Cortex XSIAM issues      |                                                    |
|                                                                           |                                                    | XSIAM network connection stories and Google Cloud  | (Analytics, IOC, BIOC, and Correlation Rules) when |                                                    |
|                                                                           |                                                    | DNS logs that are searchable in the Query Builder  | relevant from logs.                                |                                                    |
|                                                                           |                                                    | and in XQL Search.                                 |                                                    |                                                    |
|                                                                           |                                                    |                                                    | > **Note**                                         |                                                    |
|                                                                           |                                                    |                                                    | >                                                  |                                                    |
|                                                                           |                                                    |                                                    | > While Correlation Rules issues are generated on  |                                                    |
|                                                                           |                                                    |                                                    | > non-normalized and normalized logs, Analytics,   |                                                    |
|                                                                           |                                                    |                                                    | > IOC and BIOC issues are only generated on        |                                                    |
|                                                                           |                                                    |                                                    | > normalized logs.                                 |                                                    |
+---------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [Okta](#UUID698f9cbf8fc99992ef63a9c29ab549d2)                             | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                |
|                                                                           | height="0.2777777777777778in"}                     |                                                    | height="0.2777777777777778in"}                     |                                                    |
|                                                                           |                                                    |                                                    |                                                    |                                                    |
|                                                                           | Raw data is searchable in XQL Search.              |                                                    | Cortex XSIAM can generate Cortex XSIAM issues      |                                                    |
|                                                                           |                                                    |                                                    | (Analytics, IOC, BIOC, and Correlation Rules) when |                                                    |
|                                                                           |                                                    |                                                    | relevant from logs.                                |                                                    |
|                                                                           |                                                    |                                                    |                                                    |                                                    |
|                                                                           |                                                    |                                                    | > **Note**                                         |                                                    |
|                                                                           |                                                    |                                                    |                                                    |                                                    |
|                                                                           |                                                    |                                                    | - > While Correlation Rules issues are generated   |                                                    |
|                                                                           |                                                    |                                                    |   > on non-normalized and normalized logs,         |                                                    |
|                                                                           |                                                    |                                                    |   > Analytics, IOC and BIOC issues are only        |                                                    |
|                                                                           |                                                    |                                                    |   > generated on normalized logs.                  |                                                    |
|                                                                           |                                                    |                                                    |                                                    |                                                    |
|                                                                           |                                                    |                                                    | - > IOCs and BIOCs are only generated for these    |                                                    |
|                                                                           |                                                    |                                                    |   > event types: `sso` and `session_start`.        |                                                    |
+---------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [Prisma Access Browser](#UUID01e59e4df1b8a022e18f47b5255005a6)            | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                | ---                                                | ---                                                |
|                                                                           | height="0.2777777777777778in"}                     |                                                    |                                                    |                                                    |
|                                                                           |                                                    |                                                    |                                                    |                                                    |
|                                                                           | Raw data is searchable in XQL Search.              |                                                    |                                                    |                                                    |
+---------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [Windows DHCP via Elasticsearch                                           | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                | ---                                                |
| Filebeat](#UUID43a820a2b38a854f5f8f147510f2cd96)                          | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     |                                                    |                                                    |
|                                                                           |                                                    |                                                    |                                                    |                                                    |
|                                                                           | Raw data is searchable in XQL Search.              | Cortex XSIAM uses Windows DHCP logs to enrich your |                                                    |                                                    |
|                                                                           |                                                    | network logs with hostnames and MAC addresses that |                                                    |                                                    |
|                                                                           |                                                    | are searchable in XQL Search.                      |                                                    |                                                    |
+---------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [Zscaler Internet Access (ZIA)](#UUID9e297bb3c6808428c6f123df5498d4df)    | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                |
|                                                                           | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     |                                                    |
|                                                                           |                                                    |                                                    |                                                    |                                                    |
|                                                                           | Raw data is searchable in XQL Search.              | Network stories that include ZIA network           | Cortex XSIAM can generate Cortex XSIAM issues      |                                                    |
|                                                                           |                                                    | connection and firewall logs are searchable in the | (Analytics, IOC, BIOC, and Correlation Rules) when |                                                    |
|                                                                           |                                                    | Query Builder and in XQL Search.                   | relevant from logs.                                |                                                    |
|                                                                           |                                                    |                                                    |                                                    |                                                    |
|                                                                           |                                                    |                                                    | > **Note**                                         |                                                    |
|                                                                           |                                                    |                                                    |                                                    |                                                    |
|                                                                           |                                                    |                                                    | - > While Correlation Rules issues are generated   |                                                    |
|                                                                           |                                                    |                                                    |   > on non-normalized and normalized logs,         |                                                    |
|                                                                           |                                                    |                                                    |   > Analytics, IOC and BIOC issues are only        |                                                    |
|                                                                           |                                                    |                                                    |   > generated on normalized logs.                  |                                                    |
|                                                                           |                                                    |                                                    |                                                    |                                                    |
|                                                                           |                                                    |                                                    | - > Analytics, IOCs and BIOCs are only generated   |                                                    |
|                                                                           |                                                    |                                                    |   > on the Firewall data.                          |                                                    |
|                                                                           |                                                    |                                                    |                                                    |                                                    |
|                                                                           |                                                    |                                                    | - > The Zscaler Nanolog Streaming Service (NSS)    |                                                    |
|                                                                           |                                                    |                                                    |   > feed for web logs is only used for Correlation |                                                    |
|                                                                           |                                                    |                                                    |   > Rules and threat hunting.                      |                                                    |
+---------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [Zscaler Private Access (ZPA)](#UUID9813b19d9b3f21373ba410a8bc37fa77)     | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                |
|                                                                           | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     |                                                    |
|                                                                           |                                                    |                                                    |                                                    |                                                    |
|                                                                           | Raw data is searchable in XQL Search.              | Network stories that include ZPA network           | Cortex XSIAM can generate Cortex XSIAM issues      |                                                    |
|                                                                           |                                                    | connection logs are searchable in the Query        | (Analytics, IOC, BIOC, and Correlation Rules) when |                                                    |
|                                                                           |                                                    | Builder and in XQL Search.                         | relevant from logs.                                |                                                    |
|                                                                           |                                                    |                                                    |                                                    |                                                    |
|                                                                           |                                                    |                                                    | > **Note**                                         |                                                    |
|                                                                           |                                                    |                                                    |                                                    |                                                    |
|                                                                           |                                                    |                                                    | - > While Correlation Rules issues are generated   |                                                    |
|                                                                           |                                                    |                                                    |   > on non-normalized and normalized logs,         |                                                    |
|                                                                           |                                                    |                                                    |   > Analytics, IOC and BIOC issues are only        |                                                    |
|                                                                           |                                                    |                                                    |   > generated on normalized logs.                  |                                                    |
|                                                                           |                                                    |                                                    |                                                    |                                                    |
|                                                                           |                                                    |                                                    | - > The Zscaler Nanolog Streaming Service (NSS)    |                                                    |
|                                                                           |                                                    |                                                    |   > feed for web logs is only used for Correlation |                                                    |
|                                                                           |                                                    |                                                    |   > Rules and threat hunting.                      |                                                    |
+---------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+

##### Authentication services/Audit logs

+-----------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------+
| Vendor and Device Type                                                      | Raw Data Visibility                                | Normalized Log Visibility                          | Cortex XSIAM Issue Visibility                      | Vendor      |
|                                                                             |                                                    |                                                    |                                                    | Alert       |
|                                                                             |                                                    |                                                    |                                                    | Visibility  |
+=============================================================================+====================================================+====================================================+====================================================+=============+
| [Amazon S3 (audit logs)](#UUIDc1ab7bea87ed20d21accce03ccec7e3a)             | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ---         |
|                                                                             | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     |             |
|                                                                             |                                                    |                                                    |                                                    |             |
|                                                                             | Logs and stories are searchable in XQL Search      | Option to stitch audit logs with authentication    | Cortex XSIAM can generate Cortex XSIAM issues      |             |
|                                                                             |                                                    | stories that are searchable in the Query Builder   | (Analytics, IOC, BIOC, and Correlation Rules) when |             |
|                                                                             |                                                    | and XQL Search.                                    | relevant from logs.                                |             |
|                                                                             |                                                    |                                                    |                                                    |             |
|                                                                             |                                                    |                                                    | > **Note**                                         |             |
|                                                                             |                                                    |                                                    | >                                                  |             |
|                                                                             |                                                    |                                                    | > While Correlation Rules issues are generated on  |             |
|                                                                             |                                                    |                                                    | > non-normalized and normalized logs, Analytics,   |             |
|                                                                             |                                                    |                                                    | > IOC and BIOC issues are only generated on        |             |
|                                                                             |                                                    |                                                    | > normalized logs.                                 |             |
+-----------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------+
| [Azure Event Hub (audit logs, AKS                                           | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ---         |
| logs)](/document/preview/1037046#UUID-002a2d13-12a4-36f9-d2e8-b330eb0f0a1d) | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     |             |
|                                                                             |                                                    |                                                    |                                                    |             |
|                                                                             | Logs and stories are searchable in XQL Search      | Option to stitch audit logs with authentication    | Cortex XSIAM can generate Cortex XSIAM issues      |             |
|                                                                             |                                                    | stories that are searchable in the Query Builder   | (Analytics, IOC, BIOC, and Correlation Rules) when |             |
|                                                                             |                                                    | and XQL Search.                                    | relevant from logs.                                |             |
|                                                                             |                                                    |                                                    |                                                    |             |
|                                                                             |                                                    |                                                    | > **Note**                                         |             |
|                                                                             |                                                    |                                                    | >                                                  |             |
|                                                                             |                                                    |                                                    | > While Correlation Rules issues are generated on  |             |
|                                                                             |                                                    |                                                    | > non-normalized and normalized logs, Analytics,   |             |
|                                                                             |                                                    |                                                    | > IOC and BIOC issues are only generated on        |             |
|                                                                             |                                                    |                                                    | > normalized logs.                                 |             |
+-----------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------+
| [Google Cloud Platform (audit logs, GKE                                     | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ---         |
| logs)](#UUIDb95ef00075ad79e026c18a1695611928)                               | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     |             |
|                                                                             |                                                    |                                                    |                                                    |             |
|                                                                             | Raw data is searchable in XQL Search.              | Option to stitch audit logs with authentication    | Cortex XSIAM can generate Cortex XSIAM issues      |             |
|                                                                             |                                                    | stories that are searchable in the Query Builder   | (Analytics, IOC, BIOC, and Correlation Rules) when |             |
|                                                                             |                                                    | and XQL Search.                                    | relevant from logs.                                |             |
|                                                                             |                                                    |                                                    |                                                    |             |
|                                                                             |                                                    |                                                    | > **Note**                                         |             |
|                                                                             |                                                    |                                                    | >                                                  |             |
|                                                                             |                                                    |                                                    | > While Correlation Rules issues are generated on  |             |
|                                                                             |                                                    |                                                    | > non-normalized and normalized logs, Analytics,   |             |
|                                                                             |                                                    |                                                    | > IOC and BIOC issues are only generated on        |             |
|                                                                             |                                                    |                                                    | > normalized logs.                                 |             |
+-----------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------+
| [Google Workspace](#UUID1218397b9beed848e97e29c6e0dbc71e)                   | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ---         |
|                                                                             | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     |             |
|                                                                             |                                                    |                                                    |                                                    |             |
|                                                                             | Raw data is searchable in XQL Search.              | Relevant Login, Token, Google drive, SAML, Admin   | For all logs, Cortex XSIAM can generate Cortex     |             |
|                                                                             |                                                    | Console, Enterprise Groups, and Rules audit logs   | XSIAM issues (Analytics and Correlation Rules)     |             |
|                                                                             |                                                    | normalized into authentication stories. All are    | when relevant from logs.                           |             |
|                                                                             |                                                    | searchable in the Query Builder.                   |                                                    |             |
+-----------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------+
| [Microsoft 365 email](#UUIDd0bd27c38c86c7e6ef7b65788c7fe549)                | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ---         |
|                                                                             | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     |             |
|                                                                             |                                                    |                                                    |                                                    |             |
|                                                                             | Email logs are searchable in XQL Search            | Microsoft 365 normalized email stories             | For Microsoft 365 email logs, Cortex XSIAM can     |             |
|                                                                             |                                                    |                                                    | also generate Cortex XSIAM issues (Analytics, IOC, |             |
|                                                                             |                                                    |                                                    | BIOC, and Correlation Rules) when relevant from    |             |
|                                                                             |                                                    |                                                    | the email logs.                                    |             |
+-----------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------+
| [Microsoft Office                                                           | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ---         |
| 365](/document/preview/1038728#UUID-96e29ceb-6654-3926-4323-e7d527326eba)   | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     |             |
|                                                                             |                                                    |                                                    |                                                    |             |
|                                                                             | Logs and stories (Azure AD authentication and      | Azure AD authentication logs and Azure AD Sign-in  | For all Microsoft Office 365 logs, Cortex          |             |
|                                                                             | audit logs only) are searchable in XQL Search      | logs normalized into authentication stories. Azure | XSIAM can also generate Cortex XSIAM issues        |             |
|                                                                             |                                                    | AD audit logs normalized to cloud audit logs       | (Analytics, IOC, BIOC, and Correlation Rules) when |             |
|                                                                             |                                                    | stories. Exchange Online, SharePoint Online, and   | relevant from Office 365 logs.                     |             |
|                                                                             |                                                    | General audit logs normalized into stories. All    |                                                    |             |
|                                                                             |                                                    | are searchable in the Query Builder.               | > **Note**                                         |             |
|                                                                             |                                                    |                                                    | >                                                  |             |
|                                                                             |                                                    |                                                    | > While Correlation Rules issues are generated on  |             |
|                                                                             |                                                    |                                                    | > non-normalized and normalized logs, Analytics,   |             |
|                                                                             |                                                    |                                                    | > IOC and BIOC issues are only generated on        |             |
|                                                                             |                                                    |                                                    | > normalized logs.                                 |             |
+-----------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------+
| [Okta](#UUID698f9cbf8fc99992ef63a9c29ab549d2)                               | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ---         |
|                                                                             | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     |             |
|                                                                             |                                                    |                                                    |                                                    |             |
|                                                                             | Logs and stories are searchable in XQL Search      | Logs stitched with authentication stories are      | Cortex XSIAM can generate Cortex XSIAM issues      |             |
|                                                                             |                                                    | searchable in the Query Builder.                   | (Analytics, IOC, BIOC, and Correlation Rules only) |             |
|                                                                             |                                                    |                                                    | when relevant from logs.                           |             |
|                                                                             |                                                    |                                                    |                                                    |             |
|                                                                             |                                                    |                                                    | > **Note**                                         |             |
|                                                                             |                                                    |                                                    |                                                    |             |
|                                                                             |                                                    |                                                    | - > While Correlation Rules issues are generated   |             |
|                                                                             |                                                    |                                                    |   > on non-normalized and normalized logs,         |             |
|                                                                             |                                                    |                                                    |   > Analytics, IOC and BIOC issues are only        |             |
|                                                                             |                                                    |                                                    |   > generated on normalized logs.                  |             |
|                                                                             |                                                    |                                                    |                                                    |             |
|                                                                             |                                                    |                                                    | - > IOCs and BIOCs are only generated for these    |             |
|                                                                             |                                                    |                                                    |   > event types: `sso` and `session_start`.        |             |
+-----------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------+
| [OneLogin](#UUID0702bd66b5ef03d6d14c3ae2350776e7)                           | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ---         |
|                                                                             | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     |             |
|                                                                             |                                                    |                                                    |                                                    |             |
|                                                                             | Raw data is searchable in XQL Search.              | All log types are normalized into authentication   | Cortex XSIAM can generate Cortex XSIAM issues      |             |
|                                                                             |                                                    | stories, and are searchable in the Query Builder.  | (Analytics, IOC, BIOC, and Correlation Rules) when |             |
|                                                                             |                                                    |                                                    | relevant from logs.                                |             |
|                                                                             |                                                    |                                                    |                                                    |             |
|                                                                             |                                                    |                                                    | > **Note**                                         |             |
|                                                                             |                                                    |                                                    | >                                                  |             |
|                                                                             |                                                    |                                                    | > While Correlation Rules issues are generated on  |             |
|                                                                             |                                                    |                                                    | > non-normalized and normalized logs, Analytics,   |             |
|                                                                             |                                                    |                                                    | > IOC and BIOC issues are only generated on        |             |
|                                                                             |                                                    |                                                    | > normalized logs.                                 |             |
+-----------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------+
| [PingFederate](#UUIDeb7d96b621aa8922d227ac30459df61b)                       | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ---         |
|                                                                             | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     |             |
|                                                                             |                                                    |                                                    |                                                    |             |
|                                                                             | Logs and stories are searchable in XQL Search      | Logs stitched with authentication stories are      | Cortex XSIAM can generate Cortex XSIAM issues      |             |
|                                                                             |                                                    | searchable in the Query Builder.                   | (Analytics, IOC, BIOC, and Correlation Rules) when |             |
|                                                                             |                                                    |                                                    | relevant from logs.                                |             |
|                                                                             |                                                    |                                                    |                                                    |             |
|                                                                             |                                                    |                                                    | > **Note**                                         |             |
|                                                                             |                                                    |                                                    | >                                                  |             |
|                                                                             |                                                    |                                                    | > While Correlation Rules issues are generated on  |             |
|                                                                             |                                                    |                                                    | > non-normalized and normalized logs, Analytics,   |             |
|                                                                             |                                                    |                                                    | > IOC and BIOC issues are only generated on        |             |
|                                                                             |                                                    |                                                    | > normalized logs.                                 |             |
+-----------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------+
| [PingOne for Enterprise](#UUID4734781ab5de02dfda2701630e5413d7)             | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ---         |
|                                                                             | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     |             |
|                                                                             |                                                    |                                                    |                                                    |             |
|                                                                             | Raw data is searchable in XQL Search               | Logs stitched with authentication stories are      | Cortex XSIAM can generate Cortex XSIAM issues      |             |
|                                                                             |                                                    | searchable in the Query Builder.                   | (Analytics, IOC, BIOC, and Correlation Rules) when |             |
|                                                                             |                                                    |                                                    | relevant from logs.                                |             |
|                                                                             |                                                    |                                                    |                                                    |             |
|                                                                             |                                                    |                                                    | > **Note**                                         |             |
|                                                                             |                                                    |                                                    | >                                                  |             |
|                                                                             |                                                    |                                                    | > While Correlation Rules issues are generated on  |             |
|                                                                             |                                                    |                                                    | > non-normalized and normalized logs, Analytics,   |             |
|                                                                             |                                                    |                                                    | > IOC and BIOC issues are only generated on        |             |
|                                                                             |                                                    |                                                    | > normalized logs.                                 |             |
+-----------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------+

##### Operation and system logs from cloud providers

+-----------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------+
| Vendor and Device Type                                                      | Raw Data Visibility                                | Normalized Log Visibility                          | Cortex XSIAM Issue Visibility                      | Vendor      |
|                                                                             |                                                    |                                                    |                                                    | Alert       |
|                                                                             |                                                    |                                                    |                                                    | Visibility  |
+=============================================================================+====================================================+====================================================+====================================================+=============+
| [Amazon S3 (generic                                                         | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                | ![](media/rId635.png){width="0.2777777777777778in" | ---         |
| logs)](/document/preview/1039055#UUID-d5ace3ce-5a93-86f4-55f2-4943fbf09cbe) | height="0.2777777777777778in"}                     |                                                    | height="0.2777777777777778in"}                     |             |
|                                                                             |                                                    |                                                    |                                                    |             |
|                                                                             | Raw data is searchable in XQL Search.              |                                                    | Cortex XSIAM can generate Cortex XSIAM issues      |             |
|                                                                             |                                                    |                                                    | (Correlation Rules only) when relevant from logs.  |             |
+-----------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------+
| [Amazon CloudWatch (generic logs, EKS                                       | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ---         |
| logs)](#UUIDa8188a992d754bde83bf5414dd3252e8)                               | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     |             |
|                                                                             |                                                    |                                                    |                                                    |             |
|                                                                             | Raw data is searchable in XQL Search.              |                                                    | Cortex XSIAM can generate Cortex XSIAM issues      |             |
|                                                                             |                                                    |                                                    | (Analytics, IOC, BIOC, and Correlation Rules) when |             |
|                                                                             |                                                    |                                                    | relevant from logs.                                |             |
|                                                                             |                                                    |                                                    |                                                    |             |
|                                                                             |                                                    |                                                    | > **Note**                                         |             |
|                                                                             |                                                    |                                                    | >                                                  |             |
|                                                                             |                                                    |                                                    | > While Correlation Rules issues are generated on  |             |
|                                                                             |                                                    |                                                    | > non-normalized and normalized logs, Analytics,   |             |
|                                                                             |                                                    |                                                    | > IOC and BIOC issues are only generated on        |             |
|                                                                             |                                                    |                                                    | > normalized logs.                                 |             |
+-----------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------+
| [Azure Event                                                                | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                | ![](media/rId635.png){width="0.2777777777777778in" | ---         |
| Hub](/document/preview/1037046#UUID-002a2d13-12a4-36f9-d2e8-b330eb0f0a1d)   | height="0.2777777777777778in"}                     |                                                    | height="0.2777777777777778in"}                     |             |
|                                                                             |                                                    |                                                    |                                                    |             |
|                                                                             | Raw data is searchable in XQL Search.              |                                                    | Cortex XSIAM can generate Cortex XSIAM issues      |             |
|                                                                             |                                                    |                                                    | (Correlation Rules only) when relevant from logs.  |             |
+-----------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------+
| [Google Cloud Platform](#UUIDb95ef00075ad79e026c18a1695611928)              | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                | ![](media/rId635.png){width="0.2777777777777778in" | ---         |
|                                                                             | height="0.2777777777777778in"}                     |                                                    | height="0.2777777777777778in"}                     |             |
|                                                                             |                                                    |                                                    |                                                    |             |
|                                                                             | Raw data is searchable in XQL Search.              |                                                    | Cortex XSIAM can generate Cortex XSIAM issues      |             |
|                                                                             |                                                    |                                                    | (Correlation Rules only) when relevant from logs.  |             |
+-----------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------+
| [Google Kubernetes Engine](#UUID112fc862755b4d8a7d945e3800484884)           | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                | ![](media/rId635.png){width="0.2777777777777778in" | ---         |
|                                                                             | height="0.2777777777777778in"}                     |                                                    | height="0.2777777777777778in"}                     |             |
|                                                                             |                                                    |                                                    |                                                    |             |
|                                                                             | Raw data is searchable in XQL Search.              |                                                    | Cortex XSIAM can generate Cortex XSIAM issues      |             |
|                                                                             |                                                    |                                                    | (Correlation Rules only) when relevant from logs.  |             |
+-----------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------+
| [Okta](#UUID698f9cbf8fc99992ef63a9c29ab549d2)                               | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                | ![](media/rId635.png){width="0.2777777777777778in" | ---         |
|                                                                             | height="0.2777777777777778in"}                     |                                                    | height="0.2777777777777778in"}                     |             |
|                                                                             |                                                    |                                                    |                                                    |             |
|                                                                             | Raw data is searchable in XQL Search.              |                                                    | Cortex XSIAM can generate Cortex XSIAM issues      |             |
|                                                                             |                                                    |                                                    | (Analytics, IOC, BIOC, and Correlation Rules) when |             |
|                                                                             |                                                    |                                                    | relevant from logs.                                |             |
|                                                                             |                                                    |                                                    |                                                    |             |
|                                                                             |                                                    |                                                    | > **Note**                                         |             |
|                                                                             |                                                    |                                                    | >                                                  |             |
|                                                                             |                                                    |                                                    | > While Correlation Rules issues are generated on  |             |
|                                                                             |                                                    |                                                    | > non-normalized and normalized logs, Analytics,   |             |
|                                                                             |                                                    |                                                    | > IOC and BIOC issues are only generated on        |             |
|                                                                             |                                                    |                                                    | > normalized logs.                                 |             |
+-----------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------+

##### Endpoint logs

+--------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------+
| Vendor and Device Type                                 | Raw Data Visibility                                | Normalized Log Visibility                          | Cortex XSIAM Issue Visibility                      | Vendor      |
|                                                        |                                                    |                                                    |                                                    | Alert       |
|                                                        |                                                    |                                                    |                                                    | Visibility  |
+========================================================+====================================================+====================================================+====================================================+=============+
| [CrowdStrike Falcon Data                               | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ---         |
| Replicator](#UUIDe3b1d1ee6b1ee07e14e0a8831921588c)     | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     |             |
|                                                        |                                                    |                                                    |                                                    |             |
| and                                                    | Raw data in `crowdstrike_falcon_incident_raw` and  | CrowdStrike logs are normalized into `xdr_data`    | Cortex XSIAM can generate Cortex XSIAM issues      |             |
|                                                        | `crowdstrike_fdr_raw` is searchable in XQL search. | and are searchable using XQL search and the Query  | (Analytics, IOC, BIOC, and Correlation Rules) when |             |
| [CrowdStrike Streaming                                 |                                                    | Builder.                                           | relevant from logs.                                |             |
| API](#UUID608d1f5782496b45bbbb5318ab84878e)            |                                                    |                                                    |                                                    |             |
| (collection from APIs)                                 |                                                    |                                                    | > **Note**                                         |             |
|                                                        |                                                    |                                                    | >                                                  |             |
|                                                        |                                                    |                                                    | > While Correlation Rules issues are generated on  |             |
|                                                        |                                                    |                                                    | > non-normalized and normalized logs, Analytics,   |             |
|                                                        |                                                    |                                                    | > IOC and BIOC issues are only generated on        |             |
|                                                        |                                                    |                                                    | > normalized logs.                                 |             |
+--------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------+
| [Microsoft Defender for                                | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ---         |
| Endpoint](#UUID6e00ea26bd37150df8776680549e71d5)       | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     |             |
|                                                        |                                                    |                                                    |                                                    |             |
|                                                        | Raw data in `msft_defender_raw` is searchable in   | Microsoft Defender logs are normalized into        | Cortex XSIAM can generate Cortex XSIAM issues      |             |
|                                                        | XQL search.                                        | `xdr_data` and are searchable using XQL search and | (Analytics, IOC, BIOC, and Correlation Rules) when |             |
|                                                        |                                                    | the Query Builder.                                 | relevant from logs.                                |             |
|                                                        |                                                    |                                                    |                                                    |             |
|                                                        |                                                    |                                                    | > **Note**                                         |             |
|                                                        |                                                    |                                                    | >                                                  |             |
|                                                        |                                                    |                                                    | > While Correlation Rules issues are generated on  |             |
|                                                        |                                                    |                                                    | > non-normalized and normalized logs, Analytics,   |             |
|                                                        |                                                    |                                                    | > IOC and BIOC issues are only generated on        |             |
|                                                        |                                                    |                                                    | > normalized logs.                                 |             |
+--------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------+
| [SentinelOne                                           | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ---         |
| DeepVisibility](#UUIDddd95331f5b1ac2572e08780ca22d5ba) | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     |             |
|                                                        |                                                    |                                                    |                                                    |             |
|                                                        | Raw data in `sentinelone_deep_visibility_raw` is   | SentinelOne DeepVisibility logs are normalized     | Cortex XSIAM can generate Cortex XSIAM issues      |             |
|                                                        | searchable in XQL search.                          | into `xdr_data` and are searchable using XQL       | (Analytics, IOC, BIOC, and Correlation Rules) when |             |
|                                                        |                                                    | search and the Query Builder.                      | relevant from logs.                                |             |
|                                                        |                                                    |                                                    |                                                    |             |
|                                                        |                                                    |                                                    | > **Note**                                         |             |
|                                                        |                                                    |                                                    | >                                                  |             |
|                                                        |                                                    |                                                    | > While Correlation Rules issues are generated on  |             |
|                                                        |                                                    |                                                    | > non-normalized and normalized logs, Analytics,   |             |
|                                                        |                                                    |                                                    | > IOC and BIOC issues are only generated on        |             |
|                                                        |                                                    |                                                    | > normalized logs.                                 |             |
+--------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------+
| [Windows Event                                         | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" | ---         |
| Collector](#UUIDc6ef2ebad58d3dba56c7dddddee05aca)      | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     |             |
|                                                        |                                                    |                                                    |                                                    |             |
|                                                        | Windows event logs are available with agent EDR    | Windows event logs are stitched with agent EDR     | Cortex XSIAM can generate Cortex XSIAM issues      |             |
|                                                        | data and are searchable in XQL Search.             | data and are searchable in the Query Builder.      | (Analytics, IOC, BIOC, and Correlation Rules) when |             |
|                                                        |                                                    |                                                    | relevant from logs.                                |             |
|                                                        | The normalized Windows event log data is also      | The Windows event logs are also normalized into    |                                                    |             |
|                                                        | available in `microsoft_windows_raw` and is        | the common Cortex Windows event format             | > **Note**                                         |             |
|                                                        | searchable in XQL Search.                          | in `microsoft_windows_raw` and are searchable      | >                                                  |             |
|                                                        |                                                    | using the Query Builder.                           | > While Correlation Rules issues are generated on  |             |
|                                                        |                                                    |                                                    | > non-normalized and normalized logs, Analytics,   |             |
|                                                        |                                                    |                                                    | > IOC and BIOC issues are only generated on        |             |
|                                                        |                                                    |                                                    | > normalized logs.                                 |             |
+--------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+----------------------------------------------------+-------------+

##### Custom external sources

+---------------------------------------------------------+----------------------------------------------------+--------------------------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| Vendor and Device Type                                  | Raw Data Visibility                                | Normalized Log Visibility                                                                  | Cortex XSIAM Issue Visibility                      | Vendor Alert Visibility                            |
+=========================================================+====================================================+============================================================================================+====================================================+====================================================+
| [Any Vendor Sending CEF, LEEF, CISCO, CORELIGHT, or RAW | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                                                        | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" |
| formatted                                               | height="0.2777777777777778in"}                     |                                                                                            | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     |
| Syslog](#UUID66807bb80b453c44e6bb091c295ba8cf)          |                                                    |                                                                                            |                                                    |                                                    |
|                                                         | Raw data is searchable in XQL Search.              |                                                                                            | Cortex XSIAM can generate Cortex XSIAM issues      | To enable Cortex XSIAM to display alerts from      |
|                                                         |                                                    |                                                                                            | (Analytics, IOC, BIOC, and Correlation Rules) when | other vendors, you must map your alert fields to   |
|                                                         |                                                    |                                                                                            | relevant from logs.                                | the Cortex XSIAM field format (see [Ingest         |
|                                                         |                                                    |                                                                                            |                                                    | external                                           |
|                                                         |                                                    |                                                                                            | > **Note**                                         | alerts](#UUID2b159e163816b273ec23c84f52f62cd1)).   |
|                                                         |                                                    |                                                                                            | >                                                  |                                                    |
|                                                         |                                                    |                                                                                            | > While Correlation Rules issues are generated on  |                                                    |
|                                                         |                                                    |                                                                                            | > non-normalized and normalized logs, Analytics,   |                                                    |
|                                                         |                                                    |                                                                                            | > IOC and BIOC issues are only generated on        |                                                    |
|                                                         |                                                    |                                                                                            | > normalized logs.                                 |                                                    |
+---------------------------------------------------------+----------------------------------------------------+--------------------------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [Any vendor CSV files on a shared Windows               | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                                                        | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                |
| directory](#UUID7af6a43b05154972d235d92e05c1df4a)       | height="0.2777777777777778in"}                     |                                                                                            | height="0.2777777777777778in"}                     |                                                    |
|                                                         |                                                    |                                                                                            |                                                    |                                                    |
|                                                         | Raw data is searchable in XQL Search.              |                                                                                            | Cortex XSIAM can generate Cortex XSIAM issues      |                                                    |
|                                                         |                                                    |                                                                                            | (Correlation Rules only) when relevant from logs.  |                                                    |
+---------------------------------------------------------+----------------------------------------------------+--------------------------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [Any vendor logs stored in a                            | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                                                        | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                |
| database](#UUIDcda6fac648f36599bfc78b770684d6ff)        | height="0.2777777777777778in"}                     |                                                                                            | height="0.2777777777777778in"}                     |                                                    |
|                                                         |                                                    |                                                                                            |                                                    |                                                    |
|                                                         | Raw data is searchable in XQL Search.              |                                                                                            | Cortex XSIAM can generate Cortex XSIAM issues      |                                                    |
|                                                         |                                                    |                                                                                            | (Correlation Rules only) when relevant from logs.  |                                                    |
+---------------------------------------------------------+----------------------------------------------------+--------------------------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [Any vendor logs stored in files on a network           | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                                                        | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                |
| share](#UUIDf2ef3ee8c4ab9e4f1568f78fa0ee87d5)           | height="0.2777777777777778in"}                     |                                                                                            | height="0.2777777777777778in"}                     |                                                    |
|                                                         |                                                    |                                                                                            |                                                    |                                                    |
|                                                         | Raw data is searchable in XQL Search.              |                                                                                            | Cortex XSIAM can generate Cortex XSIAM issues      |                                                    |
|                                                         |                                                    |                                                                                            | (Correlation Rules only) when relevant from logs.  |                                                    |
+---------------------------------------------------------+----------------------------------------------------+--------------------------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [Any vendor logs from a third party source over FTP,    | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                                                        | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                |
| FTPS, or SFTP](#UUIDfe4286fdc5a4efc1b5e23821682829c9)   | height="0.2777777777777778in"}                     |                                                                                            | height="0.2777777777777778in"}                     |                                                    |
|                                                         |                                                    |                                                                                            |                                                    |                                                    |
|                                                         | Raw data is searchable in XQL Search.              |                                                                                            | Cortex XSIAM can generate Cortex XSIAM issues      |                                                    |
|                                                         |                                                    |                                                                                            | (Correlation Rules only) when relevant from logs.  |                                                    |
+---------------------------------------------------------+----------------------------------------------------+--------------------------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [Any vendor sending NetFlow flow                        | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" height="0.2777777777777778in"}          | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                |
| records](#UUIDaa3914c7f3c2f723764f47f5ff2e0a77)         | height="0.2777777777777778in"}                     |                                                                                            | height="0.2777777777777778in"}                     |                                                    |
|                                                         |                                                    | NetFlow events are stitched with the Agent's EDR data and other Network products to a      |                                                    |                                                    |
|                                                         | Raw data is searchable in XQL Search.              | Session Story, and are searchable in the Query Builder and in XQL.                         | Cortex XSIAM can generate Cortex XSIAM issues      |                                                    |
|                                                         |                                                    |                                                                                            | (Analytics, IOC, BIOC, and Correlation Rules) when |                                                    |
|                                                         |                                                    |                                                                                            | relevant from logs.                                |                                                    |
|                                                         |                                                    |                                                                                            |                                                    |                                                    |
|                                                         |                                                    |                                                                                            | > **Note**                                         |                                                    |
|                                                         |                                                    |                                                                                            | >                                                  |                                                    |
|                                                         |                                                    |                                                                                            | > While Correlation Rules issues are generated on  |                                                    |
|                                                         |                                                    |                                                                                            | > non-normalized and normalized logs, Analytics,   |                                                    |
|                                                         |                                                    |                                                                                            | > IOC and BIOC issues are only generated on        |                                                    |
|                                                         |                                                    |                                                                                            | > normalized logs.                                 |                                                    |
+---------------------------------------------------------+----------------------------------------------------+--------------------------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [Any vendor sending logs over                           | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                                                        | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" |
| HTTP](#UUID82957a915c448913c42057ec30158437)            | height="0.2777777777777778in"}                     |                                                                                            | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     |
|                                                         |                                                    |                                                                                            |                                                    |                                                    |
|                                                         | Raw data is searchable in XQL Search.              |                                                                                            | Cortex XSIAM can generate Cortex XSIAM issues      | To enable Cortex XSIAM to display alerts from      |
|                                                         |                                                    |                                                                                            | (Correlation Rules only) when relevant from logs.  | other vendors, you must map your alert fields to   |
|                                                         |                                                    |                                                                                            |                                                    | the Cortex XSIAM field format (see [Ingest         |
|                                                         |                                                    |                                                                                            |                                                    | external                                           |
|                                                         |                                                    |                                                                                            |                                                    | alerts](#UUID2b159e163816b273ec23c84f52f62cd1)).   |
+---------------------------------------------------------+----------------------------------------------------+--------------------------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [Apache Kafka](#UUIDf0aa33aeb07e07ff251eb0a030d7858f)   | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                                                        | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                |
|                                                         | height="0.2777777777777778in"}                     |                                                                                            | height="0.2777777777777778in"}                     |                                                    |
|                                                         |                                                    |                                                                                            |                                                    |                                                    |
|                                                         | Raw data is searchable in XQL Search.              |                                                                                            | Cortex XSIAM can generate Cortex XSIAM issues      |                                                    |
|                                                         |                                                    |                                                                                            | (Analytics, IOC, BIOC, and Correlation Rules) when |                                                    |
|                                                         |                                                    |                                                                                            | relevant from logs.                                |                                                    |
|                                                         |                                                    |                                                                                            |                                                    |                                                    |
|                                                         |                                                    |                                                                                            | > **Note**                                         |                                                    |
|                                                         |                                                    |                                                                                            | >                                                  |                                                    |
|                                                         |                                                    |                                                                                            | > While Correlation Rules issues are generated on  |                                                    |
|                                                         |                                                    |                                                                                            | > non-normalized and normalized logs, Analytics,   |                                                    |
|                                                         |                                                    |                                                                                            | > IOC and BIOC issues are only generated on        |                                                    |
|                                                         |                                                    |                                                                                            | > normalized logs.                                 |                                                    |
+---------------------------------------------------------+----------------------------------------------------+--------------------------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [BeyondTrust Privilege Management                       | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                                                        | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                |
| Cloud](#UUID2d0e630f249722aa4aa3b711b5604d70)           | height="0.2777777777777778in"}                     |                                                                                            | height="0.2777777777777778in"}                     |                                                    |
|                                                         |                                                    |                                                                                            |                                                    |                                                    |
|                                                         | Raw data is searchable in XQL Search.              |                                                                                            | Cortex XSIAM can generate Cortex XSIAM issues      |                                                    |
|                                                         |                                                    |                                                                                            | (Correlation Rules only) when relevant from logs.  |                                                    |
+---------------------------------------------------------+----------------------------------------------------+--------------------------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [Box](#UUID84f55810c4f2f2079902e3aaa494bfae)            | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" height="0.2777777777777778in"}          | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                |
|                                                         | height="0.2777777777777778in"}                     |                                                                                            | height="0.2777777777777778in"}                     |                                                    |
|                                                         |                                                    | Selected Box audit event logs are normalized into stories and are searchable in the Query  |                                                    |                                                    |
|                                                         | Raw data is searchable in XQL Search.              | Builder and in XQL.                                                                        | Cortex XSIAM can generate Cortex XSIAM issues      |                                                    |
|                                                         |                                                    |                                                                                            | (Analytics, IOC, BIOC, and Correlation Rules) when |                                                    |
|                                                         |                                                    |                                                                                            | relevant from logs.                                |                                                    |
|                                                         |                                                    |                                                                                            |                                                    |                                                    |
|                                                         |                                                    |                                                                                            | > **Note**                                         |                                                    |
|                                                         |                                                    |                                                                                            | >                                                  |                                                    |
|                                                         |                                                    |                                                                                            | > While Correlation Rules issues are generated on  |                                                    |
|                                                         |                                                    |                                                                                            | > non-normalized and normalized logs, Analytics,   |                                                    |
|                                                         |                                                    |                                                                                            | > IOC and BIOC issues are only generated on        |                                                    |
|                                                         |                                                    |                                                                                            | > normalized logs.                                 |                                                    |
+---------------------------------------------------------+----------------------------------------------------+--------------------------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [Dropbox](#UUIDc7f0682e3b8e5c52705551e2f57578e3)        | ![](media/rId635.png){width="0.2777777777777778in" |   ---------------------------------------------------- ----------------------------------- | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                |
|                                                         | height="0.2777777777777778in"}                     |    ![](media/rId635.png){width="0.2777777777777778in"   Selected Box audit event logs are  | height="0.2777777777777778in"}                     |                                                    |
|                                                         |                                                    |              height="0.2777777777777778in"}              normalized into stories and are   |                                                    |                                                    |
|                                                         | Raw data is searchable in XQL Search.              |                                                        searchable in the Query Builder and | Cortex XSIAM can generate Cortex XSIAM issues      |                                                    |
|                                                         |                                                    |                                                                      in XQL.               | (Analytics, IOC, BIOC, and Correlation Rules) when |                                                    |
|                                                         |                                                    |                                                                                            | relevant from logs.                                |                                                    |
|                                                         |                                                    |   ---------------------------------------------------- ----------------------------------- |                                                    |                                                    |
|                                                         |                                                    |                                                                                            | > **Note**                                         |                                                    |
|                                                         |                                                    |                                                                                            | >                                                  |                                                    |
|                                                         |                                                    |                                                                                            | > While Correlation Rules issues are generated on  |                                                    |
|                                                         |                                                    |                                                                                            | > non-normalized and normalized logs, Analytics,   |                                                    |
|                                                         |                                                    |                                                                                            | > IOC and BIOC issues are only generated on        |                                                    |
|                                                         |                                                    |                                                                                            | > normalized logs.                                 |                                                    |
+---------------------------------------------------------+----------------------------------------------------+--------------------------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [Elasticsearch                                          | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                                                        | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                |
| Filebeat](#UUIDe2a36474f951854cf40baa936b7d46e5)        | height="0.2777777777777778in"}                     |                                                                                            | height="0.2777777777777778in"}                     |                                                    |
|                                                         |                                                    |                                                                                            |                                                    |                                                    |
|                                                         | Raw data is searchable in XQL Search.              |                                                                                            | Cortex XSIAM can generate Cortex XSIAM issues      |                                                    |
|                                                         |                                                    |                                                                                            | (Correlation Rules only) when relevant from logs.  |                                                    |
+---------------------------------------------------------+----------------------------------------------------+--------------------------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [Forcepoint DLP](#UUID83a09bba22172571a6122851cf5034e7) | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                                                        | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                |
|                                                         | height="0.2777777777777778in"}                     |                                                                                            | height="0.2777777777777778in"}                     |                                                    |
|                                                         |                                                    |                                                                                            |                                                    |                                                    |
|                                                         | Raw data is searchable in XQL Search.              |                                                                                            | Cortex XSIAM can generate Cortex XSIAM issues      |                                                    |
|                                                         |                                                    |                                                                                            | (Correlation Rules only) when relevant from logs.  |                                                    |
+---------------------------------------------------------+----------------------------------------------------+--------------------------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [IoT Security](#UUID00b89949c9638816dbd43a39fa8a5c87)   | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" height="0.2777777777777778in"}          | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                |
|                                                         | height="0.2777777777777778in"}                     |                                                                                            | height="0.2777777777777778in"}                     |                                                    |
|                                                         |                                                    | Cortex XSIAM uses IoT Security information to improve analytics detection and assets       |                                                    |                                                    |
|                                                         | Raw data is searchable in XQL Search.              | management information.                                                                    | Cortex XSIAM can generate Cortex XSIAM issues      |                                                    |
|                                                         |                                                    |                                                                                            | (Analytics, IOC, BIOC, and Correlation Rules) when |                                                    |
|                                                         |                                                    |                                                                                            | relevant from logs.                                |                                                    |
|                                                         |                                                    |                                                                                            |                                                    |                                                    |
|                                                         |                                                    |                                                                                            | > **Note**                                         |                                                    |
|                                                         |                                                    |                                                                                            | >                                                  |                                                    |
|                                                         |                                                    |                                                                                            | > While Correlation Rules issues are generated on  |                                                    |
|                                                         |                                                    |                                                                                            | > non-normalized and normalized logs, Analytics,   |                                                    |
|                                                         |                                                    |                                                                                            | > IOC and BIOC issues are only generated on        |                                                    |
|                                                         |                                                    |                                                                                            | > normalized logs.                                 |                                                    |
+---------------------------------------------------------+----------------------------------------------------+--------------------------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [Proofpoint Targeted Attack                             | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                                                        | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                |
| Protection](#UUID21155e84fe3c389cc072d812046531cb)      | height="0.2777777777777778in"}                     |                                                                                            | height="0.2777777777777778in"}                     |                                                    |
|                                                         |                                                    |                                                                                            |                                                    |                                                    |
|                                                         | Raw data is searchable in XQL Search.              |                                                                                            | Cortex XSIAMCortex XSIAM issues (Correlation Rules |                                                    |
|                                                         |                                                    |                                                                                            | only) when relevant from logs.                     |                                                    |
+---------------------------------------------------------+----------------------------------------------------+--------------------------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [Salesforce.com](#UUID10ea85f89762a07ad8a6e09da3e8258b) | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                                                        | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                |
|                                                         | height="0.2777777777777778in"}                     |                                                                                            | height="0.2777777777777778in"}                     |                                                    |
|                                                         |                                                    |                                                                                            |                                                    |                                                    |
|                                                         | Raw data is searchable in XQL Search.              |                                                                                            | Cortex XSIAM can generate Cortex XSIAM issues      |                                                    |
|                                                         |                                                    |                                                                                            | (Correlation Rules only) when relevant from logs.  |                                                    |
+---------------------------------------------------------+----------------------------------------------------+--------------------------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [ServiceNow                                             | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                                                        | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                |
| CMDB](#UUIDc692505eebef87bc21a065a7fee0c068)            | height="0.2777777777777778in"}                     |                                                                                            | height="0.2777777777777778in"}                     |                                                    |
|                                                         |                                                    |                                                                                            |                                                    |                                                    |
|                                                         | Raw data is searchable in XQL Search.              |                                                                                            | Cortex XSIAMCortex XSIAM issues (Correlation Rules |                                                    |
|                                                         |                                                    |                                                                                            | only) when relevant from logs.                     |                                                    |
+---------------------------------------------------------+----------------------------------------------------+--------------------------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [Strata Logging                                         | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" height="0.2777777777777778in"}          | ![](media/rId635.png){width="0.2777777777777778in" | ![](media/rId635.png){width="0.2777777777777778in" |
| Service](#UUID9924d7bc3c045fc0cf8b6645ef9ca152)         | height="0.2777777777777778in"}                     |                                                                                            | height="0.2777777777777778in"}                     | height="0.2777777777777778in"}                     |
|                                                         |                                                    | Detection events are stitched with other Palo Alto Networks product logs to stories, and   |                                                    |                                                    |
|                                                         | Raw data is searchable in XQL Search.              | are searchable in the Query Builder and in XQL.                                            | Cortex XSIAM can generate Cortex XSIAM issues      | Alerts from NGFW are generated throughout Cortex   |
|                                                         |                                                    |                                                                                            | (Analytics, IOC, BIOC, and Correlation Rules) when | XSIAM when relevant.                               |
|                                                         |                                                    |                                                                                            | relevant from logs.                                |                                                    |
|                                                         |                                                    |                                                                                            |                                                    |                                                    |
|                                                         |                                                    |                                                                                            | > **Note**                                         |                                                    |
|                                                         |                                                    |                                                                                            | >                                                  |                                                    |
|                                                         |                                                    |                                                                                            | > While Correlation Rules issues are generated on  |                                                    |
|                                                         |                                                    |                                                                                            | > non-normalized and normalized logs, Analytics,   |                                                    |
|                                                         |                                                    |                                                                                            | > IOC and BIOC issues are only generated on        |                                                    |
|                                                         |                                                    |                                                                                            | > normalized logs.                                 |                                                    |
+---------------------------------------------------------+----------------------------------------------------+--------------------------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| [Workday](#UUIDdc39423b15130148f6871272af42a19d)        | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                                                        | ![](media/rId635.png){width="0.2777777777777778in" | ---                                                |
|                                                         | height="0.2777777777777778in"}                     |                                                                                            | height="0.2777777777777778in"}                     |                                                    |
|                                                         |                                                    |                                                                                            |                                                    |                                                    |
|                                                         | Raw data is searchable in XQL Search.              |                                                                                            | Cortex XSIAMCortex XSIAM issues (Correlation Rules |                                                    |
|                                                         |                                                    |                                                                                            | only) when relevant from logs.                     |                                                    |
+---------------------------------------------------------+----------------------------------------------------+--------------------------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+
| Any vendor sending alerts                               | ---                                                | ---                                                                                        | ---                                                | ![](media/rId635.png){width="0.2777777777777778in" |
|                                                         |                                                    |                                                                                            |                                                    | height="0.2777777777777778in"}                     |
|                                                         |                                                    |                                                                                            |                                                    |                                                    |
|                                                         |                                                    |                                                                                            |                                                    | Alerts are surfaced throughout Cortex XSIAM when   |
|                                                         |                                                    |                                                                                            |                                                    | relevant. To enable Cortex XSIAM to display your   |
|                                                         |                                                    |                                                                                            |                                                    | alerts, you must map your alert fields to          |
|                                                         |                                                    |                                                                                            |                                                    | the Cortex XSIAM field format (see [Ingest         |
|                                                         |                                                    |                                                                                            |                                                    | external                                           |
|                                                         |                                                    |                                                                                            |                                                    | alerts](#UUID2b159e163816b273ec23c84f52f62cd1)).   |
+---------------------------------------------------------+----------------------------------------------------+--------------------------------------------------------------------------------------------+----------------------------------------------------+----------------------------------------------------+

##### Datasets created from ingesting data

When ingesting data from an external source, Cortex XSIAM creates a
dataset that you can query using Cortex Query Language (XQL). Datasets
created in this way use the following naming convention.

                <vendor_name>_<product_name>_raw

For example: cisco_asa_raw

The datatypes used for the fields in an imported dataset are
automatically assigned based on the input content. Fields can have a
datatype of `string`, `int`, `float`, `time`, or `boolean`. All other
fields are ingested as a JSON object.

For CEF type files, when extension values are quoted, the CEF parser
automatically removes the quotes from the values. In addition, files
containing invalid UTF-8 are parsed under XQL mapping
field `_invalid_utf8`.

#### Visibility of Cortex XSIAM audit and authentication logs

You can audit and query Cortex XSIAM authentication logs and activity
logs to track and create issues for malicious activity on Cortex XSIAM.

A ![](media/rId635.png){width="0.20833333333333334in"
height="0.20833333333333334in"} indicates support and a dash (---)
indicates the feature is not supported.

+-----------------+------------------------------------------------------+------------------------------------------------------+------------------------------------------------------+
| Log type        | Raw data visibility                                  | Normalized log visibility                            | Cortex XSIAM Issue visibility                        |
+=================+======================================================+======================================================+======================================================+
| Cortex XSIAM    |  ![](media/rId635.png){width="0.20833333333333334in" |  ![](media/rId635.png){width="0.20833333333333334in" |  ![](media/rId635.png){width="0.20833333333333334in" |
| authentication  | height="0.20833333333333334in"}                      | height="0.20833333333333334in"}                      | height="0.20833333333333334in"}                      |
| logs            |                                                      |                                                      |                                                      |
|                 | Logs and stories are searchable in XQL Search.       | Cortex XSIAM authentication logs normalized into     | Cortex XSIAM can create Cortex XSIAM issues          |
|                 |                                                      | authentication stories, which are searchable in the  | (Analytics, IOC, BIOC, and Correlation Rules) when   |
|                 |                                                      | Query Builder.                                       | relevant from logs.                                  |
|                 |                                                      |                                                      |                                                      |
|                 |                                                      |                                                      | > **Note**                                           |
|                 |                                                      |                                                      | >                                                    |
|                 |                                                      |                                                      | > Cortex XSIAM can create Cortex XSIAM issues        |
|                 |                                                      |                                                      | > (Analytics, IOC, BIOC, and Correlation Rules) when |
|                 |                                                      |                                                      | > relevant from logs.                                |
+-----------------+------------------------------------------------------+------------------------------------------------------+------------------------------------------------------+
| Cortex XSIAM    |  ![](media/rId635.png){width="0.20833333333333334in" |  ![](media/rId635.png){width="0.20833333333333334in" |  ![](media/rId635.png){width="0.20833333333333334in" |
| audit logs      | height="0.20833333333333334in"}                      | height="0.20833333333333334in"}                      | height="0.20833333333333334in"}                      |
|                 |                                                      |                                                      |                                                      |
|                 | Logs and stories are searchable in XQL Search.       | Cortex XSIAM authentication logs are normalized into | Cortex XSIAM can create Cortex XSIAM issues          |
|                 |                                                      | SaaS stories which are searchable in the Query       | (Analytics, IOC, BIOC, and Correlation Rules) when   |
|                 |                                                      | Builder.                                             | relevant from logs.                                  |
|                 |                                                      |                                                      |                                                      |
|                 |                                                      |                                                      | > **Note**                                           |
|                 |                                                      |                                                      | >                                                    |
|                 |                                                      |                                                      | > Cortex XSIAM can create Cortex XSIAM issues        |
|                 |                                                      |                                                      | > (Analytics, IOC, BIOC, and Correlation Rules) when |
|                 |                                                      |                                                      | > relevant from logs.                                |
+-----------------+------------------------------------------------------+------------------------------------------------------+------------------------------------------------------+

#### External data ingestion vendor support

> **Note**
>
> Data collection may require an add-on.

To provide you with a more complete and detailed picture of the activity
involved in a case, you can ingest data from a variety of external,
third-party sources into Cortex XSIAM.

Cortex XSIAM can receive logs, or both logs and alerts, from the source.
Depending on the data source, Cortex XSIAM can provide visibility into
your external data in the form of:

- Log stitching with other logs in order to create network or
  authentication stories.

- Raw data in queries from XQL Search.

- Alerts reported by the vendor throughout Cortex XSIAM, such as in the
  issues table, cases, and views.

- Issues raised by Cortex XSIAM on log data, such as analytics findings.

To ingest data, you must set up the Syslog Collector applet on a Broker
VM within your network.

The following table summarizes the vendor data that can be ingested,
according to log or data type.

+--------------------------------------------------+-------------------------------------------------------------------------------+
| Log or Data Type                                 | Vendor Support                                                                |
+==================================================+===============================================================================+
| *Network Connections*                            | - [Amazon S3 (flow logs)](#UUID5617c36b719ce5e3c8a354c63606f30a)              |
|                                                  |                                                                               |
|                                                  | - [Amazon S3 (Route 53 logs)](#UUIDca098ab3cc1668f15f34d2c25224b507)          |
|                                                  |                                                                               |
|                                                  | - [Azure Event                                                                |
|                                                  |   Hub](/document/preview/1037046#UUID-002a2d13-12a4-36f9-d2e8-b330eb0f0a1d)   |
|                                                  |                                                                               |
|                                                  | - [Azure Network Watcher (flow logs)](#UUID1d467284cb48b500b897369b4830bfc3)  |
|                                                  |                                                                               |
|                                                  | - [Check Point FW1/VPN1](#UUID836518f132520c5006f82ed736c6eceb)               |
|                                                  |                                                                               |
|                                                  | - [Cisco ASA and Cisco AnyConnect VPN](#UUID67771daf4b0571e9089b936cb4416b17) |
|                                                  |                                                                               |
|                                                  | - [Corelight Zeek](#UUID0740e5796b8c841be59b7b5a103cf12e)                     |
|                                                  |                                                                               |
|                                                  | - [Fortinet Fortigate](#UUIDb5cc710d2c80ee7476ecc7ffce22338a)                 |
|                                                  |                                                                               |
|                                                  | - [Google Cloud Platform (flow logs, DNS                                      |
|                                                  |   logs)](#UUIDb95ef00075ad79e026c18a1695611928)                               |
|                                                  |                                                                               |
|                                                  | - [Okta](#UUID698f9cbf8fc99992ef63a9c29ab549d2)                               |
|                                                  |                                                                               |
|                                                  | - [Prisma Access Browser](#UUID01e59e4df1b8a022e18f47b5255005a6)              |
|                                                  |                                                                               |
|                                                  | - [Windows DHCP via Elasticsearch                                             |
|                                                  |   Filebeat](#UUID43a820a2b38a854f5f8f147510f2cd96)                            |
|                                                  |                                                                               |
|                                                  | - [Zscaler Internet Access (ZIA)](#UUID9e297bb3c6808428c6f123df5498d4df)      |
|                                                  |                                                                               |
|                                                  | - [Zscaler Private Access (ZPA)](#UUID9813b19d9b3f21373ba410a8bc37fa77)       |
+--------------------------------------------------+-------------------------------------------------------------------------------+
| *Authentication Services/Audit Logs*             | - [Amazon S3 (audit logs)](#UUIDc1ab7bea87ed20d21accce03ccec7e3a)             |
|                                                  |                                                                               |
|                                                  | - [Azure Event Hub (audit logs, AKS                                           |
|                                                  |   logs)](/document/preview/1037046#UUID-002a2d13-12a4-36f9-d2e8-b330eb0f0a1d) |
|                                                  |                                                                               |
|                                                  | - [Google Cloud Platform (audit logs, GKE                                     |
|                                                  |   logs)](#UUIDb95ef00075ad79e026c18a1695611928)                               |
|                                                  |                                                                               |
|                                                  | - [Google Workspace](#UUID1218397b9beed848e97e29c6e0dbc71e)                   |
|                                                  |                                                                               |
|                                                  | - [Microsoft 365 (email)](#UUIDd0bd27c38c86c7e6ef7b65788c7fe549)              |
|                                                  |                                                                               |
|                                                  | - [Microsoft Office                                                           |
|                                                  |   365](/document/preview/1038728#UUID-96e29ceb-6654-3926-4323-e7d527326eba)   |
|                                                  |                                                                               |
|                                                  | - [Okta](#UUID698f9cbf8fc99992ef63a9c29ab549d2)                               |
|                                                  |                                                                               |
|                                                  | - [OneLogin](#UUID0702bd66b5ef03d6d14c3ae2350776e7)                           |
|                                                  |                                                                               |
|                                                  | - [PingFederate](#UUIDeb7d96b621aa8922d227ac30459df61b)                       |
|                                                  |                                                                               |
|                                                  | - [PingOne for Enterprise](#UUID4734781ab5de02dfda2701630e5413d7)             |
+--------------------------------------------------+-------------------------------------------------------------------------------+
| *Operation and System Logs from Cloud Providers* | - [Amazon S3 (generic                                                         |
|                                                  |   logs)](/document/preview/1039055#UUID-d5ace3ce-5a93-86f4-55f2-4943fbf09cbe) |
|                                                  |                                                                               |
|                                                  | - [Amazon CloudWatch (generic logs, EKS                                       |
|                                                  |   logs)](#UUIDa8188a992d754bde83bf5414dd3252e8)                               |
|                                                  |                                                                               |
|                                                  | - [Azure Event                                                                |
|                                                  |   Hub](/document/preview/1037046#UUID-002a2d13-12a4-36f9-d2e8-b330eb0f0a1d)   |
|                                                  |                                                                               |
|                                                  | - [Google Cloud Platform](#UUIDb95ef00075ad79e026c18a1695611928)              |
|                                                  |                                                                               |
|                                                  | - [Google Kubernetes Engine](#UUID112fc862755b4d8a7d945e3800484884)           |
|                                                  |                                                                               |
|                                                  | - [Okta](#UUID698f9cbf8fc99992ef63a9c29ab549d2)                               |
+--------------------------------------------------+-------------------------------------------------------------------------------+
| *Endpoint Logs*                                  | - [CrowdStrike Platform](#UUID608d1f5782496b45bbbb5318ab84878e)               |
|                                                  |                                                                               |
|                                                  | - [Microsoft Defender for Endpoint](#UUID6e00ea26bd37150df8776680549e71d5)    |
|                                                  |                                                                               |
|                                                  | - [SentinelOne DeepVisibility](#UUIDddd95331f5b1ac2572e08780ca22d5ba)         |
|                                                  |                                                                               |
|                                                  | - [Windows Event Collector](#UUIDc6ef2ebad58d3dba56c7dddddee05aca)            |
+--------------------------------------------------+-------------------------------------------------------------------------------+
| *Custom External Sources*                        | - [Any Vendor Sending CEF, LEEF, CISCO, CORELIGHT, or RAW formatted           |
|                                                  |   Syslog](#UUID66807bb80b453c44e6bb091c295ba8cf)                              |
|                                                  |                                                                               |
|                                                  | - [Any vendor CSV files on a shared Windows                                   |
|                                                  |   directory](#UUID7af6a43b05154972d235d92e05c1df4a)                           |
|                                                  |                                                                               |
|                                                  | - [Any vendor logs stored in a                                                |
|                                                  |   database](#UUIDcda6fac648f36599bfc78b770684d6ff)                            |
|                                                  |                                                                               |
|                                                  | - [Any vendor logs stored in files on a network                               |
|                                                  |   share](#UUIDf2ef3ee8c4ab9e4f1568f78fa0ee87d5)                               |
|                                                  |                                                                               |
|                                                  | - [Any vendor logs from a third party source over FTP, FTPS, or               |
|                                                  |   SFTP](#UUIDfe4286fdc5a4efc1b5e23821682829c9)                                |
|                                                  |                                                                               |
|                                                  | - [Any vendor sending NetFlow flow                                            |
|                                                  |   records](#UUIDaa3914c7f3c2f723764f47f5ff2e0a77)                             |
|                                                  |                                                                               |
|                                                  | - [Any vendor sending logs over HTTP](#UUID82957a915c448913c42057ec30158437)  |
|                                                  |                                                                               |
|                                                  | - [Apache Kafka](#UUIDf0aa33aeb07e07ff251eb0a030d7858f)                       |
|                                                  |                                                                               |
|                                                  | - [BeyondTrust Privilege Management                                           |
|                                                  |   Cloud](#UUID2d0e630f249722aa4aa3b711b5604d70)                               |
|                                                  |                                                                               |
|                                                  | - [Box](#UUID84f55810c4f2f2079902e3aaa494bfae)                                |
|                                                  |                                                                               |
|                                                  | - [Dropbox](#UUIDc7f0682e3b8e5c52705551e2f57578e3)                            |
|                                                  |                                                                               |
|                                                  | - [Elasticsearch Filebeat](#UUIDe2a36474f951854cf40baa936b7d46e5)             |
|                                                  |                                                                               |
|                                                  | - [Forcepoint DLP](#UUID83a09bba22172571a6122851cf5034e7)                     |
|                                                  |                                                                               |
|                                                  | - [IoT Security](#UUID00b89949c9638816dbd43a39fa8a5c87)                       |
|                                                  |                                                                               |
|                                                  | - [Proofpoint Targeted Attack                                                 |
|                                                  |   Protection](#UUID21155e84fe3c389cc072d812046531cb)                          |
|                                                  |                                                                               |
|                                                  | - [Salesforce.com](#UUID10ea85f89762a07ad8a6e09da3e8258b)                     |
|                                                  |                                                                               |
|                                                  | - [ServiceNow CMDB](#UUIDc692505eebef87bc21a065a7fee0c068)                    |
|                                                  |                                                                               |
|                                                  | - [Strata Logging Service](#UUID9924d7bc3c045fc0cf8b6645ef9ca152)             |
|                                                  |                                                                               |
|                                                  | - [Workday](#UUIDdc39423b15130148f6871272af42a19d)                            |
|                                                  |                                                                               |
|                                                  | - [Any vendor sending alerts](#UUID2b159e163816b273ec23c84f52f62cd1)          |
+--------------------------------------------------+-------------------------------------------------------------------------------+

#### Manage instances

You can manage the Instances configured for a Data Source on the
**Data Sources** page. You can edit, delete, enable, or disable
instances, and refresh log data.

1.  Select Settings \> Data Sources.

2.  Find an instance by clicking on a Data Source name or using the
    **Search** field.

3.  In the row for the instance name, take the required action:

+-----------------------------------+-----------------------------------+
| Action                            | Instructions                      |
+===================================+===================================+
| Enable or disable an Instance     | Select or deselect the **Enable** |
|                                   | checkbox.                         |
+-----------------------------------+-----------------------------------+
| Refresh log data                  | Click the **Refresh** icon.       |
+-----------------------------------+-----------------------------------+
| Edit an Instance                  | 1.  Click the **Edit** icon.      |
|                                   |                                   |
|                                   | 2.  In **Edit Data Source**, you  |
|                                   |     can update the values in the  |
|                                   |     **Connect** and **Collect**   |
|                                   |     sections. The options under   |
|                                   |     **Recommended Content** are   |
|                                   |     view only.                    |
|                                   |                                   |
|                                   | 3.  **Test** the configuration.   |
|                                   |                                   |
|                                   | 4.  **Connect** the updated       |
|                                   |     Instance.                     |
+-----------------------------------+-----------------------------------+
| Delete an Instance                | Click the **Delete** icon.        |
|                                   |                                   |
|                                   | If you delete all the instances   |
|                                   | for a Data Source, the Data       |
|                                   | Source is not listed on the Data  |
|                                   | Sources page.                     |
+-----------------------------------+-----------------------------------+

##### Add a new data source or instance

You can add a new data source with the Data Source Onboarder. The
Onboarder installs the data source, sets up an instance, configures
playbooks and scripts, and other recommended content. The Onboarder
offers default (customizable) options and displays all configured
content in a summary screen at the end of the process.

1.  Select Settings \> Data Sources.

2.  Select one of the following options:

    - **Add Data Source**

    - **Add New Instance** for an integrated data source by clicking the
      menu in the right corner of an existing data source. Then skip to
      Step [4](#Xd1dbc91dea5e7395ef728d28d575768ffbc8cfe).

3.  Select a data source to onboard and click **Connect**.

- Hovering over a data source displays information about the data source
  and its integrations. Data sources that are already integrated are
  highlighted green and show **Connect Another Instance**. To see
  details of existing integrations, click on the number of integrations.

  The data sources are drawn from the Marketplace, Custom Collectors,
  and integrations. If you search for a data source and
  **No Data Sources Found**, click **Try searching the Marketplace**, to
  view the marketplace page prefiltered for your search. If there are no
  available options in the Marketplace, you can use one of the Custom
  Collectors to build your own.

  > **Notes**

  - > If a data source contains multiple integrations, the integration
    > configured as the default integration will used by the Data
    > Onboarder. The default integration of the content pack is
    > indicated in each content pack\'s documentation. The other
    > integrations are available for configuration in the Automation and
    > Feed Integrations page after installing the content pack.

  - > Not all content packs are supported.

  - > When adding XDR data sources the Data Source Onboarder is not
    > available, however, you can still enable the data source. Cortex
    > XSIAM then creates an instance and lists it on the Data Sources
    > page.

4.  In the **New Data Source** window, complete the mandatory fields in
    the **Connect** section.

- For more information about the fields, click the question mark icon.

5.  (Optional) Under **Collect**, select **Fetched alerts** and complete
    the fields.

6.  Under **Recommended Content**, review and customize the options.

- The items in this section are content-specific. Some options are view
  only, and others are customizable. Click on each option for more
  information:

  - **Classifiers & Mappers**

  - **Data Normalization**: Parsing rules and data models

  - **Correlations**: Correlation rules included in the pack

  - **Automation**: **Playbooks** and **Scripts** included in the pack.

  <!-- -->

  - You can select the **Playbooks** and **Scripts** that you want to
    enable. By default, recommended options are selected. Any unselected
    content is added as disabled content. Depending on the selected
    playbook, some scripts are mandatory.

  <!-- -->

  - **Dashboards & Reports**: Recommended dashboards, widgets, and
    reports

  > **Notes**

  - > If you are adding a new instance to an existing data source, these
    > options are **View** only.

  <!-- -->

  - > You can adjust the view-only options on the relevant page in the
    > system, for example Correlations, Playbooks, or Scripts.

  <!-- -->

  - > Cortex XSIAM automatically installs content packs with required
    > dependencies and updates any pre-installed optional content packs.
    > You can also **Select additional content packs** with optional
    > dependencies to be configured during connection.

7.  **Test** the configuration.

- If the test fails, you can **Run Test & Download Debug Log** to debug
  the error.

8.  **Connect** the data source.

9.  Review the configuration in the summary screen.

- If errors occurred during the test, you can click **See Details** and
  **Back to Edit** to revise your configuration. For advanced
  configuration, click on any item to open a new window to the relevant
  page in the system (for example, Correlations or Playbooks) filtered
  by the configuration.

10. Click **Finish** to return to the Data Sources page.

##### How to configure the scanning settings for supported services

1.  In Cortex XSIAM Command Center, in the lower left area, click
    Settings \> Data Sources.

2.  On the **Data Sources** screen, click a cloud service provider or
    other data source and then click the **View Details** link.

3.  On the **Cloud Instances** screen, click an instance name link. A
    screen displaying the instance name opens.

4.  At the bottom of the screen, under **Accounts** (AWS),
    **Subscriptions** (Azure), or **Projects** (GCP), right-click an
    item in the list and then in the context menu, select **Edit**.

5.  In the screen that opens, under
    **Data assets classification options**, you can deselect any data
    asset types.

- > **Note**

  > All the asset types are selected by default. When you deselect a
  > data asset type, it is not included when the system runs the next
  > scan operation, reducing the total scan time.

6.  Click **Save**.

- For more information about supported assets in Cortex Cloud Data
  Security, see [Supported assets in Cortex Cloud Data
  Security](#UUIDa01721f1b61d048e0fcb6929f39d81fe).

##### Manage cloud instances

1.  Select Settings \> Data Sources.

2.  Find the cloud instance by clicking on the CSP name or using the
    **Search** field.

3.  In the row for the cloud instance, click **View Details**. The
    **Cloud Instances** page is displayed, filtered by the CSP you
    selected.

4.  In the **Cloud Instances** page, you can filter the results by any
    heading and value.

5.  Click on an instance name to open the details pane for that
    instance.

6.  You can perform the following actions on each cloud instance:

+-----------------------------------+-----------------------------------+
| Action                            | Instructions                      |
+===================================+===================================+
| Discover Now                      | To initiate a discovery scan, in  |
|                                   | the row for the cloud instance,   |
|                                   | right-click and select            |
|                                   | **Discover Now**. Alternatively,  |
|                                   | in the details pane, click the    |
|                                   | more options icon and select      |
|                                   | **Discover Now**.                 |
+-----------------------------------+-----------------------------------+
| Enable/Disable                    | In the row for the cloud          |
|                                   | instance, right-click and select  |
|                                   | **Enable** or **Disable**.        |
|                                   | Alternatively, in the details     |
|                                   | pane, click the more options icon |
|                                   | and select **Enable** or          |
|                                   | **Disable**.                      |
+-----------------------------------+-----------------------------------+
| Delete                            | In the row for the cloud          |
|                                   | instance, right-click and select  |
|                                   | **Delete**. Alternatively, in the |
|                                   | details pane, click the more      |
|                                   | options icon and select           |
|                                   | **Delete**.                       |
+-----------------------------------+-----------------------------------+
| Create a new instance             | Click **New Instance** and select |
|                                   | the type of CSP of which you want |
|                                   | to create a new instance. Follow  |
|                                   | the onboarding wizard to define   |
|                                   | its settings.                     |
+-----------------------------------+-----------------------------------+
| Edit configuration                | In the row for the cloud          |
|                                   | instance, right-click and select  |
|                                   | **Configuration**. Alternatively, |
|                                   | in the details pane, click the    |
|                                   | edit button. Follow the           |
|                                   | onboarding wizard to edit the     |
|                                   | cloud instance\'s settings.       |
|                                   |                                   |
|                                   | You must execute the updated      |
|                                   | template in the CSP environment   |
|                                   | for the configuration changes to  |
|                                   | be applied.                       |
+-----------------------------------+-----------------------------------+

##### Update cloud permissions after Cortex release updates

This topic provides guidance on how to manage permission updates for
your cloud instances following new feature releases or bug fixes. It
outlines how users are notified of required permission changes and
provides step-by-step instructions for granting necessary permissions to
ensure continued functionality and security.

> **Warning**

- > Ensure that the user account used to modify permissions has the
  > necessary privileges within both the Cortex platform and your cloud
  > environment, for example, AWS or Azure.

- > You received a notification regarding a new version available that
  > requires permission updates, or viewed a **Needs Update** status in
  > the **Data Sources** page.

**Procedure**

1.  Navigate to the **Data Sources** page.

2.  Do the following to identify instances requiring updates:

    1.  For the relevant instance, locate the **Update Status** column.

    2.  Filter or sort by this column to quickly identify instances
        marked as **Needs Update**. The message on the page indicates
        the number of instances that need updating.

- > **Note**

  > Instances requiring updates will not change their connection status,
  > for example, Connected, Warning, Error, Disabled, due to the pending
  > permission update.

3.  Do the following to access the connector\'s permissions section:

    1.  Click the name of the specific cloud connector instance that
        requires permission updates. The connector\'s detailed view
        appears.

    2.  Within the connector\'s detailed view, locate and select the
        permissions section.

4.  Review missing permissions. In the permissions section, the missing
    permission names or changes in permission scope is indicated.

5.  Follow the on-screen instructions to grant the required permissions,
    or refer to the specific permission names or scopes provided.

6.  After making the necessary permission adjustments, click **Save** or
    **Apply Changes** within the connector\'s configuration.

7.  Return to the **Data Sources** page and verify that the updated
    status of the instance shows as up-to-date, or the update is in
    progress.

8.  Monitor the instance\'s health and functionality to confirm the
    changes have taken effect and the connector is operating as
    expected.

- If you encounter issues during the permission update process, check
  the generated health alerts for more specific details.

##### Pending cloud instances

In Cortex XSIAM, a pending cloud instance refers to a cloud instance
created after Cortex XSIAM generates an authentication template, but
before that template has been fully executed within the Cloud Service
Provider (CSP) environment.

A pending cloud instance is created each time you complete the
onboarding wizard for a new CSP and click **Save**. You can view all
cloud instances, including those in a pending state, by navigating to
**Cloud Instances**. Ensure you remove any default filters that might
exclude instances with a \"pending\" status.

A single pending instance can be leveraged to create multiple cloud
instances, all sharing the same configurations defined during the cloud
onboarding process. Pending instances are automatically deleted after 30
days.

###### Manage pending cloud instances

There are some actions that can be performed specifically on cloud
instances with a status of \"pending\".

  ---------------------------------------------------------------------------------------
  Action                              Instructions
  ----------------------------------- ---------------------------------------------------
  Manually connect an instance        After the authentication template has been executed
                                      in the CSP, you can manually connect the Cortex
                                      XSIAM cloud instance to the CSP by right-clicking
                                      the pending cloud instance and selecting
                                      **Manually connect an instance**. For more about
                                      this process, see [Manually connect a cloud
                                      instance](#UUIDcec2c8a0a3660665483543a06f28a92a).

  View Details                        To review the configuration settings defined in the
                                      onboarding wizard for a pending instance,
                                      right-click the instance and select
                                      **View Details**. This is helps you distinguish
                                      between pending instances when you want to create a
                                      new cloud instance from an existing pending
                                      instance or when you want to manually connect an
                                      instance.

  Re-download Connection Template     The authentication template that you download from
                                      the onboarding wizard is valid for seven days from
                                      when it was downloaded. If you want to create a new
                                      cloud instance from a pending instance after the
                                      authentication template has expired, you can
                                      right-click the pending instance and select
                                      Re-download Connection Template. You must then
                                      execute the template in the CSP.

  Delete                              To delete a pending instance, right-click the
                                      pending instance and select **Delete**.
  ---------------------------------------------------------------------------------------

##### Troubleshoot errors on cloud instances

To help you to troubleshoot errors on a cloud instance, Cortex XSIAM
provides the following visibility and drilldown options:

- Overall status of an instance that indicates the health of your
  instance.

- A breakdown of the security capabilities enabled on an instance,
  detailing the status of each capability along with any open errors or
  issues.

- Additional XQL drill down options to query the history of error and
  recovery events for each security capability.

**How to troubleshoot errors on a cloud instance**

1.  Go to Settings \> Data Sources.

- Under **Cloud Service Provider**, review the status of the instances
  that were onboarded for the service provider. If the status shows
  **Warning** or **Error**, hover over the service provider and click
  **View Details**.

2.  On the **Cloud Instances** page review the list of instances that
    were onboarded and their overall status. The status is displayed as
    follows:

    - **Connected:** The connector is enabled and has no issues.

    - **Warning:** The connector is enabled and has minor issues. For
      example, some accounts or capabilities are in warning or error
      status.

    - **Error:** The connector is enabled and has substantial
      errors. For example, an authentication failure, an outpost
      failure, major permissions issues, or (for organization level
      accounts) the majority of the accounts in the instance are in
      error status.

    - **Disabled:** The connector is disabled.

3.  To understand why an instance is showing a **Warning** or **Error**
    status, click on the instance name.

- The cloud instance panel provides a breakdown of the security
  capabilities and the accounts onboarded on the instance. Review the
  information in the following sections:

+-----------------------------------+-----------------------------------+
| Section                           | Context                           |
+===================================+===================================+
| Header                            | Displays the overall status of    |
|                                   | the instance and the following    |
|                                   | information about the account, as |
|                                   | specified during onboarding:      |
|                                   |                                   |
|                                   | - **Scope of the instance:** The  |
|                                   |   number of accounts onboarded on |
|                                   |   the instance and their status.  |
|                                   |   See the **Accounts** section    |
|                                   |   for more information about the  |
|                                   |   individual accounts and the     |
|                                   |   type of account (single account |
|                                   |   or organization).               |
|                                   |                                   |
|                                   | - **Scan mode:** Cloud Scan or    |
|                                   |   Outpost. For accounts using     |
|                                   |   Outpost, information is         |
|                                   |   displayed about the status of   |
|                                   |   the Outpost account and the     |
|                                   |   account ID.                     |
|                                   |                                   |
|                                   | - **Resource Tags:** Tags defined |
|                                   |   during onboarding.              |
+-----------------------------------+-----------------------------------+
| Security Capabilities             | Displays a breakdown of the       |
|                                   | security capabilities enabled on  |
|                                   | the instance and their individual |
|                                   | statuses. Click on any item that  |
|                                   | shows a warning or error status   |
|                                   | to see the open errors and issues |
|                                   | that contributed to the status:   |
|                                   |                                   |
|                                   | - **Errors** are factual objects  |
|                                   |   that are automatically created  |
|                                   |   when problems occur, and        |
|                                   |   provide insight into the        |
|                                   |   current status of the           |
|                                   |   capability. For example, if a   |
|                                   |   permission is missing, an error |
|                                   |   is displayed. Browse and filter |
|                                   |   the errors to better understand |
|                                   |   and resolve the problem.        |
|                                   |                                   |
|                                   | - **Issues** are actionable       |
|                                   |   objects that are triggered when |
|                                   |   detected problems exceed        |
|                                   |   defined thresholds. Issues are  |
|                                   |   manageable, trackable, and      |
|                                   |   provide remediation suggestions |
|                                   |   and automations.                |
|                                   |                                   |
|                                   | <!-- -->                          |
|                                   |                                   |
|                                   | - The issues displayed in the     |
|                                   |   panel are open issues that are  |
|                                   |   specifically related to the     |
|                                   |   selected connector with the     |
|                                   |   selected capability in the      |
|                                   |   observed scope (single account  |
|                                   |   or organization). Click an      |
|                                   |   issue to start investigating    |
|                                   |   it.                             |
+-----------------------------------+-----------------------------------+
| Accounts                          | Lists the accounts that are       |
|                                   | onboarded on the instance and     |
|                                   | their individual status.          |
|                                   |                                   |
|                                   | If multiple accounts are          |
|                                   | onboarded on the instance, click  |
|                                   | on each account to filter the     |
|                                   | page information by account, and  |
|                                   | drill-down to the security        |
|                                   | capability statuses for each      |
|                                   | account.                          |
+-----------------------------------+-----------------------------------+

4.  If the instance shows an Outpost error, go to the **All Outposts**
    page and find the outpost account that is being used by this
    instance. Right click the Outpost account to view the open errors
    and issues for the account.

5.  If the account shows **Permission** errors, use the side panel to
    check which permissions are missing. You can also **Edit** the
    instance to redeploy the cloud setup template, which should normally
    resolve the error.

6.  Further investigate errors by running XQL queries on the
    `cloud_health_auditing` dataset.

- This dataset records error and recovery events for the security
  capabilities in cloud instances. By querying this dataset you can see
  information about when the error started, the prevalence of the error,
  and whether there is a recurrency pattern. See the specific fields
  descriptions and query examples for each security capability.

  > **Note**

  > Errors related to collection of audit logs in the cloud instance are
  > recorded in the `collection_auditing` dataset. For more information,
  > see [Audit logs fields and query
  > examples](#X716d22fd29931301a896e5a50958fe0ac2b716a).

7.  Set up correlation rules to trigger issues when errors occur in
    cloud security capabilities. See the following examples.

###### Outpost fields and query examples

You can review Outpost entries in the `cloud_health_auditing` dataset to
see Outpost activity over time, or to search for errors on specific
accounts. Outpost entries are added to the dataset as follows:

- An error occurred on an Outpost account that disabled or prevented an
  operation. This is audited as **Error**.

- An exceptional condition occurred on an Outpost account that might
  cause problems if not resolved. This is audited as **Warning**.

- The Outpost account returns to normal function. This is audited as
  **Informational**.

The following table describes the fields for Outpost entries:

  -----------------------------------------------------------------------
  Field                               Description
  ----------------------------------- -----------------------------------
  Account                             Cloud account ID of the Outpost

  Name                                Category of the error, or a brief
                                      description of the event

  Resource ID                         Outpost ID

  Capability                          Outpost

  Region                              Region where the event occurred, or
                                      **All regions**.

  Classification                      Type of entry (Error, Warning, or
                                      Informational)

  Message                             Description of the error or
                                      **Connected** for informational
                                      entries.

  Error                               Details about the error. For
                                      informational entries this is
                                      blank.
  -----------------------------------------------------------------------

Examples of Outpost queries

- Identify Outpost errors on all Outpost accounts in the eu-west-3
  region:

<!-- -->

- dataset = cloud_health_auditing | filter capability = "Outpost" and classification = "Error" and region = "eu-west-3"

<!-- -->

- See all entries (error, warning, and recovery) for Outpost_1 on cloud
  account Account_A:

<!-- -->

- dataset = cloud_health_auditing | filter capability = "Outpost" and resource_id = “Outpost_1” and account = "Account_A"

###### Permissions fields and query examples

You can review Permissions entries in the `cloud_health_auditing`
dataset to see Permissions activity over time, or to search for errors
on specific accounts. Permissions entries are added to the dataset as
follows:

- A permission problem was found. This is audited as **Error**.

- An exceptional condition occurred that might cause problems if not
  resolved. This is audited as **Warning**.

- A permission problem is resolved. This is audited as
  **Informational**.

The following table describes the fields for Permissions entries:

  -----------------------------------------------------------------------
  Field                               Description
  ----------------------------------- -----------------------------------
  Account                             Name of the account where the event
                                      occurred, or **All accounts**.

  Connector                           Name of the connector where the
                                      event occurred

  Name                                Permission name

  Capability                          Permissions

  Classification                      Type of entry (Error, Warning, or
                                      Informational)

  Message                             Description of the error or
                                      **Granted** for informational
                                      entries.
  -----------------------------------------------------------------------

###### Discovery engine fields and query examples

You can review Discovery engine entries in the `cloud_health_auditing`
dataset to see Discovery activity over time, or to search for errors on
specific accounts. Discovery entries are added to the dataset as
follows:

- An API exec problem is found. This is audited as **Error**.

- An exceptional condition occurred that might cause problems if not
  resolved. This is audited as **Warning**.

- An API exec problem is resolved. This is audited as **Informational**.

The following table describes the fields for Discovery engine entries:

  -----------------------------------------------------------------------
  Field                               Description
  ----------------------------------- -----------------------------------
  Account                             Name of the account where the event
                                      occurred, or **All accounts**

  Connector                           Name of the connector where the
                                      event occurred

  Name                                Asset name

  Capability                          Discovery

  Region                              Region where the event occurred, or
                                      **All regions**.

  Classification                      Type of entry (Error, Warning, or
                                      Informational)

  Message                             Description of the error or
                                      **Connected** for informational
                                      entries.
  -----------------------------------------------------------------------

Examples of Discovery engine queries

- Identify API exec errors on the Discovery engine for all accounts on
  the AWS_1 connector:

<!-- -->

- dataset = cloud_health_auditing | filter capability = "Discovery" and connector = "AWS_1" and classification = “Error”

<!-- -->

- See all Discovery engine activity on connector AWS_1 for Account\_ A
  in the af-south-1 region:

<!-- -->

- dataset = cloud_health_auditing | filter capability = "Discovery" and connector = "AWS_1" and account = "accountA" and region = "af-south-1"

###### Agentless Disk Scanning (ADS) fields and query examples

You can review ADS entries in the `cloud_health_auditing` dataset to see
ADS activity over time, or to search for errors on specific accounts.
ADS entries are added to the dataset as follows:

- ADS failed to scan an asset. This is audited as **Failed**.

- ADS successfully scanned an asset. This is audited as **Scanned**.

- The asset or host is not supported by ADS. This is audited as
  **Unsupported**.

- The asset or Host was excluded from the scan. This is audited as
  **Excluded**.

  -----------------------------------------------------------------------
  Field                               Description
  ----------------------------------- -----------------------------------
  Account                             Name of the account to which the
                                      asset belongs

  Connector                           ID of the connector

  Name                                Name of the asset

  Resource ID                         Asset ID

  Capability                          ADS

  Region                              Region where the asset is located

  Classification                      Type of entry (Failed, Unsupported,
                                      Excluded, Scanned)

  Message                             Description of the error, or
                                      Connected for informational
                                      entries.

  Error                               Details about the error. For
                                      informational entries this is
                                      blank.

  Type                                Type of asset that was scanned

  Scope                               Scope of the asset (Asset, Region,
                                      or Account)
  -----------------------------------------------------------------------

Examples of ADS queries

- Identify failed ADS scans on connector
  \"a8df43e848dd42778ae7efd5a706a0fc\" for EC2 assets at the asset scope
  level, filtered by region (northamerica-northeast2-a):

<!-- -->

- dataset = cloud_health_auditing | filter capability = "ADS" and classification = "failed" and connector = “a8df43e848dd42778ae7efd5a706a0fc” and type = "EC2_INSTANCE" and scope = "Asset" and region = "northamerica-northeast2-a" 

<!-- -->

- See all ADS scans (failed and successful) on connector
  \"a8df43e848dd42778ae7efd5a706a0fc\" for EC2 assets belonging to
  Account_A:

<!-- -->

- dataset = cloud_health_auditing | filter capability = "ADS" and connector = “a8df43e848dd42778ae7efd5a706a0fc” and type = "EC2" and account = “Account_A”

###### Data Security Scanning (DSPM) fields and query examples

You can review DSPM entries in the `cloud_health_auditing` dataset to
see DSPM activity over time, or to search for errors on specific
accounts. DSPM entries are added to the dataset as follows:

- DSPM failed to scan an asset. This is audited as **Failed**.

- DSPM successfully scanned an asset. This is audited as **Success**.

The following table describes the fields for DSPM entries:

  -----------------------------------------------------------------------
  Field                               Description
  ----------------------------------- -----------------------------------
  Account                             Name of the account to which the
                                      asset belongs

  Connector                           Name of the connector where the
                                      event occurred

  Name                                Name of the asset

  Resource ID                         Asset ID

  Capability                          DSPM

  Region                              Region where the asset is located

  Classification                      Type of entry (Failed or Success)

  Message                             Description of the error, or
                                      Connected for informational
                                      entries.

  Error                               Details about the error. For
                                      informational entries this is
                                      blank.

  Type                                Type of asset that was scanned

  Scope                               Scope of the asset (Asset, Region,
                                      or Account)
  -----------------------------------------------------------------------

Examples of DSPM queries

- Identify failed DSPM scans on the AWS_1 connector for S3 asset types,
  filtered by region (ap-east-1):

<!-- -->

- dataset = cloud_health_auditing | filter capability = "DSPM" and classification = “Error” and connector = “AWS_1” and type = "S3_BUCKET" and region = "ap-east-1"

<!-- -->

- See all DSPM scans (failed and successful) on the AWS_1 connector, for
  all scanned assets on Account_A:

<!-- -->

- dataset = cloud_health_auditing | filter capability = "DSPM" and account = "Account_A" and connector = “AWS_1”

###### Registry scanning fields and query examples

You can review Registry scanning entries in the `cloud_health_auditing`
dataset to see Registry scanning activity over time, or to search for
errors on specific accounts. Registry scanning entries are added to the
dataset as follows:

- The Registry scanner failed to scan an asset. This is audited as
  **Failed**.

- The Registry scanner successfully scanned an asset. This is audited as
  **Scanned**.

The following table describes the fields for Registry scanning entries:

  -----------------------------------------------------------------------
  Field                               Description
  ----------------------------------- -----------------------------------
  Account                             Name of the account to which the
                                      asset belongs

  Connector                           Name of the connector where the
                                      event occurred

  Resource ID                         Asset ID

  Capability                          Registry

  Classification                      Type of entry (Scanned or Failed)

  Error                               Details about the error. For
                                      informational entries this is blank

  Scope                               Scope of the asset (Asset or
                                      Account)
  -----------------------------------------------------------------------

Examples of Registry scanning queries

- Identify failed scans on connector GCP_1:

<!-- -->

- dataset = cloud_health_auditing | filter capability = "Registry" and classification = “error” and connector = “GCP_1”

<!-- -->

- Review all registry scans (failed and successful) on connector GCP_1
  for asset Asset_A:

<!-- -->

- dataset = cloud_health_auditing | filter capability = "Registry" and connector = “GCP_1” and ressource_id = "Asset_A"

###### Audit logs fields and query example

You can review Audit logs entries in the `collection_auditing` dataset.
Querying this dataset can help you see the connectivity changes of an
instance over time, the escalation or recovery of the connectivity
status, and the error, warning, and informational messages related to
status changes. For more information about this dataset, see [Verify
collector connectivity](#UUIDb7462bf72e1eb1f6c344359c5f7a23dd).

The following table describes the fields for Audit logs entries:

  -----------------------------------------------------------------------
  Field                               Description
  ----------------------------------- -----------------------------------
  Instance                            Instance name

  Log type                            Type of logs affected

  Classification                      Type of entry (Error, Warning, or
                                      Informational)

  Collector type                      Type of the collector

  Description                         Description of the error, or
                                      **Connected** for informational
                                      entries.
  -----------------------------------------------------------------------

Audit logs query example

Identify disruptions (errors) in audit log collection on connector
AWS_1:

    dataset = collection_auditing | filter instance = “AWS_1” and log_type = "Audit Logs" and classification = “Error”

###### Correlation rule examples

The following examples show how to set up correlation rules to trigger
Health Collection issues when errors occur on a specific security
capability.

**Example rule for DSPM errors**

In this example, a correlation rule will trigger a Health Collection
issue if a DSPM scan fails on an AWS_S3 asset on the AWS_1 connector.

Example XQL:

    dataset = cloud_health_auditing | filter capability = "DSPM" and classification = “Error” and type = "AWS_S3" and scope = "Asset" and connector = “AWS_1”

Additional fields to specify in the correlation rule:

  -----------------------------------------------------------------------
  Field                               Value
  ----------------------------------- -----------------------------------
  Time Schedule                       Hourly

  Query time frame                    1 Hour

  Issue Suppression                   Select
                                      **Enable issue suppression**.

  Action                              Select **Generate Issue**.

  Issue Domain                        Health

  Severity                            Medium

  Category                            Collection
  -----------------------------------------------------------------------

**Example rule for Outpost errors**

In this example, a correlation rule will trigger a Health Collection
issue if an error is recorded on account Outpost_A in the us-east-1
region.

Example XQL:

    dataset = cloud_health_auditing | filter capability = "Outpost" and account = "Outpost_A" and region = "eu-west-3" and classification = "Error"

Additional fields to specify in the correlation rule:

  -----------------------------------------------------------------------
  Field                               Value
  ----------------------------------- -----------------------------------
  Time Schedule                       Hourly

  Query time frame                    1 Hour

  Issue Suppression                   Select
                                      **Enable issue suppression**.

  Action                              Select **Generate Issue**.

  Issue Domain                        Health

  Severity                            Medium

  Category                            Collection
  -----------------------------------------------------------------------

##### Manage Kubernetes Connector instances

1.  Select Settings \> Data Sources.

2.  Find the Kubernetes instance by clicking on the Kubernetes name or
    using the **Search** field.

3.  In the row for the Kubernetes instance, click **View Details**. The
    **Kubernetes Connectors** page is displayed with all deployed
    Kubernetes Connectors. To view all Kubernetes clusters, including
    ones that are not yet deployed, go to the
    **Kubernetes Connectivity Management** page.

4.  In the **Kubernetes Connectors** page, click on a cluster name to
    open the details pane for that instance.

5.  You can perform the following actions on each Kubernetes Connector
    instance:

  -----------------------------------------------------------------------------------------------------------------------------
  Action                              Instructions
  ----------------------------------- -----------------------------------------------------------------------------------------
  Edit Connector                      In the row for the Kubernetes instance, right-click and select **Edit**. Alternatively,
                                      in the details pane, click the more options icon and select **Edit Connector**. In
                                      **Edit Kubernetes Connector**, enter a name for the installer. You can edit the namespace
                                      for the connector, the scan cadence, and the version of the connector you want to
                                      install. You must execute the updated template in the Kubernetes environment for the
                                      configuration changes to be applied.

  Delete Connector                    In the row for the Kubernetes instance, right-click and select **Delete**. Alternatively,
                                      in the details pane, click the more options icon and select **Delete Connector**. To
                                      remove the connector, you must manually run Kubernetes commands to delete the resources
                                      in the Kubernetes environment. The commands are listed
                                      [here](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#delete).
  -----------------------------------------------------------------------------------------------------------------------------

###### Kubernetes Connectivity Management

Navigate to Settings \> Data Sources and find the Kubernetes instances
by clicking on the Kubernetes name or using the **Search** field. In the
**Kubernetes Connectors** page, click
**Kubernetes Connectivity Management** to view all detected Kubernetes
clusters. Here, you can check if a cluster is connected, view the
status, and see the connector version. When a new version of the
Kubernetes Connector is available, you can update it here.

#### Palo Alto Networks integrations

> **Note**
>
> Data collection may require an add-on.

Cortex XSIAM supports streaming data directly from Prisma Access
accounts, Next-Generation Firewalls (NGFW), and Panorama devices to your
Cortex XSIAM tenants using the Strata Logging Service.

New tenants (and tenants upgraded from XDR to XSIAM) will work with the
new direct integration of Next-Generation Firewall and Panorama into
Cortex. For such tenants, there's no option to use the Strata Logging
Service integration.

For tenants where a Strata Logging Service license exists, the
configured integrations, such as Next-Generation Firewall and Prisma
Access, can be migrated to Cortex XSIAM in either of the following ways
before the license expires:

- More than two weeks before the license for existing integrations with
  Strata Logging Service expires, manually migrate the integrations,
  using the corresponding **Migrate Devices** buttons on the
  **Data Sources** page. Make sure you select all your devices to
  connect directly to Cortex XSIAM.

- Two weeks prior to the end of your Strata Logging Service license,
  Cortex XSIAM will automatically migrate your integrations to your
  Strata Logging Service.

<!-- -->

- > **Note**

  > Roll-back of Strata Logging Service integration migration is not
  > supported.

##### About Palo Alto Networks integrations

Cortex XSIAM supports streaming data directly from Prisma Access
accounts, Next-Generation Firewalls (NGFW), and Panorama devices to your
Cortex XSIAM tenants using the Strata Logging Service.

Ensure you have deployed Panorama and NGFW, and hold Super User
permissions to your Customer Support Account (CSP).

After your tenant has been activated, navigate to the **Data Sources**
page to configure your integrations. All devices and accounts allocated
to your CSP accounts are available to integrate.

> **Note**
>
> For Palo Alto Networks Integrations there is an option to turn on or
> off the collection of URL and File log types. For more information,
> see [Collecting URL and File log
> types](#UUIDa5af875dc05f7298d3e7849c23153541).

New tenants (and tenants upgraded from XDR to XSIAM) will work with the
new direct integration of Next-Generation Firewall and Panorama into
Cortex. For such tenants, there's no option to use the Strata Logging
Service integration.

For tenants where a Strata Logging Service license exists, the
configured integrations, such as Next-Generation Firewall and Prisma
Access, can be migrated to Cortex XSIAM in either of the following ways
before the license expires:

- More than two weeks before the license for existing integrations with
  Strata Logging Service expires, manually migrate the integrations,
  using the corresponding **Migrate Devices** buttons on the
  **Data Sources** page. Make sure you select all your devices to
  connect directly to Cortex XSIAM.

- Two weeks prior to the end of your Strata Logging Service license,
  Cortex XSIAM will automatically migrate your integrations to your
  Strata Logging Service.

<!-- -->

- > **Note**

  > Roll-back of Strata Logging Service integration migration is not
  > supported.

##### Ingest data from Next-Generation Firewall

You can forward firewall data from your Next-Generation Firewall (NGFW)
and Panorama devices to Cortex XSIAM.

Collection of firewall data from multiple accounts is supported. Super
User permissions on both the Cortex XSIAM tenant accounts and the NGFW
or Panorama accounts are required for this use case.

New tenants (and tenants upgraded from XDR to XSIAM) will work with the
new direct integration of Next-Generation Firewall and Panorama into
Cortex. For such tenants, there's no option to use the Strata Logging
Service integration.

For tenants where a Strata Logging Service license exists, the
configured integrations, such as Next-Generation Firewall and Prisma
Access, can be migrated to Cortex XSIAM in either of the following ways
before the license expires:

- More than two weeks before the license for existing integrations with
  Strata Logging Service expires, manually migrate the integrations,
  using the corresponding **Migrate Devices** buttons on the
  **Data Sources** page. Make sure you select all your devices to
  connect directly to Cortex XSIAM.

- Two weeks prior to the end of your Strata Logging Service license,
  Cortex XSIAM will automatically migrate your integrations to your
  Strata Logging Service.

<!-- -->

- > **Note**

  > Roll-back of Strata Logging Service integration migration is not
  > supported.

> **Prerequisite**
>
> Ensure that you have completed the following on the NGFW or Panorama
> side:

- > For Panorama only, ensure that the Panorama Cloud Services plugin is
  > installed.

- > Enable log forwarding profiles on firewall rules.

> On the Cortex XSIAM side, ensure that you have user role permissions
> for **Data Collection \> Data Sources**.
>
> Configuration of data ingestion from multiple accounts requires Super
> User permissions on both the Cortex XSIAM tenant and on the device
> accounts.
>
> **Note**
>
> If your firewalls are located in a different region, or bandwidth
> issues are encountered due to large log size, you can ingest NGFW logs
> in CEF format, using the Syslog collector. However, the Syslog
> solution is not as powerful nor as comprehensive as this data
> collector, and should only be used when this data collector cannot be
> used. For more information, see [Ingest Next-Generation Firewall logs
> using the Syslog collector](#UUID8d760c93d18771cf3c89e26805fb62cd).
>
> **Note**
>
> In the following procedure, general information is provided for NGFW
> and Panorama. For detailed instructions, consult the documentation for
> your specific devices and Panorama version.

**Set up detection data ingestion**

1.  In the user interface for setting up firewalls, for Strata Logging
    Service/Cloud Logging, enable the following options directly, or
    using device templates.

- (For example, go to Device \> Setup \> Management \> Cloud Logging
  section)

  a.  Select **Enable Strata Logging Service**.

  b.  Select **Enable Enhanced Application Logging**.

  c.  (Optional, depending on your organization\'s requirements) Select
      **Enable Duplicate Logging (Cloud and On-Premise)**.

2.  Depending on your PAN-OS or Panorama version, generate either a
    certificate or PSK.

- For PAN-OS and Panorama versions 10.1 and later, each firewall
  requires a separate certificate. Certificates need to be requested
  through the Customer Support portal. To sign in to the portal, click
  [here](https://support.paloaltonetworks.com/Support/Index). For PAN-OS
  and Panorama versions 10.0 and earlier, you are only required to
  generate one global PSK for all the firewall devices.

  > **Note**

  > Cortex XSIAM does not validate your firewall credentials, you must
  > ensure the certificates or PSK details have been updated in your
  > firewalls in order for data to stream.

3.  Onboard the certificates.

4.  Define a Log Forwarding profile.

5.  Map the Log Forwarding profile to a Security Policy Rule.

6.  Verify that the connection between the firewalls and Strata Logging
    Service is valid.

7.  Push the configuration changes to the firewalls.

8.  In Cortex XSIAM, select Settings \> Data Sources.

9.  On the **Data Sources** page, click **Add Data Source**, search for
    and select **NGFW**, and click **Connect**.

10. Select **Add NGFW Device** or **Add Panorama Device**, and then do
    one of the following:

    - For devices in your account, select one or more devices from
      **Select FW/Panorama devices**.

    - To include devices from other accounts, select
      **Select devices from other accounts**, and then select one or
      more FW or Panorama devices from other accounts. For cross-account
      connections, you must have Super User permissions on the Cortex
      tenant account and the device account.

- Devices already connected are listed at the end. A device may be
  connected via Strata Logging Service, or via Cortex XSIAM. Rectify any
  streaming issues that may arise by checking configurations for the
  relevant connection type (Strata Logging Service or Cortex XSIAM).

11. To complete the onboarding process of your devices, on the
    **Next Steps to Connect Your Devices** page, expand the relevant
    device version, and follow the corresponding instructions.

12. Click **Connect** to establish the instance.

- Connection is established regardless of the firewall credential status
  and can take up to several minutes, select **Sync now** to refresh
  your instances.

13. Validate that your data is streaming. It might be necessary to
    create traffic before you verify data streaming.

- To ensure the data is streaming into your tenant:

  - In your NGFW Standalone Firewall Devices, track the
    **Last communication** timestamp.

  - Run XQL Query:
    `dataset = panw_ngfw_system_raw| filter log_source_id = "[NGFW device SN]"`

14. (*Optional*) Manage your Instance.

- After you create the NGFW instance, on the **Data Sources** page,
  expand the NGFW to track the status of your
  **Standalone Firewall Devices** and **Panorama Devices**.

  Select the ellipses to **Request Certificate**, if required, or
  **Delete** the instance.

> **Note**
>
> It might take an hour or longer after connecting the firewall in
> Cortex XSIAM until you start seeing notifications that the certificate
> has been approved, and that the logging service license has appeared
> on the firewall.

When Cortex XSIAM begins receiving detection data, the console begins
stitching logs with other Palo Alto Network-generated logs to form
stories. Use the XQL Search dataset `panw_ngfw_*_raw` to query your
data, where the following logs are supported:

- [Authentication
  Logs](https://docs.paloaltonetworks.com/cortex/cortex-data-lake/log-forwarding-schema-reference/network-logs/network-authentication-log.html):
  panw_ngfw_auth_raw

- [File Data
  Logs](https://docs.paloaltonetworks.com/strata-logging-service/log-reference/network-logs/network-file-log):
  panw_ngfw_filedata_raw

- [Global Protect
  Logs](https://docs.paloaltonetworks.com/cortex/cortex-data-lake/log-forwarding-schema-reference/network-logs/network-globalprotect-log.html):
  panw_ngfw_globalprotect_raw

- [Hipmatch
  Logs](https://docs.paloaltonetworks.com/cortex/cortex-data-lake/log-forwarding-schema-reference/network-logs/network-hip-match-log.html):
  panw_ngfw_hipmatch_raw\*

- [System
  Logs](https://docs.paloaltonetworks.com/cortex/cortex-data-lake/log-forwarding-schema-reference/common-logs/common-system-log.html):
  panw_ngfw_system_raw

- [Threat
  Logs](https://docs.paloaltonetworks.com/cortex/cortex-data-lake/log-forwarding-schema-reference/network-logs/network-threat-log.html):
  panw_ngfw_threat_raw\*

- [Traffic
  Logs](https://docs.paloaltonetworks.com/cortex/cortex-data-lake/log-forwarding-schema-reference/network-logs/network-traffic-log.html):
  panw_ngfw_traffic_raw\*

- [URL
  Logs](https://docs.paloaltonetworks.com/cortex/cortex-data-lake/log-forwarding-schema-reference/network-logs/network-url-log.html):
  panw_ngfw_url_raw\*

- [User ID
  Logs](https://docs.paloaltonetworks.com/cortex/cortex-data-lake/log-forwarding-schema-reference/network-logs/network-userid-log.html):
  panw_ngfw_userid_raw

- [Configuration
  Logs](https://docs.paloaltonetworks.com/strata-logging-service/log-reference/common-logs/common-configuration-log):
  panw_ngfw_config_raw

- [Tunnel
  Logs](https://docs.paloaltonetworks.com/strata-logging-service/log-reference/network-logs/network-tunnel-log):
  panw_ngfw_tunnel_raw

\*These datasets use the query field names as described in the [Cortex
schema](https://docs.paloaltonetworks.com/cortex/cortex-data-lake/log-forwarding-schema-reference.html)
documentation.

For stitched raw data, you can query the `xdr_data` dataset or use any
preset designated for stitched data, such as `network_story`. For query
examples, refer to the in-app XQL Library. When relevant, Cortex XSIAM
can also generate Cortex XSIAM issues (Analytics, Correlation Rules,
IOC, and BIOC only) from Strata Logging Service detection data. While
Correlation Rules issues are generated on non-normalized and normalized
logs, Analytics, IOC, and BIOC issues are only generated on normalized
logs.

> **Note**
>
> IOC and BIOC issues are applicable on stitched data only, and are not
> available on raw data.
>
> **Tip**
>
> You can see an overview of ingestion status for all log types, and a
> breakdown of each log type and its daily consumption quota on the NGFW
> Ingestion Dashboard.

###### Ingest Next-Generation Firewall logs using the Syslog collector

Use the Syslog collector to ingest Next-Generation Firewall (NGFW) logs
in CEF format. This method is useful when your firewalls are located in
a different region, or bandwidth issues are encountered due to large log
size. When possible, we recommend that you ingest NGFW logs using the
dedicated Next-Generation Firewall data collector instead of the Syslog
collector.

> **Note**
>
> In the following procedure, general information is provided for NGFW
> and Panorama. For detailed instructions, consult the documentation for
> your specific devices and Panorama version, to ensure that you have
> configured log forwarding correctly for all the log types that you
> would like to forward to Cortex XSIAM. The following steps only cover
> configuration of the custom log schema (CEF) for a given syslog
> server. They do not replace the administrator guide's configuration
> coverage of log forwarding.

**Configure the firewall/Panorama for log forwarding to Cortex XSIAM**

1.  To configure the device to include its IP address in the header of
    Syslog messages, select Panorama/Device \> Setup \> Management,
    click the **Edit** icon in the **Logging and Reporting Settings**
    section, and navigate to the **Log Export and Reporting** tab.

2.  From the **Syslog HOSTNAME Format** menu, select **ipv4-address** or
    **ipv6-address**, and click **OK**.

3.  Select Device \> Server Profiles \> Syslog, and click **Add**.

4.  Enter a server profile **Name** and **Location** (**Location**
    refers to a virtual system, if the device is enabled for virtual
    systems).

5.  On the **Servers** tab of the **Syslog Server Profiles** window,
    click **Add,** and enter the following information for the Syslog
    server:

    - **Name**

    - **Syslog Server** (IP address)

    - **Transport**, **Port** (default 514 for UDP)

    - **Facility** (default LOG_USER)

6.  Select the **Custom Log Format** tab and click configure the log
    formats as follows:

- > **Note**

  > To avoid the possible effects of line formatting, do not copy/paste
  > the message formats directly into the PAN-OS web interface. Instead,
  > paste into a text editor, remove any carriage return or line feed
  > characters, and then copy and paste into the web interface.

  > **Note**

  > From version 10.0 and later, the log format documented for log types
  > (Traffic, Threat, and URL) exceeds the maximum supported 2048
  > characters in the Custom Log Format tab on the firewall and
  > Panorama. Select the CEF keys and values to limit the number of
  > characters to 2048, as per your requirements.

  ----------------------------------------------------------------------------------------------------------------------------
  Log Type                            Custom Format
  ----------------------------------- ----------------------------------------------------------------------------------------
  Traffic                             CEF:0\|PANW\|NGFW_CEF\|\$sender_sw_version\|\$subtype\|\$type\|1\|
                                      \_\_firewall_type=firewall.traffic \_\_timestamp=\$start \_\_tz=\$high_res_timestamp
                                      log_type=\$type subtype=\$subtype log_time=\$cef-formatted-receive_time
                                      time_generated=\$cef-formatted-time_generated log_source_id=\$serial
                                      log_source_name=\$device_name sequence_no=\$seqno source_ip=\$src dest_ip=\$dst
                                      source_port=\$sport dest_port=\$dport nat_source=\$natsrc nat_dest=\$natdst
                                      nat_source_port=\$natsport nat_dest_port=\$natdport protocol=\$proto action=\$action
                                      source_user=\$srcuser dest_user=\$dstuser xff_ip=\$xff_ip app=\$app
                                      app_category=\$category_of_app app_sub_category=\$subcategory_of_app rule_matched=\$rule
                                      rule_matched_uuid=\$rule_uuid severity=1 vsys=\$vsys vsys_name=\$vsys_name
                                      from_zone=\$from to_zone=\$to inbound_if=\$inbound_if outbound_if=\$outbound_if
                                      session_id=\$sessionid source_device_category=\$src_category
                                      source_device_profile=\$src_profile source_device_model=\$src_model
                                      source_device_vendor=\$src_vendor source_device_osfamily=\$src_osfamily
                                      source_device_osversion=\$src_osversion source_device_mac=\$src_mac
                                      dest_device_category=\$dst_category dest_device_profile=\$dst_profile
                                      dest_device_model=\$dst_model dest_device_vendor=\$dst_vendor
                                      dest_device_osfamily=\$dst_osfamily dest_device_osversion=\$dst_osversion
                                      dest_device_mac=\$dst_mac bytes_sent=\$bytes_sent bytes_received=\$bytes_received
                                      packets_received=\$pkts_received packets_sent=\$pkts_sent total_time_elapsed=\$elapsed
                                      session_end_reason=\$session_end_reason url_category=\$category

  Threat                              CEF:0\|PANW\|NGFW_CEF\|\$sender_sw_version\|\$threatid\|\$type\|\$number-of-severity\|
                                      \_\_firewall_type=firewall.threat \_\_timestamp=\$cef-formatted-time_generated
                                      \_\_tz=\$high_res_timestamp log_type=\$type subtype=\$subtype
                                      log_time=\$cef-formatted-receive_time time_generated=\$cef-formatted-time_generated
                                      log_source_id=\$serial log_source_name=\$device_name sequence_no=\$seqno source_ip=\$src
                                      dest_ip=\$dst source_port=\$sport dest_port=\$dport nat_source=\$natsrc
                                      nat_dest=\$natdst nat_source_port=\$natsport nat_dest_port=\$natdport protocol=\$proto
                                      action=\$action source_user=\$srcuser dest_user=\$dstuser xff=\$xff xff_ip=\$xff_ip
                                      app=\$app app_category=\$category_of_app app_sub_category=\$subcategory_of_app
                                      rule_matched=\$rule rule_matched_uuid=\$rule_uuid severity=\$number-of-severity
                                      vsys=\$vsys vsys_name=\$vsys_name from_zone=\$from to_zone=\$to inbound_if=\$inbound_if
                                      outbound_if=\$outbound_if session_id=\$sessionid source_device_category=\$src_category
                                      source_device_profile=\$src_profile source_device_model=\$src_model
                                      source_device_vendor=\$src_vendor source_device_osfamily=\$src_osfamily
                                      source_device_osversion=\$src_osversion source_device_mac=\$src_mac
                                      dest_device_category=\$dst_category dest_device_profile=\$dst_profile
                                      dest_device_model=\$dst_model dest_device_vendor=\$dst_vendor
                                      dest_device_osfamily=\$dst_osfamily dest_device_osversion=\$dst_osversion
                                      dest_device_mac=\$dst_mac misc=\$misc threat_id=\$threatid threat_name=\$threat_name
                                      threat_category=\$thr_category direction=\$direction user_agent=\$user_agent

  URL                                 CEF:0\|PANW\|NGFW_CEF\|\$sender_sw_version\|\$subtype\|\$type\|\$number-of-severity\|
                                      \_\_firewall_type=firewall.url \_\_timestamp=\$cef-formatted-time_generated
                                      \_\_tz=\$high_res_timestamp log_type=\$type subtype=\$subtype
                                      log_time=\$cef-formatted-receive_time time_generated=\$cef-formatted-time_generated
                                      log_source_id=\$serial log_source_name=\$device_name sequence_no=\$seqno source_ip=\$src
                                      dest_ip=\$dst source_port=\$sport dest_port=\$dport nat_source=\$natsrc
                                      nat_dest=\$natdst nat_source_port=\$natsport nat_dest_port=\$natdport protocol=\$proto
                                      action=\$action source_user=\$srcuser dest_user=\$dstuser xff=\$xff xff_ip=\$xff_ip
                                      app=\$app app_category=\$category_of_app app_sub_category=\$subcategory_of_app
                                      rule_matched=\$rule rule_matched_uuid=\$rule_uuid severity=\$number-of-severity
                                      vsys=\$vsys vsys_name=\$vsys_name from_zone=\$from to_zone=\$to inbound_if=\$inbound_if
                                      outbound_if=\$outbound_if session_id=\$sessionid source_device_category=\$src_category
                                      source_device_profile=\$src_profile source_device_model=\$src_model
                                      source_device_vendor=\$src_vendor source_device_osfamily=\$src_osfamily
                                      source_device_osversion=\$src_osversion source_device_mac=\$src_mac
                                      dest_device_category=\$dst_category dest_device_profile=\$dst_profile
                                      dest_device_model=\$dst_model dest_device_vendor=\$dst_vendor
                                      dest_device_osfamily=\$dst_osfamily dest_device_osversion=\$dst_osversion
                                      dest_device_mac=\$dst_mac uri=\$misc threat_id=\$threatid threat_name=\$threat_name
                                      threat_category=\$thr_category direction=\$direction user_agent=\$user_agent
                                      url_category=\$category url_category_list=\$url_category_list content_type=\$contenttype
                                      http_method=\$http_method http_headers=\$http_headers
                                      http2_connection=\$http2_connection referer=\$referer pcap_id=\$pcap_id

  File Data                           CEF:0\|PANW\|NGFW_CEF\|\$sender_sw_version\|\$threatid\|\$type\|\$number-of-severity\|
                                      \_\_firewall_type=firewall.filedata \_\_timestamp=\$cef-formatted-time_generated
                                      \_\_tz=\$high_res_timestamp log_type=\$type subtype=\$subtype
                                      log_time=\$cef-formatted-receive_time time_generated=\$cef-formatted-time_generated
                                      log_source_id=\$serial log_source_name=\$device_name sequence_no=\$seqno source_ip=\$src
                                      dest_ip=\$dst source_port=\$sport dest_port=\$dport nat_source=\$natsrc
                                      nat_dest=\$natdst nat_source_port=\$natsport nat_dest_port=\$natdport protocol=\$proto
                                      action=\$action source_user=\$srcuser dest_user=\$dstuser xff=\$xff xff_ip=\$xff_ip
                                      app=\$app app_category=\$category_of_app app_sub_category=\$subcategory_of_app
                                      rule_matched=\$rule rule_matched_uuid=\$rule_uuid severity=\$number-of-severity
                                      vsys=\$vsys vsys_name=\$vsys_name from_zone=\$from to_zone=\$to inbound_if=\$inbound_if
                                      outbound_if=\$outbound_if session_id=\$sessionid source_device_category=\$src_category
                                      source_device_profile=\$src_profile source_device_model=\$src_model
                                      source_device_vendor=\$src_vendor source_device_osfamily=\$src_osfamily
                                      source_device_osversion=\$src_osversion source_device_mac=\$src_mac
                                      dest_device_category=\$dst_category dest_device_profile=\$dst_profile
                                      dest_device_model=\$dst_model dest_device_vendor=\$dst_vendor
                                      dest_device_osfamily=\$dst_osfamily dest_device_osversion=\$dst_osversion
                                      dest_device_mac=\$dst_mac misc=\$misc threat_id=\$threatid threat_name=\$threat_name
                                      threat_category=\$thr_category direction=\$direction user_agent=\$user_agent
                                      file_url=\$file_url filedigest=\$filedigest filetype=\$filetype pcap_id=\$pcap_id
  ----------------------------------------------------------------------------------------------------------------------------

7.  Configure **Escaping** characters as follows:

    - **Escaped Characters:** \\=

    - **Escape Character:** \\

- ![](media/rId5377.png){width="5.833333333333333in"
  height="4.710416666666666in"}

**Configure Syslog collection**

Set up a Syslog collector for the logs, as explained in [Activate Syslog
Collector](#UUIDd7d85d4a97df7ec2a052d22376f5a52e). In Task 4, ensure
that you set **Format** to CEF.

##### Ingest data from Prisma Access

You can forward data from Prisma Access to Cortex XSIAM. When your
Cortex XSIAM tenant begins receiving detection data, it begins stitching
logs with other Palo Alto Networks-generated logs to form stories. Use
the XQL Search to query the data.

Collection of data from multiple accounts is supported. Super User
permissions on both the Cortex XSIAM tenant accounts and the Prisma
Access accounts are required for this use case.

New tenants (and tenants upgraded from XDR to XSIAM) will work with the
new direct integration of Next-Generation Firewall and Panorama into
Cortex. For such tenants, there's no option to use the Strata Logging
Service integration.

For tenants where a Strata Logging Service license exists, the
configured integrations, such as Next-Generation Firewall and Prisma
Access, can be migrated to Cortex XSIAM in either of the following ways
before the license expires:

- More than two weeks before the license for existing integrations with
  Strata Logging Service expires, manually migrate the integrations,
  using the corresponding **Migrate Devices** buttons on the
  **Data Sources** page. Make sure you select all your devices to
  connect directly to Cortex XSIAM.

- Two weeks prior to the end of your Strata Logging Service license,
  Cortex XSIAM will automatically migrate your integrations to your
  Strata Logging Service.

<!-- -->

- > **Note**

  > Roll-back of Strata Logging Service integration migration is not
  > supported.

> **Prerequisite**
>
> Configuration of data ingestion from multiple accounts requires Super
> User permissions in both Cortex XSIAM tenant and Prisma Access
> accounts.

The logs ingested by Prisma Access are the same as the logs ingested by
Next-Generation Firewall. For more information, refer to [Ingest data
from Next-Generation Firewall](#UUIDf67ce98eabab86b9955104d7bb4ff0c7).

To ingest detection data from Prisma Access:

1.  Select Settings \> Data Sources.

2.  On the **Data Sources** page, click **Add Data Source**, search for
    and select **Prisma Access**, and click **Connect**.

- > **Note**

  > Cortex XSIAM does not validate your Prisma Access account
  > credentials. You must ensure the account has been deployed in order
  > for data to stream.

3.  In the **Connect Prisma Access** dialog box, you can choose to
    connect Prisma Access to this account or other accounts.

    - To connect Prisma Access to this account, click **Connect**.

    - To connect Prisma Access to other accounts, click
      **Connect Prisma Access from other accounts** and select the
      account from the accounts listed. Click **Connect**.

- Connection can take up to several minutes.

  On the **Data Sources** page, expand Prisma Access to track the status
  of your instance.

4.  Validate that your data is streaming.

- To ensure the data is streaming into your tenant, using XQL, query
  Next-Generation Firewall raw datasets `panw_ngfw_<*>_raw` using the
  field: `is_prisma_mobile`.

5.  (*Optional*) Manage your Instance.

- After you create the Prisma Access instance, on the **Data Sources**
  page, expand the Prisma Access integration to track the connection,
  or, if you want, to **Delete** the instance.

##### Ingest logs from Prisma Access Browser

Cortex Prisma Access Browser is a browser designed specifically for
enterprise use, and is fortified with security features to protect users
and organizations. You can configure Cortex XSIAM to ingest Prisma
Access Browser logs into a dataset called
`panw_prisma_access_browser_raw`, that can be queried using XQL. This
integration gives you visibility into issues that are generated by the
browser. The ingested data can also be used for performing threat
hunting queries and correlations within the Cortex platform.

Only one instance of this collector can be created per Cortex XSIAM
tenant.

1.  In Cortex XSIAM, select Settings \> Data Sources.

2.  On the **Data Sources** page, click **Add Data Source**, search for
    and select **Prisma Access Browser**, and click **Connect**.

3.  In the **Connect Prisma Access Browser** dialog box, select the
    checkbox for **Connect Prisma Access Browser to this account**.

4.  Click **Connect**.

- Connection can take up to several minutes.

  On the **Data Sources** page, expand **Prisma Access Browser** to
  track the status of your instance.

5.  Validate that data is streaming to your tenant by using XQL to query
    the dataset `panw_prisma_access_browser_raw`.

After you have created a Prisma Access Browser instance, you can use the
**Data Sources** page to view information about the integration, or
delete the instance.

##### Ingest detection data from Strata Logging Service

To streamline the connection and management of all Palo Alto Networks
generated logs across products in Cortex XSIAM with a Strata Logging
Service, Cortex XSIAM can ingest detection data from Strata Logging
Service in a more flexible manner using the Strata Logging Service data
collector.

You can configure the Strata Logging Service data collector to take logs
from other Palo Alto Networks products already logging to 1 or more
existing Strata Logging Service.

Cortex XSIAM supports streaming data directly from Prisma Access
accounts and New-Generation Firewalls (NGFW) and Panorama devices to
your Cortex XSIAM tenants using the Cortex Native Data Lake. Existing
integrations should be migrated to the Cortex Native Data Lake. Make
sure you select all your devices to connect directly to Cortex XSIAM.
Integrations not migrated manually will be migrated automatically 2
weeks before the end of the contract with Strata Logging Service.

For stitched raw data, use the XQL query `xdr_data` dataset or any
preset designated for stitched data, such as `network_story`. For query
examples, refer to the in-app XQL Library. Cortex XSIAM can also
generate Cortex XSIAM issues (Analytics, Correlation Rules, IOC, and
BIOC only) when relevant from Strata Logging Service detection data.
While Correlation Rules issues are generated on non-normalized and
normalized logs, Analytics, IOC, and BIOC issues are only generated on
normalized logs.

> **Note**
>
> IOC and BIOC issues are applicable on stitched data only and are not
> available on raw data.

To ingest detection data from Strata Logging Service.

1.  [Activate the Strata Logging
    Service](https://docs.paloaltonetworks.com/cortex/cortex-data-lake/cortex-data-lake-getting-started/activate-cortex-data-lake-toc/activate-cortex-data-lake-easy).

- You can configure Cortex XSIAM to take Palo Alto generated firewall
  logs from other Palo Alto Networks products already logging to an
  existing Strata Logging Service.

2.  Select Settings \> Data Sources.

3.  In the Strata Logging Service configuration, click the more options
    icon, and select **Add New Instance**.

4.  **Select Data Lake Instance**.

- Select one or more existing Strata Logging Service instances that you
  want to connect to this Strata Logging Service instance.

5.  **Save** your Strata Logging Service configuration.

- Once events start to come in, a green check mark appears underneath
  the **Strata Logging Service** configuration.

6.  (*Optional*) Manage your Strata Logging Service Collector.

- After you create the Strata Logging Service Collector, you can make
  additional changes, as needed.

  - **Delete** the Strata Logging Service Collector.

7.  After Cortex XSIAM begins receiving data from a Strata Logging
    Service, you can use XQL Search to search for specific data, using
    the `xdr_data` dataset.

##### Ingest alerts and assets from IoT Security

The Palo Alto Networks IoT Security solution discovers unmanaged
devices, detects behavioral anomalies, recommends policy based on risk,
and automates enforcement without the need for additional sensors or
infrastructure. The Cortex XSIAM IoT Security integration enables you to
ingest alerts and device information from your IoT Security instance.

To receive data, configure the Data Sources settings in Cortex XSIAM for
the IoT Security data collector in Settings \> Data Sources.

As soon as data collection begins, Cortex XSIAM displays the IoT
Security alerts in the Cortex XSIAM Issues table and groups them into
cases. The IoT Security issues are updated every 15 minutes. IoT
security alerts which were resolved before the integration aren't added
to the Cortex XSIAM table. Cortex XSIAM adds device activities detected
by IoT Security into the Cortex XSIAM Assets table. Device activities
are updated every five minutes.

Cortex XSIAM automatically creates a new dataset for device activities
(`panw_iot_security_devices_raw`) and a new dataset for issues
(`panw_iot_security_alerts_raw`), which you can use to initiate XQL
Search queries and create Correlation Rules.

Before you configure the **IoT Security Collector**, generate an access
key and a key ID for the integration.

1.  Log in to the **PAN IoT Security** portal and click your user name.

2.  Select **Preferences**.

3.  In the **User Role & Access** section, **Create** an API Access Key.

4.  Download and save the access key and key ID in a secure location.

For more information about the PAN IoT Secuity API, see [Get Started
with the IoT Security
API](https://docs.paloaltonetworks.com/iot/iot-security-api-reference/iot-security-api-overview/get-started-with-the-iot-security-api).

Configure the IoT Security alerts and assets collection in Cortex XSIAM.

1.  Select Settings \> Data Sources.

2.  On the **Data Sources** page, click **Add Data Source**, search for
    and select **IoT Security Collector**, and click **Connect**.

3.  Specify the following parameters.

    - **Customer ID**: Tenant domain part of the FQDN used for your
      **IoT Security** account. For example, in
      `yourcorp.iot.paloaltonetworks.com`, the customer ID is
      `yourcorp`. The customer ID is unique and case sensitive. After
      you save the integration instance, you can\'t edit the Customer
      ID.

    - **Access Key** and **Key ID** previously generated for the
      integration.

    - **Integration Scope**: Select at least one of the two values,
      **Alerts** and **Devices** depending on which information you want
      to ingest.

4.  Click **Test** to validate access, and then click **Enable**.

- When events start to come in, a green check mark appears underneath
  the **IoT Security Collector** configuration with the data and time
  that the data was last synced.

5.  (*Optional*) Manage your IOT Security Collector.

- After you enable the IOT Security Collector, you can make additional
  changes as needed. To modify a configuration, select any of the
  following options.

  - **Edit** the IOT Security Collector settings.

  - **Disable** the IOT Security Collector.

  - **Delete** the IOT Security Collector.

6.  After Cortex XSIAM begins receiving data from IOT Security, you can
    use the XQL Search to search for logs in the new datasets,
    `panw_iot_security_devices_raw` for device activities, and
    `panw_iot_security_alerts_raw` for issues.

##### Collecting URL and File log types

For Palo Alto Networks integrations, you can choose whether to collect
URL and File type logs. These logs enhance your cyber analytics,
correlation rules and visibility for investigation. However, if you want
to reduce ingestion charges, you can globally turn off collection of URL
and File log types for all **Palo Alto Networks Integrations**.

When collection is turned off, some detectors won't detect cyber attacks
or provide full context, and correlation rules won't be able to detect
cyber events. For a full list of affected detectors, see [Detectors
connected to URL and File log
types](#UUID5d66ea8c0d79ae0dddb05bbf87bdb31b).

You can also calculate the amount of ingestion that URL and File log
types are consuming by looking at the NGFW dashboard. This dashboard
provides an overview of the PAN-NGFW ingestion status of all log types
(including URL and File log types) and their daily consumption quota.
For more information, see <urn:resource:component:1159517>.

You can turn on or off **URL and File log types collection** on the
**Data Sources** page.

###### Detectors connected to URL and File log types

If you turn off **URL and File log types collection**, some detectors
are unable to detect cyber attacks or provide full context, and
correlation rules are unable to detect cyber events.

The following detectors are affected by URL logs:

- A non-browser process accessed a website UI

- Reverse SSH tunnel to external domain/IP

- Uncommon network tunnel creation

- Suspicious domain fronting behavior

- Possible watering hole SMB credential theft

- Rare connection to external IP address or host by an application using
  RMI-IIOP or LDAP protocol

- Uncommon JA3 SSL fingerprint communication to an instant messaging
  server

- PowerShell Initiates a Network Connection to GitHub

- Non-browser failed access to a pastebin-like site

- Non-browser access to a pastebin-like site

- C2 from contextual causality signal

- Massive upload to a rare storage or mail domain

- DNS Tunneling

The following detectors are affected by File logs:

- Rare AppID usage to a rare destination

- Abnormal network communication through TOR using an uncommon port

- Recurring access to rare IP

- Possible network connection to a TOR relay server

- A user accessed an uncommon AppID

- Large Upload (Generic)

- Large Upload (FTP)

- Large Upload (SMTP)

- Possible network connection to a TOR relay server

- A user accessed a resource for the first time via SSO - silent

- Access to a domain that is categorized as malicious - silent

- Recurring access to rare domain categorized as malicious - silent

- Cloud Large Upload (Generic) - disabled

#### External data ingestion

Cortex XSIAM supports data ingestion from external sources for a variety
of service types and vendors.

To ensure complete and uninterrupted data ingestion, Cortex XSIAM
collects granular data ingestion metrics and generates ingestion issues
if disruption is identified in data collection. For more information,
see [Overview of data ingestion
metrics](#UUIDea34d22cafb5700f684dc6f52342e84d).

##### External applications

You can integrate the following external applications to manage
notifications:

- Slack: To send outbound notifications to Slack. For more information,
  see [Integrate Slack for outbound
  notifications](#UUID49a9a09cf0b44048aec98ff52278bda2).

- Syslog server: To send Cortex XSIAM notifications to your Syslog
  server. For more information, see [Integrate a syslog
  receiver](#UUID218d87cf0fa5bccd27b3bb119b0567ea).

##### Ingest network connection logs

You can ingest network connection logs from different third-party
sources.

###### Ingest network flow logs from Amazon S3

You can forward network flow logs to Cortex XSIAM from Amazon Simple
Storage Service (Amazon S3).

To receive network flow logs from Amazon S3, you must first configure
data collection from Amazon S3. You can then configure the Data Sources
settings in Cortex XSIAM for Amazon S3. After you set up collection
integration, Cortex XSIAM begins receiving new logs and data from the
source.

You can either configure Amazon S3 with SQS notification manually on
your own or use the AWS CloudFormation Script that we have created for
you to make the process easier. The instructions below explain how to
configure Cortex XSIAM to receive network flow logs from Amazon S3 using
SQS. To perform these steps manually, see Configure Data Collection from
Amazon S3 Manually.

> **Note**
>
> For more information on configuring data collection from Amazon S3,
> see the Amazon S3 Documentation.

When Cortex XSIAM begins receiving logs, the app automatically creates
an Amazon S3 Cortex Query Language (XQL) dataset (`aws_s3_raw`). This
enables you to search the logs with XQL Search using the dataset. For
example, queries refer to the in-app XQL Library. For enhanced cloud
protection, you can also configure Cortex XSIAM to ingest network flow
logs as Cortex XSIAM network connection stories, which you can query
with XQL Search using the `xdr_data` dataset with the preset called
`network_story`. Cortex XSIAM can also generate Cortex XSIAM issues
(Analytics, Correlation Rules, IOC, and BIOC) when relevant from Amazon
S3 logs. While Correlation Rules issues are generated on non-normalized
and normalized logs, Analytics, IOC, and BIOC issues are only generated
on normalized logs.

Enhanced cloud protection provides the following:

- Normalization of cloud logs

- Cloud logs stitching

- Enrichment with cloud data

- Detection based on cloud analytics

- Cloud-tailored investigations

Be sure you do the following tasks before you begin configuring data
collection from Amazon S3 using the AWS CloudFormation Script.

- Ensure that you have the proper permissions to run AWS CloudFormation
  with the script provided in Cortex XSIAM. You need at a minimum the
  following permissions in AWS for an Amazon S3 bucket and Amazon Simple
  Queue Service (SQS):

  - **Amazon S3 bucket**: `GetObject`

  - **SQS**: `ChangeMessageVisibility`, `ReceiveMessage`, and
    `DeleteMessage`.

- Ensure that you can access your Amazon Virtual Private Cloud (VPC) and
  have the necessary permissions to create flow logs.

- Determine how you want to provide access to Cortex XSIAM to your logs
  and perform API operations. You have the following options:

  - Designate an AWS IAM user, where you will need to know the Account
    ID for the user and have the relevant permissions to create an
    access key/id for the relevant IAM user. This is the default option
    as explained in Configure the Amazon S3 Collection in Cortex XSIAM
    by selecting **Access Key**.

  - Create an assumed role in AWS to delegate permissions to a Cortex
    XSIAM AWS service. This role grants Cortex XSIAM access to your flow
    logs. For more information, see [Creating a role to delegate
    permissions to an AWS
    service](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-service.html).
    This is the **Assumed Role** option as described in the Configure
    the Amazon S3 collection in Cortex XSIAM. For more information on
    creating an assumed role for Cortex XSIAM, see [Create an assumed
    role](#UUIDb11b8a8d194ebbe449559e38629b008d).

<!-- -->

- To collect Amazon S3 logs that use server-side encryption (SSE), the
  user role must have an IAM policy that states that Cortex XSIAM has
  kms:Decrypt permissions. With this permission, Amazon S3 automatically
  detects if a bucket is encrypted and decrypts it. If you want to
  collect encrypted logs from different accounts, you must have the
  decrypt permissions for the user role also in the key policy for the
  master account Key Management Service (KMS). For more information, see
  [Allowing users in other accounts to use a KMS
  key](https://docs.aws.amazon.com/kms/latest/developerguide/key-policy-modifying-external-accounts.html).

Configure Cortex XSIAM to receive network flow logs from Amazon S3 using
the CloudFormation Script.

1.  Download the CloudFormation Script in Cortex XSIAM.

    a.  Select Settings \> Data Sources.

    b.  On the **Data Sources** page, click **Add Data Source**, search
        for and select **Amazon S3**, and click **Connect**.

    c.  To provide access to Cortex XSIAM to your logs and to perform
        API operations using a designated AWS IAM user, leave the
        **Access Key** option selected. Otherwise, select
        **Assumed Role**, and ensure that you Create an Assumed Role for
        before continuing with these instructions.

    d.  For the **Log Type**, select **Flow Logs** to configure your log
        collection to receive network flow logs from Amazon S3, and the
        following text is displayed under the field
        **Download CloudFormation Script. See instructions here.**

    e.  Click the **Download CloudFormation Script.** link to download
        the script to your computer.

2.  Create a new Stack in the CloudFormation Console with the script you
    downloaded from Cortex XSIAM.

- For more information on creating a Stack, see [Creating a stack on the
  AWS CloudFormation
  console](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-console-create-stack.html).

  a.  Log in to the [CloudFormation
      Console](https://console.aws.amazon.com/cloudformation/).

  b.  From the CloudFormation \> Stacks page, ensure that you have
      selected the correct region for your configuration.

  c.  Select Create Stack \> With new resources (standard).

  d.  Specify the template that you want AWS CloudFormation to use to
      create your stack. This template is the script that you downloaded
      from Cortex XSIAM , which will create an Amazon S3 bucket, Amazon
      Simple Queue Service (SQS) queue, and Queue Policy. Configure the
      following settings in the **Specify template** page.

      - Prerequisite - Prepare template \> Prepare template: Select
        **Template is ready**.

      - **Specify Template**

        - **Template source**: Select **Upload a template file**.

        - **Upload a template file**: **Choose file**, and select the
          `cortex-xdr-create-s3-with-sqs-flow-logs.json` file that you
          downloaded from Cortex XDR.

        <!-- -->

        - ![](media/rId5400.png){width="5.833333333333333in"
          height="3.675in"}

  e.  Click **Next**.

  f.  In the **Specify stack details** page, configure the following
      stack details.

      - **Stack name**: Specify a descriptive name for your stack.

      - Parameters \> Cortex XDR Flow Logs Integration

        - **Bucket Name**: Specify the name of the S3 bucket to create,
          where you can leave the default populated name as
          **xdr-flow-logs** or create a new one. The name must be
          unique.

        - **Publisher Account ID**: Specify the AWS IAM user account ID
          with whom you are sharing access.

        - **Queue Name**: Specify the name for your Amazon SQS queue to
          create, where you can leave the default populated name as
          **xdr-flow** or create a new one. The name must be unique.

        <!-- -->

        - ![](media/rId5403.png){width="5.833333333333333in"
          height="3.711457786526684in"}

  g.  Click **Next**.

  h.  In the **Configure stack options** page, there is nothing to
      configure, so click **Next**.

  i.  In the **Review** page, look over the stack configurations
      settings that you have configured and if they are correct, click
      **Create stack**. If you need to make a change, click **Edit**
      beside the particular step that you want to update.

  - The stack is created and is opened with the **Events** tab
    displayed. It can take a few minutes for the new Amazon S3 bucket,
    SQS queue, and Queue Policy to be created. Click **Refresh** to get
    updates. Once everything is created, leave the stack opened in the
    current browser, because you will need to access information in the
    stack for other steps detailed below.

    > **Note**

    > For the Amazon S3 bucket created using CloudFormation, it is the
    > customer's responsibility to define a retention policy by creating
    > a **Lifecycle rule** in the **Management** tab. We recommend
    > setting the retention policy to at least 7 days to ensure that the
    > data is retrieved under all circumstances.

3.  Configure your Amazon Virtual Private Cloud (VPC) with flow logs:

    1.  Open the [Amazon VPC
        Console](https://console.aws.amazon.com/vpc/.), and in the
        **Resources by Region** listed, select **VPCs** to view the VPCs
        configured for the current region selected. To select another
        VPC from another region, select **See all regions**, and select
        one of them.

    - > **Note**

      > To create a new VPC, click **Launch VPC Wizard**. For more
      > information, see [AWS VPC Flow
      > Logs](https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html).

    2.  From the list of **Your VPCs**, select the checkbox beside the
        VPC that you want to configure to create flow logs, and then
        select Actions \> Create flow log.

    - ![](media/rId5408.png){width="5.833333333333333in"
      height="2.559374453193351in"}

    3.  Configure the following **Flow log settings**:

        - **Name - optional**: (*Optional*) Specify a descriptive name
          for your VPC flow log.

        - **Filter**: Select **All** types of traffic to capture.

        - **Maximum aggregation interval**: If you anticipate a heavy
          flow of traffic, select **1 minute**. Otherwise, leave the
          default setting as **10 minutes**.

        - **Destination**: Select **Send to an Amazon S3 bucket** as the
          destination to publish the flow log data.

        - **S3 bucket ARN**:Specify the Amazon Resource Name (ARN) for
          your Amazon S3 bucket.

        <!-- -->

        - You can retrieve your bucket's ARN by opening another instance
          of the AWS Management Console in a browser window and opening
          the [Amazon S3 console](https://console.aws.amazon.com/s3/).
          In the **Buckets** section, select the bucket that you created
          for collecting the Amazon S3 flow logs when you created your
          stack, click **Copy ARN**, and paste the ARN in this field.

          ![](media/rId5412.png){width="5.833333333333333in"
          height="1.5677077865266842in"}

        <!-- -->

        - **Log record format**: Select **Custom Format**, and in the
          **Log Format** field, specify the following fields to include
          in the flow log record, which you can select from the list
          displayed:

          - **account-id**

          - **action**

          - **az-id**

          - **bytes**

          - **dstaddr**

          - **dstport**

          - **end**

          - **flow-direction**

          - **instance-id**

          - **interface-id**

          - **packets**

          - **log-status**

          - **pkt-srcaddr**

          - **pkt-dstaddr**

          - **protocol**

          - **region**

          - **srcaddr**

          - **srcport**

          - **start**

          - **sublocation-id**

          - **sublocation-type**

          - **subnet-id**

          - **tcp-flags**

          - **type**

          - **vpc-id**

          - **version**

    4.  Click **Create flow log**.

    - Once the flow log is created, a message indicating that the flow
      log was successfully created is displayed at the top of the
      **Your VPCs** page.

      In addition, if you open your Amazon S3 bucket configurations, by
      selecting the bucket from the [Amazon S3
      console](https://console.aws.amazon.com/s3/), the **Objects** tab
      contains a folder called `AWSLogs/` to collect the flow logs.

4.  Configure access keys for the AWS IAM user that Cortex XSIAM uses
    for API operations.

- > **Note**

  - > It is the responsibility of the customer's organization to ensure
    > that the user who performs this task of creating the access key is
    > designated with the relevant permissions. Otherwise, this can
    > cause the process to fail with errors.

  - > Skip this step if you are using an **Assumed Role** for Cortex
    > XSIAM.

  1.  Open the [AWS IAM Console](https://console.aws.amazon.com/iam/),
      and in the navigation pane, select Access management \> Users.

  2.  Select the **User name** of the AWS IAM user.

  3.  Select the **Security credentials** tab, scroll down to the
      **Access keys** section, and click **Create access key**.

  4.  Click the copy icon next to the **Access key ID** and
      **Secret access key** keys, where you must click
      **Show secret access key** to see the secret key and record them
      somewhere safe before closing the window. You will need to provide
      these keys when you edit the Access policy of the SQS queue and
      when setting the **AWS Client ID** and **AWS Client Secret** in
      Cortex XSIAM. If you forget to record the keys and close the
      window, you will need to generate new keys and repeat this
      process.

  > **Note**

  > For more information, see [Managing access keys for IAM
  > users](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html).

5.  When you create an Assumed Role in Cortex XSIAM, ensure that you
    edit the policy that defines the permissions for the role with the
    S3 Bucket ARN and **SQS ARN**, which is taken from the Stack you
    created.

- > **Note**

  > Skip this step if you are using an **Access Key** to provide access
  > to Cortex XSIAM.

6.  Configure the Amazon S3 collection in Cortex XSIAM.

    a.  Select Settings \> Data Sources.

    b.  In the **Amazon S3** configuration, click **Add Instance** to
        begin a new configuration.

    c.  Set these parameters, where the parameters change depending on
        whether you configured an **Access Key** or **Assumed Role**.

        - **SQS URL**: Specify the **SQS URL**, which is taken from the
          stack you created. In the browser you left open after creating
          the stack, open the **Outputs** tab, copy the **Value** of the
          **QueueURL** and paste it in this field.

        - **Name**: Specify a descriptive name for your log collection
          configuration.

        - When setting an **Access Key**, set these parameters.

          - **AWS Client ID**: Specify the **Access key ID**, which you
            received when you created access keys for the AWS IAM user
            in AWS.

          - **AWS Client Secret**: Specify the **Secret access key** you
            received when you created access keys for the AWS IAM user
            in AWS.

        - When setting an **Assumed Role**, set these parameters.

          - **Role ARN**: Specify the **Role ARN** for the Assumed Role
            you created for in AWS.

          - **External Id**:Specify the **External Id** for the Assumed
            Role you created for in AWS.

        - **Log Type**: Select **Flow Logs** to configure your log
          collection to receive network flow logs from Amazon S3. When
          configuring network flow log collection, the following
          additional field is displayed for
          **Enhanced Cloud Protection**.

        <!-- -->

        - You can **Normalize and enrich flow logs** by selecting the
          checkbox. If selected, Cortex XSIAM ingests the network flow
          logs as XDR network connection stories, which you can query
          using XQL Search from the `xdr_data` dataset using the preset
          called `network_story`.

    d.  Click **Test** to validate access, and then click **Enable**.

    - When events start to come in, a green check mark appears
      underneath the **Amazon S3** configuration with the number of logs
      received.

####### Create an assumed role

If you do not designate a separate AWS IAM user to provide access to
Cortex XSIAM to your logs and to perform API operations, you can create
an assumed role in AWS to delegate permissions to a Cortex XSIAM AWS
service. This role grants Cortex XSIAM access to your logs. For more
information, see [Creating a role to delegate permissions to an AWS
service](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-service.html).

When setting up any type of Amazon S3 Collector in Cortex XSIAM, these
instructions explain setting up an **Assumed Role**.

1.  Log in to the AWS Management Console to create a role for Cortex
    XSIAM.

- Refer to the [AWS
  instructions](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-service.html)
  for guidance.

  a.  Create the role in the same region as your AWS account, and use
      the following values and options when creating the role.

      - Type of Trusted \> Another AWS Account, and specify the
        **Account ID** as `006742885340`. When using a Cortex XSIAM
        FedRAMP environment, specify the **Account ID** as
        `685269782068`.

      - Select **Options** for the **Require external ID**, which is a
        unique alphanumeric string, and generate a secure UUIDv4 using
        an [Online UUID
        Generator](https://www.uuidgenerator.net/version4). Copy the
        **External ID** as you will use this when configuring the Amazon
        S3 Collector in Cortex XSIAM .

      <!-- -->

      - > **Note**

        > In AWS this is an optional field to configure, but this must
        > be configured to set up the Amazon S3 Collector in Cortex
        > XSIAM .

      <!-- -->

      - Do not enable MFA. Verify that **Require MFA** is not selected.

  - ![](media/rId5418.png){width="5.833333333333333in"
    height="4.477083333333334in"}

  b.  Click **Next** and add the AWS Managed Policy for
      **Security Audit**.

  - ![](media/rId5421.png){width="5.833333333333333in"
    height="4.367707786526684in"}

    Then, add a role name and create the role. In this workflow, later,
    you will create the granular policies and edit the role to attach
    the additional policies.

2.  Create the policy that defines the permissions for the Cortex XSIAM
    role.

    a.  Select **IAM** on the AWS Management Console.

    b.  In the navigation pane on the left, select Access Management \>
        Policies \> Create Policy.

    c.  Select the **JSON** tab.

    - Copy the following JSON policy and paste it within the editor
      window.

      > **Note**

      > The `<s3-arn>` and `<sqs-arn>` placeholders. These will be
      > filled out later depending on which Amazon S3 logs you are
      > configuring, including network flow logs, audit logs, or generic
      > logs.

          {
              "Version": "2012-10-17",
              "Statement": [
                  {
                      "Effect": "Allow",
                      "Action": "s3:GetObject",
                      "Resource": "<s3-arn>/*"
                  },
                  {
                      "Effect": "Allow",
                       "Action": [
                          "sqs:ReceiveMessage",
                          "sqs:DeleteMessage",
                          "sqs:ChangeMessageVisibility"
                      ],
                      "Resource": "<sqs-arn>"
                  }
              ]
          }

    d.  Review and create the policy.

3.  Edit the role you created in Step 1 and attach the policy to the
    role.

4.  Copy the **Role ARN**.

- ![](media/rId5424.png){width="5.833333333333333in"
  height="3.1427077865266844in"}

5.  Continue with the task for the applicable Amazon S3 logs you want to
    configure.

- The following type of logs are available.

  - [Ingest network flow logs from Amazon
    S3](#UUID5617c36b719ce5e3c8a354c63606f30a).

  - [Ingest network Route 53 logs from Amazon
    S3](#UUIDca098ab3cc1668f15f34d2c25224b507)

  - [Ingest audit logs from AWS Cloud
    Trail](#UUIDc1ab7bea87ed20d21accce03ccec7e3a).

  - [/document/preview/1039055#UUID-d5ace3ce-5a93-86f4-55f2-4943fbf09cbe](/document/preview/1039055#UUID-d5ace3ce-5a93-86f4-55f2-4943fbf09cbe).

####### Configure data collection from Amazon S3 manually

> **Note**
>
> Requires the Cortex Cloud Runtime Security or Data Collection add-on.

There are various reasons why you may need to configure data collection
from Amazon S3 manually, as opposed to using the CloudFormation Script
provided in Cortex XSIAM. For example, if your organization does not use
CloudFormation scripts, you will need to follow the instructions below,
which explain at a high-level how to perform these steps manually with a
link to the relevant topic in the Amazon S3 documentation with the
detailed steps to follow.

As soon as Cortex XSIAM begins receiving logs, the app automatically
creates an Amazon S3 Cortex Query Language (XQL) dataset (`aws_s3_raw`).
This enables you to search the logs with XQL Search using the dataset.
For example queries, refer to the in-app XQL Library. For enhanced cloud
protection, you can also configure Cortex XSIAM to ingest network flow
logs as Cortex XSIAM network connection stories, which you can query
with XQL Search using the `xdr_dataset` dataset with the preset called
`network_story`. Cortex XSIAM can also generate Cortex XSIAM issues
(Analytics, Correlations, IOC, and BIOC) when relevant from Amazon S3
logs. While Correlation Rules issues are generated on non-normalized and
normalized logs, Analytics, IOC, and BIOC issues are only generated on
normalized logs.

Enhanced cloud protection provides:

- Normalization of cloud logs

- Cloud logs stitching

- Enrichment with cloud data

- Detection based on cloud analytics

- Cloud-tailored investigations

Be sure you do the following tasks before you begin configuring data
collection manually from Amazon CloudWatch to Amazon S3.

> **Note**
>
> If you already have an Amazon S3 bucket configured with VPC flow logs
> that you want to use for this configuration, you do not need to
> perform the prerequisite steps detailed in the first two bullets.

- Ensure that you have at a minimum the following permissions in AWS for
  an Amazon S3 bucket and Amazon Simple Queue Service (SQS).

  - **Amazon S3 bucket**: `GetObject`

  - **SQS**: `ChangeMessageVisibility`, `ReceiveMessage`, and
    `DeleteMessage`.

- Create a dedicated Amazon S3 bucket for collecting network flow logs
  with the default settings. For more information, see [Creating a
  bucket using the Amazon S3
  Console](https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html).

<!-- -->

- > **Note**

  > It is your responsibility to define a retention policy for your
  > Amazon S3 bucket by creating a **Lifecycle rule** in the
  > **Management** tab. We recommend setting the retention policy to at
  > least 7 days to ensure that the data is retrieved under all
  > circumstances.

<!-- -->

- Ensure that you can access your Amazon Virtual Private Cloud (VPC) and
  have the necessary permissions to create flow logs.

- Determine how you want to provide access to Cortex XSIAM to your logs
  and perform API operations. You have the following options.

  - Designate an AWS IAM user, where you will need to know the Account
    ID for the user and have the relevant permissions to create an
    access key/id for the relevant IAM user. This is the default option
    as explained in Configure the Amazon S3 collection by selecting
    **Access Key**.

  - Create an assumed role in AWS to delegate permissions to a Cortex
    XSIAM AWS service. This role grants Cortex XSIAM access to your flow
    logs. For more information, see [Creating a role to delegate
    permissions to an AWS
    service](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-service.html).
    This is the **Assumed Role** option as described in the Configure
    the Amazon S3 collection. For more information on creating an
    assumed role for Cortex XSIAM , see [Create an assumed
    role](#UUIDb11b8a8d194ebbe449559e38629b008d).

<!-- -->

- To collect Amazon S3 logs that use server-side encryption (SSE), the
  user role must have an IAM policy that states that Cortex XSIAM has
  kms:Decrypt permissions. With this permission, Amazon S3 automatically
  detects if a bucket is encrypted and decrypts it. If you want to
  collect encrypted logs from different accounts, you must have the
  decrypt permissions for the user role also in the key policy for the
  master account Key Management Service (KMS). For more information, see
  [Allowing users in other accounts to use a KMS
  key](https://docs.aws.amazon.com/kms/latest/developerguide/key-policy-modifying-external-accounts.html).

Configure Cortex XSIAM to receive network flow logs from Amazon S3
manually.

1.  Log in to the [AWS Management
    Console](https://console.aws.amazon.com/).

2.  From the menu bar, ensure that you have selected the correct region
    for your configuration.

3.  Configure your Amazon Virtual Private Cloud (VPC) with flow logs.
    For more information, see [AWS VPC Flow
    Logs](https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html).

- > **Note**

  > If you already have an Amazon S3 bucket configured with VPC flow
  > logs, skip this step and go to Configure an Amazon Simple Queue
  > Service (SQS).

4.  Configure an Amazon Simple Queue Service (SQS). For more
    information, see [Configuring Amazon SQS queues
    (console)](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configuring.html).

- > **Note**

  > Ensure that you create your Amazon S3 bucket and Amazon SQS queue in
  > the same region.

5.  Configure an event notification to your Amazon SQS whenever a file
    is written to your Amazon S3 bucket. For more information, see
    [Amazon S3 Event
    Notifications](https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html).

6.  Configure access keys for the AWS IAM user that Cortex XSIAM uses
    for API operations. For more information, see [Managing access keys
    for IAM
    users](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html).

- > **Note**

  - > It is the responsibility of the customer's organization to ensure
    > that the user who performs this task of creating the access key is
    > designated with the relevant permissions. Otherwise, this can
    > cause the process to fail with errors.

  - > Skip this step if you are using an **Assumed Role** for Cortex
    > XSIAM.

7.  Update the Access Policy of your SQS queue and grant the required
    permissions mentioned above to the relevant IAM user. For more
    information, see [Granting permissions to publish event notification
    messages to a
    destination](https://docs.aws.amazon.com/AmazonS3/latest/userguide/grant-destinations-permissions-to-s3.html).

- > **Note**

  > Skip this step if you are using an **Assumed Role** for Cortex
  > XSIAM.

8.  Configure the Amazon S3 collection in Cortex XSIAM.

    a.  Select Settings \> Data Sources.

    b.  On the **Data Sources** page, click **Add Data Source**, search
        for and select **Amazon S3**, and click **Connect**.

    c.  Set these parameters, where the parameters change depending on
        whether you configured an **Access Key** or **Assumed Role**.

        - To provide access to Cortex XSIAM to your logs and perform API
          operations using a designated AWS IAM user, leave the
          **Access Key** option selected. Otherwise, select
          **Assumed Role**, and ensure that you Create an Assumed Role
          for Cortex XSIAM before continuing with these instructions. In
          addition, when you create an Assumed Role for Cortex XSIAM,
          ensure that you edit the policy that defines the permissions
          for the role with the Amazon S3 Bucket ARN and SQS ARN.

        - **SQS URL**: Specify the **SQS URL**, which is the ARN of the
          Amazon SQS that you configured in the AWS Management Console.
          For more information on how to retrieve your Amazon SQS ARN,
          see the **Specify SQS queue** field when you configure an
          event notification to your Amazon SQS whenever a file is
          written to your Amazon S3 bucket.

        - **Name**: Specify a descriptive name for your log collection
          configuration.

        - When setting an **Access Key**, set these parameters.

          - **AWS Client ID**: Specify the **Access key ID**, which you
            received when you created access keys for the AWS IAM user
            in AWS.

          - **AWS Client Secret**: Specify the **Secret access key** you
            received when you created access keys for the AWS IAM user
            in AWS.

        - When setting an **Assumed Role**, set these parameters.

          - **Role ARN**: Specify the **Role ARN** for the Assumed Role
            for Cortex XSIAM in AWS.

          - **External Id**: Specify the **External Id** for the Assumed
            Role for Cortex XSIAM in AWS.

        - **Log Type**: Select **Flow Logs** to configure your log
          collection to receive network flow logs from Amazon S3. When
          configuring network flow log collection, the following
          additional field is displayed for
          **Enhanced Cloud Protection**.

        <!-- -->

        - You can **Normalize and enrich flow logs** by selecting the
          checkbox. When selected, Cortex XSIAM ingests the network flow
          logs as Cortex XSIAM network connection stories, which you can
          query using XQL Search from the `xdr_dataset` dataset using
          the preset called `network_story`.

    d.  Click **Test** to validate access, and then click **Enable**.

    - Once events start to come in, a green check mark appears
      underneath the **Amazon S3** configuration with the number of logs
      received.

###### Ingest network Route 53 logs from Amazon S3

You can forward network AWS Route 53 DNS logs to Cortex XSIAM from
Amazon Simple Storage Service (Amazon S3).

To receive network Route 53 DNS logs from Amazon S3, you must first
configure data collection from Amazon S3. You can then configure the
Collection Integrations settings in Cortex XSIAM for Amazon S3. After
you set up collection integration, Cortex XSIAM begins receiving new
logs and data from the source.

You can configure Amazon S3 with SQS notification using the AWS
CloudFormation Script that we have created for you to make the process
easier. The instructions below explain how to configure Cortex XSIAM to
receive network Route 53 DNS logs from Amazon S3 using SQS.

> **Note**
>
> For more information on configuring data collection from Amazon S3 for
> Route 53 DNS logs, see the [AWS
> Documentation](https://aws.amazon.com/blogs/aws/log-your-vpc-dns-queries-with-route-53-resolver-query-logs/).

When Cortex XSIAM begins receiving logs, the app automatically creates
an Amazon Route 53 Cortex Query Language (XQL) dataset
(`amazon_route53_raw`). This enables you to search the logs with XQL
Search using the dataset. For example, queries refer to the in-app XQL
Library. For enhanced cloud protection, you can also configure Cortex
XSIAM to ingest network Route 53 DNS logs as Cortex XSIAM network
connection stories, which you can query with XQL Search using the
`xdr_data` dataset with the preset called `network_story`. Cortex XSIAM
can also generate Cortex XSIAM issues (Analytics, Correlation Rules,
IOC, and BIOC) when relevant from Amazon Route 53 DNS logs. While
Correlation Rules issues are generated on non-normalized and normalized
logs, Analytics, IOC, and BIOC issues are only generated on normalized
logs.

Enhanced cloud protection provides:

- Normalization of cloud logs

- Cloud logs stitching

- Enrichment with cloud data

- Detection based on cloud analytics

- Cloud-tailored investigations

Be sure you do the following tasks before you begin configuring data
collection from Amazon S3 using the AWS CloudFormation Script.

- Ensure that you have the proper permissions to run AWS CloudFormation
  with the script provided in Cortex XSIAM. You need at a minimum the
  following permissions in AWS for an Amazon S3 bucket and Amazon Simple
  Queue Service (SQS):

  - **Amazon S3 bucket**: `GetObject`

  - **SQS**: `ChangeMessageVisibility`, `ReceiveMessage`, and
    `DeleteMessage`.

- Ensure that you can access your Amazon Virtual Private Cloud (VPC) and
  have the necessary permissions to create Route 53 Resolver Query logs.

- Determine how you want to provide access to Cortex XSIAM to your logs
  and perform API operations. You have the following options.

  - Designate an AWS IAM user, where you will need to know the Account
    ID for the user and have the relevant permissions to create an
    access key/id for the relevant IAM user. This is the default option
    when you configure the Amazon S3 collection by selecting
    **Access Key**.

  - Create an assumed role in AWS to delegate permissions to a Cortex
    XSIAM AWS service. This role grants Cortex XSIAM access to your flow
    logs. For more information, see [Creating a role to delegate
    permissions to an AWS
    service](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-service.html).
    This is the **Assumed Role** option when you configure the Amazon S3
    collection in Cortex XSIAM. For more information on creating an
    assumed role for Cortex XSIAM, see [Create an assumed
    role](#UUIDb11b8a8d194ebbe449559e38629b008d).

<!-- -->

- To collect Amazon S3 logs that use server-side encryption (SSE), the
  user role must have an IAM policy that states that Cortex XSIAM has
  kms:Decrypt permissions. With this permission, Amazon S3 automatically
  detects if a bucket is encrypted and decrypts it. If you want to
  collect encrypted logs from different accounts, you must have the
  decrypt permissions for the user role also in the key policy for the
  master account Key Management Service (KMS). For more information, see
  [Allowing users in other accounts to use a KMS
  key](https://docs.aws.amazon.com/kms/latest/developerguide/key-policy-modifying-external-accounts.html).

Configure Cortex XSIAM to receive network Route 53 DNS logs from Amazon
S3 using the CloudFormation Script.

1.  Download the CloudFormation Script in Cortex XSIAM.

    a.  Select Settings \> Data Sources.

    b.  On the **Data Sources** page, click **Add Data Source**, search
        for and select **Amazon S3**, and click **Connect**.

    c.  To provide access to Cortex XSIAM to your logs and to perform
        API operations using a designated AWS IAM user, leave the
        **Access Key** option selected. Otherwise, select
        **Assumed Role**, and ensure that you Create an Assumed Role for
        before continuing with these instructions.

    d.  For the **Log Type**, select **Route 53** to configure your log
        collection to receive network Route 53 DNS logs from Amazon S3,
        and the following text is displayed under the field
        **Download CloudFormation Script. See instructions here.**

    e.  Click the **Download CloudFormation Script.** link to download
        the script to your computer.

2.  Create a new Stack in the CloudFormation Console with the script you
    downloaded from Cortex XSIAM.

- > **Note**

  > For more information on creating a Stack, see [Creating a stack on
  > the AWS CloudFormation
  > console](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-console-create-stack.html).

  a.  Log in to the [CloudFormation
      Console](https://console.aws.amazon.com/cloudformation/).

  b.  From the CloudFormation \> Stacks page, ensure that you have
      selected the correct region for your configuration.

  c.  Select Create Slack \> With new resources (standard).

  d.  Specify the template that you want AWS CloudFormation to use to
      create your stack. This template is the script that you downloaded
      from Cortex XSIAM, which will create an Amazon S3 bucket, Amazon
      Simple Queue Service (SQS) queue, and Queue Policy. Configure the
      following settings in the **Specify template** page.

      - Prerequisite - Prepare template \> Prepare template: Select
        **Template is ready**.

      - **Specify Template**

        - **Template source**: Select **Upload a template file**.

        - **Upload a template file**: **Choose file**, and select the
          `CloudFormation-Script.json` file that you downloaded.

  e.  Click **Next**.

  f.  In the **Specify stack details** page, configure the following
      stack details.

      - **Stack name**: Specify a descriptive name for your stack.

      - Parameters \> Cortex XDR Flow Logs Integration

        - **Bucket Name**: Specify the name of the S3 bucket to create,
          where you can leave the default populated name as
          **xdr-route53-logs** or create a new one. The name must be
          unique.

        - **Publisher Account ID**: Specify the AWS IAM user account ID
          with whom you are sharing access.

        - **Queue Name**: Specify the name for your Amazon SQS queue to
          create, where you can leave the default populated name as
          **xdr-route53** or create a new one. The name must be unique.

  g.  Click **Next**.

  h.  In the **Configure stack options** page, there is nothing to
      configure, so click **Next**.

  i.  In the **Review** page, look over the stack configurations
      settings that you have configured and if they are correct, click
      **Create stack**. If you need to make a change, click **Edit**
      beside the particular step that you want to update.

  - The stack is created and is opened with the **Events** tab
    displayed. It can take a few minutes for the new Amazon S3 bucket,
    SQS queue, and Queue Policy to be created. Click **Refresh** to get
    updates. Once everything is created, leave the stack opened in the
    current browser as you will need to access information in the stack
    for other steps detailed below.

    > **Note**

    > For the Amazon S3 bucket created using CloudFormation, it is the
    > customer's responsibility to define a retention policy by creating
    > a **Lifecycle rule** in the **Management** tab. We recommend
    > setting the retention policy to at least 7 days to ensure that the
    > data is retrieved under all circumstances.

3.  Configure Route 53 Query Logging in AWS.

    a.  Log in to the [AWS Management
        Console](https://console.aws.amazon.com/).

    b.  From the menu bar, ensure that you have selected the correct
        region for your configuration.

    c.  Search for **Route 53** and select Resolver \> Query Logging.

    d.  **Configure query logging**.

    e.  Set the following parameters in the different sections on the
        **Configure query logging** page.

        - **Query logging configuration name**

          - **Name**: Specify a name for your Resolver query logging
            configuration.

        - **Query logs destination**

          - **Destination for query logs**: Select **S3 bucket** as the
            place where you want Resolver to publish query logs.

          - **Amazon S3 bucket**: **Browse S3** to select the Amazon S3
            bucket created after running the CloudFormation script,
            which is by default called **xdr-route53-logs** or select
            the one that you created.

        - **VPCs to log queries for**

          - **Add VPC**: Clicking the **Add VPC** button opens the
            **Add VPC** page, where you can choose the VPCs that you
            want to log queries for. When you are done, click **Add**.

    f.  Click **Configure query logging**.

4.  Configure access keys for the AWS IAM user that Cortex XSIAM uses
    for API operations.

- > **Note**

  - > It is the responsibility of the customer's organization to ensure
    > that the user who performs this task of creating the access key is
    > designated with the relevant permissions. Otherwise, this can
    > cause the process to fail with errors.

  - > Skip this step if you are using an **Assumed Role** for Cortex
    > XSIAM.

  a.  Open the [AWS IAM Console](https://console.aws.amazon.com/iam/),
      and in the navigation pane, select Access management \> Users.

  b.  Select the **User name** of the AWS IAM user.

  c.  Select the **Security credentials** tab, scroll down to the
      **Access keys** section, and click **Create access key**.

  d.  Click the copy icon next to the **Access key ID** and
      **Secret access key** keys, where you must click
      **Show secret access key** to see the secret key and record them
      somewhere safe before closing the window. You will need to provide
      these keys when you edit the Access policy of the SQS queue and
      when setting the **AWS Client ID** and **AWS Client Secret** in
      Cortex XSIAM. If you forget to record the keys and close the
      window, you will need to generate new keys and repeat this
      process.

  - > **Note**

    > For more information, see [Managing access keys for IAM
    > users](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html).

5.  When you create an Assumed Role, ensure that you edit the policy
    that defines the permissions for the role with the **S3 Bucket ARN**
    and **SQS ARN**, which is taken from the stack you created.

- > **Note**

  > Skip this step if you are using an **Access Key** to provide access
  > to Cortex XSIAM.

6.  Configure the Amazon S3 collection in Cortex XSIAM.

    a.  Select Settings \> Data Sources.

    b.  In the **Amazon S3** configuration, click **Add Instance** to
        begin a new configuration.

    c.  Set these parameters, where the parameters change depending on
        whether you configured an **Access Key** or **Assumed Role**.

        - **SQS URL**: Specify the **SQS URL**, which is taken from the
          stack you created. In the browser you left open after creating
          the stack, open the **Outputs** tab, copy the **Value** of the
          **QueueURL** and paste it in this field.

        - **Name**: Specify a descriptive name for your log collection
          configuration.

        - When setting an **Access Key**, set these parameters.

          - **AWS Client ID**: Specify the **Access key ID**, which you
            received when you created access keys for the AWS IAM user
            in AWS.

          - **AWS Client Secret**: Specify the **Secret access key** you
            received when you created access keys for the AWS IAM user
            in AWS.

        - When setting an **Assumed Role**, set these parameters.

          - **Role ARN**: Specify the **Role ARN** for the Assumed Role
            you created for Cortex XSIAMin AWS.

          - **External Id**: Specify the **External Id** for the Assumed
            Role you created for Cortex XSIAM in AWS.

        - **Log Type**: Select **Route 53** to configure your log
          collection to receive network Route 53 DNS logs from Amazon
          S3. When configuring network Route 53 log collection, the
          following additional field is displayed for
          **Enhanced Cloud Protection**.

        <!-- -->

        - You can **Normalize DNS logs** by selecting the checkbox
          (default configuration). When selected, Cortex XSIAM ingests
          the network Route 53 DNS logs as XDR network connection
          stories, which you can query using XQL Search from the
          `xdr_data` dataset using the preset called `network_story`.

    d.  Click **Test** to validate access, and then click **Enable**.

    - When events start to come in, a green check mark appears
      underneath the **Amazon S3** configuration with the number of logs
      received.

###### Ingest logs from Check Point firewalls

If you use Check Point FW1/VPN1 firewalls, you can still take advantage
of Cortex XSIAM investigation and detection capabilities by forwarding
your Check Point firewall logs to Cortex XSIAM. Check Point firewall
logs can be used as the sole data source, however, you can also use
Check Point firewall logs in conjunction with Palo Alto Networks
firewall logs and additional data sources.

Cortex XSIAM can stitch data from Check Point firewalls with other logs
to make up network stories searchable in the Query Builder and in Cortex
Query Language (XQL) queries. Cortex XSIAM can also return raw data from
Check Point firewalls in XQL queries.

> **Note**

- > Logs with `sessionid = 0` are dropped.

- > Destination Port data is available only in the raw logs.

In terms of alerts, Cortex XSIAM can both surface native Check Point
firewall alerts and generate its own issues on network activity. Issues
are displayed throughout Cortex XSIAM issue, case, and investigation
views.

To integrate your logs, you first need to set up an applet in a Broker
VM within your network to act as a Syslog Collector. You then configure
your Check Point firewall policy to log all traffic and set up the Log
Exporter on your Check Point Log Server to forward logs to the Syslog
Collector in a CEF format.

When Cortex XSIAM starts to receive logs, the app can begin stitching
network connection logs with other logs to form network stories. Cortex
XSIAM can also analyze your logs to generate Analytics issues, and can
apply IOC, BIOC, and Correlation Rule matching. You can also use queries
to search your network connection logs.

1.  Ensure that your Check Point firewalls meet the following
    requirements.

- Check Point software version: R77.30, R80.10, R80.20, R80.30, or
  R80.40

2.  Increase log storage for Check Point firewall logs.

- As an estimate for initial sizing, note that the average Check Point
  log size is roughly 700 bytes. For proper sizing calculations, test
  the log sizes and log rates produced by your Check Point firewalls.
  For more information, see Manage Your Log Storage within Cortex XSIAM.

3.  [Activate the Syslog
    Collector](#UUIDd7d85d4a97df7ec2a052d22376f5a52e).

4.  Configure the Check Point firewall to forward Syslog events in CEF
    format to the Syslog Collector.

- Configure your firewall policy to log all traffic and set up the Log
  Exporter to forward logs to the Syslog Collector. For more information
  on setting up Log Exporter, see the Check Point documentation.

###### Ingest logs from Cisco ASA firewalls and AnyConnect

If you use Cisco ASA firewalls or Cisco AnyConnect VPN, you can take
advantage of Cortex XSIAM investigation and detection capabilities by
forwarding your firewall and AnyConnect VPN logs to Cortex XSIAM. This
enables Cortex XSIAM to examine your network traffic to detect anomalous
behavior. Cortex XSIAM can use Cisco ASA firewall logs and AnyConnect
VPN logs as the sole data source, but can also use Cisco ASA firewall
logs in conjunction with Palo Alto Networks firewall logs. For
additional endpoint context, you can also use Cortex XSIAM to collect
and alert on endpoint data.

When Cortex XSIAM starts to receive logs, the app can begin stitching
network connection logs with other logs to form network stories. Cortex
XSIAM can also analyze your logs to generate Analytics issues, and can
apply IOC, BIOC, and Correlation Rules matching. You can also use
queries to search your network connection logs using the Cisco Cortex
Query Language (XQL) dataset (`cisco_asa_raw`).

To integrate your logs, you first need to set up an applet in a Broker
VM within your network to act as a Syslog Collector. You then configure
forwarding on your log devices to send logs to the Syslog Collector in a
**CISCO** format.

1.  Verify that your Cisco ASA firewall and Cisco AnyConnect VPN logs
    meet the following requirements.

    - Syslog in Cisco-ASA format

    - Must include `timestamps`

    - Only supports the following messages.

      - For Cisco ASA firewall: 302013, 302014, 302015, 302016

      - For Cisco AnyConnect VPN: 113039, 716001, 722022, 722033,
        722034, 722051, 722055, 722053, 113019, 716002, 722023, 722037

2.  [Activate the Syslog
    Collector](#UUIDd7d85d4a97df7ec2a052d22376f5a52e).

3.  Increase log storage for Cisco ASA firewall and Cisco AnyConnect VPN
    logs.

- As an estimate for initial sizing, note that the average Cisco ASA log
  size is roughly 180 bytes. For proper sizing calculations, test the
  log sizes and log rates produced by your Cisco ASA firewalls and Cisco
  AnyConnect VPN logs. For more information, see Manage Your Log Storage
  within Cortex XSIAM.

4.  Configure the Cisco ASA firewall and Cisco AnyConnect VPN, or the
    log devices forwarding logs from Cisco, to log to the Syslog
    Collector in a **CISCO** format.

- Configure your firewall and AnyConnect VPN policies to log all traffic
  and forward the traffic logs to the Syslog Collector in a **CISCO**
  format. By logging all traffic, you enable Cortex XSIAM to detect
  anomalous behavior from Cisco ASA firewall logs and Cisco AnyConnect
  VPN logs. For more information on setting up Log Forwarding on Cisco
  ASA firewalls or Cisco AnyConnect VPN, see the Cisco ASA Series
  documentation.

###### Ingest logs from Corelight Zeek

If you use Corelight Zeek sensors for network monitoring, you can still
take advantage of Cortex XSIAM investigation and detection capabilities
by forwarding your network connection logs to Cortex XSIAM. This enables
Cortex XSIAM to examine your network traffic to detect anomalous
behavior. Cortex XSIAM can use Corelight Zeek logs as the sole data
source, but can also use logs in conjunction with Palo Alto Networks or
third-party firewall logs. For additional endpoint context, you can also
use Cortex XSIAM to collect and alert on endpoint data.

As soon as Cortex XSIAM starts to receive logs, the app can begin
stitching network connection logs with other logs to form network
stories. Cortex XSIAM can also analyze your logs to generate Analytics
issues, and can apply IOC, BIOC, and Correlation Rule matching. You can
also use queries to search your network connection logs.

To integrate your logs, you first need to set up an applet in a Broker
VM within your network to act as a Syslog Collector. You then configure
forwarding on your Corelight Zeek sensors (using the default Syslog
export option of RFC5424 over TCP) to send logs to the Syslog Collector.

1.  [Activate the Syslog
    Collector](#UUIDd7d85d4a97df7ec2a052d22376f5a52e).

- During activation, you define the **Listening Port** over which you
  want the Syslog Collector to receive logs. You must also set TCP as
  the transport **Protocol** and Corelight as the **Syslog Format**.

2.  Increase log storage for Corelight Zeek logs.

- For proper sizing calculations, test the log sizes and log rates
  produced by your Corelight Zeek Sensors. Then adjust your Cortex XSIAM
  log storage. For more information, see Manage Your Log Storage within
  Cortex XSIAM.

3.  Forward logs to the Syslog Collector.

- Cortex XSIAM can receive logs from Corelight Zeek sensors that use the
  Syslog export option of RFC5424 over TCP.

  a.  In the Syslog configuration of Corelight Zeek (Sensor \> Export),
      specify the details for your Syslog Collector including the
      hostname or IP address of the Broker VM and corresponding
      listening port that you defined during activation of the Syslog
      Collector, default Syslog format (RFC5424), and any log exclusions
      or filters.

  b.  Save your Syslog configuration to apply the configuration to your
      Corelight Zeek Sensors.

  For full setup instructions, see the Corelight Zeek documentation.

###### Ingest logs from Fortinet Fortigate firewalls

If you use Fortinet Fortigate firewalls, you can still take advantage of
Cortex XSIAM investigation and detection capabilities by forwarding your
firewall logs to Cortex XSIAM . This enables Cortex XSIAM to examine
your network traffic to detect anomalous behavior. Cortex XSIAM can use
Fortinet Fortigate firewall logs as the sole data source, but can also
use Fortinet Fortigate firewall logs in conjunction with Palo Alto
Networks firewall logs. For additional endpoint context, you can also
use Cortex XSIAM to collect and alert on endpoint data.

When Cortex XSIAM starts to receive logs, the app can begin stitching
network connection logs with other logs to form network stories. Cortex
XSIAM can also analyze your logs to generate Analytics issues, and can
apply IOC, BIOC, and Correlation Rule matching. You can also use queries
to search your network connection logs.

To integrate your logs, you first need to set up an applet in a Broker
VM within your network to act as a Syslog collector. You then configure
forwarding on your log devices to send logs to the Syslog collector in a
CEF format.

1.  Verify that your Fortinet Fortigate firewalls meet the following
    requirements.

    - Must use FortiOS 6.2.1 or a later release

    - `timestamp` must be in nanoseconds

2.  [Activate the Syslog
    Collector](#UUIDd7d85d4a97df7ec2a052d22376f5a52e).

3.  Increase log storage for Fortinet Fortigate firewall logs.

- As an estimate for initial sizing, note that the average Fortinet
  Fortigate log size is roughly 1,070 bytes. For proper sizing
  calculations, test the log sizes and log rates produced by your
  Fortinet Fortigate firewalls. For more information, see Manage Your
  Log Storage within Cortex XSIAM.

4.  Configure the log device that receives Fortinet Fortigate firewall
    logs to forward Syslog events to the Syslog collector in a CEF
    format.

- Configure your firewall policy to log all traffic and forward the
  traffic logs to the Syslog collector in a CEF format. By logging all
  traffic, you enable Cortex XSIAM to detect anomalous behavior from
  Fortinet Fortigate firewall logs. For more information on setting up
  Log Forwarding on Fortinet Fortigate firewalls, see the Fortinet
  FortiOS documentation.

###### Ingest logs from Microsoft Azure Event Hub

Cortex XSIAM can ingest different types of data from Microsoft Azure
Event Hub using the **Microsoft Azure Event Hub** data collector. To
receive logs from Azure Event Hub, you must configure the Data Sources
settings in Cortex XSIAM based on your Microsoft Azure Event Hub
configuration. After you set up data collection, Cortex XSIAM begins
receiving new logs and data from the source.

When Cortex XSIAM begins receiving logs, the app creates a new dataset
(`MSFT_Azure_raw`) that you can use to initiate XQL Search queries. For
example, queries refer to the in-app XQL Library. For enhanced cloud
protection, you can also configure Cortex XSIAM to normalize Azure Event
Hub audit logs, including Azure Kubernetes Service (AKS) audit logs,
with other Cortex XSIAM authentication stories across all cloud
providers using the same format, which you can query with XQL Search
using the `cloud_audit_logs` dataset. For logs that you do not configure
Cortex XSIAM to normalize, you can change the default dataset. Cortex
XSIAM can also generate Cortex XSIAM issues (Analytics, IOC, BIOC, and
Correlation Rules) when relevant from Azure Event Hub logs. While
Correlation Rules issues are generated on non-normalized and normalized
logs, Analytics, IOC, and BIOC issues are only raised on normalized
logs.

Enhanced cloud protection provides:

- Normalization of cloud logs

- Cloud logs stitching

- Enrichment with cloud data

- Detection based on cloud analytics

- Cloud-tailored investigations

> **Warning**

- > Misconfiguration of Event Hub resources could cause ingestion
  > delays.

- > In an existing Event Hub integration, do not change the mapping to a
  > different Event Hub.

- > Do not use the same Event Hub for more than two purposes.

The following table provides a brief description of the different types
of Azure audit logs you can collect.

> **Note**
>
> For more information on Azure Event Hub audit logs, see [Overview of
> Azure platform
> logs](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/platform-logs-overview).

+-----------------------------------+-----------------------------------+
| Type of data                      | Description                       |
+===================================+===================================+
| Activity logs                     | Retrieves events related to the   |
|                                   | operations on each Azure resource |
|                                   | in the subscription from the      |
|                                   | outside in addition to updates on |
|                                   | Service Health events.            |
|                                   |                                   |
|                                   | > **Note**                        |
|                                   | >                                 |
|                                   | > These logs are from the         |
|                                   | > management plane.               |
+-----------------------------------+-----------------------------------+
| Azure Active Directory (AD)       | Contain the history of sign-in    |
| Activity logs and Azure Sign-in   | activity and audit trail of       |
| logs                              | changes made in Azure AD for a    |
|                                   | particular tenant.                |
|                                   |                                   |
|                                   | > **Note**                        |
|                                   | >                                 |
|                                   | > Even though you can collect     |
|                                   | > Azure AD Activity logs and      |
|                                   | > Azure Sign-in logs using the    |
|                                   | > Azure Event Hub data collector, |
|                                   | > we recommend using the          |
|                                   | > Microsoft Office 365 data       |
|                                   | > collector, because it is easier |
|                                   | > to configure. In addition,      |
|                                   | > ensure that you do not          |
|                                   | > configure both collectors to    |
|                                   | > collect the same types of logs, |
|                                   | > because if you do so, you will  |
|                                   | > be creating duplicate data in   |
|                                   | > Cortex XSIAM.                   |
+-----------------------------------+-----------------------------------+
| Resource logs, including AKS      | Retrieves events related to       |
| audit logs                        | operations that were performed    |
|                                   | within an Azure resource.         |
|                                   |                                   |
|                                   | > **Note**                        |
|                                   | >                                 |
|                                   | > These logs are from the data    |
|                                   | > plane.                          |
+-----------------------------------+-----------------------------------+

> **Note**
>
> If you want to ingest raw Microsoft Defender for Endpoint events, use
> the Microsoft Defender log collector. For more information, see
> [Ingest raw EDR events from Microsoft Defender for
> Endpoint](#UUID6e00ea26bd37150df8776680549e71d5).
>
> **Prerequisite**
>
> Ensure that you do the following tasks before you begin configuring
> data collection from Azure Event Hub.

- > Before you set up an Azure Event Hub, calculate the quantity of data
  > that you expect to send to Cortex XSIAM, taking into account
  > potential data spikes and potential increases in data ingestion,
  > because partitions cannot be modified after creation. Use this
  > information to ascertain the optimal number of partitions and
  > Throughput Units (for Azure Basic or Standard) or Processing Units
  > (for Azure Premium). Configure your Event Hub accordingly.

- > Create an Azure Event Hub. We recommend using a dedicated Azure
  > Event Hub for this Cortex XSIAM integration. For more information,
  > see [Quickstart: Create an event hub using Azure
  > portal](https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-create).

- > Each partition can support a throughput of up to 1 MB/s.

- > Ensure the format for the logs you want collected from the Azure
  > Event Hub is either JSON or raw.

Configure the Azure Event Hub collection in Cortex XSIAM:

1.  In the Microsoft Azure console, open the **Event Hubs** page, and
    select the Azure Event Hub that you created for collection in Cortex
    XSIAM.

2.  Record the following parameters from your configured event hub,
    which you will need when configuring data collection in Cortex
    XSIAM.

    - Your event hub's consumer group.

      1.  Select Entities \> Event Hubs, and select your event hub.

      2.  Select Entities \> Consumer groups, and select your event hub.

      3.  In the Consumer group table, copy the applicable value listed
          in the **Name** column for your Cortex XSIAM data collection
          configuration.

    - Your event hub's connection string for the designated policy.

      1.  Select Settings \> Shared access policies.

      2.  In the Shared access policies table, select the applicable
          policy.

      3.  Copy the **Connection string-primary key**.

    - Your storage account connection string required for partitions
      lease management and checkpointing in Cortex XSIAM.

      1.  Open the **Storage accounts** page, and either create a new
          storage account or select an existing one, which will contain
          the storage account connection string.

      2.  Select Security + networking \> Access keys, and click
          **Show keys**.

      3.  Copy the applicable **Connection string**.

3.  Configure diagnostic settings for the relevant log types you want to
    collect and then direct these diagnostic settings to the designated
    Azure Event Hub.

    a.  Open the Microsoft Azure console.

    b.  Your navigation is dependent on the type of logs you want to
        configure.

+-----------------------------------+---------------------------------------------------------------------------------+
| Log type                          | Navigation path                                                                 |
+===================================+=================================================================================+
| Activity logs                     | Select Azure services \> Activity log \> Export Activity Logs, and              |
|                                   | **+Add diagnostic setting**.                                                    |
+-----------------------------------+---------------------------------------------------------------------------------+
| Azure AD Activity logs and Azure  | 1.  Select Azure services \> Azure Active Directory.                            |
| Sign-in logs                      |                                                                                 |
|                                   | 2.  Select Monitoring \> Diagnostic settings, and **+Add diagnostic setting**.  |
+-----------------------------------+---------------------------------------------------------------------------------+
| Resource logs, including AKS      | 1.  Search for **Monitor**, and select Settings \> Diagnostic settings.         |
| audit logs                        |                                                                                 |
|                                   | 2.  From your list of available resources, select the resource that you want to |
|                                   |     configure for log collection, and then select **+Add diagnostic setting**.  |
|                                   |                                                                                 |
|                                   | - > **Note**                                                                    |
|                                   |                                                                                 |
|                                   |   > For every resource that you want to confiure, you\'ll have to repeat this   |
|                                   |   > step, or use [Azure                                                         |
|                                   |   > policy](https://learn.microsoft.com/en-us/azure/governance/policy/overview) |
|                                   |   > for a general configuration.                                                |
+-----------------------------------+---------------------------------------------------------------------------------+

a.  Set the following parameters:

    - **Diagnostic setting name**: Specify a name for your Diagnostic
      setting.

    - **Logs Categories**/**Metrics**: The options listed are dependent
      on the type of logs you want to configure. For Activity logs and
      Azure AD logs and Azure Sign-in logs, the option is called
      **Logs Categories**, and for Resource logs it\'s called
      **Metrics**.

+-----------------------------------+--------------------------------------+
| Log type                          | Log categories/metrics               |
+===================================+======================================+
| Activity logs                     | Select from the list of applicable   |
|                                   | Activity log categories, the ones    |
|                                   | that you want to configure your      |
|                                   | designated resource to collect. We   |
|                                   | recommend selecting all of the       |
|                                   | options.                             |
|                                   |                                      |
|                                   | - **Administrative**                 |
|                                   |                                      |
|                                   | - **Security**                       |
|                                   |                                      |
|                                   | - **ServiceHealth**                  |
|                                   |                                      |
|                                   | - **Alert**                          |
|                                   |                                      |
|                                   | - **Recommendation**                 |
|                                   |                                      |
|                                   | - **Policy**                         |
|                                   |                                      |
|                                   | - **Autoscale**                      |
|                                   |                                      |
|                                   | - **ResourceHealth**                 |
+-----------------------------------+--------------------------------------+
| Azure AD Activity logs and Azure  | Select from the list of applicable   |
| Sign-in logs                      | Azure AD Activity and Azure Sign-in  |
|                                   | **Logs Categories**, the ones that   |
|                                   | you want to configure your           |
|                                   | designated resource to collect. You  |
|                                   | can select any of the following      |
|                                   | categories to collect these types of |
|                                   | Azure logs.                          |
|                                   |                                      |
|                                   | - Azure AD Activity logs:            |
|                                   |                                      |
|                                   |   - **AuditLogs**                    |
|                                   |                                      |
|                                   | - Azure Sign-in logs:                |
|                                   |                                      |
|                                   |   - **SignInLogs**                   |
|                                   |                                      |
|                                   |   - **NonInteractiveUserSignInLogs** |
|                                   |                                      |
|                                   |   - **ServicePrincipalSignInLogs**   |
|                                   |                                      |
|                                   |   - **ManagedIdentitySignInLogs**    |
|                                   |                                      |
|                                   |   - **ADFSSignInLogs**               |
|                                   |                                      |
|                                   | > **Note**                           |
|                                   | >                                    |
|                                   | > There are additional log           |
|                                   | > categories displayed. We recommend |
|                                   | > selecting all the available        |
|                                   | > options.                           |
+-----------------------------------+--------------------------------------+
| Resource logs, including AKS      | The list displayed is dependent on   |
| audit logs                        | the resource that you selected. We   |
|                                   | recommend selecting all the options  |
|                                   | available for the resource.          |
+-----------------------------------+--------------------------------------+

- **Destination details**: Select **Stream to event hub**, where
  additional parameters are displayed that you need to configure. Ensure
  that you set the following parameters using the same settings for the
  Azure Event Hub that you created for the collection.

  - **Subscription**: Select the applicable **Subscription** for the
    Azure Event Hub.

  - **Event hub namespace**: Select the applicable **Subscription** for
    the Azure Event Hub.

  - (*Optional*) **Event hub name**: Specify the name of your Azure
    Event Hub.

  - **Event hub policy**: Select the applicable **Event hub policy** for
    your Azure Event Hub.

a.  **Save** your settings.

<!-- -->

4.  Configure the Azure Event Hub collection in Cortex XSIAM.

    a.  Select Settings \> Data Sources.

    b.  On the **Data Sources** page, click **Add Data Source**, search
        for and select **Azure Event Hub**, and click **Connect**.

    c.  Set these parameters.

        - **Name**: Specify a descriptive name for your log collection
          configuration.

        - **Event Hub Connection String**: Specify your event hub's
          connection string for the designated policy.

        - **Storage Account Connection String**: Specify your storage
          account's connection string for the designated policy.

        - **Consumer Group**: Specify your event hub's consumer group.

        - **Log Format**: Select the log format for the logs collected
          from the Azure Event Hub as **Raw**, **JSON**, **CEF**,
          **LEEF**, **Cisco-asa**, or **Corelight**.

        <!-- -->

        - > **Note**

          > When you **Normalize and enrich audit logs**, the log format
          > is automatically configured. As a result, the **Log Format**
          > option is removed and is no longer available to configure
          > (default).

          - **CEF** or **LEEF**: The **Vendor** and **Product** defaults
            to **Auto-Detect**.

          <!-- -->

          - > **Note**

            > For a **Log Format** set to **CEF** or **LEEF**, Cortex
            > XSIAM reads events row by row to look for the **Vendor**
            > and **Product** configured in the logs. When the values
            > are populated in the event log row, Cortex XSIAM uses
            > these values even if you specified a value in the
            > **Vendor** and **Product** fields in the Azure Event Hub
            > data collector settings. Yet, when the values are blank in
            > the event log row, Cortex XSIAM uses the **Vendor** and
            > **Product** that you specified in the Azure Event Hub data
            > collector settings. If you did not specify a **Vendor** or
            > **Product** in the Azure Event Hub data collector
            > settings, and the values are blank in the event log row,
            > the values for both fields are set to **unknown**.

          <!-- -->

          - **Cisco-asa**: The following fields are automatically set
            and not configurable.

            - **Vendor**: **Cisco**

            - **Product**: **ASA**

          <!-- -->

          - Cisco data can be queried in XQL Search using the
            `cisco_asa_raw` dataset.

          <!-- -->

          - **Corelight**: The following fields are automatically set
            and not configurable.

            - **Vendor**: **Corelight**

            - **Product**: **Zeek**

          <!-- -->

          - Corelight data can be queried in XQL Search using the
            `corelight_zeek_raw` dataset.

          <!-- -->

          - **Raw** or **JSON**: The following fields are automatically
            set and are configurable.

            - **Vendor**: **Msft**

            - **Product**: **Azure**

          <!-- -->

          - Raw or JSON data can be queried in XQL Search using the
            `msft_azure_raw` dataset.

        <!-- -->

        - **Vendor** and **Product**: Specify the **Vendor** and
          **Product** for the type of logs you are ingesting.

        <!-- -->

        - The **Vendor** and **Product** are used to define the name of
          your Cortex Query Language (XQL) dataset
          (`<vendor>_<product>_raw`). The **Vendor** and **Product**
          values vary depending on the **Log Format** selected. To
          uniquely identify the log source, consider changing the values
          if the values are configurable.

          > **Note**

          > When you **Normalize and enrich audit logs**, the **Vendor**
          > and **Product** fields are automatically configured, so
          > these fields are removed as available options (default).

        <!-- -->

        - **Normalize and enrich audit logs**: (Optional) For enhanced
          cloud protection, you can **Normalize and enrich audit logs**
          by selecting the checkbox (default). If selected, Cortex XSIAM
          normalizes and enriches Azure Event Hub audit logs with other
          Cortex XSIAM authentication stories across all cloud providers
          using the same format. You can query this normalized data with
          XQL Search using the `cloud_audit_logs` dataset.

    d.  Click **Test** to validate access, and then click **Enable**.

    - When events start to come in, a green check mark appears
      underneath the **Azure Event Hub** configuration with the amount
      of data received.

###### Ingest network flow logs from Microsoft Azure Network Watcher

To receive network security group (NSG) or Virtual network (VNet) flow
logs from Azure Network Watcher, you must configure data collection from
Microsoft Azure Network Watcher using an Azure Function provided by
Cortex XSIAM. This Azure Function requires a token that is generated
when you configure your Azure Network Watcher Collector in Cortex XSIAM.
After you have configured the Cortex XSIAM collector and successfully
deployed the Azure Function to your Azure account, Cortex XSIAM will
start receiving and ingesting network flow logs from Azure Network
Watcher.

The Azure Network Watcher Collector is deployed using an ARM template.
During deployment, the template retrieves keys using the `listKeys`
function, and your app can bind to the blob storage using the connection
string generated from those keys. After deployment, this binding works
without the need to provide any connection string manually, because the
keys were already retrieved and injected during deployment.

In addition to the user-specified storage account that captures the log
blobs, the template also creates a secondary, internal storage account
for internal operations related to the function app. This internal
storage account is used by the function app for operations such as
storing function state, and intermediate processing. To enhance
security, public network access is disabled, and the account is
restricted to private endpoints only. This additional internal storage
account allows the function app to securely store data without relying
on the user-specified storage account for internal processes. This
separation enhances data security and isolation between user-facing
storage and internal application operations. VNet integration is
required only for the internal storage account\'s internal operations.
The user-specified storage account used for NSG or VNet flow logs does
not require VNet integration.

When Cortex XSIAM begins receiving logs, the app creates a new dataset
(`MSFT_Azure_raw`) that you can use to initiate XQL Search queries. For
example queries, refer to the in-app XQL Library. For enhanced cloud
protection, you can also configure Cortex XSIAM to ingest network flow
logs as Cortex XSIAM network connection stories, which you can query
with XQL Search using the `xdr_data` dataset with the preset called
`network_story`. Cortex XSIAM can also generate Cortex XSIAM issues
(Analytics, Correlation Rules, IOC, and BIOC) when relevant from Azure
Network Watcher flow logs. While Correlation Rules issues are raised on
non-normalized and normalized logs, Analytics, IOC, and BIOC issues are
only generated on normalized logs.

Enhanced cloud protection provides:

- Normalization of cloud logs

- Cloud logs stitching

- Enrichment with cloud data

- Detection based on cloud analytics

- Cloud-tailored investigations

> **Prerequisite**

- > For NSG:

  - > Ensure that your NSG flow logs in Azure Network Watcher conform to
    > the requirements as outlined in Microsoft documentation. For more
    > information, see [Introduction to flow logging for network
    > security
    > groups](https://docs.microsoft.com/en-us/azure/network-watcher/network-watcher-nsg-flow-logging-overview#enabling-nsg-flow%20logs).

  - > [Enable NSG flow logs in the Microsoft Azure
    > Portal](https://docs.microsoft.com/en-us/azure/network-watcher/network-watcher-nsg-flow-logging-portal).

- > For VNet:

  - > Ensure that your VNet flow logs in Azure Network Watcher conform
    > to the requirements as outlined in Microsoft documentation. For
    > more information, see [Introduction to flow logging for virtual
    > networks](https://learn.microsoft.com/en-us/azure/network-watcher/vnet-flow-logs-overview).

  - > [Enable VNet flow logs in the Microsoft Azure
    > Portal](https://learn.microsoft.com/en-us/azure/network-watcher/vnet-flow-logs-manage).

- > Ensure that you have an Azure subscription with user role
  > permissions to deploy ARM templates and create the required
  > resources.

<!-- -->

- > The `listKeys` function in an Azure Resource Manager (ARM) template
  > retrieves the storage account keys, and it requires special
  > permissions to execute. Specifically, the user or identity running
  > the ARM template needs the following permission:
  > `Microsoft.Storage/storageAccounts/listKeys/action`. If the user or
  > service principal running the ARM template has the necessary user
  > role (such as Owner or Storage Account Contributor), permission is
  > implicitly granted for the template to retrieve the storage account
  > keys.

<!-- -->

- > Perform this procedure in the order shown below, because you need to
  > save a token and a URL from Cortex XSIAM in earlier steps, and use
  > them in Azure in later steps.

1.  Configure the Azure Network Watcher collection in Cortex XSIAM.

    a.  Select Settings \> Data Sources.

    b.  On the **Data Sources** page, click **Add Data Source**, search
        for and select **Azure Network Watcher**, and click **Connect**.

    c.  Set these parameters:

        - **Name**: Specify a meaningful name for your log collection
          configuration.

        - **Enhanced Cloud Protection**: (*Optional*) For enhanced cloud
          protection, you can normalize and enrich flow logs by
          selecting the **Use flow logs in analytics** checkbox. If
          selected, Cortex XSIAM ingests network flow logs as Cortex
          XSIAM network connection stories, which you can query with XQL
          Search using the `xdr_data` dataset with the preset called
          `network_story`.

    d.  Click **Save & Generate Token**. The token is displayed in a
        popup.

    - Click the copy icon next to the key and save the copy of this
      token somewhere safe. You will need to provide this token when you
      configure the Azure Function and set the **Cortex Access Token**
      value. If you forget to record the token and close the window, you
      will need to generate a new one and repeat this process. When you
      are finished, click **Done** to close the window.

    e.  On the **Integrations** page for the Azure Network Watch
        Collector that you created, click the **Copy API URL** icon and
        save a copy of the URL somewhere safe. You will need to provide
        this URL when you configure the Azure Function and set the
        **Cortex Http Endpoint** value.

2.  Configure the Azure Function provided by Cortex XSIAM.

    a.  Do one of the following, depending on the flow log type:

        - For NSG, open this [Azure
          Function](https://github.com/PaloAltoNetworks/cortex-azure-functions/tree/master/nsg-flow-logs)
          provided by Cortex XSIAM.

        - For VNet, open this [Azure
          Function](https://github.com/PaloAltoNetworks/cortex-azure-functions/tree/master/vnet-flow-logs)
          provided by Cortex XSIAM.

    b.  Click **Deploy to Azure**.

    c.  Log in to Azure, and if necessary, complete authentication
        procedures.

    d.  Set these parameters, where some fields are mandatory to set and
        others may already be populated for you.

        - **Subscription**: Specify the Azure subscription that you want
          to use for the App Configuration. If your account has only one
          subscription, it is automatically selected.

        - **Resource group**: Specify or create a resource group for
          your App Configuration store resource.

        - **Region**: Specify the Azure region that you want to use.

        - **Unique Name**: Enter a unique name for the function app. The
          name that you provide will be concatenated to some of the
          resource names, to make it easier to locate the related
          resources later on. The name must only contain alphanumeric
          characters (letters and numbers, no special symbols) and must
          contain no more than 10 characters.

        - **Cortex Access Token**: Cortex HTTP authorization key that
          you recorded when you configured the Azure Network Watcher
          collection in Cortex XSIAM in an earlier step.

        - **Target Storage Account Name**: Enter the name of the Azure
          Storage Account that was created during the NSG or VNet flow
          logs setup in Azure Network Watcher, where the log blobs are
          being stored.

        - **Target Container Name**: This field should be left empty for
          most use cases.

        <!-- -->

        - For NSG, the default value
          `insights-logs-networksecuritygroupflowevent`  is the name
          that is automatically created for the container during
          configuration of the network watcher.

          For VNet, the default value `insights-logs-flowlogflowevent`
          is the name that is automatically created for the container
          during configuration of the network watcher.

        <!-- -->

        - **Location**: The region where all the resources will be
          deployed (leave blank to use the same region as the resource
          group).

        - **Cortex Http Endpoint**: Specify the API URL that you
          recorded when you configured the Azure Network Watcher
          collection in Cortex XSIAM.

        - **Remote Package**: The URL of the remote package ZIP file
          containing the Azure Function code. Keep the default value,
          unless instructed otherwise.

    e.  Click **Review + Create** to confirm your settings for the Azure
        Function.

    f.  Click **Create**. It can take a few minutes until the deployment
        is complete.

- > **Note**

  > In addition to your storage account, the template automatically
  > creates another storage account that is required by the function app
  > for internal use only. The internal storage account name is prefixed
  > with `cortex` and is followed by a unique suffix based on the
  > resource group, storage account, and container names.

  After events start to come in, a green check mark appears underneath
  the **Azure Network Watcher** configuration that you created in Cortex
  XSIAM, and the amount of data received is displayed.

###### Ingest logs and data from Okta

To receive logs and data from Okta, you must configure the Data Sources
settings in Cortex XSIAM. After you set up data collection, Cortex XSIAM
immediately begins receiving new logs and data from the source. The
information from Okta is then searchable in XQL Search using the
`okta_sso_raw` dataset. In addition, depending on the event type, data
is normalized to either `xdr_data` or `saas_audit_logs` datasets.

You can collect all types of events from Okta. When setting up the Okta
data collector in Cortex XSIAM, a field called **Okta Filter** is
available to configure collection for events of your choosing. All
events are collected by default unless you define an Okta API Filter
expression for collecting the data, such as
`filter=eventType eq “user.session.start”.\n`. For Okta information to
be woven into authentication stories, `“user.authentication.sso”` events
must be collected.

The Okta API enforces concurrent rate limits. The Okta data collector is
built with a mechanism which reduces the amount of requests whenever an
error is received from the Okta API indicating that too many requests
have already been sent. In addition, to ensure you are properly notified
about this, an alert is displayed in the **Notification Area** and a
record is added to the **Management Audit Logs**.

Before you begin configuring data collection from Okta, ensure your Okta
user has administrator privileges with a role that can create API
tokens, such as the read-only administrator, Super administrator, and
Organization administrator. For more information, see the [Okta
Administrators
Documentation](https://help.okta.com/en-us/Content/Topics/Security/Administrators.htm?cshid=ext_Security_Administrators).

To configure the Okta collection in Cortex XSIAM:

1.  Identify the domain name of your Okta service.

- From the Dashboard of your Okta console, note your **Org URL**.

  For more information, see the [Okta
  Documentation](https://developer.okta.com/docs/guides/find-your-domain/findorg/).

  ![](media/rId5454.png){width="5.833333333333333in"
  height="2.253124453193351in"}

2.  Obtain your authentication token in Okta.

    a.  Select API \> Tokens.

    b.  **Create Token** and record the token value.

    - This is your only opportunity to record the value.

3.  Select Settings \> Data Sources.

4.  On the **Data Sources** page, click **Add Data Source**, search for
    and select **Okta**, and click **Connect**.

5.  Integrate the Okta authentication service with Cortex XSIAM.

    a.  Specify the **OKTA DOMAIN** (Org URL) that you identified on
        your Okta console.

    b.  Specify the **TOKEN** used to authenticate with Okta.

    c.  Specify the **Okta Filter** to configure collection for events
        of your choosing. **All events** are collected by default unless
        you define an Okta API Filter expression for collecting the
        data, such as `filter=eventType eq “user.session.start”.\n`. For
        Okta information to be weaved into authentication stories,
        `“user.authentication.sso”` events must be collected.

    d.  **Test** the connection settings.

    e.  If successful, **Enable** Okta log collection.

    - Once events start to come in, a green check mark appears
      underneath the **Okta** configuration with the amount of data
      received.

6.  After Cortex XSIAM begins receiving information from the service,
    you can Create an XQL Query to search for specific data. When
    including authentication events, you can also Create an
    Authentication Query to search for specific authentication data.

###### Ingest logs from Windows DHCP using Elasticsearch Filebeat

You can configure Cortex XSIAM to receive Windows DHCP logs using
Elasticsearch Filebeat with the following data collectors.

####### Ingest Windows DHCP logs with an XDR Collector profile

Extend Cortex XSIAM visibility into logs from Windows DHCP using an XDR
Collector Windows Filebeat profile.

You can enrich network logs with Windows DHCP data when defining data
collection in an XDR Collector Windows Filebeat profile. When you add a
XDR Collector Windows Filebeat profile using the Elasticsearch Filebeat
default configuration file called `filebeat.yml`, you can define whether
the collected data undergoes follow-up processing in the backend for
Windows DHCP data. Cortex XSIAM uses Windows DHCP logs to enrich your
network logs with hostnames and MAC addresses that are searchable in XQL
Search using the Windows DHCP Cortex Query Language (XQL) dataset
(`microsoft_dhcp_raw`).

While this enrichment is also available when configuring a [Windows DHCP
Collector](#Xf0b1c8b8f2fa2fccfcd1cf87754a913ba7dcf90) for a cloud data
collection integration, we recommend configuring Cortex XSIAM to receive
Windows DHCP logs with an XDR Collector Windows Filebeat profile because
it's the ideal setup configuration.

Configure Cortex XSIAM to receive logs from Windows DHCP using an XDR
Collector Windows Filebeat profile.

1.  [/document/preview/1048000#UUID-2abc9608-309b-6443-d660-6952529ca0a0](/document/preview/1048000#UUID-2abc9608-309b-6443-d660-6952529ca0a0).

- Follow the steps for creating a Windows Filebeat profile as described
  in
  [/document/preview/1048000#UUID-2abc9608-309b-6443-d660-6952529ca0a0](/document/preview/1048000#UUID-2abc9608-309b-6443-d660-6952529ca0a0),
  and in the **Filebeat Configuration File** area, ensure that you
  select and **Add** the **DHCP** template. The template\'s content will
  be displayed here, and is editable.

2.  To configure collection of Windows DHCP data, edit the template text
    as necessary for your system.

- You can enrich network logs with Windows DHCP data when defining data
  collection by setting the `vendor` to `“microsoft”` , and `product` to
  `“dhcp”` in the `filebeat.yml` file, which you can then query in the
  `microsoft_dhcp_raw` dataset.

  > **Note**

  > To avoid formatting issues in `filebeat.yml`, we recommend that you
  > edit the text file inside the user interface, instead of copying it
  > and editing it elsewhere. Validate the syntax of the YML file before
  > you finish creating the profile.

####### Ingest Windows DHCP logs with the Windows DHCP Collector

Extend Cortex XSIAM visibility into logs from Windows DHCP using
Elasticsearch Filebeat with the Windows DHCP data collector.

To receive Windows DHCP logs, you must configure data collection from
Windows DHCP via Elasticsearch Filebeat. This is configured by setting
up a Windows DHCP Collector in Cortex XSIAM and installing and
configuring an Elasticsearch Filebeat agent on your Windows DHCP Server.
Cortex XSIAM supports using Filebeat up to version 8.0.1 with the
Windows DHCP Collector.

Certain settings in the Elasticsearch Filebeat default configuration
file called `filebeat.yml` must be populated with values provided when
you configure the Data Sources settings in Cortex XSIAM for the Windows
DHCP Collector. To help you configure the `filebeat.yml` correctly,
Cortex XSIAM provides an example file that you can download and
customize. After you set up collection integration, Cortex XSIAM begins
receiving new logs and data from the source.

> **Note**
>
> For more information on configuring the `filebeat.yml` file, see the
> Elastic Filebeat Documentation.

Windows DHCP logs are stored as CSV (comma-separated values) log files.
The logs rotate by days (`DhcpSrvLog-<day>.log`), and each file contains
two sections: `Event ID Meaning` and the events list.

As soon as Cortex XSIAM begins receiving logs, the app automatically
creates a Windows DHCP XQL dataset (`microsoft_dhcp_raw`). Cortex XSIAM
uses Windows DHCP logs to enrich your network logs with hostnames and
MAC addresses that are searchable in XQL Search using the Windows DHCP
Cortex Query Language (XQL) dataset.

Configure Cortex XSIAM to receive logs from Windows DHCP via
Elasticsearch Filebeat with the Windows DHCP collector.

1.  Configure the Windows DHCP Collector in Cortex XSIAM.

    a.  Select Settings \> Data Sources.

    b.  On the **Data Sources** page, click **Add Data Source**, search
        for and select **Windows DHCP**, and click **Connect**.

    c.  (*Optional*) **Download example filebeat.yml file**.

    - To help you configure your `filebeat.yml` file correctly, Cortex
      XSIAM provides an example `filebeat.yml` file that you can
      download and customize. To download this file, use the link
      provided in this dialog box.

      > **Note**

      > To avoid formatting issues in your `filebeat.yml`, we recommend
      > that you use the download example file to make your
      > customizations. Do not copy and paste the code syntax examples
      > provided later in this procedure into your file.

    d.  Specify a descriptive **Name** for your log collection
        configuration.

    e.  **Save & Generate Token**. The token is displayed in a blue box,
        which is blurred out in the image below.

    - Click the copy icon next to the key and record it somewhere safe.
      You will need to provide this key when you set the `api_key` value
      in the **Elasticsearch Output** section in the `filebeat.yml` file
      as explained in **Step #2**. If you forget to record the key and
      close the window you will need to generate a new key and repeat
      this process.

    f.  Select **Done** to close the window.

    g.  In the **Integrations** page for the Windows DHCP Collector that
        you created, select **Copy api url** and record it somewhere
        safe. You will need to provide this URL when you set the `hosts`
        value in the **Elasticsearch Output** section in the
        `filebeat.yml` file as explained in **Step #2**.

2.  Configure an Elasticsearch Filebeat agent on your Windows DHCP
    Server.

    a.  Navigate to the Elasticsearch Filebeat installation directory,
        and open the `filebeat.yml` file to configure data collection
        with Cortex XSIAM. We recommend that you use the download
        example file provided by Cortex XSIAM.

    b.  Update the following sections and tags in the `filebeat.yml`
        file. The example code below details the specific sections to
        make these changes in the file.

        - **Filebeat inputs**: Define the paths to crawl and fetch. The
          code below provides an example of how to configure the
          **Filebeat inputs** section in the `filebeat.yml` file with
          these paths configured.

        <!-- -->

        - # ============================== Filebeat inputs ===============================
              filebeat.inputs:
                # Each - is an input. Most options can be set at the input level, so
                # you can use different inputs for various configurations.
                # Below are the input specific configurations.
                - type: log  
                  # Change to true to enable this input configuration.  
                  enabled: true  
                  # Paths that should be crawled and fetched. Glob based paths.  
                  paths:       
                    - c:\Windows\System32\dhcp\DhcpSrvLog*.log    

        <!-- -->

        - **Elasticsearch Output**: Set the `hosts` and `api_key`, where
          both of these values are obtained when you configured the
          Windows DHCP Collector in Cortex XSIAM as explained in
          **Step #1**. The code below provides an example of how to
          configure the **Elasticsearch Output** section in the
          `filebeat.yml` file and indicates which settings need to be
          obtained from Cortex XSIAM.

        <!-- -->

        - # ---------------------------- Elasticsearch Output ----------------------------
              output.elasticsearch:  
                enabled: true  
                # Array of hosts to connect to.    
                hosts: ["OBTAIN THIS URL FROM CORTEX XDR"]  
                # Protocol - either `http` (default) or `https`.  
                protocol: "https"  
                compression_level: 5  
                # Authentication credentials - either API key or username/password. 
                api_key: "OBTAIN THIS KEY FROM CORTEX XDR"

        <!-- -->

        - **Processors**: Set the `tokenizer` and add a
          `drop_event processor` to drop all events that do not start
          with an event ID. The code below provides an example of how to
          configure the **Processors** section in the `filebeat.yml`
          file and indicates which settings need to be obtained from
          Cortex XSIAM.

        <!-- -->

        - > **Note**

          > The `tokenizer` definition is dependent on the Windows
          > server version that you are using as the log format differs.

          > -For platforms earlier than Windows Server 2008, use
          > `"%{id},%{date},%{time},%{description},%{ipAddress},%{hostName},%{macAddress}"`

          > -For Windows Server 2008 and 2008 R2, use
          > `"%{id},%{date},%{time},%{description},%{ipAddress},%{hostName},%{macAddress},%{userName},%{transactionID},%{qResult},%{probationTime},%{correlationID}"`

          > For Windows Server 2012 and above, use
          > `"%{id},%{date},%{time},%{description},%{ipAddress},%{hostName},%{macAddress},%{userName},%{transactionID},%{qResult},%{probationTime},%{correlationID},%{dhcid},%{vendorClassHex},%{vendorClassASCII},%{userClassHex},%{userClassASCII},%{relayAgentInformation},%{dnsRegError}"`

              # ================================= Processors =================================
              processors:  
                - add_host_metadata:      
                  when.not.contains.tags: forwarded  
                - drop_event.when.not.regexp.message: "^[0-9]+,.*"  
                - dissect:       
                  tokenizer: "%{id},%{date},%{time},%{description},%{ipAddress},%{hostName},%{macAddress},%{userName},%{transactionID},%{qResult},%{probationTime},%{correlationID},%{dhcid},%{vendorClassHex},%{vendorClassASCII},%{userClassHex},%{userClassASCII},%{relayAgentInformation},%{dnsRegError}"  
                - drop_fields:       
                  fields: ["message"]  
                - add_locale: ~
                - rename:
                    fields:
                      - from: "event.timezone"
                        to: "dissect.timezone"
                    ignore_missing: true
                    fail_on_error: false
                - add_cloud_metadata: ~  
                - add_docker_metadata: ~  
                - add_kubernetes_metadata: ~

3.  Verify the status of the integration.

- Return to the **Integrations** page and view the statistics for the
  log collection configuration.

4.  After Cortex XSIAM begins receiving logs from Windows DHCP via
    Elasticsearch Filebeat, you can use the XQL Search to search for
    logs in the new dataset (`microsoft_dhcp_raw`).

###### Ingest logs from Zscaler Internet Access

If you use Zscaler Internet Access (ZIA) in your network, you can
forward your firewall and network logs to Cortex XSIAM for analysis.
This enables you to take advantage of Cortex XSIAM anomalous behavior
detection and investigation capabilities. Cortex XSIAM can use the
firewall and network logs from ZIA as the sole data source, and can also
use these firewall and network logs from ZIA in conjunction with Palo
Alto Networks firewall and network logs. For additional endpoint
context, you can also use Cortex XSIAM to collect and alert on endpoint
data.

To integrate your logs, you first need to set up an applet in a broker
VM within your network to act as a Syslog Collector. You then configure
forwarding on your log devices to send logs to the Syslog collector in a
**CEF** format. To provide seamless log ingestion, Cortex XSIAM
automatically maps the fields in your traffic logs to the Cortex XSIAM
log format.

When Cortex XSIAM starts to receive logs, the app performs these
actions.

- Begins stitching network connection and firewall logs with other logs
  to form network stories. Cortex XSIAM can also analyze your logs to
  generate Analytics issues and can apply IOC, BIOC, and Correlation
  Rule matching. You can also use queries to search your network
  connection logs.

- Creates a Zscaler Cortex Query Language (XQL) dataset, which enables
  you to search the logs using XQL Search. The Zscaler XQL datasets are
  dependent on the ZIA NSS Feed that you\'ve configured for the types of
  logs you want to collect.

  - Firewall logs: `zscaler_nssfwlog_raw`

  - Web logs: `zscalar_nssweblog_raw`

To ingest logs from Zscaler Internet Access (ZIA):

1.  [Activate the Syslog
    Collector](#UUIDd7d85d4a97df7ec2a052d22376f5a52e).

2.  Increase log storage for ZIA logs. For more information, see Manage
    Your Log Storage.

3.  Configure NSS log forwarding in Zscaler Internet Access to the
    Syslog Collector in a **CEF** format.

    a.  In the Zscaler Internet Access application, select
        Administration \> Nanolog Streaming Service.

    b.  In the **NSS Feeds** tab, **Add NSS Feed**.

    c.  In the **Add NSS Feed** screen, configure the fields for the
        Cortex XSIAM Syslog Collector.

    - The steps below differ depending on the type of NSS Feed you are
      configuring to collect either firewall logs or web logs. For more
      information on all the configurations available on the screen, see
      the ZIA documentation:

      - Firewall logs: See [Adding NSS Feeds for Firewall
        Logs](https://help.zscaler.com/zia/adding-nss-feeds-firewall-logs).

      - Web logs: See [Adding NSS Feeds for Web
        Logs](https://help.zscaler.com/zia/adding-nss-feeds-web-logs).

      The following image displays the fields required to add an NSS
      feed.

      ![](media/rId5464.png){width="5.833333333333333in"
      height="6.554306649168854in"}

      - **NSS Type**: Select either **NSS for Web** (default) to collect
        web logs or **NSS for Firewall** to collect firewall logs.

      - **SIEM TCP Port**: Specify the port that you set when activating
        the Syslog Collector in Cortex XSIAM. See [Activate the Syslog
        Collector](#Xdb3bd51c62a5cbce81cba8b22e504d7c061f9c4).

      - **SIEM IP Address**: Specify the IP that you set when activating
        the Syslog Collector in Cortex XSIAM. See [Activate the Syslog
        Collector](#Xdb3bd51c62a5cbce81cba8b22e504d7c061f9c4).

      - **Feed Escape Character**: Specify the feed escape character as
        `=`.

      - **Feed Output Type**: Select **Custom**.

      - **Feed Output Format**: Specify the output format, which is
        dependent on the type of logs you are collecting as defined in
        the **NSS Type** field:

  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  Log type                            Feed output format
  ----------------------------------- ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  Firewall logs                       `%s{mon} %02d{dd} %02d{hh}:%02d{mm}:%02d{ss} zscaler-nss-fw CEF:0|Zscaler|NSSFWlog|5.7|%s{action}|%s{rulelabel}|3|act=%s{action} suser=%s{login} src=%s{csip} spt=%d{csport} dst=%s{cdip} dpt=%d{cdport} deviceTranslatedAddress=%s{ssip} deviceTranslatedPort=%d{ssport} destinationTranslatedAddress=%s{sdip} destinationTranslatedPort=%d{sdport} sourceTranslatedAddress=%s{tsip} sourceTranslatedPort=%d{tsport} proto=%s{ipproto} tunnelType=%s{ttype} dnat=%s{dnat} stateful=%s{stateful} spriv=%s{location} reason=%s{rulelabel} in=%ld{inbytes} out=%ld{outbytes} rt=%s{mon} %02d{dd} %02d{hh}:%02d{mm}:%02d{ss} deviceDirection=1 cs1=%s{dept} cs1Label=dept cs2=%s{nwsvc} cs2Label=nwService cs3=%s{nwapp} cs3Label=nwApp cs4=%s{aggregate} cs4Label=aggregated cs6=%s{threatname} cs6label=threatname cn1=%d{durationms} cn1Label=durationms cn2=%d{numsessions} cn2Label=numsessions cs5Label=ipCat cs5=%s{ipcat} cat=%s{threatcat} destCountry=%s{destcountry} avgduration=%d{avgduration}`

  Web logs                            `%s{mon} %02d{dd} %02d{hh}:%02d{mm}:%02d{ss} zscaler-nss CEF:0|Zscaler|NSSWeblog|5.0|%s{action}|%s{reason}|3|act=%s{action} app=%s{proto} cat=%s{urlcat} dhost=%s{ehost} dst=%s{sip} src=%s{cip} in=%d{respsize} outcome=%s{respcode} out=%d{reqsize} request=%s{eurl} rt=%s{mon} %02d{dd} %d{yy} %02d{hh}:%02d{mm}:%02d{ss} sourceTranslatedAddress=%s{cintip} requestClientApplication=%s{ua} requestMethod=%s{reqmethod} suser=%s{login} spriv=%s{location} externalId=%d{recordid} fileType=%s{filetype} reason=%s{reason} destinationServiceName=%s{appname} cn1=%d{riskscore} cn1Label=riskscore cs1=%s{dept} cs1Label=dept cs2=%s{urlsupercat} cs2Label=urlsupercat cs3=%s{appclass} cs3Label=appclass cs4=%s{malwarecat} cs4Label=malwarecat cs5=%s{threatname} cs5Label=threatname cs6=%s{dlpeng} cs6Label=dlpeng ZscalerNSSWeblogURLClass=%s{urlclass} ZscalerNSSWeblogDLPDictionaries=%s{dlpdict} requestContext=%s{ereferer} contenttype=%s{contenttype} unscannabletype=%s{unscannabletype} deviceowner=%s{deviceowner} devicehostname=%s{devicehostname}\n`
  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

d.  Click **Save**.

e.  Click **Save** and activate the change according to the [Zscaler
    Internet Access (ZIA)
    documentation](https://help.zscaler.com/zia/saving-and-activating-changes-admin-portal).

###### Ingest logs from Zscaler Private Access

If you use Zscaler Private Access (ZPA) in your network as an
alternative to VPNs, you can forward your network logs to Cortex XSIAM
for analysis. This enables you to take advantage of Cortex XSIAM
anomalous behavior detection and investigation capabilities. Cortex
XSIAM can use the network logs from ZPA as the sole data source, and can
also use these network logs from ZPA in conjunction with Palo Alto
Networks network logs.

When Cortex XSIAM starts to receive logs, the following actions are
performed:

- Stitching network connection logs with other logs to form network
  stories. Cortex XSIAM can also analyze your logs to apply IOC, BIOC,
  and Correlation Rules matching. You can also use queries to search
  your network connection logs.

- Creates a Zscaler Cortex Query Language (XQL) dataset
  (`zscaler_zpa_raw`), which enables you to search the logs using XQL
  Search.

To integrate your logs, you first need to set up an applet in a Broker
VM within your network to act as a Syslog Collector. You then configure
forwarding on your log devices to send logs to the Syslog collector in a
**LEEF** format. To provide seamless log ingestion, Cortex XSIAM
automatically maps the fields in your traffic logs to the Cortex XSIAM
log format.

**Prerequisite Step**

Before you can add a log receiver in Zscaler Private Access, as
explained in the task below, you must first deploy your App Connectors.
For more information, see [App Connector Deployment Guides for Supported
Platforms](https://help.zscaler.com/zpa/app-connector-management/app-connector-deployment-guides-supported-platforms).

To ingest logs from Zscaler Private Access (ZPA):

1.  [Activate the Syslog
    Collector](#UUIDd7d85d4a97df7ec2a052d22376f5a52e).

2.  Increase log storage for ZPA logs. For more information, see Manage
    Your Log Storage.

3.  Configure ZPA log forwarding in Zscaler Private Access to the Syslog
    Collector in a **LEEF** format.

    1.  In the Zscaler Private Access application, select Administration
        \> Log Receivers.

    2.  Click **Add Log Receiver**.

    - > **Note**

      > For more information on configuring the parameters on the
      > screen, see the Zscaler Private Access (ZPA) documentation for
      > [Configuring a Log
      > Receiver](https://help.zscaler.com/zpa/configuring-log-receiver).

    3.  In the **Add Log Receiver** window, configure the following
        fields on the **Log Receiver** tab:

        - **Name**: Specify a name for the log receiver. The name cannot
          contain special characters, with the exception of periods (.),
          hyphens (-), and underscores ( \_ ).

        - **Description**: (Optional) Specify a log receiver
          description.

        - **Domain or IP Address**: Specify the fully qualified domain
          name (FQDN) or IP address for the log receiver that you set
          when activating the Syslog Collector in Cortex XSIAM. See
          [Activate Syslog
          Collector](#UUIDd7d85d4a97df7ec2a052d22376f5a52e).

        - **TCP Port**: Specify the TCP port number used by the log
          receiver that you set when activating the Syslog Collector in
          Cortex XSIAM. See [Activate Syslog
          Collector](#UUIDd7d85d4a97df7ec2a052d22376f5a52e).

        - **TLS Encryption**: Toggle to **Enabled** to encrypt traffic
          between the log receiver and your Syslog Collector in Cortex
          XSIAMusing mutually authenticated TLS communication. To use
          this setting, the log receiver must support TLS communication.
          For more information, see [About the Log Streaming
          Service](https://help.zscaler.com/zpa/about-log-streaming-service#tlsencryption).

        - **App Connector Groups**: (Optional) Select the App Connector
          groups that can forward logs to the receiver, and
          click **Done**. You can search for a specific group,
          click **Select All** to apply all groups, or
          click **Clear Selection** to remove all selections.

    4.  Click **Next**.

    5.  Configure the following fields in the **Log Stream** tab:

        - **Log Type**: Select the log type you want to collect, where
          only the following logs types are currently supported to
          collect with your Syslog Collector in Cortex XSIAM:

        <!-- -->

        - > **Note**

          > You can only configure a ZPA log receiver to collect one
          > type of log with your Syslog Collector in Cortex XSIAM. To
          > configure more that one log type, you\'ll need to add
          > another log receiver.

          - **User Activity**: Information on end user requests to
            applications. For more information, see [User Activity Log
            Fields](https://help.zscaler.com/zpa/about-user-activity-log-fields).

          - **User Status**: Information related to an end user\'s
            availability and connection to ZPA. For more information,
            see [User Status Log
            Fields](https://help.zscaler.com/zpa/about-user-status-log-fields).

          - **App Connector Status**: Information related to an App
            Connector\'s availability and connection to ZPA. For more
            information, see [About App Connector Status Log
            Fields](https://help.zscaler.com/zpa/about-connector-status-log-fields).

          - **Audit Logs**: Session information for all admins accessing
            the ZPA Admin Portal. For more information, See [About Audit
            Log
            Fields](https://help.zscaler.com/zpa/about-audit-log-fields) and [About
            Audit Logs](https://help.zscaler.com/zpa/about-audit-logs).

        <!-- -->

        - **Log Template**: Select a **Custom** template.

        - **Log Stream Content**: Create the log template that you
          require, according to the **Log Type** you\'ve selected, using
          the Zscaler documentation mentioned in previous steps as a
          reference.

        <!-- -->

        - If you copy and modify the following examples in the table
          below, validate your log template using an editor, ensuring
          that there are no additional spaces or line breaks, and then
          copy and paste it into the **Log Stream Content** field.

+-----------------------------------+-------------------------------------------------------------------------------------------------+
| Log type                          | Log template                                                                                    |
+===================================+=================================================================================================+
| User activity                     |     LEEF:1.0|Zscaler|ZPA|4.1|%s{ConnectionStatus}%s{InternalReason}|cat=ZPA User                |
|                                   |     Activity\tdevTime=%s{LogTimestamp:epoch}\tCustomer=%s{Customer}\tSessionID=%s               |
|                                   |     {SessionID}\tConnectionID=%s{ConnectionID}\tInternalReason=%s{InternalReason}               |
|                                   |     \tConnectionStatus=%s{ConnectionStatus}\tproto=%d{IPProtocol}                               |
|                                   |     \tDoubleEncryption=%d{DoubleEncryption}\tusrName=%s{Username}                               |
|                                   |     \tdstPort=%d{ServicePort}\tsrc=%s{ClientPublicIP}\tsrcPreNAT=%s{ClientPrivateIP}            |
|                                   |     \tClientLatitude=%f{ClientLatitude}\tClientLongitude=%f{ClientLongitude}                    |
|                                   |     \tClientCountryCode=%s{ClientCountryCode}\tClientZEN=%s{ClientZEN}                          |
|                                   |     \tpolicy=%s{Policy}\tConnector=%s{Connector}\tConnectorZEN=%s{ConnectorZEN}                 |
|                                   |     \tConnectorIP=%s{ConnectorIP}\tConnectorPort=%d{ConnectorPort}                              |
|                                   |     \tApplicationName=%s{Host}\tApplicationSegment=%s{Application}\tAppGroup=%s{AppGroup}       |
|                                   |     \tServer=%s{Server}\tdst=%s{ServerIP}\tServerPort=%d{ServerPort}                            |
|                                   |     \tPolicyProcessingTime=%d{PolicyProcessingTime}\tServerSetupTime=%d{ServerSetupTime}        |
|                                   |     \tTimestampConnectionStart:iso8601=%s{TimestampConnectionStart:iso8601}                     |
|                                   |     \tTimestampConnectionEnd:iso8601=%s{TimestampConnectionEnd:iso8601}                         |
|                                   |     \tTimestampCATx:iso8601=%s{TimestampCATx:iso8601}                                           |
|                                   |     \tTimestampCARx:iso8601=%s{TimestampCARx:iso8601}                                           |
|                                   |     \tTimestampAppLearnStart:iso8601=%s{TimestampAppLearnStart:iso8601}                         |
|                                   |     \tTimestampZENFirstRxClient:iso8601=%s{TimestampZENFirstRxClient:iso8601}                   |
|                                   |     \tTimestampZENFirstTxClient:iso8601=%s{TimestampZENFirstTxClient:iso8601}                   |
|                                   |     \tTimestampZENLastRxClient:iso8601=%s{TimestampZENLastRxClient:iso8601}                     |
|                                   |     \tTimestampZENLastTxClient:iso8601=%s{TimestampZENLastTxClient:iso8601}                     |
|                                   |     \tTimestampConnectorZENSetupComplete:iso8601=%s{TimestampConnectorZENSetupComplete:iso8601} |
|                                   |     \tTimestampZENFirstRxConnector:iso8601=%s{TimestampZENFirstRxConnector:iso8601}             |
|                                   |     \tTimestampZENFirstTxConnector:iso8601=%s{TimestampZENFirstTxConnector:iso8601}             |
|                                   |     \tTimestampZENLastRxConnector:iso8601=%s{TimestampZENLastRxConnector:iso8601}               |
|                                   |     \tTimestampZENLastTxConnector:iso8601=%s{TimestampZENLastTxConnector:iso8601}               |
|                                   |     \tZENTotalBytesRxClient=%d{ZENTotalBytesRxClient}\tZENBytesRxClient=%d{ZENBytesRxClient}    |
|                                   |     \tZENTotalBytesTxClient=%d{ZENTotalBytesTxClient}\tZENBytesTxClient=%d{ZENBytesTxClient}    |
|                                   |     \tZENTotalBytesRxConnector=%d{ZENTotalBytesRxConnector}                                     |
|                                   |     \tZENBytesRxConnector=%d{ZENBytesRxConnector}                                               |
|                                   |     \tZENTotalBytesTxConnector=%d{ZENTotalBytesTxConnector}                                     |
|                                   |     \tZENBytesTxConnector=%d{ZENBytesTxConnector}\tIdp=%s{Idp}\n                                |
+-----------------------------------+-------------------------------------------------------------------------------------------------+
| User status                       |     LEEF:1.0|Zscaler|ZPA|4.1|%s{SessionStatus}|cat=ZPA User Status                              |
|                                   |     \tdevTime=%s{LogTimestamp:epoch}\tCustomer=%s{Customer}                                     |
|                                   |     \tusrName=%s{Username}\tSessionID=%s{SessionID}\tSessionStatus=%s{SessionStatus}            |
|                                   |     \tVersion=%s{Version}\tZEN=%s{ZEN}\tCertificateCN=%s{CertificateCN}                         |
|                                   |     \tsrcPreNAT=%s{PrivateIP}\tsrc=%s{PublicIP}\tLatitude=%f{Latitude}                          |
|                                   |     \tLongitude=%f{Longitude}\tCountryCode=%s{CountryCode}                                      |
|                                   |     \tTimestampAuthentication:iso8601=%s{TimestampAuthentication:iso8601}                       |
|                                   |     \tTimestampUnAuthentication:iso8601=%s{TimestampUnAuthentication:iso8601}                   |
|                                   |     \tdstBytes=%d{TotalBytesRx}\tsrcBytes=%d{TotalBytesTx}\tIdp=%s{Idp}                         |
|                                   |     \tidentHostName=%s{Hostname}\tPlatform=%s{Platform}\tClientType=%s{ClientType}              |
|                                   |     \tTrustedNetworks=%s(,){TrustedNetworks}\tTrustedNetworksNames=%s(,){TrustedNetworksNames}  |
|                                   |     \tSAMLAttributes=%s{SAMLAttributes}\tPosturesHit=%s(,){PosturesHit}                         |
|                                   |     \tPosturesMiss=%s(,){PosturesMiss}\tZENLatitude=%f{ZENLatitude}                             |
|                                   |     \tZENLongitude=%f{ZENLongitude}\tZENCountryCode=%s{ZENCountryCode}\n                        |
+-----------------------------------+-------------------------------------------------------------------------------------------------+
| App connector status              |     LEEF:1.0|Zscaler|ZPA|4.1|%s{SessionStatus}|cat=Connector Status                             |
|                                   |     \tdevTime=%s{LogTimestamp:epoch}\tCustomer=%s{Customer}\tSessionID=%s{SessionID}            |
|                                   |     \tSessionType=%s{SessionType}\tVersion=%s{Version}\tPlatform=%s{Platform}                   |
|                                   |     \tZEN=%s{ZEN}\tConnector=%s{Connector}\tConnectorGroup=%s{ConnectorGroup}                   |
|                                   |     \tsrcPreNAT=%s{PrivateIP}\tsrc=%s{PublicIP}\tLatitude=%f{Latitude}                          |
|                                   |     \tLongitude=%f{Longitude}\tCountryCode=%s{CountryCode}                                      |
|                                   |     \tTimestampAuthentication:iso8601=%s{TimestampAuthentication:iso8601}                       |
|                                   |     \tTimestampUnAuthentication:iso8601=%s{TimestampUnAuthentication:iso8601}                   |
|                                   |     \tCPUUtilization=%d{CPUUtilization}\tMemUtilization=%d{MemUtilization}                      |
|                                   |     \tServiceCount=%d{ServiceCount}\tInterfaceDefRoute=%s{InterfaceDefRoute}                    |
|                                   |     \tDefRouteGW=%s{DefRouteGW}\tPrimaryDNSResolver=%s{PrimaryDNSResolver}                      |
|                                   |     \tHostStartTime=%s{HostStartTime}\tConnectorStartTime=%s{ConnectorStartTime}                |
|                                   |     \tNumOfInterfaces=%d{NumOfInterfaces}\tBytesRxInterface=%d{BytesRxInterface}                |
|                                   |     \tPacketsRxInterface=%d{PacketsRxInterface}\tErrorsRxInterface=%d{ErrorsRxInterface}        |
|                                   |     \tDiscardsRxInterface=%d{DiscardsRxInterface}\tBytesTxInterface=%d{BytesTxInterface}        |
|                                   |     \tPacketsTxInterface=%d{PacketsTxInterface}\tErrorsTxInterface=%d{ErrorsTxInterface}        |
|                                   |     \tDiscardsTxInterface=%d{DiscardsTxInterface}\tTotalBytesRx=%d{TotalBytesRx}                |
|                                   |     \tTotalBytesTx=%d{TotalBytesTx}\n                                                           |
+-----------------------------------+-------------------------------------------------------------------------------------------------+
| Audit logs                        |     LEEF:1.0|Zscaler|ZPA|4.1|%s{auditOperationType}|cat=ZPA_Audit_Log\t                         |
|                                   |     devTime=%s{modifiedTime:epoch}\t                                                            |
|                                   |     creationTime=%s{creationTime:iso8601}\t                                                     |
|                                   |     requestId=%s{requestId}\t                                                                   |
|                                   |     sessionId=%s{sessionId}\t                                                                   |
|                                   |     auditOldValue=%s{auditOldValue}\t                                                           |
|                                   |     auditNewValue=%s{auditNewValue}\t                                                           |
|                                   |     auditOperationType=%s{auditOperationType}\t                                                 |
|                                   |     objectType=%s{objectType}\t                                                                 |
|                                   |     objectName=%s{objectName}\t                                                                 |
|                                   |     objectId=%d{objectId}\t                                                                     |
|                                   |     accountName=%d{customerId}\t                                                                |
|                                   |     usrName=%s{modifiedByUser}\n                                                                |
+-----------------------------------+-------------------------------------------------------------------------------------------------+

- (Optional) You can define a streaming **Policy** for the log receiver.
  This entails configuring the **SAML Attributes**,
  **Application Segments**, **Segment Groups**, **Client Types**, and
  **Session Statuses**. For more information on configuring these
  settings, see the [Log Stream
  instructions](https://help.zscaler.com/zpa/configuring-log-receiver#Step2).

6.  Click **Next**.

7.  In the **Review** tab, verify your log receiver configuration.

8.  Click **Save**.

##### Ingest authentication logs and data

When you ingest authentication logs and data from an external source,
Cortex XSIAM can weave that information into authentication stories. An
authentication story unites logs and data regardless of the information
source (for example, from an on-premise KDC or from a cloud-based
authentication service) into a uniform schema. To search authentication
stories, you can use the Query Builder or XQL Search.

Cortex XSIAM can ingest authentication logs and data from various
authentication services.

###### Ingest audit logs from AWS Cloud Trail

You can forward audit logs for the relative service to Cortex XSIAM from
AWS CloudTrail.

To receive audit logs from Amazon Simple Storage Service (Amazon S3) via
AWS CloudTrail, you must first configure data collection from Amazon S3.
You can then configure the Data Sources settings in Cortex XSIAM for
Amazon S3. After you set up collection integration, Cortex XSIAM begins
receiving new logs and data from the source.

We do not recommend ingestion of data from an AWS commercial environment
into a FedRAMP-certified Cortex XSIAM tenant. However, if you must do
so, contact Customer Support for assistance.

> **Note**
>
> For more information on configuring data collection from Amazon S3
> using AWS CloudTrail, see the [AWS CloudTrail
> Documentation](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-create-a-trail-using-the-console-first-time.html).

When Cortex XSIAM begins receiving logs, the app automatically creates
an Amazon S3 Cortex Query Language (XQL) dataset (`aws_s3_raw`). This
enables you to search the logs with XQL Search using the dataset. For
example queries, refer to the in-app XQL Library.

For enhanced cloud protection, you can also configure Cortex XSIAM to
stitch Amazon S3 audit logs with other Cortex XSIAM authentication
stories across all cloud providers using the same format, which you can
query with XQL Search using the `cloud_audit_logs` dataset. Cortex XSIAM
can also generate Cortex XSIAM issues (Analytics, IOC, BIOC, and
Correlation Rules), when relevant, from Amazon S3 logs. While
Correlation Rules issues are generated on non-normalized and normalized
logs, Analytics, IOC, and BIOC issues are only generated on normalized
logs.

Enhanced cloud protection provides the following:

- Normalization of cloud logs

- Cloud logs stitching

- Enrichment with cloud data

- Detection based on cloud analytics

- Cloud-tailored investigations

**Prerequisite Steps**

Be sure you do the following tasks before you begin configuring data
collection from Amazon S3 via AWS CloudTrail.

- Ensure that you have the proper permissions to access AWS CloudTrail
  and have the necessary permissions to create audit logs. The following
  permissions in AWS are the minimum requirements for an Amazon S3
  bucket and Amazon Simple Queue Service (SQS).

  - **Amazon S3 bucket**: `GetObject`

  - **SQS**: `ChangeMessageVisibility`, `ReceiveMessage`, and
    `DeleteMessage`.

- Determine how you want to provide access to Cortex XSIAM to your logs
  and to perform API operations. You have the following options:

  - Designate an AWS IAM user, where you will need to know the Account
    ID for the user and have the relevant permissions to create an
    access key/id for the relevant IAM user. This is the default option
    as explained in Configure the Amazon S3 collection by selecting
    **Access Key**.

  - Create an assumed role in AWS to delegate permissions to a Cortex
    XSIAM AWS service. This role grants Cortex XSIAM access to your flow
    logs. For more information, see [Creating a role to delegate
    permissions to an AWS
    service](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-service.html).
    This is the **Assumed Role** option described in the Amazon S3
    collection configuration.

<!-- -->

- To collect Amazon S3 logs that use server-side encryption (SSE), the
  user role must have an IAM policy that states that Cortex XSIAM has
  kms:Decrypt permissions. With this permission, Amazon S3 automatically
  detects if a bucket is encrypted and decrypts it. If you want to
  collect encrypted logs from different accounts, you must have the
  decrypt permissions for the user role also in the key policy for the
  master account Key Management Service (KMS). For more information, see
  [Allowing users in other accounts to use a KMS
  key](https://docs.aws.amazon.com/kms/latest/developerguide/key-policy-modifying-external-accounts.html).

To configure Cortex XSIAM to receive audit logs from Amazon S3 via AWS
Cloudtrail:

1.  Log in to the [AWS Management
    Console](https://console.aws.amazon.com/).

2.  From the menu bar, ensure that you have selected the correct region
    for your configuration.

3.  Configure an AWS CloudTrail trail with audit logs.

- > **Note**

  - > For more information on creating an AWS CloudTrail trail, see
    > [Create a
    > trail](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-create-a-trail-using-the-console-first-time.html).

  - > If you already have an Amazon S3 bucket configured with AWS
    > CloudTrail audit logs, skip this step and go to Configure an
    > Amazon Simple Queue Service (SQS).

  a.  Open the [CloudTrail
      Console](https://console.aws.amazon.com/cloudtrail/), and click
      **Create trail**.

  b.  Configure the following settings for your CloudTrail trail, where
      the default settings should be configured unless otherwise
      indicated.

      - **Trail name**: Specify a descriptive name for your CloudTrail
        trail.

      - **Storage location**: Select **Create new S3 bucket** to
        configure a new Amazon S3 bucket, and specify a unique name in
        the **Trail log bucket and folder** field, or select
        **Use existing S3 bucket** and **Browse** to the S3 bucket you
        already created. If you select an existing Amazon S3 bucket, the
        bucket policy must grant CloudTrail permission to write to it.
        For information about manually editing the bucket policy, see
        [Amazon S3 Bucket Policy for
        CloudTrail](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/create-s3-bucket-policy-for-cloudtrail.html).

      <!-- -->

      - > **Note**

        > It is your organization\'s responsibility to define a
        > retention policy for your Amazon S3 bucket by creating a
        > **Lifecycle rule** in the **Management** tab. We recommend
        > setting the retention policy to at least 7 days to ensure that
        > the data is retrieved under all circumstances.

      <!-- -->

      - **Customer managed AWS KMS key**: You can either select a
        **New** key and specify the **AWS KMS alias**, or select an
        **Existing** key, and select the **AWS KMS alias**. The KMS key
        and S3 bucket must be in the same region.

      - **SNS notification delivery**: (*Optional*) If you want to be
        notified whenever CloudTrail publishes a new log to your Amazon
        S3 bucket, click **Enabled**. Amazon Simple Notification Service
        (Amazon SNS) manages these notifications, which are sent for
        every log file delivery to your S3 bucket, as opposed to every
        event. When you enable this option, you can either
        **Create a new SNS topic** by selecting **New** and the
        **SNS topic** is displayed in the field, or use an **Existing**
        topic and select the **SNS topic**. For more information, see
        [Configure SNS Notifications for
        CloudTrail](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/configure-sns-notifications-for-cloudtrail.html).

  - > **Note**

    > The **CloudWatch Logs - optional** settings are not supported and
    > should be left disabled.

  c.  Click **Next**, and configure the following **Choose log events**
      settings.

      - **Event type**: Leave the default **Management events** checkbox
        selected to capture audit logs. Depending on your system
        requirements, you can also select **Data events** to log the
        resource operations performed on or within a resource, or
        **Insights events** to identify unusual activity, errors, or
        user behavior in your account. Based on your selection,
        additional fields are displayed on the screen to configure under
        section headings with the same name as the event type.

      - **Management events** section: Configure the following settings.

      <!-- -->

      - \-**API activity**: For **Management events**, select the API
        activities you want to log. By default, the **Read** and
        **Write** activities are logged.

        \-**Exclude AWS KMS events**: (*Optional*) If you want to filter
        AWS Key Management Service (AWS KMS) events out of your trail,
        select the checkbox. By default, all AWS KMS events are
        included.

      <!-- -->

      - **Data events** section: (*Optional*) This section is displayed
        when you configure the **Event type** to include
        **Data events**, which relate to resource operations performed
        on or within a resource, such as reading and writing to a S3
        bucket. For more information on configuring these optional
        settings in AWS CloudTrail, see [Creating a
        trail](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-create-a-trail-using-the-console-first-time.html).

      - **Insights events** section: (*Optional*) This section is
        displayed when you configure the **Event type** to include
        **Insight events**, which relate to unusual activities, errors,
        or user behavior on your account. For more information on
        configuring these optional settings in AWS CloudTrail, see
        [Creating a
        trail](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-create-a-trail-using-the-console-first-time.html).

  d.  Click **Next**.

  e.  In the **Review and create** page, look over the trail
      configurations settings that you have configured and if they are
      correct, click **Create trail**. If you need to make a change,
      click **Edit** beside the particular step that you want to update.

  - The new trail is listed in the **Trails** page, which lists the
    trails in your account from all Regions. It can take up to 15
    minutes for CloudTrail to begin publishing log files. You can see
    the log files in the S3 bucket that you specified. For more
    information, see [Creating a
    trail](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-create-a-trail-using-the-console-first-time.html).

4.  Configure an Amazon Simple Queue Service (SQS).

- > **Note**

  > Ensure that you create your Amazon S3 bucket and Amazon SQS queue in
  > the same region.

  a.  In the [Amazon SQS Console](https://console.aws.amazon.com/sqs/),
      click **Create Queue**.

  b.  Configure the following settings, where the default settings
      should be configured unless otherwise indicated.

      - **Type**: Select **Standard** queue (default).

      - **Name**: Specify a descriptive name for your SQS queue.

      - **Configuration** section: Leave the default settings for the
        various fields.

      - Access policy \> Choose method: Select **Advanced** and update
        the Access policy code in the editor window to enable your
        Amazon S3 bucket to publish event notification messages to your
        SQS queue. Use this sample code as a guide for defining the
        `“Statement”` with the following definitions:

      <!-- -->

      - \-`“Resource”`: Leave the automatically generated ARN for the
        SQS queue that is set in the code, which uses the format
        `“arn:sns:Region:account-id:topic-name”`.

        You can retrieve your bucket's ARN by opening the [Amazon S3
        Console](https://console.aws.amazon.com/s3/) in a browser
        window. In the **Buckets** section, select the bucket that you
        created for collecting the Amazon S3 flow logs, click
        **Copy ARN**, and paste the ARN in the field.

        ![](media/rId5412.png){width="5.833333333333333in"
        height="1.5677077865266842in"}

        > **Note**

        > For more information on granting permissions to publish
        > messages to an SQS queue, see [Granting permissions to publish
        > event notification messages to a
        > destination](https://docs.aws.amazon.com/AmazonS3/latest/userguide/grant-destinations-permissions-to-s3.html).

            {
              "Version": "2012-10-17",
              "Statement": [
                {
                  "Effect": "Allow",
                  "Principal": {
                    "Service": "s3.amazonaws.com"
                  },
                  "Action": "SQS:SendMessage",
                  "Resource": "[Leave automatically generated ARN for the SQS queue defined by AWS]",
                  "Condition": {
                    "ArnLike": {
                      "aws:SourceArn": "[ARN of your Amazon S3 bucket]"
                    }
                  }
                },
              ]
            }

      <!-- -->

      - **Dead-letter queue** section: We recommend that you configure a
        queue for sending undeliverable messages by selecting
        **Enabled**, and then in the **Choose queue** field selecting
        the queue to send the messages. You may need to create a new
        queue for this, if you do not already have one set up. For more
        information, see [Amazon SQS dead-letter
        queues](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html).

  c.  Click **Create queue**.

  - Once the SQS is created, a message indicating that the queue was
    successfully configured is displayed at the top of the page.

5.  Configure an event notification to your Amazon SQS whenever a file
    is written to your Amazon S3 bucket.

    a.  Open the [Amazon S3 Console](https://console.aws.amazon.com/s3/)
        and in the **Properties** tab of your Amazon S3 bucket, scroll
        down to the **Event notifications** section, and click
        **Create event notification**.

    b.  Configure the following settings.

        - **Event name**: Specify a descriptive name for your event
          notification containing up to 255 characters.

        - **Prefix**: Do not set a prefix as the Amazon S3 bucket is
          meant to be a dedicated bucket for collecting audit logs.

        - **Event types**: Select **All object create events** for the
          type of event notifications that you want to receive.

        - **Destination**: Select **SQS queue** to send notifications to
          an SQS queue to be read by a server.

        - **Specify SQS queue**: You can either select
          **Choose from your SQS queues** and then select the
          **SQS queue**, or select **Enter SQS queue ARN** and specify
          the ARN in the **SQS queue** field.

        <!-- -->

        - You can retrieve your SQS queue ARN by opening another
          instance of the AWS Management Console in a browser window,
          and opening the [Amazon SQS
          Console](https://console.aws.amazon.com/sqs/), and selecting
          the Amazon SQS that you created. In the **Details** section,
          under **ARN**, click the copy icon
          (![](media/rId5488.png){width="0.125in"
          height="0.20833333333333334in"})), and paste the ARN in the
          field.

          ![](media/rId5491.png){width="5.833333333333333in"
          height="2.216666666666667in"}

    c.  Click **Save changes**.

    - Once the event notification is created, a message indicating that
      the event notification was successfully created is displayed at
      the top of the page.

      > **Note**

      > If your receive an error when trying to save your changes, you
      > should ensure that the permissions are set up correctly.

6.  Configure access keys for the AWS IAM user that Cortex XSIAM uses
    for API operations.

- > **Note**

  - > It your organization\'s responsibility to ensure that the user who
    > performs this task of creating the access key is designated with
    > the relevant permissions. Otherwise, this can cause the process to
    > fail with errors.

  - > Skip this step if you are using an **Assumed Role** for Cortex
    > XSIAM.

  a.  Open the [AWS IAM Console](https://console.aws.amazon.com/iam/),
      and in the navigation pane, select Access management \> Users.

  b.  Select the **User name** of the AWS IAM user.

  c.  Select the **Security credentials** tab, scroll down to the
      **Access keys** section, and click **Create access key**.

  d.  Click the copy icon next to the **Access key ID** and
      **Secret access key** keys, where you must click
      **Show secret access key** to see the secret key and record them
      somewhere safe before closing the window. You will need to provide
      these keys when you edit the Access policy of the SQS queue and
      when setting the **AWS Client ID** and **AWS Client Secret** in
      Cortex XSIAM. If you forget to record the keys and close the
      window, you will need to generate new keys and repeat this
      process.

  - > **Note**

    > For more information, see [Managing access keys for IAM
    > users](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html).

7.  Update the Access policy of your Amazon SQS queue.

- > **Note**

  > Skip this step if you are using an **Assumed Role** for Cortex
  > XSIAM.

  a.  In the [Amazon SQS Console](https://console.aws.amazon.com/sqs/),
      select the SQS queue that you created in Configure an Amazon
      Simple Queue Service (SQS).

  b.  Select the **Access policy** tab, and **Edit** the Access policy
      code in the editor window to enable the IAM user to perform
      operations on the Amazon SQS with permissions to
      `SQS:ChangeMessageVisibility`, `SQS:DeleteMessage`, and
      `SQS:ReceiveMessage`. Use this sample code as a guide for defining
      the `“Sid”: “__receiver_statement”` with the following
      definitions:

      - `“aws:SourceArn”`: Specify the ARN of the AWS IAM user. You can
        retrieve the **User ARN** from the **Security credentials** tab,
        which you accessed when configuring access keys for the AWS API
        user.

      - `“Resource”`: Leave the automatically generated ARN for the SQS
        queue that is set in the code, which uses the format
        `“arn:sns:Region:account-id:topic-name”`.

      <!-- -->

      - > **Note**

        > For more information on granting permissions to publish
        > messages to an SQS queue, see [Granting permissions to publish
        > event notification messages to a
        > destination](https://docs.aws.amazon.com/AmazonS3/latest/userguide/grant-destinations-permissions-to-s3.html).

            {
              "Version": "2012-10-17",
              "Statement": [
                {
                  "Effect": "Allow",
                  "Principal": {
                    "Service": "s3.amazonaws.com"
                  },
                  "Action": "SQS:SendMessage",
                  "Resource": "[Leave automatically generated ARN for the SQS queue defined by AWS]",
                  "Condition": {
                    "ArnLike": {
                      "aws:SourceArn": "[ARN of your Amazon S3 bucket]"
                    }
                  }
                },
               {
                  "Sid": "__receiver_statement",
                  "Effect": "Allow",
                  "Principal": {
                    "AWS": "[Add the ARN for the AWS IAM user]"
                  },
                  "Action": [
                    "SQS:ChangeMessageVisibility",
                    "SQS:DeleteMessage",
                    "SQS:ReceiveMessage"
                  ],
                  "Resource": "[Leave automatically generated ARN for the SQS queue defined by AWS]"
                }
              ]
            }

8.  Configure the Amazon S3 collection in Cortex XSIAM.

    a.  Select Settings \> Data Sources.

    b.  On the **Data Sources** page, click **Add Data Source**, search
        for and select **Amazon S3**, and click **Connect**.

    c.  Set these parameters, where the parameters change depending on
        whether you configured an **Access Key** or **Assumed Role**.

        - To provide access to Cortex XSIAM to your logs and perform API
          operations using a designated AWS IAM user, leave the
          **Access Key** option selected. Otherwise, select
          **Assumed Role**, and ensure that you Create an Assumed Role
          for Cortex XSIAM before continuing with these instructions. In
          addition, when you create an Assumed Role for Cortex XSIAM,
          ensure that you edit the policy that defines the permissions
          for the role with the Amazon S3 Bucket ARN and SQS ARN.

        - **SQS URL**: Specify the **SQS URL**, which is the ARN of the
          Amazon SQS that you configured in the AWS Management Console.

        - **Name**: Specify a descriptive name for your log collection
          configuration.

        - When setting an **Access Key**, set these parameters.

          - **AWS Client ID**: Specify the **Access key ID**, which you
            received when you configured access keys for the AWS IAM
            user in AWS.

          - **AWS Client Secret**: Specify the **Secret access key** you
            received when you configured access keys for the AWS IAM
            user in AWS.

        - When setting an **Assumed Role**, set these parameters.

          - **Role ARN**: Specify the **Role ARN** for the Assumed Role
            you created for in AWS.

          - **External Id**: Specify the **External Id** for the Assumed
            Role you created for in AWS.

        - **Log Type**: Select **Audit Logs** to configure your log
          collection to receive audit logs from Amazon S3 via AWS
          CloudTrail. When configuring audit log collection, the
          following additional field is displayed for
          **Enhanced Cloud Protection**.

        <!-- -->

        - You can **Normalize and enrich audit logs** by selecting the
          checkbox. If selected, Cortex XSIAM stitches Amazon S3 audit
          logs with other Cortex XSIAM authentication stories across all
          cloud providers using the same format, which you can query
          with XQL Search using the `cloud_audit_logs` dataset.

    d.  Click **Test** to validate access, and then click **Enable**.

    - Once events start to come in, a green check mark appears
      underneath the **Amazon S3** configuration with the number of logs
      received.

###### Ingest logs and data from a GCP Pub/Sub

If you use the Pub/Sub messaging service from Global Cloud Platform
(GCP), you can send logs and data from your GCP instance to Cortex
XSIAM. Data from GCP is then searchable in Cortex XSIAM to provide
additional information and context to your investigations using the GCP
Cortex Query Language (XQL) dataset, which is dependent on the type of
GCP logs collected. For example queries, refer to the in-app XQL
Library. You can configure a Google Cloud Platform collector to receive
generic, flow, audit, or Google Cloud DNS logs. When configuring generic
logs, you can receive logs in a Raw, JSON, CEF, LEEF, Cisco, or
Corelight format.

You can also configure Cortex XSIAM to normalize different GCP logs as
part of the enhanced cloud protection, which you can query with XQL
Search using the applicable dataset. Cortex XSIAM can also generate
Cortex XSIAM issues (Analytics, IOC, BIOC, and Correlation Rules), when
relevant, from GCP logs. While Correlation Rules isssues are generated
on non-normalized and normalized logs, Analytics, IOC, and BIOC issues
are only raised on normalized logs.

Enhanced cloud protection provides the following:

- Normalization of cloud logs

- Cloud logs stitching

- Enrichment with cloud data

- Detection based on cloud analytics

- Cloud-tailored investigations

The following table lists the various GCP log types the XQL datasets you
can use to query in XQL Search:

+-----------------------+------------------------------+-----------------------+
| GCP log type          | Dataset                      | Dataset with          |
|                       |                              | normalized data       |
+=======================+==============================+=======================+
| Audit logs, including | `google_cloud_logging_raw`   | `cloud_audit_logs`    |
| Google Kubernetes     |                              |                       |
| Engine (GKE) audit    |                              |                       |
| logs                  |                              |                       |
+-----------------------+------------------------------+-----------------------+
| Generic logs          | Log Format types:            | N/A                   |
|                       |                              |                       |
|                       | - **CEF** or `LEEF`:         |                       |
|                       |   Automatically detected     |                       |
|                       |   from either the logs or    |                       |
|                       |   the user\'s input in the   |                       |
|                       |   User Interface.            |                       |
|                       |                              |                       |
|                       | - **Cisco**: `cisco_asa_raw` |                       |
|                       |                              |                       |
|                       | - **Corelight**:             |                       |
|                       |   `corelight_zeek_raw`       |                       |
|                       |                              |                       |
|                       | - **JSON or Raw**:           |                       |
|                       |   `google_cloud_logging_raw` |                       |
+-----------------------+------------------------------+-----------------------+
| Google Cloud DNS logs | `google_dns_raw`             | `xdr_data`: Once      |
|                       |                              | configured, Cortex    |
|                       |                              | XSIAM ingests Google  |
|                       |                              | Cloud DNS logs as XDR |
|                       |                              | network connection    |
|                       |                              | stories, which you    |
|                       |                              | can query with XQL    |
|                       |                              | Search using the      |
|                       |                              | `xdr_data` dataset    |
|                       |                              | with the preset       |
|                       |                              | called                |
|                       |                              | `network_story`.      |
+-----------------------+------------------------------+-----------------------+
| Network flow logs     | `google_cloud_logging_raw`   | `xdr_data`: Once      |
|                       |                              | configured, Cortex    |
|                       |                              | XSIAM ingests network |
|                       |                              | flow logs as XDR      |
|                       |                              | network connection    |
|                       |                              | stories, which you    |
|                       |                              | can query with XQL    |
|                       |                              | Search using the      |
|                       |                              | `xdr_data` dataset    |
|                       |                              | with the preset       |
|                       |                              | called                |
|                       |                              | `network_story`.      |
+-----------------------+------------------------------+-----------------------+

> **Note**
>
> When collecting flow logs, we recommend that you include GKE
> annotations in your logs, which enable you to view the names of the
> containers that communicated with each other. GKE annotations are only
> included in logs if appended manually using the custom metadata
> configuration in GCP. For more information, see [VPC Flow Logs
> Overview](https://cloud.google.com/vpc/docs/flow-logs#customizing_metadata_fields).
> In addition, to customize metadata fields, you must use the gcloud
> command-line interface or the API. For more information, see [Using
> VPC Flow
> Logs](https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging_for_an_existing_subnet).

To receive logs and data from GCP, you must first set up log forwarding
using a Pub/Sub topic in GCP. You can configure GCP settings using
either the GCP web interface or a GCP cloud shell terminal. After you
set up your service account in GCP, you configure the Data Collection
settings in Cortex XSIAM. The setup process requires the subscription
name and authentication key from your GCP instance.

After you set up log collection, Cortex XSIAM immediately begins
receiving new logs and data from GCP.

####### Set up log forwarding using the GCP web interface

1.  In Cortex XSIAM, set up Data Collection.

    a.  Select Settings \> Data Sources.

    b.  On the **Data Sources** page, click **Add Data Source**, search
        for and select **Google Cloud Platform**, and click **Connect**.

    c.  Specify the **Subscription Name** that you previously noted or
        copied.

    d.  Browse to the JSON file containing your authentication key for
        the service account.

    e.  Select the **Log Type** as one of the following, where your
        selection changes the options displayed.

        - **Flow or Audit Logs**: When selecting this log type, you can
          decide whether to normalize and enrich the logs as part of the
          enhanced cloud protection.

          - (*Optional*) You can
            **Normalize and enrich flow and audit logs** by selecting
            the checkbox (default). If selected, Cortex XSIAM ingests
            the network flow logs as Cortex XSIAM network connection
            stories, which you can query using XQL Search from the
            `xdr_dataset` dataset with the preset called
            `network_story`. In addition, you can configure Cortex XSIAM
            to normalize GCP audit logs, which you can query with XQL
            Search using the `cloud_audit_logs` dataset.

          - The **Vendor** is automatically set to **Google** and
            **Product** to **Cloud Logging**, which is not configurable.
            This means that all GCP data for the flow and audit logs,
            whether it\'s normalized or not, can be queried in XQL
            Search using the `google_cloud_logging_raw` dataset.

        - **Generic**: When selecting this log type, you can configure
          the following settings.

          - **Log Format**: Select the log format type as **Raw**,
            **JSON**, **CEF**, **LEEF**, **Cisco**, or **Corelight**.

            - **CEF** or **LEEF**: The **Vendor** and **Product**
              defaults to **Auto-Detect**.

            <!-- -->

            - > **Note**

              > For a **Log Format** set to **CEF** or **LEEF**, Cortex
              > XSIAM reads events row by row to look for the **Vendor**
              > and **Product** configured in the logs. When the values
              > are populated in the event log row, Cortex XSIAM uses
              > these values even if you specified a value in the
              > **Vendor** and **Product** fields in the GCP data
              > collector settings. Yet, when the values are blank in
              > the event log row, Cortex XSIAM uses the **Vendor** and
              > **Product** that you specified in the GCP data collector
              > settings. If you did not specify a **Vendor** or
              > **Product** in the GCP data collector settings, and the
              > values are blank in the event log row, the values for
              > both fields are set to **unknown**.

            <!-- -->

            - **Cisco**: The following fields are automatically set and
              not configurable.

              - **Vendor**: **Cisco**

              - **Product**: **ASA**

            <!-- -->

            - Cisco data can be queried in XQL Search using the
              `cisco_asa_raw` dataset.

            <!-- -->

            - **Corelight**: The following fields are automatically set
              and not configurable.

              - **Vendor**: **Corelight**

              - **Product**: **Zeek**

            <!-- -->

            - Corelight data can be queried in XQL Search using the
              `corelight_zeek_raw` dataset.

            <!-- -->

            - **Raw** or **JSON**: The following fields are
              automatically set and are configurable.

              - **Vendor**: **Google**

              - **Product**: **Cloud Logging**

            <!-- -->

            - Raw or JSON data can be queried in XQL Search using the
              `google_cloud_logging_raw` dataset.

              Cortex XSIAM supports logs in single line format or
              multiline format. For a **JSON** format, multiline logs
              are collected automatically when the **Log Format** is
              configured as **JSON**. When configuring a **Raw** format,
              you must also define the **Multiline Parsing Regex** as
              explained below.

          - **Vendor**: (*Optional*) Specify a particular vendor name
            for the GCP generic data collection, which is used in the
            GCP XQL dataset `<Vendor>_<Product>_raw` that Cortex XSIAM
            creates as soon as it begins receiving logs.

          - **Product**: (*Optional*) Specify a particular product name
            for the GCP generic data collection, which is used in the
            GCP XQL dataset name `<Vendor>_<Product>_raw` that Cortex
            XSIAM creates as soon as it begins receiving logs.

          - **Multiline Parsing Regex**: (*Optional*) This option is
            only displayed when the **Log Format** is set to **Raw**,
            where you can set the regular expression that identifies
            when the multiline event starts in logs with multilines. It
            is assumed that when a new event begins, the previous one
            has ended.

        - **Google Cloud DNS**: When selecting this log type, you can
          configure whether to normalize the logs as part of the
          enhanced cloud protection.

          - *Optional*) You can **Normalize DNS logs** by selecting the
            checkbox (default). If selected, Cortex XSIAM ingests the
            Google Cloud DNS logs as Cortex XSIAM network connection
            stories, which you can query using XQL Search from the
            `xdr_dataset` dataset with the preset called
            `network_story`.

          - The **Vendor** is automatically set to **Google** and
            **Product** to **DNS** , which is not configurable. This
            means that all Google Cloud DNS logs, whether it\'s
            normalized or not, can be queried in XQL Search using the
            `google_dns_raw` dataset.

    f.  **Test** the provided settings and, if successful, proceed to
        **Enable** log collection.

<!-- -->

1.  Log in to your GCP account.

2.  Set up log forwarding from GCP to Cortex XSIAM.

    a.  Select Logging \> Logs Router.

    b.  Select Create Sink \> Cloud Pub/Sub topic, and then click
        **Next**.

    c.  To filter only specific types of data, select the filter or
        desired resource.

    d.  In the **Edit Sink** configuration, define a descriptive
        **Sink Name**.

    e.  Select Sink Destination \> Create new Cloud Pub/Sub topic.

    f.  Enter a descriptive **Name** that identifies the sink purpose
        for Cortex XSIAM, and then **Create**.

    g.  **Create Sink** and then **Close** when finished.

3.  Create a subscription for your Pub/Sub topic.

    a.  Select the menu icon in G Cloud, and then select Pub/Sub \>
        Topics.

    b.  Select the name of the topic you created in the previous steps.
        Use the filters if necessary.

    c.  Select Create Subscription \> Create subscription.

    d.  Enter a unique Subscription ID.

    e.  Choose **Pull** as the **Delivery Type**.

    f.  **Create** the subscription.

    - After the subscription is set up, G Cloud displays statistics and
      settings for the service.

    g.  In the subscription details, identify and note your
        **Subscription Name**.

    - Optionally, use the copy button to copy the name to the clipboard.
      You will need the name when you configure Collection in Cortex
      XSIAM.

4.  Create a service account and authentication key.

- You will use the key to enable Cortex XSIAM to authenticate with the
  subscription service.

  a.  Select the menu icon, and then select IAM & Admin \> Service
      Accounts.

  b.  **Create Service Account**.

  c.  Enter a **Service account name** and then **Create**.

  d.  Select a role for the account: Pub/Sub \> Pub/Sub Subscriber.

  e.  Click Continue \> Done.

  f.  Locate the service account by name, using the filters to refine
      the results, if needed.

  g.  Click the **Actions** menu identified by the three dots in the row
      for the service account and then **Create Key**.

  h.  Select JSON as the key type, and then **Create**.

  - After you create the service account key, G Cloud automatically
    downloads it.

5.  After Cortex XSIAM begins receiving information from the GCP Pub/Sub
    service, you can use the XQL Query language to search for specific
    data.

####### Set up log forwarding using the GCP cloud shell terminal

1.  In Cortex XSIAM, set up Data Collection.

    a.  Select Settings \> Data Sources.

    b.  On the **Data Sources** page, click **Add Data Source**, search
        for and select **Google Cloud Platform**, and click **Connect**.

    c.  Specify the **Subscription Name** that you previously noted or
        copied.

    d.  Browse to the JSON file containing your authentication key for
        the service account.

    e.  Select the **Log Type** as one of the following, where your
        selection changes the options displayed.

        - **Flow or Audit Logs**: When selecting this log type, you can
          decide whether to normalize and enrich the logs as part of the
          enhanced cloud protection.

          - (*Optional*) You can
            **Normalize and enrich flow and audit logs** by selecting
            the checkbox (default). If selected, Cortex XSIAM ingests
            the network flow logs as Cortex XSIAM network connection
            stories, which you can query using XQL Search from the
            `xdr_dataset` dataset with the preset called
            `network_story`. In addition, you can configure Cortex XSIAM
            to normalize GCP audit logs, which you can query with XQL
            Search using the `cloud_audit_logs` dataset.

          - The **Vendor** is automatically set to **Google** and
            **Product** to **Cloud Logging**, which is not configurable.
            This means that all GCP data for the flow and audit logs,
            whether it\'s normalized or not, can be queried in XQL
            Search using the `google_cloud_logging_raw` dataset.

        - **Generic**: When selecting this log type, you can configure
          the following settings.

          - **Log Format**: Select the log format type as **Raw**,
            **JSON**, **CEF**, **LEEF**, **Cisco**, or **Corelight**.

            - **CEF** or **LEEF**: The **Vendor** and **Product**
              defaults to **Auto-Detect**.

            <!-- -->

            - > **Note**

              > For a **Log Format** set to **CEF** or **LEEF**, Cortex
              > XSIAM reads events row by row to look for the **Vendor**
              > and **Product** configured in the logs. When the values
              > are populated in the event log row, Cortex XSIAM uses
              > these values even if you specified a value in the
              > **Vendor** and **Product** fields in the GCP data
              > collector settings. Yet, when the values are blank in
              > the event log row, Cortex XSIAM uses the **Vendor** and
              > **Product** that you specified in the GCP data collector
              > settings. If you did not specify a **Vendor** or
              > **Product** in the GCP data collector settings, and the
              > values are blank in the event log row, the values for
              > both fields are set to **unknown**.

            <!-- -->

            - **Cisco**: The following fields are automatically set and
              not configurable.

              - **Vendor**: **Cisco**

              - **Product**: **ASA**

            <!-- -->

            - Cisco data can be queried in XQL Search using the
              `cisco_asa_raw` dataset.

            <!-- -->

            - **Corelight**: The following fields are automatically set
              and not configurable.

              - **Vendor**: **Corelight**

              - **Product**: **Zeek**

            <!-- -->

            - Corelight data can be queried in XQL Search using the
              `corelight_zeek_raw` dataset.

            <!-- -->

            - **Raw** or **JSON**: The following fields are
              automatically set and are configurable.

              - **Vendor**: **Google**

              - **Product**: **Cloud Logging**

            <!-- -->

            - Raw or JSON data can be queried in XQL Search using the
              `google_cloud_logging_raw` dataset.

              Cortex XSIAM supports logs in single line format or
              multiline format. For a **JSON** format, multiline logs
              are collected automatically when the **Log Format** is
              configured as **JSON**. When configuring a **Raw** format,
              you must also define the **Multiline Parsing Regex** as
              explained below.

          - **Vendor**: (*Optional*) Specify a particular vendor name
            for the GCP generic data collection, which is used in the
            GCP XQL dataset `<Vendor>_<Product>_raw` that Cortex XSIAM
            creates as soon as it begins receiving logs.

          - **Product**: (*Optional*) Specify a particular product name
            for the GCP generic data collection, which is used in the
            GCP XQL dataset name `<Vendor>_<Product>_raw` that Cortex
            XSIAM creates as soon as it begins receiving logs.

          - **Multiline Parsing Regex**: (*Optional*) This option is
            only displayed when the **Log Format** is set to **Raw**,
            where you can set the regular expression that identifies
            when the multiline event starts in logs with multilines. It
            is assumed that when a new event begins, the previous one
            has ended.

        - **Google Cloud DNS**: When selecting this log type, you can
          configure whether to normalize the logs as part of the
          enhanced cloud protection.

          - *Optional*) You can **Normalize DNS logs** by selecting the
            checkbox (default). If selected, Cortex XSIAM ingests the
            Google Cloud DNS logs as Cortex XSIAM network connection
            stories, which you can query using XQL Search from the
            `xdr_dataset` dataset with the preset called
            `network_story`.

          - The **Vendor** is automatically set to **Google** and
            **Product** to **DNS** , which is not configurable. This
            means that all Google Cloud DNS logs, whether it\'s
            normalized or not, can be queried in XQL Search using the
            `google_dns_raw` dataset.

    f.  **Test** the provided settings and, if successful, proceed to
        **Enable** log collection.

<!-- -->

1.  Launch the GCP cloud shell terminal or use your preferred shell with
    gcloud installed.

- ![](media/rId5500.png){width="5.833333333333333in"
  height="1.8302077865266841in"}

2.  Define your project ID.

- gcloud config set project <PROJECT_ID>
                           

3.  Create a Pub/Sub topic.

- gcloud pubsub topics create <TOPIC_NAME>
                           

4.  Create a subscription for this topic.

- gcloud pubsub subscriptions create <SUBSCRIPTION_NAME> --topic=<TOPIC_NAME>
                           

  Note the subscription name you define in this step as you will need it
  to set up log ingestion from Cortex XSIAM.

5.  Create a logging sink.

- During the logging sink creation, you can also define additional log
  filters to exclude specific logs. To filter logs, supply the optional
  parameter `--log-filter=``<LOG_FILTER>`

      gcloud logging sinks create <SINK_NAME> pubsub.googleapis.com/projects/<PROJECT_ID>/topics/<TOPIC_NAME> --log-filter=<LOG_FILTER>
                           

  If setup is successful, the console displays a summary of your log
  sink settings:

      Created [https://logging.googleapis.com/v2/projects/PROJECT_ID/sinks/SINK_NAME]. Please remember to grant `serviceAccount:LOGS_SINK_SERVICE_ACCOUNT` \ the Pub/Sub Publisher role on the topic. More information about sinks can be found at /logging/docs/export/configure_export

6.  Grant log sink service account to publish to the new topic.

- Note the `serviceAccount` name from the previous step and use it to
  define the service for which you want to grant publish access.

      gcloud pubsub topics add-iam-policy-binding <TOPIC_NAME> --member serviceAccount:<LOGS_SINK_SERVICE_ACCOUNT> --role=roles/pubsub.publisher

7.  Create a service account.

- For example, use cortex-xdr-sa as the service account name and Cortex
  XSIAM Service Account as the display name.

      gcloud iam service-accounts create <SERVICE_ACCOUNT> --description="<DESCRIPTION>" --display-name="<DISPLAY_NAME>"

8.  Grant the IAM role to the service account.

- gcloud pubsub subscriptions add-iam-policy-binding <SUBSCRIPTION_NAME> --member serviceAccount:<SERVICE_ACCOUNT>@<PROJECT_ID>.iam.gserviceaccount.com --role=roles/pubsub.subscriber

9.  Create a JSON key for the service account.

- You will need the JSON file to enable Cortex XSIAM to authenticate
  with the GCP service. Specify the file destination and filename using
  a .json extension.

      gcloud iam service-accounts keys create <OUTPUT_FILE> --iam-account <SERVICE_ACCOUNT>@<PROJECT_ID>.iam.gserviceaccount.com

10. After Cortex XSIAM begins receiving information from the GCP Pub/Sub
    service, you can use the XQL Query language to search for specific
    data.

###### Ingest logs and data from Google Workspace

Cortex XSIAM can ingest the following types of data from Google
Workspace, where most of the data is collected as audit events from
various Google reports, using the **Google Workspace** data collector.

- Google Chrome

- Admin Console

- Google Chat

- Enterprise Groups

- Login

- Rules

- Google drive

- Token

- User Accounts

- SAML

- Alerts

- Emails---Requires a compliance mailbox to ingest email data (not email
  reports).

  - All message details except email headers and email content
    (`payload.body`, `payload.parts`, and `snippet`).

  - Attachment details, when **Get Attachment Info** is selected,
    includes file name, size, and hash calculation.

The following Google APIs are required to collect the different types of
data from Google Workspace.

- For all data types, except emails: [Admin SDK
  API](https://developers.google.com/admin-sdk).

- For all data types, except alerts and emails: [Admin Reports
  API](https://developers.google.com/admin-sdk/reports/reference/rest)
  (part of Admin SDK API).

<!-- -->

- > **Note**

  > For all types of data collected via the Admin Reports API, except
  > alerts and emails, the log events are collected with a preset lag
  > time as reported by Google Workspace. For more information on these
  > lag times for the different types of data, see [Google Workspace
  > Data retention and lag
  > times](https://support.google.com/a/answer/7061566?hl=en).

<!-- -->

- Alerts require implemention of an additional API: [Alert Center
  API](https://developers.google.com/admin-sdk/alertcenter) (part of
  Admin SDK API).

- Emails require implemention of the [Gmail
  API](https://developers.google.com/gmail/api).

To receive logs from Google Workspace for any of the data types except
emails, you must first enable the Google Workspace Admin SDK API with a
user with access to the Admin SDK Reports API. For emails, you must set
up a compliance email account as explained in the prerequisite steps
below and then enable the Google Workspace Gmail API. Once implemented,
you can then configure the Data Sources settings in Cortex XSIAM. After
you set up data collection, Cortex XSIAM begins receiving new logs and
data from the source.

When Cortex XSIAM begins receiving logs, the app creates a new dataset
for the different types of data that you are collecting, which you can
use to initiate XQL Search queries. For example queries, refer to the
in-app XQL Library. For all logs, Cortex XSIAM can generate Cortex XSIAM
issues for Correlation Rules only, when relevant from Google Workspace
logs.

For the different types of data you can collect using the Google
Workspace data collector, the following table lists the different
datasets, vendors, and products automatically configured, and whether
the data is normalized.

  ------------------------------------------------------------------------------------------------
  Data type         Dataset                                    Vendor            Product
  ----------------- ------------------------------------------ ----------------- -----------------
  Google Chrome     `google_workspace_chrome_raw`              Google            Workspace Chrome

  Admin console     `google_workspace_admin_console_raw`       Google            Workspace Admin
                                                                                 Console

  Google Chat       `google_workspace_chat_raw`                Google            Workspace Chat

  Enterprise groups `google_workspace_enterprise_groups_raw`   Google            Workspace
                                                                                 Enterprise Groups

  Login             `google_workspace_login_raw`               Google            Workspace Login

  Rules             `google_workspace_rules_raw`               Google            Workspace Rules

  Google Drive      `google_workspace_drive_raw`               Google            Workspace Drive

  Token             `google_workspace_token_raw`               Google            Workspace Token

  User accounts     `google_workspace_user_accounts_raw`       Google            Workspace User
                                                                                 Accounts

  SAML              `google_workspace_saml_raw`                Google            Workspace SAML

  Alerts            `google_workspace_alerts_raw`              Google            Workspace Alerts

  Emails            `google_gmail_raw`                         Google            Gmail
  ------------------------------------------------------------------------------------------------

**Prerequisite Steps**

Be sure you do the following tasks before you begin configuring data
collection from Google Workspace using the instructions detailed below.

- When configuring data collection for all data types except emails, you
  need to complete the Google Workspace Reports API Prerequisites to set
  up the Google Workspace Admin SDK environment. This entails completing
  the instructions for **Set up the basics** and
  **Set up a Google API Console project** *without* activating the
  Reports API service as this will be explained in greater detail in the
  task below. For more information on these Google Workspace
  prerequisite steps, see [Reports API
  Prerequisites](https://developers.google.com/admin-sdk/reports/v1/guides/prerequisites).

- When you only want to collect Google Workspace alerts without
  configuring any other data types, you need to set up a [Cloud Platform
  project](https://developers.google.com/admin-sdk/alertcenter/quickstart/java).

- Before you can collect Google emails, you need to set up the
  following:

  1.  A compliance email account.

  2.  The organization's Google Workspace account administrator can now
      set up a BCC to this compliance email account for all incoming and
      outgoing emails of any user in the organization.

      1.  Login to the [Admin direct routing
          URL](https://admin.google.com/ac/apps/gmail/routing) in Google
          Workspace for the user account that you want to configure.

      2.  Double-click **Routing**, and set the following parameters in
          the **Add setting** dialog.

          - **Routing**: Configure the compliance email account that you
            want to receive a BCC for emails from this user account
            using the format `BCC TO <compliance email>`. For example,
            `BCC TO admin@organization.com`.

          - Select **Inbound** and **Outbound** to ensure all incoming
            and outgoing emails are sent.

          - (Optional) To configure another email address to receive a
            BCC for emails from this account, select
            **Add more recipients** in the **Also deliver to** section,
            and then click **Add**.

          - Click **Show options**, and from the list displayed select
            Account types to affect \> Users.

          - **Save** your changes.

<!-- -->

- This configuration ensures to forward every message sent to a user
  account to a defined compliance mailbox. After the Google Workspace
  data collector ingests the emails, they are deleted from the
  compliance mailbox to prevent email from building up over time
  (nothing touches the actual users' mailboxes).

  > **Note**

  - > Spam emails from the compliance email account, and from all other
    > monitored email accounts, are not collected.

  - > Any draft emails written in the compliance email account are
    > collected by the Google Workspace data collector, and are then
    > deleted even if the email was never sent.

To set up the Google Workspace integration:

1.  Complete the applicable prerequisite steps for the types of data you
    want to collect from Google Workspace.

2.  Log in to your [GCP account](https://console.cloud.google.com/).

3.  Perform Google Workspace Domain-Wide Delegation of Authority when
    collecting any type of data from Google Workspace except Google
    Emails.

- When collecting any type of data from Google Workspace except emails,
  you need to set up Google Workspace enterprise applications to access
  users' data without any manual authorization. This is performed by
  following these steps.

  > **Note**

  > For more information on the entire process, see [Perform Google
  > Workspace Domain-Wide Delegation of
  > Authority](https://developers.google.com/admin-sdk/reports/v1/guides/delegation).

  a.  Enable the Admin SDK API to create a service account and set
      credentials for this service account.

  - As you complete this step, you need to gather information related to
    your service account, including the Client ID, Private key file, and
    Email address, which you will need to use later on in this task.

    1.  Select the menu icon \> APIs & Services \> Library.

    2.  Search for the `Admin SDK API`, and select the API from the
        results list.

    3.  **Enable** the Admin SDK API.

    4.  Select APIs & Services \> Credentials.

    5.  Select + CREATE CREDENTIALS \> Service account.

    6.  Set the following **Service account details** in the applicable
        fields.

        - Specify a service account name. This name is automatically
          used to populate the following field as the service account
          ID, where the name is changed to lowercase letters and all
          spaces are changed to hyphens.

        - Specify the service account ID, where you can either leave the
          default service account ID or add a new one. This service
          account ID is used to set the service account email using the
          following format:
          `<id>@<project name>.iam.gserviceaccount.com`.

        - (*Optional*) Specify a service account description.

    7.  **CREATE AND CONTINUE**.

    8.  (*Optional*) Decide whether you want to
        **Grant this service account access to project** or
        **Grant users access to this service account**.

    9.  Click **Done**.

    10. Select your newly created **Service Account** from the list.

    11. Create a service account private key and download the private
        key file as a JSON file.

    - In the **Keys** tab, select ADD KEY \> Create new key, leave the
      default **Key type** set to **JSON**, and **CREATE** the private
      key. Once you've downloaded the new private key pair to your
      machine, ensure that you store it in a secure location, because
      it's the only copy of this key. You will need to browse to this
      JSON file when configuring the Google Workplace data collector in
      Cortex XSIAM.

  b.  When collecting alerts, enable the Alert Center API to create a
      service account and set credentials for this service account.

  - > **Note**

    > When collecting Google Workspace alerts with other types of data,
    > except emails, you need to configure a service account in Google
    > with the applicable permissions to collect events from the Google
    > Reports API and alerts from the Alert Center API. If you prefer to
    > use different service accounts to collect events and alerts
    > separately, you\'ll need to create two service accounts with
    > different instances of the Google Workspace data collector. One
    > instance to collect events with a certain service account, and
    > another instance to collect alerts using another service account.
    > The instructions below explain how to set up one Google Workspace
    > instance to collect both event and alerts.

    1.  Select the menu icon \> APIs & Services \> Library.

    2.  Search for the `Alert Center API`, and select the API from the
        results list.

    3.  **Enable** the Alert Center API.

    4.  Select APIs & Services \> Credentials.

    5.  Select the same service account in the **Service Accounts**
        section that you created for the Admin SDK API above.

  c.  Delegate domain-wide authority to your service account with the
      Admin Reports API and Alert Center API scopes.

      1.  Open the [Google Admin Console](https://admin.google.com).

      2.  Select Security \> Access and data control \> API controls.

      3.  Scroll down to the **Domain wide delegation** section, and
          select **MANAGE DOMAIN WIDE DELEGATION**.

      4.  Click **Add new**.

      5.  Set the following settings to define permissions for the Admin
          SDK API.

          - **Client ID**: Specify the service account's **Unique ID**,
            which you can obtain from the [Service accounts
            page](https://console.cloud.google.com/iam-admin/serviceaccounts)
            by clicking the email of the service account to view further
            details. When creating a single Google Workspace data
            collector instance to collect both events and alert data,
            provide the same service account ID as the Admin SDK API.

          - In the **OAuth scopes (comma-delimited)** field, paste in
            the first of the two Admin Reports API scopes:
            `https://www.googleapis.com/auth/admin.reports.audit.readonly`

          - In the following **OAuth scopes (comma-delimited)** field,
            paste in the second Admin Reports API scope:
            `https://www.googleapis.com/auth/admin.reports.usage.readonly`

          <!-- -->

          - > **Note**

            > For more information on the Admin Reports API scopes, see
            > [OAuth 2.0 Scopes for Google
            > APIs](https://developers.google.com/identity/protocols/oauth2/scopes).

          <!-- -->

          - When collecting alerts, add the following Alert Center API
            scope: `https://www.googleapis.com/auth/apps.alerts`

      6.  **Authorize** the domain-wide authority to your service
          account.

      - This ensures that your service account now has domain-wide
        access to the Google Admin SDK Reports API and Google Workspace
        Alert Center API, if configured, for all of the users of your
        domain.

4.  Enable the Gmail API to collect Google emails.

- When you are configuring the Google Workspace data collector to
  collect Google emails, the instruction differ depending on whether you
  are configuring the collection along with other types of data with the
  Admin SDK API already set up or you are configuring the collection to
  only include emails using only the Gmail API. The steps below explain
  both scenarios.

  a.  Select the menu icon \> APIs & Services \> Library.

  b.  Search for the `Gmail API`, and select the API from the results
      list.

  c.  **Enable** the Gmail API.

  d.  Select APIs & Services \> Credentials.

  - The instructions for setting up credentials differ depending on
    whether you are setting up the Gmail API together with the Admin SDK
    API as you are collecting other data types, or you are configuring
    collection for emails only with the Gmail API.

    - When you've already set up the Admin SDK API, verify that the same
      [Service Account](#X8971cdff326e05d92ae5196071f767552dec3c1) that
      you configured for the Admin SDK API is listed, and continue on to
      the next step.

    - When you're only collecting Google emails without the Admin SDK
      API, complete these steps.

      1.  Select + CREATE CREDENTIALS \> Service account.

      2.  Set the following **Service account details** in the
          applicable fields.

      - -Specify a service account name. This name is automatically used
        to populate the following field as the service account ID, where
        the name is changed to lowercase letters and all spaces are
        changed to hyphens.

        -Specify the service account ID, where you can either leave the
        default service account ID or add a new one. This service
        account ID is used to set the service account email using the
        following format: `<id>@<project name>.iam.gserviceaccount.com`.

        -(*Optional*) Specify a service account description.

      3.  **CREATE AND CONTINUE**.

      4.  (*Optional*) Decide whether you want to
          **Grant this service account access to project** or
          **Grant users access to this service account**.

      5.  Click **Done**.

      6.  Select your newly created **Service Account** from the list.

      7.  Create a service account private key and download the private
          key file as a JSON file.

      - In the **Keys** tab, select ADD KEY \> Create new key, leave the
        default **Key type** set to **JSON**, and **CREATE** the private
        key. Once you've downloaded the new private key pair to your
        machine, ensure that you store it in a secure location as it's
        the only copy of this key. You will need to browse to this JSON
        file when configuring the Google Workplace data collector in
        Cortex XSIAM .

  e.  Delegate domain-wide authority to your service account with the
      Gmail API scopes.

      1.  Open the [Google Admin Console](https://admin.google.com).

      2.  Select Security \> Access and data control \> API controls.

      3.  Scroll down to the **Domain wide delegation** section, and
          select **MANAGE DOMAIN WIDE DELEGATION**.

      - This step explains how the following Gmail API scopes are added.

        - `https://mail.google.com/`

        - `https://www.googleapis.com/auth/gmail.addons.current.action.compose`

        - `https://www.googleapis.com/auth/gmail.addons.current.message.action`

        - `https://www.googleapis.com/auth/gmail.addons.current.message.metadata`

        - `https://www.googleapis.com/auth/gmail.addons.current.message.readonly`

        - `https://www.googleapis.com/auth/gmail.compose`

        - `https://www.googleapis.com/auth/gmail.insert`

        - `https://www.googleapis.com/auth/gmail.labels`

        - `https://www.googleapis.com/auth/gmail.metadata`

        - `https://www.googleapis.com/auth/gmail.modify`

        - `https://www.googleapis.com/auth/gmail.readonly`

        - `https://www.googleapis.com/auth/gmail.send`

        - `https://www.googleapis.com/auth/gmail.settings.basic`

        - `https://www.googleapis.com/auth/gmail.settings.sharing`

        <!-- -->

        - > **Note**

          > For more information on the Gmail API scopes, see [OAuth 2.0
          > Scopes for Google
          > APIs](https://developers.google.com/identity/protocols/oauth2/scopes).

        The instructions differ depending on whether you are setting up
        the Gmail API together with the Admin SDK API as you are
        collecting other data types, or you are configuring collection
        for emails only with the Gmail API.

        - When you've already set up the Admin SDK API, **Edit** the
          same [Service
          Account](#X8971cdff326e05d92ae5196071f767552dec3c1) that you
          configured for the Admin SDK API, and add the Gmail API scopes
          listed above.

        - When you're only collecting Google emails without the Admin
          SDK API, click **Add New**, and set the following settings to
          define permissions for the Admin SDK API.

        <!-- -->

        - \-**Client ID**---Specify the service account's **Unique ID**,
          which you can obtain from the [Service accounts
          page](https://console.cloud.google.com/iam-admin/serviceaccounts)
          by clicking the email of the service account to view further
          details.

          In the **OAuth scopes (comma-delimited)** field, paste in the
          first of the Gmail API scopes listed above, and continue
          adding in the rest of the scopes.

          **Authorize** the domain-wide authority to your service
          account.

          This ensures that your service account now has domain-wide
          access to the Google Gmail API for all of the users of your
          domain.

5.  Prepare your service account to impersonate a user with access to
    the Admin SDK Reports API when collecting any type of data from
    Google Workspace except Google emails.

- Only users with access to the Admin APIs can access the Admin SDK
  Reports API. Therefore, your service account needs to be set up to
  impersonate one of these users to access the Admin SDK Reports API.
  This means that when collecting any type of data from Google Workspace
  except Google emails, you need to designate a user whose **Roles**
  permissions are set to access reports, where Security \> Reports is
  selected. This user's email will be required when configuring the
  Google Workspace data collector in Cortex XSIAM.

  a.  In the [Google Admin Console](https://admin.google.com), select
      Directory \> Users.

  b.  From the list of users listed, select the user configured with the
      necessary permissions in **Admin roles and privileges** to view
      reports, such as a **Super Admin**, that you want to set up your
      service account to impersonate.

  c.  Record the email of this user as you will need it in Cortex XSIAM
      .

6.  In Cortex XSIAM, select Settings \> Data Sources.

7.  On the **Data Sources** page, click **Add Data Source**, search for
    and select **Google Workspace**, and click **Connect**.

8.  Integrate the applicable Google Workspace service with Cortex XSIAM.

    a.  Specify a descriptive **Name** for your log collection
        integration.

    b.  **Browse** to the JSON file containing your [service account
        key](#X8971cdff326e05d92ae5196071f767552dec3c1) **Credentials**
        for the Google Workspace Admin SDK API that you enabled. If
        you're only collecting Google emails, ensure that you **Browse**
        to the JSON file containing your [service account private
        key](#X09bbfaca69aa2c71ec935e2b248337060bd9304) **Credentials**
        for the Gmail API that you enabled.

    c.  Select the types of data that you want to **Collect** from
        Google Workspace.

        - **Google Chrome**: [Chrome browser and Chrome OS
          events](https://developers.google.com/admin-sdk/reports/v1/appendix/activity/chrome)
          included in the Chrome activity reports.

        - **Admin Console**: Account information about different types
          of [administrator activity
          events](https://developers.google.com/admin-sdk/reports/v1/appendix/activity/admin-event-names)
          included in the Admin console application\'s activity reports.

        - **Google Chat**: [Chat activity
          events](https://developers.google.com/admin-sdk/reports/v1/appendix/activity/chat)
          included in the Chat activity reports.

        - **Enterprise Groups**: [Enterprise group activity
          events](https://developers.google.com/admin-sdk/reports/v1/appendix/activity/groups-enterprise)
          included in the Enterprise Groups activity reports.

        - **Login**: Account information about different types of [login
          activity
          events](https://developers.google.com/admin-sdk/reports/v1/appendix/activity/login)
          included in the Login application\'s activity reports.

        - **Rules**: [Rules activity
          events](https://developers.google.com/admin-sdk/reports/v1/appendix/activity/rules)
          included in the Rules activity report.

        - **Google drive**: [Google Drive activity
          events](https://developers.google.com/admin-sdk/reports/v1/appendix/activity/drive)
          included in the Google Drive application\'s activity reports.

        - **Token**: [Token activity
          events](https://developers.google.com/admin-sdk/reports/v1/appendix/activity/token)
          included in the Token application\'s activity reports.

        - **User Accounts**: Account information about different types
          of [User Accounts activity
          events](https://developers.google.com/admin-sdk/reports/v1/appendix/activity/user-accounts)
          included in the User Accounts application\'s activity reports.

        - **SAML**: [SAML activity
          events](https://developers.google.com/admin-sdk/reports/v1/appendix/activity/saml)
          included in the SAML activity report.

        - **Alerts**: Alerts from the [Alert Center API beta
          version](http:// https://developers.google.com/admin-sdk/alertcenter/guides),
          which is still subject to change.

        - **Emails**: Collects email data (not emails reports). All
          message details except email headers and email content
          (`payload.body`, `payload.parts`, and `snippet`).

        <!-- -->

        - > **Note**

          > For more information about the events collected from the
          > various Google Reports, see [Google Workspace Reports API
          > Documentation](https://developers.google.com/admin-sdk/reports/reference/rest/v1/activities/list#ApplicationName).

    - For all options selected, except **Emails**, you must specify the
      **Service Account Email**. This is the email account of the user
      with access to the Admin SDK Reports API that you [prepared your
      service account to
      impersonate](#Xf22fdbfadcfb04090418f944351634cbc091ca3).

      When selecting **Emails**, configure the following.

      - **Audit Email Account**: Specify the email address for the
        [compliance mailbox](#Xfef3c3d80d952899c197254c733c57bc7db26d1)
        that you set up.

      - **Get Attachment Info** from the ingested email, which includes
        file name, size, and hash calculation.

    d.  **Test** the connection settings.

    - To test the connection, you must select one or more log types.
      Cortex XSIAM then tests the connection settings for the selected
      log types.

    e.  If successful, **Enable** Google Workspace log collection.

###### Ingest logs from Microsoft Azure Event Hub

Cortex XSIAM can ingest different types of data from Microsoft Azure
Event Hub using the **Microsoft Azure Event Hub** data collector. To
receive logs from Azure Event Hub, you must configure the Data Sources
settings in Cortex XSIAM based on your Microsoft Azure Event Hub
configuration. After you set up data collection, Cortex XSIAM begins
receiving new logs and data from the source.

When Cortex XSIAM begins receiving logs, the app creates a new dataset
(`MSFT_Azure_raw`) that you can use to initiate XQL Search queries. For
example, queries refer to the in-app XQL Library. For enhanced cloud
protection, you can also configure Cortex XSIAM to normalize Azure Event
Hub audit logs, including Azure Kubernetes Service (AKS) audit logs,
with other Cortex XSIAM authentication stories across all cloud
providers using the same format, which you can query with XQL Search
using the `cloud_audit_logs` dataset. For logs that you do not configure
Cortex XSIAM to normalize, you can change the default dataset. Cortex
XSIAM can also generate Cortex XSIAM issues (Analytics, IOC, BIOC, and
Correlation Rules) when relevant from Azure Event Hub logs. While
Correlation Rules issues are generated on non-normalized and normalized
logs, Analytics, IOC, and BIOC issues are only raised on normalized
logs.

Enhanced cloud protection provides:

- Normalization of cloud logs

- Cloud logs stitching

- Enrichment with cloud data

- Detection based on cloud analytics

- Cloud-tailored investigations

> **Warning**

- > Misconfiguration of Event Hub resources could cause ingestion
  > delays.

- > In an existing Event Hub integration, do not change the mapping to a
  > different Event Hub.

- > Do not use the same Event Hub for more than two purposes.

The following table provides a brief description of the different types
of Azure audit logs you can collect.

> **Note**
>
> For more information on Azure Event Hub audit logs, see [Overview of
> Azure platform
> logs](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/platform-logs-overview).

+-----------------------------------+-----------------------------------+
| Type of data                      | Description                       |
+===================================+===================================+
| Activity logs                     | Retrieves events related to the   |
|                                   | operations on each Azure resource |
|                                   | in the subscription from the      |
|                                   | outside in addition to updates on |
|                                   | Service Health events.            |
|                                   |                                   |
|                                   | > **Note**                        |
|                                   | >                                 |
|                                   | > These logs are from the         |
|                                   | > management plane.               |
+-----------------------------------+-----------------------------------+
| Azure Active Directory (AD)       | Contain the history of sign-in    |
| Activity logs and Azure Sign-in   | activity and audit trail of       |
| logs                              | changes made in Azure AD for a    |
|                                   | particular tenant.                |
|                                   |                                   |
|                                   | > **Note**                        |
|                                   | >                                 |
|                                   | > Even though you can collect     |
|                                   | > Azure AD Activity logs and      |
|                                   | > Azure Sign-in logs using the    |
|                                   | > Azure Event Hub data collector, |
|                                   | > we recommend using the          |
|                                   | > Microsoft Office 365 data       |
|                                   | > collector, because it is easier |
|                                   | > to configure. In addition,      |
|                                   | > ensure that you do not          |
|                                   | > configure both collectors to    |
|                                   | > collect the same types of logs, |
|                                   | > because if you do so, you will  |
|                                   | > be creating duplicate data in   |
|                                   | > Cortex XSIAM.                   |
+-----------------------------------+-----------------------------------+
| Resource logs, including AKS      | Retrieves events related to       |
| audit logs                        | operations that were performed    |
|                                   | within an Azure resource.         |
|                                   |                                   |
|                                   | > **Note**                        |
|                                   | >                                 |
|                                   | > These logs are from the data    |
|                                   | > plane.                          |
+-----------------------------------+-----------------------------------+

> **Note**
>
> If you want to ingest raw Microsoft Defender for Endpoint events, use
> the Microsoft Defender log collector. For more information, see
> [Ingest raw EDR events from Microsoft Defender for
> Endpoint](#UUID6e00ea26bd37150df8776680549e71d5).
>
> **Prerequisite**
>
> Ensure that you do the following tasks before you begin configuring
> data collection from Azure Event Hub.

- > Before you set up an Azure Event Hub, calculate the quantity of data
  > that you expect to send to Cortex XSIAM, taking into account
  > potential data spikes and potential increases in data ingestion,
  > because partitions cannot be modified after creation. Use this
  > information to ascertain the optimal number of partitions and
  > Throughput Units (for Azure Basic or Standard) or Processing Units
  > (for Azure Premium). Configure your Event Hub accordingly.

- > Create an Azure Event Hub. We recommend using a dedicated Azure
  > Event Hub for this Cortex XSIAM integration. For more information,
  > see [Quickstart: Create an event hub using Azure
  > portal](https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-create).

- > Each partition can support a throughput of up to 1 MB/s.

- > Ensure the format for the logs you want collected from the Azure
  > Event Hub is either JSON or raw.

Configure the Azure Event Hub collection in Cortex XSIAM:

1.  In the Microsoft Azure console, open the **Event Hubs** page, and
    select the Azure Event Hub that you created for collection in Cortex
    XSIAM.

2.  Record the following parameters from your configured event hub,
    which you will need when configuring data collection in Cortex
    XSIAM.

    - Your event hub's consumer group.

      1.  Select Entities \> Event Hubs, and select your event hub.

      2.  Select Entities \> Consumer groups, and select your event hub.

      3.  In the Consumer group table, copy the applicable value listed
          in the **Name** column for your Cortex XSIAM data collection
          configuration.

    - Your event hub's connection string for the designated policy.

      1.  Select Settings \> Shared access policies.

      2.  In the Shared access policies table, select the applicable
          policy.

      3.  Copy the **Connection string-primary key**.

    - Your storage account connection string required for partitions
      lease management and checkpointing in Cortex XSIAM.

      1.  Open the **Storage accounts** page, and either create a new
          storage account or select an existing one, which will contain
          the storage account connection string.

      2.  Select Security + networking \> Access keys, and click
          **Show keys**.

      3.  Copy the applicable **Connection string**.

3.  Configure diagnostic settings for the relevant log types you want to
    collect and then direct these diagnostic settings to the designated
    Azure Event Hub.

    a.  Open the Microsoft Azure console.

    b.  Your navigation is dependent on the type of logs you want to
        configure.

+-----------------------------------+---------------------------------------------------------------------------------+
| Log type                          | Navigation path                                                                 |
+===================================+=================================================================================+
| Activity logs                     | Select Azure services \> Activity log \> Export Activity Logs, and              |
|                                   | **+Add diagnostic setting**.                                                    |
+-----------------------------------+---------------------------------------------------------------------------------+
| Azure AD Activity logs and Azure  | 1.  Select Azure services \> Azure Active Directory.                            |
| Sign-in logs                      |                                                                                 |
|                                   | 2.  Select Monitoring \> Diagnostic settings, and **+Add diagnostic setting**.  |
+-----------------------------------+---------------------------------------------------------------------------------+
| Resource logs, including AKS      | 1.  Search for **Monitor**, and select Settings \> Diagnostic settings.         |
| audit logs                        |                                                                                 |
|                                   | 2.  From your list of available resources, select the resource that you want to |
|                                   |     configure for log collection, and then select **+Add diagnostic setting**.  |
|                                   |                                                                                 |
|                                   | - > **Note**                                                                    |
|                                   |                                                                                 |
|                                   |   > For every resource that you want to confiure, you\'ll have to repeat this   |
|                                   |   > step, or use [Azure                                                         |
|                                   |   > policy](https://learn.microsoft.com/en-us/azure/governance/policy/overview) |
|                                   |   > for a general configuration.                                                |
+-----------------------------------+---------------------------------------------------------------------------------+

a.  Set the following parameters:

    - **Diagnostic setting name**: Specify a name for your Diagnostic
      setting.

    - **Logs Categories**/**Metrics**: The options listed are dependent
      on the type of logs you want to configure. For Activity logs and
      Azure AD logs and Azure Sign-in logs, the option is called
      **Logs Categories**, and for Resource logs it\'s called
      **Metrics**.

+-----------------------------------+--------------------------------------+
| Log type                          | Log categories/metrics               |
+===================================+======================================+
| Activity logs                     | Select from the list of applicable   |
|                                   | Activity log categories, the ones    |
|                                   | that you want to configure your      |
|                                   | designated resource to collect. We   |
|                                   | recommend selecting all of the       |
|                                   | options.                             |
|                                   |                                      |
|                                   | - **Administrative**                 |
|                                   |                                      |
|                                   | - **Security**                       |
|                                   |                                      |
|                                   | - **ServiceHealth**                  |
|                                   |                                      |
|                                   | - **Alert**                          |
|                                   |                                      |
|                                   | - **Recommendation**                 |
|                                   |                                      |
|                                   | - **Policy**                         |
|                                   |                                      |
|                                   | - **Autoscale**                      |
|                                   |                                      |
|                                   | - **ResourceHealth**                 |
+-----------------------------------+--------------------------------------+
| Azure AD Activity logs and Azure  | Select from the list of applicable   |
| Sign-in logs                      | Azure AD Activity and Azure Sign-in  |
|                                   | **Logs Categories**, the ones that   |
|                                   | you want to configure your           |
|                                   | designated resource to collect. You  |
|                                   | can select any of the following      |
|                                   | categories to collect these types of |
|                                   | Azure logs.                          |
|                                   |                                      |
|                                   | - Azure AD Activity logs:            |
|                                   |                                      |
|                                   |   - **AuditLogs**                    |
|                                   |                                      |
|                                   | - Azure Sign-in logs:                |
|                                   |                                      |
|                                   |   - **SignInLogs**                   |
|                                   |                                      |
|                                   |   - **NonInteractiveUserSignInLogs** |
|                                   |                                      |
|                                   |   - **ServicePrincipalSignInLogs**   |
|                                   |                                      |
|                                   |   - **ManagedIdentitySignInLogs**    |
|                                   |                                      |
|                                   |   - **ADFSSignInLogs**               |
|                                   |                                      |
|                                   | > **Note**                           |
|                                   | >                                    |
|                                   | > There are additional log           |
|                                   | > categories displayed. We recommend |
|                                   | > selecting all the available        |
|                                   | > options.                           |
+-----------------------------------+--------------------------------------+
| Resource logs, including AKS      | The list displayed is dependent on   |
| audit logs                        | the resource that you selected. We   |
|                                   | recommend selecting all the options  |
|                                   | available for the resource.          |
+-----------------------------------+--------------------------------------+

- **Destination details**: Select **Stream to event hub**, where
  additional parameters are displayed that you need to configure. Ensure
  that you set the following parameters using the same settings for the
  Azure Event Hub that you created for the collection.

  - **Subscription**: Select the applicable **Subscription** for the
    Azure Event Hub.

  - **Event hub namespace**: Select the applicable **Subscription** for
    the Azure Event Hub.

  - (*Optional*) **Event hub name**: Specify the name of your Azure
    Event Hub.

  - **Event hub policy**: Select the applicable **Event hub policy** for
    your Azure Event Hub.

a.  **Save** your settings.

<!-- -->

4.  Configure the Azure Event Hub collection in Cortex XSIAM.

    a.  Select Settings \> Data Sources.

    b.  On the **Data Sources** page, click **Add Data Source**, search
        for and select **Azure Event Hub**, and click **Connect**.

    c.  Set these parameters.

        - **Name**: Specify a descriptive name for your log collection
          configuration.

        - **Event Hub Connection String**: Specify your event hub's
          connection string for the designated policy.

        - **Storage Account Connection String**: Specify your storage
          account's connection string for the designated policy.

        - **Consumer Group**: Specify your event hub's consumer group.

        - **Log Format**: Select the log format for the logs collected
          from the Azure Event Hub as **Raw**, **JSON**, **CEF**,
          **LEEF**, **Cisco-asa**, or **Corelight**.

        <!-- -->

        - > **Note**

          > When you **Normalize and enrich audit logs**, the log format
          > is automatically configured. As a result, the **Log Format**
          > option is removed and is no longer available to configure
          > (default).

          - **CEF** or **LEEF**: The **Vendor** and **Product** defaults
            to **Auto-Detect**.

          <!-- -->

          - > **Note**

            > For a **Log Format** set to **CEF** or **LEEF**, Cortex
            > XSIAM reads events row by row to look for the **Vendor**
            > and **Product** configured in the logs. When the values
            > are populated in the event log row, Cortex XSIAM uses
            > these values even if you specified a value in the
            > **Vendor** and **Product** fields in the Azure Event Hub
            > data collector settings. Yet, when the values are blank in
            > the event log row, Cortex XSIAM uses the **Vendor** and
            > **Product** that you specified in the Azure Event Hub data
            > collector settings. If you did not specify a **Vendor** or
            > **Product** in the Azure Event Hub data collector
            > settings, and the values are blank in the event log row,
            > the values for both fields are set to **unknown**.

          <!-- -->

          - **Cisco-asa**: The following fields are automatically set
            and not configurable.

            - **Vendor**: **Cisco**

            - **Product**: **ASA**

          <!-- -->

          - Cisco data can be queried in XQL Search using the
            `cisco_asa_raw` dataset.

          <!-- -->

          - **Corelight**: The following fields are automatically set
            and not configurable.

            - **Vendor**: **Corelight**

            - **Product**: **Zeek**

          <!-- -->

          - Corelight data can be queried in XQL Search using the
            `corelight_zeek_raw` dataset.

          <!-- -->

          - **Raw** or **JSON**: The following fields are automatically
            set and are configurable.

            - **Vendor**: **Msft**

            - **Product**: **Azure**

          <!-- -->

          - Raw or JSON data can be queried in XQL Search using the
            `msft_azure_raw` dataset.

        <!-- -->

        - **Vendor** and **Product**: Specify the **Vendor** and
          **Product** for the type of logs you are ingesting.

        <!-- -->

        - The **Vendor** and **Product** are used to define the name of
          your Cortex Query Language (XQL) dataset
          (`<vendor>_<product>_raw`). The **Vendor** and **Product**
          values vary depending on the **Log Format** selected. To
          uniquely identify the log source, consider changing the values
          if the values are configurable.

          > **Note**

          > When you **Normalize and enrich audit logs**, the **Vendor**
          > and **Product** fields are automatically configured, so
          > these fields are removed as available options (default).

        <!-- -->

        - **Normalize and enrich audit logs**: (Optional) For enhanced
          cloud protection, you can **Normalize and enrich audit logs**
          by selecting the checkbox (default). If selected, Cortex XSIAM
          normalizes and enriches Azure Event Hub audit logs with other
          Cortex XSIAM authentication stories across all cloud providers
          using the same format. You can query this normalized data with
          XQL Search using the `cloud_audit_logs` dataset.

    d.  Click **Test** to validate access, and then click **Enable**.

    - When events start to come in, a green check mark appears
      underneath the **Azure Event Hub** configuration with the amount
      of data received.

###### Ingest logs from Microsoft Office 365

> **Note**

- > Ingesting Microsoft Entra ID (formerly known as Azure AD)
  > authentication and audit events from Microsoft Graph API requires a
  > Microsoft Azure Premium 1 or Premium 2 license. Alternatively, if
  > the directory type is Azure AD B2C, the sign-in reports are
  > accessible through the API without any additional license
  > requirement.

- > To ingest **email** logs and data from Microsoft Office 365, use the
  > dedicated data collector. For more information, see [Ingest logs and
  > data from Microsoft 365](#UUIDd0bd27c38c86c7e6ef7b65788c7fe549).

Cortex XSIAM can ingest the following logs and data from Microsoft
Office 365 Management Activity API and Microsoft Graph API using the
**Office 365** data collector. Alerts are collected with a delay of 5
minutes. If your organization requires collection that is closer to
real-time collection, we recommend using the Microsoft Azure Event Hub
integration instead.

To ingest email logs and data from Microsoft Office 365, use the
dedicated data collector. For more information, see [Ingest logs and
data from Microsoft 365](#UUIDd0bd27c38c86c7e6ef7b65788c7fe549).

- Microsoft Office 365 audit events from Management Activity API, which
  provides information about various user, administrator, system, and
  policy actions and events from Office 365, Microsoft Entra ID
  (formerly known as Azure AD) and MDO activity logs.

<!-- -->

- > **Note**

  > When auditing is turned off from the default setting, you need to
  > first [turn on
  > auditing](https://learn.microsoft.com/en-us/microsoft-365/compliance/turn-audit-log-search-on-or-off?view=o365-worldwide#verify-the-auditing-status-for-your-organization)
  > for your organization to collect Microsoft Office 365 audit events
  > from the Management Activity API. Log duplication of up to 5% in
  > Microsoft products is considered normal. In some cases, such as
  > login to a portal using MFA, two log entries are recorded by design.

<!-- -->

- Microsoft Entra ID (Azure AD) authentication and audit events from
  Microsoft Graph API.

<!-- -->

- When collecting Azure AD Authentication Logs, Cortex XSIAM also
  collects by default all sign-in event types from a beta version of
  Microsoft Graph API, which is still subject to change. In addition to
  classic interactive user sign-ins, selecting this option allows you to
  collect.

  - Non-interactive user sign-ins.

  - Service principal sign-ins.

  - Managed Identities for Azure resource sign-ins.

  > **Note**

  > To address [Azure reporting
  > latency](https://docs.microsoft.com/en-us/azure/active-directory/reports-monitoring/reference-reports-latencies),
  > there is a 10-minute latency period for Cortex XSIAM to receive
  > Azure AD logs.

<!-- -->

- Microsoft 365 alerts from Microsoft Graph Security API are available
  for different products.

  - Microsoft Graph Security API v1: Alerts from the following products
    are available via the Microsoft Graph Security API v1:

    - Microsoft Defender for Cloud, Azure Active Directory Identity
      Protection, Microsoft Defender for Cloud Apps, Microsoft Defender
      for Endpoint, Microsoft Defender for Identity, Microsoft 365,
      Azure Information Protection, and Azure Sentinel.

  - Microsoft Graph Security API v2: Alerts (alerts_v2) from the
    following products are available via the Microsoft Graph Security
    API v2 beta version, which is still subject to change:

    - Microsoft 365 Defender unified alerts API, which serves alerts
      from Microsoft 365 Defender, Microsoft Defender for Endpoint,
      Microsoft Defender for Office 365, Microsoft Defender for
      Identity, Microsoft Defender for Cloud Apps, and Microsoft Purview
      Data Loss Prevention (including any future new signals integrated
      into M365D).

  <!-- -->

  - > **Note**

    > You can also implement the corresponding Cortex Data Model (XDM)
    > mappings for these Microsoft Graph Security API v2 alerts using
    > [Cortex Marketplace](#UUIDbd7bb72a06838e5b74c7b3b159596497) via
    > the Microsoft Graph Security content pack.

<!-- -->

- To view alerts from the various products via the Microsoft Graph
  Security API versions, you need to ensure that you\'ve set up the
  applicable licenses in Office 365. The table below lists the various
  licenses required for the different Microsoft Defender products. For
  more information on other Microsoft product licenses, see the
  Microsoft documentation.

  ---------------------------------------------------------------------------------
  Product      Standalone  E3 license    E3 +    E5 license     E5          E5
                license                Security              Security   Compliance
                                        add-on               license     license
                                       license                         
  ----------- ------------ ---------- ---------- ---------- ---------- ------------
  Microsoft        ✓           ✓          ✓         ---        ---         ---
  Defender                                                             
  for                                                                  
  Endpoint                                                             
  Plan 1                                                               

  Microsoft       ---         ---         ✓          ✓          ✓          ---
  Defender                                                             
  for                                                                  
  Endpoint                                                             
  Plan 2                                                               

  Microsoft       ---         ---         ✓          ✓          ✓          ---
  Defender                                                             
  for                                                                  
  Identity                                                             

  Microsoft        ✓          ---        ---        ---        ---         ---
  Defender                                                             
  for Office                                                           
  365 Plan 1                                                           

  Microsoft        ✓          ---         ✓          ✓          ✓          ---
  Defender                                                             
  for Office                                                           
  365 Plan 2                                                           

  Microsoft       ---         ---         ✓          ✓          ✓           ✓
  Defender                                                             
  for Cloud                                                            
  Apps                                                                 
  ---------------------------------------------------------------------------------

> **Note**
>
> For more information, see the [Office 365 Management Activity API
> schema](https://docs.microsoft.com/en-us/office/office-365-management-api/office-365-management-activity-api-schema).

To receive logs from Microsoft Office 365, you must first configure the
Data Sources settings in Cortex XSIAM. After you set up data collection,
Cortex XSIAM begins receiving new logs and data from the source.

When Cortex XSIAM begins receiving logs, the app creates a new dataset
for the different types of logs and data that you are collecting, which
you can use to initiate XQL Search queries. For example queries, refer
to the in-app XQL Library. For all Microsoft Office 365 logs, Cortex
XSIAM can also generate Cortex XSIAM issues (Analytics, IOC, BIOC, and
Correlation Rules), when relevant, from Office 365 logs. While
Correlation Rules issues are generated on non-normalized and normalized
logs, Analytics, IOC, and BIOC issues are only generated on normalized
logs.

For the different types of data you can collect using the Office 365
data collector, the following table lists the different datasets,
vendors, and products automatically configured, and whether the data is
normalized.

+-----------------+-----------------------------------+-----------------+--------------------------+
| Data type       | Dataset                           | Vendor          | Product                  |
+=================+===================================+=================+==========================+
| Microsoft       |                                   |                 |                          |
| Office 365      |                                   |                 |                          |
| audit events    |                                   |                 |                          |
| from Management |                                   |                 |                          |
| Activity API    |                                   |                 |                          |
+-----------------+-----------------------------------+-----------------+--------------------------+
| - Microsoft     | `msft_o365_azure_ad_raw`          | `msft`          | `O365 Azure AD`          |
|   Entra ID      |                                   |                 |                          |
|   (Azure AD)    |                                   |                 |                          |
+-----------------+-----------------------------------+-----------------+--------------------------+
| - Exchange      | `msft_o365_exchange_online_raw`   | `msft`          | `O365 Exchange Online`   |
|   Online        |                                   |                 |                          |
+-----------------+-----------------------------------+-----------------+--------------------------+
| - SharePoint    | `msft_o365_sharepoint_online_raw` | `msft`          | `O365 Sharepoint Online` |
|   Online        |                                   |                 |                          |
+-----------------+-----------------------------------+-----------------+--------------------------+
| - DLP           | `msft_o365_dlp_raw`               | `msft`          | `O365 DLP`               |
+-----------------+-----------------------------------+-----------------+--------------------------+
| - General       | `msft_o365_general_raw`           | `msft`          | `O365 General`           |
+-----------------+-----------------------------------+-----------------+--------------------------+
| Microsoft Entra | `msft_azure_ad_raw`               | `msft`          | `Azure AD`               |
| ID (Azure AD)   |                                   |                 |                          |
| authentication  |                                   |                 |                          |
| events from     |                                   |                 |                          |
| Microsoft Graph |                                   |                 |                          |
| API             |                                   |                 |                          |
+-----------------+-----------------------------------+-----------------+--------------------------+
| Microsoft Entra | `msft_azure_ad_audit_raw`         | `msft`          | `Azure AD Audit`         |
| ID (Azure AD)   |                                   |                 |                          |
| audit events    |                                   |                 |                          |
| from Microsoft  |                                   |                 |                          |
| Graph API       |                                   |                 |                          |
+-----------------+-----------------------------------+-----------------+--------------------------+
| Alerts from     | `msft_graph_security_alerts_raw`  | `msft`          | `Security Alerts`        |
| Microsoft Graph |                                   |                 |                          |
| Security API v1 |                                   |                 |                          |
| and v2          |                                   |                 |                          |
+-----------------+-----------------------------------+-----------------+--------------------------+

\***Note**: For the `saas_audit_logs` dataset, the Vendor is **saas**
and Product is **Audit Logs**.

> **Note**
>
> In FedRAMP environments, Azure sign-in logs are not supported, due to
> vendor technical constraints.

To set up the Office 365 integration:

1.  From the Microsoft Entra ID console (formerly Azure AD console),
    create an app for Cortex XSIAM with the applicable API permissions
    for the logs and data you want to collect as detailed in the
    following table.

+-----------------------------------+-----------------------------------+
| Log type and data                 | API/Permission name               |
+===================================+===================================+
| Microsoft Office 365 audit events |                                   |
| from Management Activity API      |                                   |
+-----------------------------------+-----------------------------------+
| -Azure AD                         | Office 365 Management APIs \>     |
|                                   | ActivityFeed.Read                 |
+-----------------------------------+-----------------------------------+
| -Exchange Online                  | Office 365 Management APIs \>     |
|                                   | ActivityFeed.Read                 |
+-----------------------------------+-----------------------------------+
| -Sharepoint Online                | Office 365 Management APIs \>     |
|                                   | ActivityFeed.Read                 |
+-----------------------------------+-----------------------------------+
| -DLP                              | Office 365 Management APIs \>     |
|                                   | ActivityFeed.ReadDlp              |
+-----------------------------------+-----------------------------------+
| -General                          | Office 365 Management APIs \>     |
|                                   | ActivityFeed.Read                 |
+-----------------------------------+-----------------------------------+
| Azure AD authentication and audit | - Microsoft Graph \>              |
| events from Microsoft Graph API   |   AuditLog.Read.All               |
|                                   |                                   |
|                                   | - Microsoft Graph \>              |
|                                   |   Directory.Read.All              |
+-----------------------------------+-----------------------------------+
| Alerts from Microsoft Graph       | - Microsoft Graph \>              |
| Security API v1 and v2            |   SecurityAlert.Read.All          |
|                                   |                                   |
|                                   | - Microsoft Graph \>              |
|                                   |   SecurityEvents.Read.All         |
+-----------------------------------+-----------------------------------+

- For more information on Microsoft Azure, see the following
  instructions in the Microsoft documentation portal.

  - [Register an
    app](https://docs.microsoft.com/en-us/azure/active-directory/develop/quickstart-register-app).

  - [Add API permissions with type
    Application](https://docs.microsoft.com/en-us/azure/active-directory/develop/quickstart-configure-app-access-web-apis#add-permissions-to-access-web-apis).

  - [Create an application
    secret](https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal#create-a-new-application-secret).

2.  In Cortex XSIAM, select Settings \> Data Sources.

3.  On the **Data Sources** page, click **Add Data Source**, search for
    and select **Office 365**, and click **Connect**.

4.  Integrate the applicable Microsoft Entra ID (Azure AD) service with
    Cortex XSIAM.

    a.  Specify the **Tenant Domain** of your Microsoft Entra ID tenant.

    b.  Obtain the **Application Client ID** and **Secret** for your
        Microsoft Entra ID (Azure AD) service from the Microsoft Entra
        ID console, and specify the values in Cortex XSIAM.

    - These values enable Cortex XSIAM to authenticate with your
      Microsoft Entra ID (Azure AD) service.

    c.  Select the types of logs that you want to receive from Office
        365.

    - The following options are available.

      - **Office 365 Management Activity API**

        - **Cloud Environment**: select the cloud environment used by
          your organization:

          - **Enterprise**: Default option for non-US Government tenants

          - **GCC**: US Government Compliant Cloud tenants

          - **GCC High**: US Government Compliant Cloud High tenants

          - **DoD**: US Department of Defense tenants

        - **Azure AD**: Includes subset of Azure AD audit events and
          Azure AD authentication events. There can be significant
          overlap between these and the **Azure AD Authentication Logs**
          originating from Microsoft Graph API.

        <!-- -->

        - > **Note**

          > Use this option when you don't want to grant permissions for
          > Azure AD Authentication and Azure AD Audit.

        <!-- -->

        - **Exchange Online**: Includes audit logs on [Azure Exchange
          mailboxes](https://docs.microsoft.com/en-us/office/office-365-management-api/office-365-management-activity-api-schema#exchange-mailbox-schema)
          and [Exchange admin
          activities](https://docs.microsoft.com/en-us/office/office-365-management-api/office-365-management-activity-api-schema#exchange-admin-schema)
          on the Office 365 Exchange.

        - **Sharepoint Online**: Includes audit events on Sharepoint and
          OneDrive activities.

        - **DLP**: Includes Microsoft 365 DLP events for Exchange,
          Sharepoint, and OneDrive.

        - **General**: Includes audit logs for [various Microsoft 365
          applications](https://docs.microsoft.com/en-us/office/office-365-management-api/office-365-management-activity-api-schema),
          such as Power BI and Microsoft Forms.

      - **Microsoft Graph API**

        - **Cloud Environment**: select the cloud environment used by
          your organization:

          - **Global Service**: Default option for non-US Government
            tenants

          - **Government L4**: US Government Layer 4 tenants

          - **Government L5 (DOD)**: US Government Layer 5 tenants

        - **Azure AD Authentication Logs** and
          **Collect all sign-in event types**: [Azure AD Sign-in
          logs](https://docs.microsoft.com/en-us/azure/active-directory/reports-monitoring/concept-sign-ins)
          includes by default all sign-in event types from a beta
          version of Microsoft Graph API, which is still subject to
          change. In addition to classic interactive user sign-ins,
          selecting the **Collect all sign-in event types** allows you
          to collect.

        <!-- -->

        - -Non-interactive user sign-ins.

          -Service principal sign-ins.

          -Managed Identities for Azure resource sign-ins.

        <!-- -->

        - **Azure AD Audit Logs**: [Azure AD Audit
          logs](https://docs.microsoft.com/en-us/azure/active-directory/reports-monitoring/concept-audit-logs)
          includes different categories, such as User Management, Group
          Management and Application Management.

        - **Alerts**: When this checkbox is selected, alerts from the
          following products are collected via the Microsoft Graph
          Security API v1:

          - Microsoft Defender for Cloud, Azure Active Directory
            Identity Protection, Microsoft Defender for Cloud Apps,
            Microsoft Defender for Endpoint, Microsoft Defender for
            Identity, Microsoft 365, Azure Information Protection, and
            Azure Sentinel.

          - **Use Microsoft Graph API v2**: When this checkbox is also
            selected, alerts (alerts_v2) from the following products are
            only collected via the Microsoft Graph Security API v2 beta
            version, which is still subject to change:

            - Microsoft 365 Defender unified alerts API, which serves
              alerts from Microsoft 365 Defender, Microsoft Defender for
              Endpoint, Microsoft Defender for Office 365, Microsoft
              Defender for Identity, Microsoft Defender for Cloud Apps,
              and Microsoft Purview Data Loss Prevention (including any
              future new signals integrated into M365D).

        - **Emails**: Deprecated. Use the dedicated email collector
          instead. For more information, see [Ingest logs and data from
          Microsoft 365](#UUIDd0bd27c38c86c7e6ef7b65788c7fe549).

    d.  Click **Test** to test the connection settings.

    - To test the connection, you must select one or more log types.
      Cortex XSIAM then tests the connection settings for the selected
      log types.

    e.  If successful, click **Enable** to enable Office 365 log
        collection.

###### Ingest logs and data from Microsoft 365

The Microsoft 365 email collector fetches email metadata through
Microsoft Graph API, using an authorized app. A compliance mailbox is
not required.

> **Note**
>
> Email data: The subject, body, and attachments are visible only with
> an Email Security module license.
>
> **Note**
>
> For other logs from Microsoft Office 365, use the Office 365 data
> collector. For more information, see [Ingest logs from Microsoft
> Office 365](#UUID44e507825fe7595361178c99286a2b4b).
>
> **Prerequisite**

- > A user account with the Microsoft Azure Account Administrator role
  > is required to set up a new Microsoft 365 email collector.

- > The following Microsoft Graph API permissions are required:

  - > Mailbox access (read-write)

    - > Read and write mail in all mailboxes

    - > Read contacts in all mailboxes

    - > Read all user mailbox settings

  - > User information, groups, and directory data (read-only)

    - > Read directory data

    - > Read all groups

    - > Read all users\' full profiles

You can narrow down the scope of ingested mailboxes by:

- Microsoft 365 Group

- Distribution List

- Mail-enabled Security Group

- Mail-enabled Users

**Datasets**

The Microsoft 365 collector ingests data into the following datasets:

- `msft_o365_emails_raw`

- `msft_o365_users_raw`

- `msft_o365_groups_raw`

- `msft_o365_devices_raw`

- `msft_o365_mailboxes_raw`

- `msft_o365_rules_raw`

- `msft_o365_contacts_raw`

- `msft_o365_protected_emails_raw` - requires the Email Security module
  license.

- `o365_email_threat_submission_policies` - requires the Email Security
  module license.

**Data Encryption**

Cortex XSIAM stores email metadata as plain text, and encrypts emails\'
subject and body. The email body is saved for 48 hours, and then
deleted. Analytical detectors analyze raw and encrypted email data, and
when necessary, create issues. When an issue with severity higher than
or equal to Medium is created for a malicious email, the raw email,
including its subject and body (decrypted), is attached to the issue as
an artifact. Therefore, you will not be able to perform threat hunting
based on email subject and body. Only email metadata such as date, From,
or To, are available for threat hunting purposes.

####### Configure ingestion into Cortex XSIAM

1.  In Settings \> Data Sources, click **Add Data Source**, search for
    and select **Microsoft 365**, and click **Connect**.

2.  In the wizard that opens, ensure that you have configured the items
    listed on the **Permissions** page, and then click **Next**.

3.  To confirm that you know that API authorization consent is required,
    click **OK**.

4.  Select the Microsoft account from which you want to collect email
    data.

5.  Click **Next**.

6.  Enter your password for the Microsoft account, and click
    **Sign in**.

7.  If you are asked to perform authentication using your
    organization\'s authentication tools, do so.

8.  For the list of of permissions that Cortex Email Security requires,
    click **Accept**.

9.  On the **Scope** page, select one of the following:

    - **Entire organization**: Emails will be collected from all
      mailboxes in your organization.

    - **Specific groups**: Enter the email addresses of group names,
      such as Microsoft 365 Groups, Mail-enabled Security Groups,
      Distribution Lists, or Mail-enabled Users.

10. Click **Next**.

11. On the **Details** page, enter a meaningful instance name, and click
    **Next**.

12. On the **Summary** page, check your configurations, and then click
    **Create**.

After data starts to come in, a green check mark appears below the
**Microsoft 365** configuration, along with the amount of data received.

###### Ingest logs and data from Okta

To receive logs and data from Okta, you must configure the Data Sources
settings in Cortex XSIAM. After you set up data collection, Cortex XSIAM
immediately begins receiving new logs and data from the source. The
information from Okta is then searchable in XQL Search using the
`okta_sso_raw` dataset. In addition, depending on the event type, data
is normalized to either `xdr_data` or `saas_audit_logs` datasets.

You can collect all types of events from Okta. When setting up the Okta
data collector in Cortex XSIAM, a field called **Okta Filter** is
available to configure collection for events of your choosing. All
events are collected by default unless you define an Okta API Filter
expression for collecting the data, such as
`filter=eventType eq “user.session.start”.\n`. For Okta information to
be woven into authentication stories, `“user.authentication.sso”` events
must be collected.

The Okta API enforces concurrent rate limits. The Okta data collector is
built with a mechanism which reduces the amount of requests whenever an
error is received from the Okta API indicating that too many requests
have already been sent. In addition, to ensure you are properly notified
about this, an alert is displayed in the **Notification Area** and a
record is added to the **Management Audit Logs**.

Before you begin configuring data collection from Okta, ensure your Okta
user has administrator privileges with a role that can create API
tokens, such as the read-only administrator, Super administrator, and
Organization administrator. For more information, see the [Okta
Administrators
Documentation](https://help.okta.com/en-us/Content/Topics/Security/Administrators.htm?cshid=ext_Security_Administrators).

To configure the Okta collection in Cortex XSIAM:

1.  Identify the domain name of your Okta service.

- From the Dashboard of your Okta console, note your **Org URL**.

  For more information, see the [Okta
  Documentation](https://developer.okta.com/docs/guides/find-your-domain/findorg/).

  ![](media/rId5454.png){width="5.833333333333333in"
  height="2.253124453193351in"}

2.  Obtain your authentication token in Okta.

    a.  Select API \> Tokens.

    b.  **Create Token** and record the token value.

    - This is your only opportunity to record the value.

3.  Select Settings \> Data Sources.

4.  On the **Data Sources** page, click **Add Data Source**, search for
    and select **Okta**, and click **Connect**.

5.  Integrate the Okta authentication service with Cortex XSIAM.

    a.  Specify the **OKTA DOMAIN** (Org URL) that you identified on
        your Okta console.

    b.  Specify the **TOKEN** used to authenticate with Okta.

    c.  Specify the **Okta Filter** to configure collection for events
        of your choosing. **All events** are collected by default unless
        you define an Okta API Filter expression for collecting the
        data, such as `filter=eventType eq “user.session.start”.\n`. For
        Okta information to be weaved into authentication stories,
        `“user.authentication.sso”` events must be collected.

    d.  **Test** the connection settings.

    e.  If successful, **Enable** Okta log collection.

    - Once events start to come in, a green check mark appears
      underneath the **Okta** configuration with the amount of data
      received.

6.  After Cortex XSIAM begins receiving information from the service,
    you can Create an XQL Query to search for specific data. When
    including authentication events, you can also Create an
    Authentication Query to search for specific authentication data.

###### Ingest logs and data from OneLogin

Cortex XSIAM can ingest different types of data from OneLogin accounts
using the OneLogin data collector.

To receive logs and data from OneLogin via the OneLogin REST APIs, you
must configure the Data Sources settings in Cortex XSIAM based on your
OneLogin credentials. After you set up data collection, Cortex XSIAM
begins receiving new logs and data from the source.

When Cortex XSIAM begins receiving logs, the app creates a new dataset
for the different types of data collected and normalizes the ingested
data into authentication stories, where specific relevant events are
collected in the `authentication_story` preset for the `xdr_data`
dataset. You can search these datasets using XQL Search queries. For all
logs, Cortex XSIAM can generate Cortex XSIAM issues (Analytics,
Correlation Rules, IOC, and BIOC), when relevant from OneLogin logs.
While Correlation Rules issues are generated on non-normalized and
normalized logs, Analytics, IOC, and BIOC issues are only generated on
normalized logs.

The following table provides a description of the different types of
data you can collect, the collection method and fetch interval for the
data collected, and the name of the dataset to use in Cortex Query
Language (XQL) queries.

  -----------------------------------------------------------------------------------------
  Data type            Description      Collection     Fetch interval Dataset name
                                        method                        
  -------------------- ---------------- -------------- -------------- ---------------------
  **Log collection**                                                  

  Events               User logins,     Appends data   30 seconds     onelogin_events_raw
                       administrative                                 
                       operations,                                    
                       provisioning,                                  
                       and a list of                                  
                       all OneLogin                                   
                       event types                                    

  **Directory**                                                       

  Users                Lists of users   Overwrites     10 minutes     onelogin_users_raw
                                        data                          

  Groups               Lists of groups  Overwrites     10 minutes     onelogin_groups_raw
                                        data                          

  Apps                 Lists of apps    Overwrites     10 minutes     onelogin_apps_raw
                                        data                          
  -----------------------------------------------------------------------------------------

Before you configure Cortex XSIAM data collection from OneLogin, make
sure you have the following.

- An Advanced OneLogin account.

- Owner or administrator permissions in your OneLogin account which
  enable Cortex XSIAM to access the OneLogin account and generate the
  OAuth 2.0 access token.

- A Cortex XSIAM user account with permissions to Read Log Collections,
  for example an Instance Administrator.

Configure Cortex XSIAM to receive logs and data from OneLogin.

1.  Log in to OneLogin as an account owner or administrator.

2.  Under Administration \> Developers \> API Credentials, [Create a New
    Credential](https://developers.onelogin.com/api-docs/1/getting-started/working-with-api-credentials)
    with scope **Read All**.

3.  In the credential details page, copy the Client ID and the Client
    Secret, and save them somewhere safe. You will need to provide these
    keys when you configure the OneLogin data collector in Cortex XSIAM
    .

4.  Select Settings \> Data Sources.

5.  On the **Data Sources** page, click **Add Data Source**, search for
    and select **OneLogin**, and click **Connect**.

6.  Configure the following parameters.

    - **Domain**: Specify the domain of the OneLogin instance. The
      domain name must be in the format
      `https://<subdomain-name>.onelogin.com`.

    - **Name**: Specify a descriptive and unique name for the
      configuration.

    - **Client ID**: Specify the Client ID for the OneLogin API
      credential pair.

    - **Secret**: Specify the Client Secret for the OneLogin API
      credential pair.

    - **Collect**: Select the types of data to collect. By default, all
      the options are selected.

      - Log Collection

        - **Events**: Retrieves user logins, administrative operations,
          provisioning, and OneLogin event types. After normalization,
          the event types are enriched with the event name and
          description.

      <!-- -->

      - > **Note**

        > Event data is collected every 30 seconds.

      <!-- -->

      - Directory

        - **Users**: Retrieves lists of users.

        - **Groups**: Retrieves lists of groups.

        - **Apps**: Retrieves lists of apps.

      <!-- -->

      - > **Note**

        > Inventory data snapshots are collected every 10 minutes.

7.  **Test** the connection settings. If successful, **Enable** the
    OneLogin log collection.

- When events start to come in, a green check mark appears underneath
  the OneLogin configuration.

###### Ingest authentication logs from PingFederate

To receive authentication logs from PingFederate, you must first write
Audit and Provisioner Audit Logs to CEF in PingFederate and then set up
a Syslog Collector in Cortex XSIAM to receive the logs. After you set up
log collection, Cortex XSIAM immediately begins receiving new
authentication logs from the source. Cortex XSIAM creates a dataset
named `ping_identity_pingfederate_raw`. Logs from PingFederate are
searchable in Cortex Query Language (XQL) queries using the dataset and
surfaced, when relevant, in authentication stories.

1.  [Activate the Syslog
    Collector](#UUIDd7d85d4a97df7ec2a052d22376f5a52e).

2.  Set up PingFederate to write logs in CEF.

- To set up the integration, you must have an account for the
  PingFederate management dashboard and access to create a subscription
  for SSO logs.

  In your PingFederate deployment, [write audit logs in
  CEF](https://docs.pingidentity.com/bundle/pingfederate-102/page/obk1564002980895.html).
  During this set up you will need the IP address and port you
  configured in the Syslog Collector.

3.  To search for specific authentication logs or data, you can Create
    an Authentication Query or use the XQL Search.

###### Ingest authentication logs and data from PingOne

To receive authentication logs and data from PingOne for Enterprise, you
must first set up a Poll subscription in PingOne and then configure the
Collection Integrations settings in Cortex XSIAM. After you set up
collection integration, Cortex XSIAM immediately begins receiving new
authentication logs and data from the source. These logs and data are
then searchable in Cortex XSIAM.

1.  Set up PingOne for Enterprise to send logs and data.

- To set up the integration, you must have an account for the PingOne
  management dashboard and access to create a subscription for SSO logs.

  From the PingOne Dashboard:

  a.  [Set up a Poll
      subscription](https://docs.pingidentity.com/bundle/pingone/page/stz1564020498800.html).

      1.  Select Reporting \> Subscriptions \> Add Subscription.

      2.  Enter a **NAME** for the subscription.

      3.  Select **Poll** as the subscription type.

      4.  Leave the remaining defaults and select **Done**.

  b.  Identify your account ID and subscription ID.

      1.  Select the subscription you just set up and note the part of
          the poll URL between /reports/ and /poll-subscriptions. This
          is your PingOne account ID.

      - For example:

        `https://admin-api.pingone.com/v3/reports/1234567890asdfghjk-123456-zxcvbn/poll-subscriptions/***-0912348765-4567-98012***/events`

        In this URL, the account ID is
        `1234567890asdfghjk-123456-zxcvbn`.

      2.  Next, note the part of the poll URL between
          /poll-subscriptions/ and /events. This is your subscription
          ID.

      - In the example above, the subscription ID is
        `***-0912348765-4567-98012***`.

2.  Select Settings \> Data Sources.

3.  On the **Data Sources** page, click **Add Data Source**, search for
    and select **PingOne**, and click **Connect**.

4.  Connect Cortex XSIAM to your PingOne for Enterprise authentication
    service.

    a.  Enter your PingOne **ACCOUNT ID**.

    b.  Enter your PingOne **SUBSCRIPTION ID**.

    c.  Enter your PingOne **USER NAME**.

    d.  Enter your PingOne **PASSWORD**.

    e.  **Test** the connection settings.

    f.  If successful, **Enable** PingOne authentication log collection.

- After configuration is complete, Cortex XSIAM begins receiving
  information from the authentication service. From the **Integrations**
  page, you can view the log collection summary.

5.  To search for specific authentication logs or data, you can Create
    an Authentication Query or Create an XQL Query.

##### Ingest operation and system logs from cloud providers

You can ingest operation and system logs from supported cloud providers
into Cortex XSIAM.

###### Ingest generic logs from Amazon S3

> **Note**
>
> Requires the Cortex Cloud Runtime Security or Data Collection add-on.

You can forward generic logs for the relative service to Cortex XSIAM
from Amazon S3.

To receive generic data from Amazon Simple Storage Service (Amazon S3),
you must first configure data collection from Amazon S3. You can then
configure the Data Sources settings in Cortex XSIAM for Amazon S3. After
you set up collection integration, Cortex XSIAM begins receiving new
logs and data from the source.

> **Note**
>
> For more information on configuring data collection from Amazon S3,
> see the Amazon S3 Documentation.

When Cortex XSIAM begins receiving logs, the app automatically creates
an Amazon S3 Cortex Query Language (XQL) dataset
(`<Vendor>_<Product>_raw`). This enables you to search the logs using
XQL Search with the dataset. For example queries, refer to the in-app
XQL Library. Cortex XSIAM can also generate Cortex XSIAM issues
(Correlation Rules only), when relevant, from Amazon S3 logs.

> **Note**
>
> You need to set up an Amazon S3 data collector to receive generic logs
> when collecting logs from BeyondTrust Privilege Management Cloud. For
> more information, see [Ingest logs from BeyondTrust Privilege
> Management Cloud](#UUID2d0e630f249722aa4aa3b711b5604d70).
>
> **Note**
>
> If you want to ingest raw EDR events from SentinelOne DeepVisibility,
> use the SentinelOne DeepVisibility log collector. For more
> information, see [Ingest raw EDR events from SentinelOne
> DeepVisibility](#UUIDddd95331f5b1ac2572e08780ca22d5ba).

**Prerequisites**

Perform the following tasks before you begin configuring data collection
from Amazon S3:

- Create a dedicated Amazon S3 bucket, which collects the generic logs
  that you want capture. For more information, see [Creating a bucket
  using the Amazon S3
  Console](https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html).

<!-- -->

- > **Note**

  > It is the customer's responsibility to define a retention policy for
  > your Amazon S3 bucket by creating a **Lifecycle rule** in the
  > **Management** tab. We recommend setting the retention policy to at
  > least 7 days to ensure that the data is retrieved under all
  > circumstances.

<!-- -->

- The logs collected by your dedicated Amazon S3 bucket must adhere to
  the following guidelines.

  - Each log file must use the 1 log per line format.

  <!-- -->

  - By default, multi-line format is not supported. It can only be used
    for `raw` format when you specifically configure your environment
    for that use case.

  <!-- -->

  - The log format must be compressed as gzip or uncompressed.

  - For best performance, we recommend limiting each file size to up to
    50 MB (compressed).

- Ensure that you have at a minimum the following permissions in AWS for
  an Amazon S3 bucket and Amazon Simple Queue Service (SQS).

  - **Amazon S3 bucket**: `GetObject`

  - **SQS**: `ChangeMessageVisibility`, `ReceiveMessage`, and
    `DeleteMessage`.

- Determine how you want to provide access to Cortex XSIAM to your logs
  and perform API operations. You have the following options:

  - Designate an AWS IAM user, where you will need to know the Account
    ID for the user and have the relevant permissions to create an
    access key/id for the relevant IAM user.

  - Create an assumed role in AWS to delegate permissions to a Cortex
    XSIAM AWS service. This role grants Cortex XSIAM access to your flow
    logs. For more information, see [Creating a role to delegate
    permissions to an AWS
    service](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-service.html).
    This is the **Assumed Role** option described in the configure the
    Amazon S3 collection in Cortex XSIAM. For more information on
    creating an assumed role for Cortex XSIAM, see [Create an assumed
    role](#UUIDb11b8a8d194ebbe449559e38629b008d).

<!-- -->

- To collect Amazon S3 logs that use server-side encryption (SSE), the
  user role must have an IAM policy that states that Cortex XSIAM has
  kms:Decrypt permissions. With this permission, Amazon S3 automatically
  detects if a bucket is encrypted and decrypts it. If you want to
  collect encrypted logs from different accounts, you must have the
  decrypt permissions for the user role also in the key policy for the
  master account Key Management Service (KMS). For more information, see
  [Allowing users in other accounts to use a KMS
  key](https://docs.aws.amazon.com/kms/latest/developerguide/key-policy-modifying-external-accounts.html).

Configure Cortex XSIAM to receive generic logs from Amazon S3:

1.  Log in to the [AWS Management
    Console](https://console.aws.amazon.com/).

2.  From the menu bar, ensure that you have selected the correct region
    for your configuration.

3.  Configure an Amazon Simple Queue Service (SQS).

- > **Note**

  > Ensure that you create your Amazon S3 bucket and Amazon SQS queue in
  > the same region.

  a.  In the [Amazon SQS Console](https://console.aws.amazon.com/sqs/),
      click **Create Queue**.

  b.  Configure the following settings, where the default settings
      should be configured unless otherwise indicated.

      - **Type**: Select **Standard** queue (default).

      - **Name**: Specify a descriptive name for your SQS queue.

      - **Configuration** section: Leave the default settings for the
        various fields.

      - Access policy \> Choose method: Select **Advanced** and update
        the Access policy code in the editor window to enable your
        Amazon S3 bucket to publish event notification messages to your
        SQS queue. Use this sample code as a guide for defining the
        `“Statement”` with the following definitions.

      <!-- -->

      - \-`“Resource”`: Leave the automatically generated ARN for the
        SQS queue that is set in the code, which uses the format
        `“arn:sns:Region:account-id:topic-name”`.

        You can retrieve your bucket's ARN by opening the [Amazon S3
        Console](https://console.aws.amazon.com/s3/) in a browser
        window. In the **Buckets** section, select the bucket that you
        created for collecting the Amazon S3 flow logs, click
        **Copy ARN**, and paste the ARN in the field.

        ![](media/rId5412.png){width="5.833333333333333in"
        height="1.5677077865266842in"}

        > **Note**

        > For more information on granting permissions to publish
        > messages to an SQS queue, see [Granting permissions to publish
        > event notification messages to a
        > destination](https://docs.aws.amazon.com/AmazonS3/latest/userguide/grant-destinations-permissions-to-s3.html).

            {
              "Version": "2012-10-17",
              "Statement": [
                {
                  "Effect": "Allow",
                  "Principal": {
                    "Service": "s3.amazonaws.com"
                  },
                  "Action": "SQS:SendMessage",
                  "Resource": "[Leave automatically generated ARN for the SQS queue defined by AWS]",
                  "Condition": {
                    "ArnLike": {
                      "aws:SourceArn": "[ARN of your Amazon S3 bucket]"
                    }
                  }
                }
              ]
            }

      <!-- -->

      - **Dead-letter queue** section: We recommend that you configure a
        queue for sending undeliverable messages by selecting
        **Enabled**, and then in the **Choose queue** field selecting
        the queue to send the messages. You may need to create a new
        queue for this, if you do not already have one set up. For more
        information, see [Amazon SQS dead-letter
        queues](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html).

  c.  Click **Create queue**.

  - Once the SQS is created, a message indicating that the queue was
    successfully configured is displayed at the top of the page.

4.  Configure an event notification to your Amazon SQS whenever a file
    is written to your Amazon S3 bucket.

    a.  Open the [Amazon S3 Console](https://console.aws.amazon.com/s3/)
        and in the **Properties** tab of your Amazon S3 bucket, scroll
        down to the **Event notifications** section, and click
        **Create event notification**.

    b.  Configure the following settings:

        - **Event name**: Specify a descriptive name for your event
          notification containing up to 255 characters.

        - **Prefix**: Do not set a prefix as the Amazon S3 bucket is
          meant to be a dedicated bucket for collecting only network
          flow logs.

        - **Event types**: Select **All object create events** for the
          type of event notifications that you want to receive.

        - **Destination**: Select **SQS queue** to send notifications to
          an SQS queue to be read by a server.

        - **Specify SQS queue**: You can either select
          **Choose from your SQS queues** and then select the
          **SQS queue**, or select **Enter SQS queue ARN** and specify
          the ARN in the **SQS queue** field.

        <!-- -->

        - You can retrieve your SQS queue ARN by opening another
          instance of the AWS Management Console in a browser window,
          and opening the [Amazon SQS
          Console](https://console.aws.amazon.com/sqs/), and selecting
          the Amazon SQS that you created. In the **Details** section,
          under **ARN**, click the copy icon
          (![](media/rId5488.png){width="0.16666666666666666in"
          height="0.20833333333333334in"})), and paste the ARN in the
          field.

          ![](media/rId5491.png){width="5.833333333333333in"
          height="2.216666666666667in"}

    c.  Click **Save changes**.

    - Once the event notification is created, a message indicating that
      the event notification was successfully created is displayed at
      the top of the page.

      > **Note**

      > If your receive an error when trying to save your changes, you
      > should ensure that the permissions are set up correctly.

5.  Configure access keys for the AWS IAM user.

- > **Note**

  - > It is the responsibility of your organization to ensure that the
    > user who performs this task of creating the access key is assigned
    > the relevant permissions. Otherwise, this can cause the process to
    > fail with errors.

  - > Skip this step if you are using an **Assumed Role** for Cortex
    > XSIAM.

  a.  Open the [AWS IAM Console](https://console.aws.amazon.com/iam/),
      and in the navigation pane, select Access management \> Users.

  b.  Select the **User name** of the AWS IAM user.

  c.  Select the **Security credentials** tab, and scroll down to the
      **Access keys** section, and click **Create access key**.

  d.  Click the copy icon () next to the **Access key ID** and
      **Secret access key** keys, where you must click
      **Show secret access key** to see the secret key, and record them
      somewhere safe before closing the window. You will need to provide
      these keys when you edit the Access policy of the SQS queue and
      when setting the **AWS Client ID** and **AWS Client Secret** in
      Cortex XSIAM. If you forget to record the keys and close the
      window, you will need to generate new keys and repeat this
      process.

  - > **Note**

    > For more information, see [Managing access keys for IAM
    > users](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html).

6.  Update the Access policy of your Amazon SQS queue.

- > **Note**

  > Skip this step if you are using an **Assumed Role** for Cortex
  > XSIAM.

  a.  In the [Amazon SQS Console](https://console.aws.amazon.com/sqs/),
      select the SQS queue that you created when you configured an
      Amazon Simple Queue Service (SQS).

  b.  Select the **Access policy** tab, and **Edit** the Access policy
      code in the editor window to enable the IAM user to perform
      operations on the Amazon SQS with permissions to
      `SQS:ChangeMessageVisibility`, `SQS:DeleteMessage`, and
      `SQS:ReceiveMessage`. Use this sample code as a guide for defining
      the `“Sid”: “__receiver_statement”` with the following
      definitions.

      - `“aws:SourceArn”`: Specify the ARN of the AWS IAM user. You can
        retrieve the **User ARN** from the **Security credentials** tab,
        which you accessed when you configured access keys for the AWS
        API user.

      - `“Resource”`: Leave the automatically generated ARN for the SQS
        queue that is set in the code, which uses the format
        `“arn:sns:Region:account-id:topic-name”`.

      <!-- -->

      - > **Note**

        > For more information on granting permissions to publish
        > messages to an SQS queue, see [Granting permissions to publish
        > event notification messages to a
        > destination](https://docs.aws.amazon.com/AmazonS3/latest/userguide/grant-destinations-permissions-to-s3.html).

            {
              "Version": "2012-10-17",
              "Statement": [
                {
                  "Effect": "Allow",
                  "Principal": {
                    "Service": "s3.amazonaws.com"
                  },
                  "Action": "SQS:SendMessage",
                  "Resource": "[Leave automatically generated ARN for the SQS queue defined by AWS]",
                  "Condition": {
                    "ArnLike": {
                      "aws:SourceArn": "[ARN of your Amazon S3 bucket]"
                    }
                  }
                },
               {
                  "Sid": "__receiver_statement",
                  "Effect": "Allow",
                  "Principal": {
                    "AWS": "[Add the ARN for the AWS IAM user]"
                  },
                  "Action": [
                    "SQS:ChangeMessageVisibility",
                    "SQS:DeleteMessage",
                    "SQS:ReceiveMessage"
                  ],
                  "Resource": "[Leave automatically generated ARN for the SQS queue defined by AWS]"
                }
              ]
            }

7.  Configure the Amazon S3 collection in Cortex XSIAM.

    a.  Select Settings \> Data Sources.

    b.  On the **Data Sources** page, click **Add Data Source**, search
        for and select **Amazon S3**, and click **Connect**.

    c.  Set these parameters, where the parameters change depending on
        whether you configured an **Access Key** or **Assumed Role**.

        - To provide access to Cortex XSIAM to your logs and perform API
          operations using a designated AWS IAM user, leave the
          **Access Key** option selected. Otherwise, select
          **Assumed Role**, and ensure that you create an Assumed Role
          for Cortex XSIAM before continuing with these instructions. In
          addition, when you create an Assumed Role for Cortex XSIAM,
          ensure that you edit the policy that defines the permissions
          for the role with the Amazon S3 Bucket ARN and SQS ARN.

        - **SQS URL**: Specify the **SQS URL**, which is the ARN of the
          Amazon SQS that you configured in the AWS Management Console.

        - **Name**: Specify a descriptive name for your log collection
          configuration.

        - When setting an **Access Key**, set these parameters.

          - **AWS Client ID**: Specify the **Access key ID**, which you
            received when you configured access keys for the AWS IAM
            user in AWS.

          - **AWS Client Secret**: Specify the **Secret access key** you
            received when you configured access keys for the AWS IAM
            user in AWS.

        - When setting an **Assumed Role**, set these parameters.

          - **Role ARN**: Specify the **Role ARN** for the Assumed Role
            you created for Cortex XSIAM in AWS.

          - **External Id**: Specify the **External Id** for the Assumed
            Role you created for Cortex XSIAM in AWS.

        - **Log Type**: Select **Generic** to configure your log
          collection to receive generic logs from Amazon S3, which can
          include different types of data, such as file and metadata.
          When selecting this option, the following additional fields
          are displayed.

          - **Log Format**: Select the log format type as **Raw**,
            **JSON**, **CEF**, **LEEF**, **Cisco**, **Corelight**, or
            **Beyondtrust Cloud ECS**.

          <!-- -->

          - > **Note**

            > -The **Vendor** and **Product** defaults to
            > **Auto-Detect** when the **Log Format** is set to **CEF**
            > or **LEEF**.

            > -For a **Log Format** set to **CEF** or **LEEF**, Cortex
            > XSIAM reads events row by row to look for the **Vendor**
            > and **Product** configured in the logs. When the values
            > are populated in the event log row, Cortex XSIAM uses
            > these values even if you specified a value in the
            > **Vendor** and **Product** fields in the Amazon S3 data
            > collector settings. Yet, when the values are blank in the
            > event log row, Cortex XSIAM uses the **Vendor** and
            > **Product** that you specified in these fields in the
            > Amazon S3 data collector settings. If you did not specify
            > a **Vendor** or **Product** in the Amazon S3 data
            > collector settings, and the values are blank in the event
            > log row, the values for both fields are set to
            > **unknown**.

            For a **Log Format** set to **Beyondtrust Cloud ECS**, the
            following fields are automatically set and are not
            configurable:

            \-**Vendor**: **Beyondtrust**

            \-**Product**: **Privilege Management**

            \-**Compression**: **Uncompressed**

            For more information, see [Ingest logs from BeyondTrust
            Privilege Management
            Cloud](#UUID2d0e630f249722aa4aa3b711b5604d70).

            For a **Log Format** set to **Cisco**, the following fields
            are automatically set and not configurable.

            \-**Vendor**: **Cisco**

            \-**Product**: **ASA**

            For a **Log Format** set to **Corelight**, the following
            fields are automatically set and not configurable:

            \-**Vendor**: **Corelight**

            \-**Product**: **Zeek**

            For a **Log Format** set to **Raw** or **JSON**, the
            following fields are automatically set and are configurable.

            \-**Vendor**: **AMAZON**

            \-**Product**: **AWS**

            Cortex XSIAM supports logs in single line format or
            multiline format. For a **JSON** format, multiline logs are
            collected automatically when the **Log Format** is
            configured as **JSON**. When configuring a **Raw** format,
            you must also define the **Multiline Parsing Regex** as
            explained below.

          <!-- -->

          - **Vendor**: (*Optional*) Specify a particular vendor name
            for the Amazon S3 generic data collection, which is used in
            the Amazon S3 XQL dataset `<Vendor>_<Product>_raw` that
            Cortex XSIAM creates as soon as it begins receiving logs.

          - **Product**: (*Optional*) Specify a particular product name
            for the Amazon S3 generic data collection, which is used in
            the Amazon S3 XQL dataset name `<Vendor>_<Product>_raw` that
            Cortex XSIAM creates as soon as it begins receiving logs.

          - **Compression**: Select whether the logs are compressed into
            a **gzip** file or are **uncompressed**.

          - **Multiline Parsing Regex**: (*Optional*) This option is
            only displayed when the **Log Format** is set to **Raw**,
            where you can set the regular expression that identifies
            when the multiline event starts in logs with multilines. It
            is assumed that when a new event begins, the previous one
            has ended.

    d.  Click **Test** to validate access, and then click **Enable**.

    - When events start to come in, a green check mark appears
      underneath the **Amazon S3** configuration with the number of logs
      received.

###### Ingest logs from Amazon CloudWatch

You can forward generic and Elastic Kubernetes Service (EKS) logs to
Cortex XSIAM from Amazon CloudWatch. When forwarding EKS logs, the
following log types are included:

- API Server: Logs pertaining to API requests to the cluster.

- Audit: Logs pertaining to cluster access via the Kubernetes API.

- Authenticator: Logs pertaining to authentication requests into the
  cluster.

- Scheduler: Logs pertaining to scheduling decisions.

- Controller Manager: Logs pertaining to the state of cluster
  controllers.

You can ingest generic logs of the raw data or in a JSON format from
Amazon Kinesis Firehose. EKS logs are automatically ingested in a JSON
format from Amazon Kinesis Firehose. To enable log forwarding, you set
up Amazon Kinesis Firehose and then add that to your Amazon CloudWatch
configuration. After you complete the set up process, logs from the
respective service are then searchable in Cortex XSIAM to provide
additional information and context to your investigations.

As soon as Cortex XSIAM begins receiving logs, the application
automatically creates one of the following Cortex Query Language (XQL)
datasets depending on the type of logs you\'ve configured:

- Generic: `<Vendor>_<Product>_raw`

- EKS: `amazon_eks_raw`

These datasets enable you to search the logs in XQL Search. For example,
queries refer to the in-app XQL Library. For enhanced cloud protection,
you can also configure Cortex XSIAM to normalize EKS audit logs, which
you can query with XQL Search using the `cloud_audit_logs` dataset.
Cortex XSIAM can also generate Cortex XSIAM issues (Analytics, IOC,
BIOC, and Correlation Rules) when relevant from AWS logs. While
Correlation Rules issues are generated on non-normalized and normalized
logs, Analytics, IOC, and BIOC issues are only generated on normalized
logs.

Enhanced cloud protection provides the following:

- Normalization of cloud logs

- Cloud logs stitching

- Enrichment with cloud data

- Detection based on cloud analytics

- Cloud-tailored investigations

To set up Amazon CloudWatch integration, you require certain permissions
in AWS. You need a role that enables access to configuring Amazon
Kinesis Firehose.

1.  Set up the Amazon CloudWatch integration in Cortex XSIAM.

    a.  Select Settings \> Data Sources.

    b.  On the **Data Sources** page, click **Add Data Source**, search
        for and select **Amazon CloudWatch**, and click **Connect**.

    c.  Specify a descriptive **Name** for your log collection
        configuration.

    d.  Select the **Log Type** as one of the following, where your
        selection changes the options displayed:

        - **Generic**: When selecting this log type, you can configure
          the following settings:

          - **Log Format**: Choose the format of the data input source
            (CloudWatch) that you\'ll export to Cortex XSIAM , either
            **JSON** or **Raw**.

          - Specify the **Vendor** and **Product** for the type of
            generic logs you are ingesting.

          <!-- -->

          - The vendor and product are used to define the name of your
            XQL dataset (`<Vendor>_<Product>_raw`). If you do not define
            a vendor or product, Cortex XSIAM uses the default values of
            Amazon and AWS with the resulting dataset name as
            `amazon_aws_raw`. To uniquely identify the log source,
            consider changing the values.

        - **EKS**: When selecting this log type, the following options
          are displayed:

          - The **Vendor** is automatically set to **Amazon** and
            **Product** to **EKS** , and is non-configurable. This means
            that all data for the EKS logs, whether it\'s normalized or
            not, can be queried in XQL Search using the `amazon_eks_raw`
            dataset.

          - (Optional) You can decide whether to
            **Normalize and enrich audit logs** as part of the enhanced
            cloud protection by selecting the checkbox (default). If
            selected, Cortex XSIAM is configured to normalize EKS audit
            logs, which you can query with XQL Search using the
            `cloud_audit_logs` dataset.

    e.  **Save & Generate Token**.

    - Click the copy icon next to the key and record it somewhere safe.
      You will need to provide this key when you set up output settings
      in AWS Kinesis Firehose. If you forget to record the key and close
      the window you will need to generate a new key and repeat this
      process.

    f.  Select **Done** to close the window.

2.  Create a Kinesis Data Firehose delivery stream to your chosen
    destination.

    a.  Log in to the AWS Management Console, and open the [Kinesis
        console](https://console.aws.amazon.com/kinesis).

    b.  Select Data Firehose \> Create delivery stream.

    - ![](media/rId5563.png){width="5.833333333333333in"
      height="1.2833333333333334in"}

    c.  Define the name and source for your stream.

        - **Delivery stream name**: Enter a descriptive name for your
          stream configuration.

        - **Source**: Select **Direct PUT or other sources**.

        - **Server-side encryption for source records in the delivery stream**:
          Ensure this option is disabled.

    - Click **Next** to proceed to the process record configuration.

    d.  Define the process records.

        - Transform source records with AWS Lambda: Set the Data
          Transformation as **Disabled**.

        - Convert record format: Set Record format conversion as
          **Disabled**.

    - Click **Next** to proceed to the destination configuration.

    e.  Choose a destination for the logs.

    - Choose **HTTP Endpoint** as the destination and configure the HTTP
      endpoint configuration settings:

      - **HTTP endpoint name**: Specify the name you used to identify
        your AWS log collection configuration in Cortex XSIAM.

      - **HTTP endpoint URL**: Copy the API URL associated with your log
        collection from the Cortex XSIAM management console. The URL
        will include your tenant name
        (`https://api-<tenant external URL>/logs/v1/aws)`.

      - **Access key**: Paste in the token key you recorded earlier
        during the configuration of your Cortex XSIAM log collection
        settings.

      - **Content encoding**: Select **GZIP**. Disabling content
        encoding may result in high egress costs.

      - **Retry duration**: Enter **300** seconds.

      - S3 bucket: Set the **S3 backup mode** as **Failed data only**.
        For the S3 bucket, we recommend that you create a dedicated
        bucket for Cortex XSIAM integration.

      Click **Next** to proceed to the settings configuration.

    f.  Configure additional settings.

        - HTTP endpoint buffer conditions: Set the **Buffer size** as
          **1** MiB and the **Buffer interval** as **60** seconds.

        - S3 buffer conditions: Use the default settings for
          **Buffer size** as **5** MiB and **Buffer interval** as
          **300** seconds unless you have alternative sizing
          preferences.

        - S3 compression and encryption: Choose your desired compression
          and encryption settings.

        - **Error logging**: Select **Enabled**.

        - **Permissions**: **Create or update IAM role** option.

    - Select **Next**.

    g.  Review your configuration and **Create delivery stream**.

    - When your delivery stream is ready, the status changes from
      Creating to Active.

3.  To begin forwarding logs, add the Kinesis Firehose instance to your
    Amazon CloudWatch configuration.

- To do this, [add a subscription filter for Amazon Kinesis
  Firehose](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html#FirehoseExample).

4.  Verify the status of the integration.

- Return to the **Integrations** page and view the statistics for the
  log collection configuration.

5.  After Cortex XSIAM begins receiving logs from your Amazon services,
    you can use the XQL Search to search for logs in the new dataset.

###### Ingest logs and data from a GCP Pub/Sub

If you use the Pub/Sub messaging service from Global Cloud Platform
(GCP), you can send logs and data from your GCP instance to Cortex
XSIAM. Data from GCP is then searchable in Cortex XSIAM to provide
additional information and context to your investigations using the GCP
Cortex Query Language (XQL) dataset, which is dependent on the type of
GCP logs collected. For example queries, refer to the in-app XQL
Library. You can configure a Google Cloud Platform collector to receive
generic, flow, audit, or Google Cloud DNS logs. When configuring generic
logs, you can receive logs in a Raw, JSON, CEF, LEEF, Cisco, or
Corelight format.

You can also configure Cortex XSIAM to normalize different GCP logs as
part of the enhanced cloud protection, which you can query with XQL
Search using the applicable dataset. Cortex XSIAM can also generate
Cortex XSIAM issues (Analytics, IOC, BIOC, and Correlation Rules), when
relevant, from GCP logs. While Correlation Rules isssues are generated
on non-normalized and normalized logs, Analytics, IOC, and BIOC issues
are only raised on normalized logs.

Enhanced cloud protection provides the following:

- Normalization of cloud logs

- Cloud logs stitching

- Enrichment with cloud data

- Detection based on cloud analytics

- Cloud-tailored investigations

The following table lists the various GCP log types the XQL datasets you
can use to query in XQL Search:

+-----------------------+------------------------------+-----------------------+
| GCP log type          | Dataset                      | Dataset with          |
|                       |                              | normalized data       |
+=======================+==============================+=======================+
| Audit logs, including | `google_cloud_logging_raw`   | `cloud_audit_logs`    |
| Google Kubernetes     |                              |                       |
| Engine (GKE) audit    |                              |                       |
| logs                  |                              |                       |
+-----------------------+------------------------------+-----------------------+
| Generic logs          | Log Format types:            | N/A                   |
|                       |                              |                       |
|                       | - **CEF** or `LEEF`:         |                       |
|                       |   Automatically detected     |                       |
|                       |   from either the logs or    |                       |
|                       |   the user\'s input in the   |                       |
|                       |   User Interface.            |                       |
|                       |                              |                       |
|                       | - **Cisco**: `cisco_asa_raw` |                       |
|                       |                              |                       |
|                       | - **Corelight**:             |                       |
|                       |   `corelight_zeek_raw`       |                       |
|                       |                              |                       |
|                       | - **JSON or Raw**:           |                       |
|                       |   `google_cloud_logging_raw` |                       |
+-----------------------+------------------------------+-----------------------+
| Google Cloud DNS logs | `google_dns_raw`             | `xdr_data`: Once      |
|                       |                              | configured, Cortex    |
|                       |                              | XSIAM ingests Google  |
|                       |                              | Cloud DNS logs as XDR |
|                       |                              | network connection    |
|                       |                              | stories, which you    |
|                       |                              | can query with XQL    |
|                       |                              | Search using the      |
|                       |                              | `xdr_data` dataset    |
|                       |                              | with the preset       |
|                       |                              | called                |
|                       |                              | `network_story`.      |
+-----------------------+------------------------------+-----------------------+
| Network flow logs     | `google_cloud_logging_raw`   | `xdr_data`: Once      |
|                       |                              | configured, Cortex    |
|                       |                              | XSIAM ingests network |
|                       |                              | flow logs as XDR      |
|                       |                              | network connection    |
|                       |                              | stories, which you    |
|                       |                              | can query with XQL    |
|                       |                              | Search using the      |
|                       |                              | `xdr_data` dataset    |
|                       |                              | with the preset       |
|                       |                              | called                |
|                       |                              | `network_story`.      |
+-----------------------+------------------------------+-----------------------+

> **Note**
>
> When collecting flow logs, we recommend that you include GKE
> annotations in your logs, which enable you to view the names of the
> containers that communicated with each other. GKE annotations are only
> included in logs if appended manually using the custom metadata
> configuration in GCP. For more information, see [VPC Flow Logs
> Overview](https://cloud.google.com/vpc/docs/flow-logs#customizing_metadata_fields).
> In addition, to customize metadata fields, you must use the gcloud
> command-line interface or the API. For more information, see [Using
> VPC Flow
> Logs](https://cloud.google.com/vpc/docs/using-flow-logs#enabling_vpc_flow_logging_for_an_existing_subnet).

To receive logs and data from GCP, you must first set up log forwarding
using a Pub/Sub topic in GCP. You can configure GCP settings using
either the GCP web interface or a GCP cloud shell terminal. After you
set up your service account in GCP, you configure the Data Collection
settings in Cortex XSIAM. The setup process requires the subscription
name and authentication key from your GCP instance.

After you set up log collection, Cortex XSIAM immediately begins
receiving new logs and data from GCP.

####### Set up log forwarding using the GCP web interface

1.  In Cortex XSIAM, set up Data Collection.

    a.  Select Settings \> Data Sources.

    b.  On the **Data Sources** page, click **Add Data Source**, search
        for and select **Google Cloud Platform**, and click **Connect**.

    c.  Specify the **Subscription Name** that you previously noted or
        copied.

    d.  Browse to the JSON file containing your authentication key for
        the service account.

    e.  Select the **Log Type** as one of the following, where your
        selection changes the options displayed.

        - **Flow or Audit Logs**: When selecting this log type, you can
          decide whether to normalize and enrich the logs as part of the
          enhanced cloud protection.

          - (*Optional*) You can
            **Normalize and enrich flow and audit logs** by selecting
            the checkbox (default). If selected, Cortex XSIAM ingests
            the network flow logs as Cortex XSIAM network connection
            stories, which you can query using XQL Search from the
            `xdr_dataset` dataset with the preset called
            `network_story`. In addition, you can configure Cortex XSIAM
            to normalize GCP audit logs, which you can query with XQL
            Search using the `cloud_audit_logs` dataset.

          - The **Vendor** is automatically set to **Google** and
            **Product** to **Cloud Logging**, which is not configurable.
            This means that all GCP data for the flow and audit logs,
            whether it\'s normalized or not, can be queried in XQL
            Search using the `google_cloud_logging_raw` dataset.

        - **Generic**: When selecting this log type, you can configure
          the following settings.

          - **Log Format**: Select the log format type as **Raw**,
            **JSON**, **CEF**, **LEEF**, **Cisco**, or **Corelight**.

            - **CEF** or **LEEF**: The **Vendor** and **Product**
              defaults to **Auto-Detect**.

            <!-- -->

            - > **Note**

              > For a **Log Format** set to **CEF** or **LEEF**, Cortex
              > XSIAM reads events row by row to look for the **Vendor**
              > and **Product** configured in the logs. When the values
              > are populated in the event log row, Cortex XSIAM uses
              > these values even if you specified a value in the
              > **Vendor** and **Product** fields in the GCP data
              > collector settings. Yet, when the values are blank in
              > the event log row, Cortex XSIAM uses the **Vendor** and
              > **Product** that you specified in the GCP data collector
              > settings. If you did not specify a **Vendor** or
              > **Product** in the GCP data collector settings, and the
              > values are blank in the event log row, the values for
              > both fields are set to **unknown**.

            <!-- -->

            - **Cisco**: The following fields are automatically set and
              not configurable.

              - **Vendor**: **Cisco**

              - **Product**: **ASA**

            <!-- -->

            - Cisco data can be queried in XQL Search using the
              `cisco_asa_raw` dataset.

            <!-- -->

            - **Corelight**: The following fields are automatically set
              and not configurable.

              - **Vendor**: **Corelight**

              - **Product**: **Zeek**

            <!-- -->

            - Corelight data can be queried in XQL Search using the
              `corelight_zeek_raw` dataset.

            <!-- -->

            - **Raw** or **JSON**: The following fields are
              automatically set and are configurable.

              - **Vendor**: **Google**

              - **Product**: **Cloud Logging**

            <!-- -->

            - Raw or JSON data can be queried in XQL Search using the
              `google_cloud_logging_raw` dataset.

              Cortex XSIAM supports logs in single line format or
              multiline format. For a **JSON** format, multiline logs
              are collected automatically when the **Log Format** is
              configured as **JSON**. When configuring a **Raw** format,
              you must also define the **Multiline Parsing Regex** as
              explained below.

          - **Vendor**: (*Optional*) Specify a particular vendor name
            for the GCP generic data collection, which is used in the
            GCP XQL dataset `<Vendor>_<Product>_raw` that Cortex XSIAM
            creates as soon as it begins receiving logs.

          - **Product**: (*Optional*) Specify a particular product name
            for the GCP generic data collection, which is used in the
            GCP XQL dataset name `<Vendor>_<Product>_raw` that Cortex
            XSIAM creates as soon as it begins receiving logs.

          - **Multiline Parsing Regex**: (*Optional*) This option is
            only displayed when the **Log Format** is set to **Raw**,
            where you can set the regular expression that identifies
            when the multiline event starts in logs with multilines. It
            is assumed that when a new event begins, the previous one
            has ended.

        - **Google Cloud DNS**: When selecting this log type, you can
          configure whether to normalize the logs as part of the
          enhanced cloud protection.

          - *Optional*) You can **Normalize DNS logs** by selecting the
            checkbox (default). If selected, Cortex XSIAM ingests the
            Google Cloud DNS logs as Cortex XSIAM network connection
            stories, which you can query using XQL Search from the
            `xdr_dataset` dataset with the preset called
            `network_story`.

          - The **Vendor** is automatically set to **Google** and
            **Product** to **DNS** , which is not configurable. This
            means that all Google Cloud DNS logs, whether it\'s
            normalized or not, can be queried in XQL Search using the
            `google_dns_raw` dataset.

    f.  **Test** the provided settings and, if successful, proceed to
        **Enable** log collection.

<!-- -->

1.  Log in to your GCP account.

2.  Set up log forwarding from GCP to Cortex XSIAM.

    a.  Select Logging \> Logs Router.

    b.  Select Create Sink \> Cloud Pub/Sub topic, and then click
        **Next**.

    c.  To filter only specific types of data, select the filter or
        desired resource.

    d.  In the **Edit Sink** configuration, define a descriptive
        **Sink Name**.

    e.  Select Sink Destination \> Create new Cloud Pub/Sub topic.

    f.  Enter a descriptive **Name** that identifies the sink purpose
        for Cortex XSIAM, and then **Create**.

    g.  **Create Sink** and then **Close** when finished.

3.  Create a subscription for your Pub/Sub topic.

    a.  Select the menu icon in G Cloud, and then select Pub/Sub \>
        Topics.

    b.  Select the name of the topic you created in the previous steps.
        Use the filters if necessary.

    c.  Select Create Subscription \> Create subscription.

    d.  Enter a unique Subscription ID.

    e.  Choose **Pull** as the **Delivery Type**.

    f.  **Create** the subscription.

    - After the subscription is set up, G Cloud displays statistics and
      settings for the service.

    g.  In the subscription details, identify and note your
        **Subscription Name**.

    - Optionally, use the copy button to copy the name to the clipboard.
      You will need the name when you configure Collection in Cortex
      XSIAM.

4.  Create a service account and authentication key.

- You will use the key to enable Cortex XSIAM to authenticate with the
  subscription service.

  a.  Select the menu icon, and then select IAM & Admin \> Service
      Accounts.

  b.  **Create Service Account**.

  c.  Enter a **Service account name** and then **Create**.

  d.  Select a role for the account: Pub/Sub \> Pub/Sub Subscriber.

  e.  Click Continue \> Done.

  f.  Locate the service account by name, using the filters to refine
      the results, if needed.

  g.  Click the **Actions** menu identified by the three dots in the row
      for the service account and then **Create Key**.

  h.  Select JSON as the key type, and then **Create**.

  - After you create the service account key, G Cloud automatically
    downloads it.

5.  After Cortex XSIAM begins receiving information from the GCP Pub/Sub
    service, you can use the XQL Query language to search for specific
    data.

####### Set up log forwarding using the GCP cloud shell terminal

1.  In Cortex XSIAM, set up Data Collection.

    a.  Select Settings \> Data Sources.

    b.  On the **Data Sources** page, click **Add Data Source**, search
        for and select **Google Cloud Platform**, and click **Connect**.

    c.  Specify the **Subscription Name** that you previously noted or
        copied.

    d.  Browse to the JSON file containing your authentication key for
        the service account.

    e.  Select the **Log Type** as one of the following, where your
        selection changes the options displayed.

        - **Flow or Audit Logs**: When selecting this log type, you can
          decide whether to normalize and enrich the logs as part of the
          enhanced cloud protection.

          - (*Optional*) You can
            **Normalize and enrich flow and audit logs** by selecting
            the checkbox (default). If selected, Cortex XSIAM ingests
            the network flow logs as Cortex XSIAM network connection
            stories, which you can query using XQL Search from the
            `xdr_dataset` dataset with the preset called
            `network_story`. In addition, you can configure Cortex XSIAM
            to normalize GCP audit logs, which you can query with XQL
            Search using the `cloud_audit_logs` dataset.

          - The **Vendor** is automatically set to **Google** and
            **Product** to **Cloud Logging**, which is not configurable.
            This means that all GCP data for the flow and audit logs,
            whether it\'s normalized or not, can be queried in XQL
            Search using the `google_cloud_logging_raw` dataset.

        - **Generic**: When selecting this log type, you can configure
          the following settings.

          - **Log Format**: Select the log format type as **Raw**,
            **JSON**, **CEF**, **LEEF**, **Cisco**, or **Corelight**.

            - **CEF** or **LEEF**: The **Vendor** and **Product**
              defaults to **Auto-Detect**.

            <!-- -->

            - > **Note**

              > For a **Log Format** set to **CEF** or **LEEF**, Cortex
              > XSIAM reads events row by row to look for the **Vendor**
              > and **Product** configured in the logs. When the values
              > are populated in the event log row, Cortex XSIAM uses
              > these values even if you specified a value in the
              > **Vendor** and **Product** fields in the GCP data
              > collector settings. Yet, when the values are blank in
              > the event log row, Cortex XSIAM uses the **Vendor** and
              > **Product** that you specified in the GCP data collector
              > settings. If you did not specify a **Vendor** or
              > **Product** in the GCP data collector settings, and the
              > values are blank in the event log row, the values for
              > both fields are set to **unknown**.

            <!-- -->

            - **Cisco**: The following fields are automatically set and
              not configurable.

              - **Vendor**: **Cisco**

              - **Product**: **ASA**

            <!-- -->

            - Cisco data can be queried in XQL Search using the
              `cisco_asa_raw` dataset.

            <!-- -->

            - **Corelight**: The following fields are automatically set
              and not configurable.

              - **Vendor**: **Corelight**

              - **Product**: **Zeek**

            <!-- -->

            - Corelight data can be queried in XQL Search using the
              `corelight_zeek_raw` dataset.

            <!-- -->

            - **Raw** or **JSON**: The following fields are
              automatically set and are configurable.

              - **Vendor**: **Google**

              - **Product**: **Cloud Logging**

            <!-- -->

            - Raw or JSON data can be queried in XQL Search using the
              `google_cloud_logging_raw` dataset.

              Cortex XSIAM supports logs in single line format or
              multiline format. For a **JSON** format, multiline logs
              are collected automatically when the **Log Format** is
              configured as **JSON**. When configuring a **Raw** format,
              you must also define the **Multiline Parsing Regex** as
              explained below.

          - **Vendor**: (*Optional*) Specify a particular vendor name
            for the GCP generic data collection, which is used in the
            GCP XQL dataset `<Vendor>_<Product>_raw` that Cortex XSIAM
            creates as soon as it begins receiving logs.

          - **Product**: (*Optional*) Specify a particular product name
            for the GCP generic data collection, which is used in the
            GCP XQL dataset name `<Vendor>_<Product>_raw` that Cortex
            XSIAM creates as soon as it begins receiving logs.

          - **Multiline Parsing Regex**: (*Optional*) This option is
            only displayed when the **Log Format** is set to **Raw**,
            where you can set the regular expression that identifies
            when the multiline event starts in logs with multilines. It
            is assumed that when a new event begins, the previous one
            has ended.

        - **Google Cloud DNS**: When selecting this log type, you can
          configure whether to normalize the logs as part of the
          enhanced cloud protection.

          - *Optional*) You can **Normalize DNS logs** by selecting the
            checkbox (default). If selected, Cortex XSIAM ingests the
            Google Cloud DNS logs as Cortex XSIAM network connection
            stories, which you can query using XQL Search from the
            `xdr_dataset` dataset with the preset called
            `network_story`.

          - The **Vendor** is automatically set to **Google** and
            **Product** to **DNS** , which is not configurable. This
            means that all Google Cloud DNS logs, whether it\'s
            normalized or not, can be queried in XQL Search using the
            `google_dns_raw` dataset.

    f.  **Test** the provided settings and, if successful, proceed to
        **Enable** log collection.

<!-- -->

1.  Launch the GCP cloud shell terminal or use your preferred shell with
    gcloud installed.

- ![](media/rId5500.png){width="5.833333333333333in"
  height="1.8302077865266841in"}

2.  Define your project ID.

- gcloud config set project <PROJECT_ID>
                           

3.  Create a Pub/Sub topic.

- gcloud pubsub topics create <TOPIC_NAME>
                           

4.  Create a subscription for this topic.

- gcloud pubsub subscriptions create <SUBSCRIPTION_NAME> --topic=<TOPIC_NAME>
                           

  Note the subscription name you define in this step as you will need it
  to set up log ingestion from Cortex XSIAM.

5.  Create a logging sink.

- During the logging sink creation, you can also define additional log
  filters to exclude specific logs. To filter logs, supply the optional
  parameter `--log-filter=``<LOG_FILTER>`

      gcloud logging sinks create <SINK_NAME> pubsub.googleapis.com/projects/<PROJECT_ID>/topics/<TOPIC_NAME> --log-filter=<LOG_FILTER>
                           

  If setup is successful, the console displays a summary of your log
  sink settings:

      Created [https://logging.googleapis.com/v2/projects/PROJECT_ID/sinks/SINK_NAME]. Please remember to grant `serviceAccount:LOGS_SINK_SERVICE_ACCOUNT` \ the Pub/Sub Publisher role on the topic. More information about sinks can be found at /logging/docs/export/configure_export

6.  Grant log sink service account to publish to the new topic.

- Note the `serviceAccount` name from the previous step and use it to
  define the service for which you want to grant publish access.

      gcloud pubsub topics add-iam-policy-binding <TOPIC_NAME> --member serviceAccount:<LOGS_SINK_SERVICE_ACCOUNT> --role=roles/pubsub.publisher

7.  Create a service account.

- For example, use cortex-xdr-sa as the service account name and Cortex
  XSIAM Service Account as the display name.

      gcloud iam service-accounts create <SERVICE_ACCOUNT> --description="<DESCRIPTION>" --display-name="<DISPLAY_NAME>"

8.  Grant the IAM role to the service account.

- gcloud pubsub subscriptions add-iam-policy-binding <SUBSCRIPTION_NAME> --member serviceAccount:<SERVICE_ACCOUNT>@<PROJECT_ID>.iam.gserviceaccount.com --role=roles/pubsub.subscriber

9.  Create a JSON key for the service account.

- You will need the JSON file to enable Cortex XSIAM to authenticate
  with the GCP service. Specify the file destination and filename using
  a .json extension.

      gcloud iam service-accounts keys create <OUTPUT_FILE> --iam-account <SERVICE_ACCOUNT>@<PROJECT_ID>.iam.gserviceaccount.com

10. After Cortex XSIAM begins receiving information from the GCP Pub/Sub
    service, you can use the XQL Query language to search for specific
    data.

###### Ingest logs from Google Kubernetes Engine

Instead of forwarding Google Kubernetes Engine (GKE) logs directly to
Google StackDrive, Cortex XSIAM can ingest container logs from GKE using
Elasticsearch Filebeat. To receive logs, you must install Filebeat on
your containers and enable Data Collection settings for Filebeat.

When Cortex XSIAM begins receiving logs, the app automatically creates
an Cortex Query Language (XQL) dataset using the vendor and product name
that you specify during Filebeat setup. It is recommended to specify a
descriptive name. For example, if you specify `google` as the vendor and
`kubernetes` as the product, the dataset name will be
`google_kubernetes_raw`. If you leave the product and vendor blank,
Cortex XSIAM assigns the dataset a name of `container_container_raw`.

After Cortex XSIAM creates the dataset, you can search your GKE logs
using XQL Search.

1.  Install Filebeat on your containers.

- For more information, see
  <https://www.elastic.co/guide/en/beats/filebeat/current/running-on-kubernetes.html>.

2.  Ingest Logs from Elasticsearch Filebeat.

- Record your token key and API URL for the Filebeat Collector instance
  as you will need these later in this workflow.

3.  Deploy a Filebeat as a DaemonSet on Kubernetes.

- This ensures there is a running instance of Filebeat on each node of
  the cluster.

  a.  Download the manifest file to a location where you can edit it.

  - `curl -L -O https://raw.githubusercontent.com/elastic/beats/7.10/deploy/kubernetes/filebeat-kubernetes.yaml`

  b.  Open the YAML file in your preferred text editor.

  c.  Remove the `cloud.id` and `cloud.auth` lines.

  - ![](media/rId5575.png){width="5.833333333333333in"
    height="5.045833333333333in"}

  d.  For the `output.elasticsearch` configuration, replace the `hosts`,
      `username`, and `password` with environment variable references
      for `hosts` and `api_key`, and add a field and value for
      `compression_level` and `bulk_max_size`.

  - ![](media/rId5578.png){width="5.833333333333333in"
    height="1.0241721347331583in"}

  e.  In the `DaemonSet` configuration, locate the `env` configuration
      and replace `ELASTIC_CLOUD_AUTH`, `ELASTIC_CLOUD_ID`,
      `ELASTICSEARCH_USERNAME`, `ELASTICSEARCH_PASSWORD`,
      `ELASTICSEARCH_HOST`, `ELASTICSEARCH_PORT` and their relative
      values with the following.

      - `ELASTICSEARCH_ENDPOINT`: Specify the API URL for your Cortex
        XSIAM tenant. You can copy the URL from the Filebeat Collector
        instance you set up for GKE in the Cortex XSIAM management
        console (Settings \>
        (![](media/rId5581.png){width="0.20833333333333334in"
        height="0.20833333333333334in"}) \> Configurations \> Data
        Collection \> Custom Collectors \> Copy API URL. The URL will
        include your tenant name
        (`https://api-tenant external URL:443/logs/v1/filebeat)`

      - `ELASTICSEARCH_API_KEY`: Specify the token key you recorded
        earlier during the configuration of your Filebeat Collector
        instance.

  - After you configure these settings your configuration should look
    like the following image.

    ![](media/rId5584.png){width="5.833333333333333in"
    height="5.133333333333334in"}

  f.  Save your changes.

4.  If you use RedHat OpenShift, you must also specify additional
    settings.

- See
  [https://www.elastic.co/guide/en/beats/filebeat/7.10/running-on-kubernetes.html](https://www.elastic.co/guide/en/beats/filebeat/7.10/running-on-kubernetes.html#_red_hat_openshift_configuration).

5.  Deploy Filebeat on your Kubernetes.

- `kubectl create -f filebeat-kubernetes.yaml`

  This deploys Filebeat in the kube-system namespace. If you want to
  deploy the Filebeat configuration in other namespaces, change the
  namespace values in the YAML file (in any YAML inside this file) and
  add `-n <your_namespace>`.

  After you deploy your configuration, the Filebeat DameonSet runs
  throughout your containers to forward logs to Cortex XSIAM. You can
  review the configuration from the Kubernetes Engine console: Workloads
  \> Filebeat \> YAML.

  > **Note**

  > Cortex XSIAM supports logs in single line format or multiline
  > format. For more information on handling messages that span multiple
  > lines of text in Elasticsearch Filebeat, see [Manage Multiline
  > Messages](https://www.elastic.co/guide/en/beats/filebeat/current/multiline-examples.html).

6.  After Cortex XSIAM begins receiving logs from GKE, you can use the
    XQL Search to search for logs in the new dataset.

###### Ingest logs from Microsoft Azure Event Hub

Cortex XSIAM can ingest different types of data from Microsoft Azure
Event Hub using the **Microsoft Azure Event Hub** data collector. To
receive logs from Azure Event Hub, you must configure the Data Sources
settings in Cortex XSIAM based on your Microsoft Azure Event Hub
configuration. After you set up data collection, Cortex XSIAM begins
receiving new logs and data from the source.

When Cortex XSIAM begins receiving logs, the app creates a new dataset
(`MSFT_Azure_raw`) that you can use to initiate XQL Search queries. For
example, queries refer to the in-app XQL Library. For enhanced cloud
protection, you can also configure Cortex XSIAM to normalize Azure Event
Hub audit logs, including Azure Kubernetes Service (AKS) audit logs,
with other Cortex XSIAM authentication stories across all cloud
providers using the same format, which you can query with XQL Search
using the `cloud_audit_logs` dataset. For logs that you do not configure
Cortex XSIAM to normalize, you can change the default dataset. Cortex
XSIAM can also generate Cortex XSIAM issues (Analytics, IOC, BIOC, and
Correlation Rules) when relevant from Azure Event Hub logs. While
Correlation Rules issues are generated on non-normalized and normalized
logs, Analytics, IOC, and BIOC issues are only raised on normalized
logs.

Enhanced cloud protection provides:

- Normalization of cloud logs

- Cloud logs stitching

- Enrichment with cloud data

- Detection based on cloud analytics

- Cloud-tailored investigations

> **Warning**

- > Misconfiguration of Event Hub resources could cause ingestion
  > delays.

- > In an existing Event Hub integration, do not change the mapping to a
  > different Event Hub.

- > Do not use the same Event Hub for more than two purposes.

The following table provides a brief description of the different types
of Azure audit logs you can collect.

> **Note**
>
> For more information on Azure Event Hub audit logs, see [Overview of
> Azure platform
> logs](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/platform-logs-overview).

+-----------------------------------+-----------------------------------+
| Type of data                      | Description                       |
+===================================+===================================+
| Activity logs                     | Retrieves events related to the   |
|                                   | operations on each Azure resource |
|                                   | in the subscription from the      |
|                                   | outside in addition to updates on |
|                                   | Service Health events.            |
|                                   |                                   |
|                                   | > **Note**                        |
|                                   | >                                 |
|                                   | > These logs are from the         |
|                                   | > management plane.               |
+-----------------------------------+-----------------------------------+
| Azure Active Directory (AD)       | Contain the history of sign-in    |
| Activity logs and Azure Sign-in   | activity and audit trail of       |
| logs                              | changes made in Azure AD for a    |
|                                   | particular tenant.                |
|                                   |                                   |
|                                   | > **Note**                        |
|                                   | >                                 |
|                                   | > Even though you can collect     |
|                                   | > Azure AD Activity logs and      |
|                                   | > Azure Sign-in logs using the    |
|                                   | > Azure Event Hub data collector, |
|                                   | > we recommend using the          |
|                                   | > Microsoft Office 365 data       |
|                                   | > collector, because it is easier |
|                                   | > to configure. In addition,      |
|                                   | > ensure that you do not          |
|                                   | > configure both collectors to    |
|                                   | > collect the same types of logs, |
|                                   | > because if you do so, you will  |
|                                   | > be creating duplicate data in   |
|                                   | > Cortex XSIAM.                   |
+-----------------------------------+-----------------------------------+
| Resource logs, including AKS      | Retrieves events related to       |
| audit logs                        | operations that were performed    |
|                                   | within an Azure resource.         |
|                                   |                                   |
|                                   | > **Note**                        |
|                                   | >                                 |
|                                   | > These logs are from the data    |
|                                   | > plane.                          |
+-----------------------------------+-----------------------------------+

> **Note**
>
> If you want to ingest raw Microsoft Defender for Endpoint events, use
> the Microsoft Defender log collector. For more information, see
> [Ingest raw EDR events from Microsoft Defender for
> Endpoint](#UUID6e00ea26bd37150df8776680549e71d5).
>
> **Prerequisite**
>
> Ensure that you do the following tasks before you begin configuring
> data collection from Azure Event Hub.

- > Before you set up an Azure Event Hub, calculate the quantity of data
  > that you expect to send to Cortex XSIAM, taking into account
  > potential data spikes and potential increases in data ingestion,
  > because partitions cannot be modified after creation. Use this
  > information to ascertain the optimal number of partitions and
  > Throughput Units (for Azure Basic or Standard) or Processing Units
  > (for Azure Premium). Configure your Event Hub accordingly.

- > Create an Azure Event Hub. We recommend using a dedicated Azure
  > Event Hub for this Cortex XSIAM integration. For more information,
  > see [Quickstart: Create an event hub using Azure
  > portal](https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-create).

- > Each partition can support a throughput of up to 1 MB/s.

- > Ensure the format for the logs you want collected from the Azure
  > Event Hub is either JSON or raw.

Configure the Azure Event Hub collection in Cortex XSIAM:

1.  In the Microsoft Azure console, open the **Event Hubs** page, and
    select the Azure Event Hub that you created for collection in Cortex
    XSIAM.

2.  Record the following parameters from your configured event hub,
    which you will need when configuring data collection in Cortex
    XSIAM.

    - Your event hub's consumer group.

      1.  Select Entities \> Event Hubs, and select your event hub.

      2.  Select Entities \> Consumer groups, and select your event hub.

      3.  In the Consumer group table, copy the applicable value listed
          in the **Name** column for your Cortex XSIAM data collection
          configuration.

    - Your event hub's connection string for the designated policy.

      1.  Select Settings \> Shared access policies.

      2.  In the Shared access policies table, select the applicable
          policy.

      3.  Copy the **Connection string-primary key**.

    - Your storage account connection string required for partitions
      lease management and checkpointing in Cortex XSIAM.

      1.  Open the **Storage accounts** page, and either create a new
          storage account or select an existing one, which will contain
          the storage account connection string.

      2.  Select Security + networking \> Access keys, and click
          **Show keys**.

      3.  Copy the applicable **Connection string**.

3.  Configure diagnostic settings for the relevant log types you want to
    collect and then direct these diagnostic settings to the designated
    Azure Event Hub.

    a.  Open the Microsoft Azure console.

    b.  Your navigation is dependent on the type of logs you want to
        configure.

+-----------------------------------+---------------------------------------------------------------------------------+
| Log type                          | Navigation path                                                                 |
+===================================+=================================================================================+
| Activity logs                     | Select Azure services \> Activity log \> Export Activity Logs, and              |
|                                   | **+Add diagnostic setting**.                                                    |
+-----------------------------------+---------------------------------------------------------------------------------+
| Azure AD Activity logs and Azure  | 1.  Select Azure services \> Azure Active Directory.                            |
| Sign-in logs                      |                                                                                 |
|                                   | 2.  Select Monitoring \> Diagnostic settings, and **+Add diagnostic setting**.  |
+-----------------------------------+---------------------------------------------------------------------------------+
| Resource logs, including AKS      | 1.  Search for **Monitor**, and select Settings \> Diagnostic settings.         |
| audit logs                        |                                                                                 |
|                                   | 2.  From your list of available resources, select the resource that you want to |
|                                   |     configure for log collection, and then select **+Add diagnostic setting**.  |
|                                   |                                                                                 |
|                                   | - > **Note**                                                                    |
|                                   |                                                                                 |
|                                   |   > For every resource that you want to confiure, you\'ll have to repeat this   |
|                                   |   > step, or use [Azure                                                         |
|                                   |   > policy](https://learn.microsoft.com/en-us/azure/governance/policy/overview) |
|                                   |   > for a general configuration.                                                |
+-----------------------------------+---------------------------------------------------------------------------------+

a.  Set the following parameters:

    - **Diagnostic setting name**: Specify a name for your Diagnostic
      setting.

    - **Logs Categories**/**Metrics**: The options listed are dependent
      on the type of logs you want to configure. For Activity logs and
      Azure AD logs and Azure Sign-in logs, the option is called
      **Logs Categories**, and for Resource logs it\'s called
      **Metrics**.

+-----------------------------------+--------------------------------------+
| Log type                          | Log categories/metrics               |
+===================================+======================================+
| Activity logs                     | Select from the list of applicable   |
|                                   | Activity log categories, the ones    |
|                                   | that you want to configure your      |
|                                   | designated resource to collect. We   |
|                                   | recommend selecting all of the       |
|                                   | options.                             |
|                                   |                                      |
|                                   | - **Administrative**                 |
|                                   |                                      |
|                                   | - **Security**                       |
|                                   |                                      |
|                                   | - **ServiceHealth**                  |
|                                   |                                      |
|                                   | - **Alert**                          |
|                                   |                                      |
|                                   | - **Recommendation**                 |
|                                   |                                      |
|                                   | - **Policy**                         |
|                                   |                                      |
|                                   | - **Autoscale**                      |
|                                   |                                      |
|                                   | - **ResourceHealth**                 |
+-----------------------------------+--------------------------------------+
| Azure AD Activity logs and Azure  | Select from the list of applicable   |
| Sign-in logs                      | Azure AD Activity and Azure Sign-in  |
|                                   | **Logs Categories**, the ones that   |
|                                   | you want to configure your           |
|                                   | designated resource to collect. You  |
|                                   | can select any of the following      |
|                                   | categories to collect these types of |
|                                   | Azure logs.                          |
|                                   |                                      |
|                                   | - Azure AD Activity logs:            |
|                                   |                                      |
|                                   |   - **AuditLogs**                    |
|                                   |                                      |
|                                   | - Azure Sign-in logs:                |
|                                   |                                      |
|                                   |   - **SignInLogs**                   |
|                                   |                                      |
|                                   |   - **NonInteractiveUserSignInLogs** |
|                                   |                                      |
|                                   |   - **ServicePrincipalSignInLogs**   |
|                                   |                                      |
|                                   |   - **ManagedIdentitySignInLogs**    |
|                                   |                                      |
|                                   |   - **ADFSSignInLogs**               |
|                                   |                                      |
|                                   | > **Note**                           |
|                                   | >                                    |
|                                   | > There are additional log           |
|                                   | > categories displayed. We recommend |
|                                   | > selecting all the available        |
|                                   | > options.                           |
+-----------------------------------+--------------------------------------+
| Resource logs, including AKS      | The list displayed is dependent on   |
| audit logs                        | the resource that you selected. We   |
|                                   | recommend selecting all the options  |
|                                   | available for the resource.          |
+-----------------------------------+--------------------------------------+

- **Destination details**: Select **Stream to event hub**, where
  additional parameters are displayed that you need to configure. Ensure
  that you set the following parameters using the same settings for the
  Azure Event Hub that you created for the collection.

  - **Subscription**: Select the applicable **Subscription** for the
    Azure Event Hub.

  - **Event hub namespace**: Select the applicable **Subscription** for
    the Azure Event Hub.

  - (*Optional*) **Event hub name**: Specify the name of your Azure
    Event Hub.

  - **Event hub policy**: Select the applicable **Event hub policy** for
    your Azure Event Hub.

a.  **Save** your settings.

<!-- -->

4.  Configure the Azure Event Hub collection in Cortex XSIAM.

    a.  Select Settings \> Data Sources.

    b.  On the **Data Sources** page, click **Add Data Source**, search
        for and select **Azure Event Hub**, and click **Connect**.

    c.  Set these parameters.

        - **Name**: Specify a descriptive name for your log collection
          configuration.

        - **Event Hub Connection String**: Specify your event hub's
          connection string for the designated policy.

        - **Storage Account Connection String**: Specify your storage
          account's connection string for the designated policy.

        - **Consumer Group**: Specify your event hub's consumer group.

        - **Log Format**: Select the log format for the logs collected
          from the Azure Event Hub as **Raw**, **JSON**, **CEF**,
          **LEEF**, **Cisco-asa**, or **Corelight**.

        <!-- -->

        - > **Note**

          > When you **Normalize and enrich audit logs**, the log format
          > is automatically configured. As a result, the **Log Format**
          > option is removed and is no longer available to configure
          > (default).

          - **CEF** or **LEEF**: The **Vendor** and **Product** defaults
            to **Auto-Detect**.

          <!-- -->

          - > **Note**

            > For a **Log Format** set to **CEF** or **LEEF**, Cortex
            > XSIAM reads events row by row to look for the **Vendor**
            > and **Product** configured in the logs. When the values
            > are populated in the event log row, Cortex XSIAM uses
            > these values even if you specified a value in the
            > **Vendor** and **Product** fields in the Azure Event Hub
            > data collector settings. Yet, when the values are blank in
            > the event log row, Cortex XSIAM uses the **Vendor** and
            > **Product** that you specified in the Azure Event Hub data
            > collector settings. If you did not specify a **Vendor** or
            > **Product** in the Azure Event Hub data collector
            > settings, and the values are blank in the event log row,
            > the values for both fields are set to **unknown**.

          <!-- -->

          - **Cisco-asa**: The following fields are automatically set
            and not configurable.

            - **Vendor**: **Cisco**

            - **Product**: **ASA**

          <!-- -->

          - Cisco data can be queried in XQL Search using the
            `cisco_asa_raw` dataset.

          <!-- -->

          - **Corelight**: The following fields are automatically set
            and not configurable.

            - **Vendor**: **Corelight**

            - **Product**: **Zeek**

          <!-- -->

          - Corelight data can be queried in XQL Search using the
            `corelight_zeek_raw` dataset.

          <!-- -->

          - **Raw** or **JSON**: The following fields are automatically
            set and are configurable.

            - **Vendor**: **Msft**

            - **Product**: **Azure**

          <!-- -->

          - Raw or JSON data can be queried in XQL Search using the
            `msft_azure_raw` dataset.

        <!-- -->

        - **Vendor** and **Product**: Specify the **Vendor** and
          **Product** for the type of logs you are ingesting.

        <!-- -->

        - The **Vendor** and **Product** are used to define the name of
          your Cortex Query Language (XQL) dataset
          (`<vendor>_<product>_raw`). The **Vendor** and **Product**
          values vary depending on the **Log Format** selected. To
          uniquely identify the log source, consider changing the values
          if the values are configurable.

          > **Note**

          > When you **Normalize and enrich audit logs**, the **Vendor**
          > and **Product** fields are automatically configured, so
          > these fields are removed as available options (default).

        <!-- -->

        - **Normalize and enrich audit logs**: (Optional) For enhanced
          cloud protection, you can **Normalize and enrich audit logs**
          by selecting the checkbox (default). If selected, Cortex XSIAM
          normalizes and enriches Azure Event Hub audit logs with other
          Cortex XSIAM authentication stories across all cloud providers
          using the same format. You can query this normalized data with
          XQL Search using the `cloud_audit_logs` dataset.

    d.  Click **Test** to validate access, and then click **Enable**.

    - When events start to come in, a green check mark appears
      underneath the **Azure Event Hub** configuration with the amount
      of data received.

###### Ingest logs and data from Okta

To receive logs and data from Okta, you must configure the Data Sources
settings in Cortex XSIAM. After you set up data collection, Cortex XSIAM
immediately begins receiving new logs and data from the source. The
information from Okta is then searchable in XQL Search using the
`okta_sso_raw` dataset. In addition, depending on the event type, data
is normalized to either `xdr_data` or `saas_audit_logs` datasets.

You can collect all types of events from Okta. When setting up the Okta
data collector in Cortex XSIAM, a field called **Okta Filter** is
available to configure collection for events of your choosing. All
events are collected by default unless you define an Okta API Filter
expression for collecting the data, such as
`filter=eventType eq “user.session.start”.\n`. For Okta information to
be woven into authentication stories, `“user.authentication.sso”` events
must be collected.

The Okta API enforces concurrent rate limits. The Okta data collector is
built with a mechanism which reduces the amount of requests whenever an
error is received from the Okta API indicating that too many requests
have already been sent. In addition, to ensure you are properly notified
about this, an alert is displayed in the **Notification Area** and a
record is added to the **Management Audit Logs**.

Before you begin configuring data collection from Okta, ensure your Okta
user has administrator privileges with a role that can create API
tokens, such as the read-only administrator, Super administrator, and
Organization administrator. For more information, see the [Okta
Administrators
Documentation](https://help.okta.com/en-us/Content/Topics/Security/Administrators.htm?cshid=ext_Security_Administrators).

To configure the Okta collection in Cortex XSIAM:

1.  Identify the domain name of your Okta service.

- From the Dashboard of your Okta console, note your **Org URL**.

  For more information, see the [Okta
  Documentation](https://developer.okta.com/docs/guides/find-your-domain/findorg/).

  ![](media/rId5454.png){width="5.833333333333333in"
  height="2.253124453193351in"}

2.  Obtain your authentication token in Okta.

    a.  Select API \> Tokens.

    b.  **Create Token** and record the token value.

    - This is your only opportunity to record the value.

3.  Select Settings \> Data Sources.

4.  On the **Data Sources** page, click **Add Data Source**, search for
    and select **Okta**, and click **Connect**.

5.  Integrate the Okta authentication service with Cortex XSIAM.

    a.  Specify the **OKTA DOMAIN** (Org URL) that you identified on
        your Okta console.

    b.  Specify the **TOKEN** used to authenticate with Okta.

    c.  Specify the **Okta Filter** to configure collection for events
        of your choosing. **All events** are collected by default unless
        you define an Okta API Filter expression for collecting the
        data, such as `filter=eventType eq “user.session.start”.\n`. For
        Okta information to be weaved into authentication stories,
        `“user.authentication.sso”` events must be collected.

    d.  **Test** the connection settings.

    e.  If successful, **Enable** Okta log collection.

    - Once events start to come in, a green check mark appears
      underneath the **Okta** configuration with the amount of data
      received.

6.  After Cortex XSIAM begins receiving information from the service,
    you can Create an XQL Query to search for specific data. When
    including authentication events, you can also Create an
    Authentication Query to search for specific authentication data.

##### Ingest endpoint data

Cortex XSIAM enables you to ingest endpoint data.

The following endpoint data can be ingested by Cortex XSIAM:

- [SentinelOne DeepVisibility raw EDR
  events](#UUIDddd95331f5b1ac2572e08780ca22d5ba)

- [Microsoft Defender for Endpoint raw EDR
  events](#UUID6e00ea26bd37150df8776680549e71d5)

- [CrowdStrike Falcon Data Replicator raw EDR
  events](#UUIDe3b1d1ee6b1ee07e14e0a8831921588c)

- [CrowdStrike alerts and metadata, using CrowdStrike
  APIs](#UUID608d1f5782496b45bbbb5318ab84878e)

- [Windows Events](#UUIDc6ef2ebad58d3dba56c7dddddee05aca) and other data
  using [other Broker VM data collector
  applets](#UUIDcbf1b89c5ead42300cc50bf56b56d6d0)

###### Ingest alerts and metadata from CrowdStrike APIs

> **Note**
>
> To enable some of the APIs, you may need to reach out to CrowdStrike
> support.

To receive CrowdStrike API real-time alerts and logs, you must first
configure data collection from CrowdStrike APIs. You can then configure
the Data Sources settings in Cortex XSIAM for the CrowdStrike APIs.

> **Note**
>
> For more information on configuring data collection from CrowdStrike
> APIs, see the CrowdStrike Documentation.

When Cortex XSIAM begins receiving alerts and logs, it automatically
creates a CrowdStrike API XQL dataset
(`crowdstrike_falcon_incident_raw`). You can use the issues created by
Cortex XSIAM in rules, and search the logs using XQL Search. For example
queries, refer to the in-app XQL Library.

1.  Configure data collection from CrowdStrike APIs.

    a.  In the CrowdStrike Falcon application, select
        ![](media/rId5594.png){width="0.3072911198600175in"
        height="0.20833333333333334in"} Support \> API Clients and Keys.

    b.  Under the **OAuth2 API Clients** section,
        **Add new API client**.

    c.  Configure your new API client with these settings:

    - ![](media/rId5597.png){width="5.833333333333333in"
      height="4.557291119860017in"}

      - **CLIENT NAME**: Specify a name for the new API client.

      - **DESCRIPTION**: (*Optional*) Specify a description for the new
        API client.

      - API SCOPES \> Event streams: Select the **Read** permissions
        check box.

      - API SCOPES \> Hosts: Select the **Read** permissions check box.

    d.  Click **ADD**.

    e.  Copy the values for the **CLIENT ID**, **SECRET**, and
        **BASE URL**, and save them, because you will need them when you
        configure the Data Collection settings in Cortex XSIAM.

    - > **Note**

      > Ensure that you save the **SECRET** value because this is the
      > only time that it is displayed.

      ![](media/rId5600.png){width="5.833333333333333in"
      height="3.1135411198600176in"}

    f.  Click **DONE**.

2.  Configure the CrowdStrike Platform collection in Cortex XSIAM.

    a.  In Cortex XSIAM, select Settings \> Data Sources.

    b.  On the **Data Sources** page, click **Add Data Source**, search
        for and select **CrowdStrike Platform**, and click **Connect**.

    c.  Set these parameters:

        - **Name**: Specify a descriptive name for your log collection
          configuration, preferably the same **CLIENT NAME** used when
          adding a new client API in the CrowdStrike Falcon application,
          as explained above.

        - **Base URL**: Specify the **BASE URL** you received when you
          created the client API in the CrowdStrike Falcon application,
          as explained above.

        - **Client ID**: Specify the **CLIENT ID** you received when you
          created the client API in the CrowdStrike Falcon application,
          as explained above.

        - **Secret**: Specify the **SECRET** you received when you
          created the client API in the CrowdStrike Falcon application,
          as explained above.

        - **Collect**: Select the items that you want to collect
          (**Alerts**, **Hosts**).

    d.  Click **Test** to validate access, and then click **Enable**.

- When events start to come in, a green check mark appears below the
  **CrowdStrike Platform** configuration, along with the amount of data
  received.

###### Ingest raw EDR events from CrowdStrike Falcon Data Replicator

Cortex XSIAM enables ingestion of raw EDR event data from CrowdStrike
Falcon Data Replicator (FDR), streamed to Amazon S3. In addition to all
standard SIEM capabilities, this integration unlocks some advanced
Cortex XSIAM features, enabling comprehensive analysis of data from all
sources, enhanced detection and response, and deeper visibility into
CrowdStrike FDR data.

Key benefits include:

- Querying all raw event data received from CrowdStrike FDR using XQL.

- Querying critical modeled and unified EDR data via the `xdr_data`
  dataset.

- Enriching case and issue investigations with relevant context.

- Grouping issues with issues from other sources to accelerate the
  scoping process of cases, and to cut investigation time.

- Leveraging the data for analytics-based detection.

- Utilizing the data for rule-based detection, including correlation
  rules, BIOC, and IOC.

- Leveraging the data within playbooks for case response.

When Cortex XSIAM begins receiving EDR events from CrowdStrike FDR, it
automatically creates a new dataset labeled `crowdstrike_fdr_raw`,
allowing you to query all CrowdStrike FDR events using XQL. For example
XQL queries, refer to the in-app XQL Library.

In addition, Cortex XSIAM parses and maps critical data into the
`xdr_data` dataset and XDM data model, enabling unified querying and
investigation across all supported EDR vendors\' data, and unlocking key
benefits like stitching and advanced analytics. While mapped data from
all supported EDR vendors, including CrowdStrike, will be available in
the `xdr_data` dataset, it\'s important to note that third-party EDR
data present some limitations.

Third-party agents, including CrowdStrike, typically provide less data
compared to our native agents, and do not include the same level of
optimization for causality analysis and cloud-based analytics.
Furthermore, external EDR rate limits and filters might restrict the
availability of critical data required for comprehensive analytics. As a
result, only a subset of our analytics-based detectors will function
with third-party EDR data.

Raw event data from CrowdStrike FDR lacks key contextual information. To
enhance its usability, we allocate additional resources to stitch it
with other event data and data sources. Therefore, enabling the
CrowdStrike FDR integration might temporarily make the tenant
unavailable for a maintenance period of up to an hour.

We are continuously enhancing our support and using advanced techniques
to enrich missing third-party data, while somehow replicating some
proprietary functionalities available with our agents. This approach
maximizes value for our customers using third-party EDRs within existing
constraints. However, it's important to recognize that the level of
comprehensiveness achieved with our native agents cannot be matched, as
much of the logic happens on the agent itself. These capabilities are
unique, and are not found in typical SIEMs. Many of them, along with
their underlying logic, are patented by Palo Alto Networks. Therefore,
they should be regarded as added value beyond standard SIEM
functionalities for customers who are not using our agents.

> **Prerequisite**
>
> Ensure that your organization has a license for the CrowdStrike Falcon
> Data Replicator (FDR).
>
> Ensure that CrowdStrike FDR is enabled. CrowdStrike FDR can only be
> enabled by CrowdStrike Support. If CrowdStrike FDR is not enabled,
> submit a support ticket through the CrowdStrike support portal.
>
> Follow these steps to check if CrowdStrike FDR is enabled:

1.  > Log in to the CrowdStrike Falcon user interface using an account
    > that has view/create permission for the **API clients and keys**
    > page.

2.  > Navigate to Support \> API Clients and Keys.

3.  > Verify that **FDR AWS S3 Credentials and SQS Queue** is listed.

> **Note**
>
> Due to limitations with the S3 bucket used by CrowdStrike, data can
> only be collected once, by one system.
>
> **Note**
>
> For more information on configuring data collection from CrowdStrike
> via Falcon Data Replicator, see CrowdStrike documentation.

####### Task 1: Create a CrowdStrike FDR feed

1.  In the CrowdStrike user interface, select Support and resources \>
    Resources and Tools \> Falcon data replicator.

2.  Click the **FDR feeds** tab.

3.  Click **Create feed**.

4.  Enter a feed name.

5.  In Falcon Flight Control deployments, there is an option called
    **Select which CID will manage this feed**. In typical environments,
    the parent CID manages the feed for all of its child CIDs. This
    creates an aggregated feed that has data from all of the child CIDs.
    For information about aggregated feeds, and how they compare to
    individual feeds, see CrowdStrike documentation.

    - To set up an aggregated feed, select the parent CID.

    - To set up an individual feed, select a child CID or select both a
      parent CID and the **Exclude Child CIDs** option.

    - To exclude only some of the child CIDs, don't select the
      **Exclude Child CIDs** option. Instead, select
      **Customize your FDR feed** in the next step.

6.  Set the feed status.

7.  Select the method for creating your feed, from the following
    options:

    - **Create your FDR feed with default settings**, where you get the
      recommended settings, including all current and future events, all
      secondary events (if available), and no partitions.

    - **Customize your FDR feed**, where you start with the option to
      use a filter to get the specific events that you want in the feed.
      You can then customize secondary events and partitioning.

8.  Include secondary events. They are required for data stitching and
    enrichment.

9.  Optionally, in Flight Control deployments, edit the existing child
    CIDs included in the feed, and choose whether future CIDs are
    automatically included, by using the **Include future CIDs** option.

10. Click **Create feed**.

11. From the summary page that appears, copy and save all the
    information shown on the page somewhere safe, for later use. This
    page includes the credentials that are required for setting up an
    SQS consumer.

- > **Note**

  > Ensure that you copy the **Secret**, and store it in a safe place.
  > You will not be able to retrieve it later. If you need a new secret,
  > you must reset the feed credentials.

####### Task 2: Configure CrowdStrike Falcon Data Replicator

1.  Log in to CrowdStrike Falcon using an account that has view/create
    permission for the **API clients and keys** page.

2.  Navigate to ![](media/rId5594.png){width="0.3072911198600175in"
    height="0.20833333333333334in"}Support \> API Clients and Keys.

3.  On the same line as **FDR AWS S3 Credentials and SQS Queue**, click
    **Create new credentials**.

- > **Note**

  > CrowdStrike Falcon Data Replicator only supports one FDR credential
  > configuration.

4.  Configure your new FDR credentials.

- ![](media/rId5607.png){width="5.833333333333333in"
  height="4.389583333333333in"}

5.  Copy the values for the **CLIENT ID**, **SECRET**,
    **S3 IDENTIFIER**, and **SQS URL**, and save them somewhere safe,
    because you will need them when you configure data collection in
    Cortex XSIAM.

- > **Note**

  > Ensure that you save the **SECRET** value, because this is the only
  > time that it is displayed. You can go back to this page later to
  > copy the other credentials, but you will not have access to the
  > secret again.

6.  Click **DONE**.

####### Task 3: Configure ingestion into Cortex XSIAM

1.  In Cortex XSIAM, select Settings \> Data Sources.

2.  On the **Data Sources** page, click **Add Data Source**, search for
    and select **CrowdStrike Falcon Data Replicator**, and click
    **Connect**.

3.  Set these parameters:

    - **Name**: Specify a descriptive name for your log collection
      configuration.

    - **SQS URL**: Specify the **SQS URL** you received when you created
      the FDR credential in CrowdStrike Falcon, as explained above.

    - **AWS Client ID**: Specify the **CLIENT ID** you received when you
      created the FDR credential in CrowdStrike Falcon, as explained
      above.

    - **AWS Client Secret**: Specify the **SECRET** you received when
      you created the FDR credential in CrowdStrike Falcon, as explained
      above.

4.  Click **Test** to validate access, and then click **Enable**.

When events start to come in, a green check mark appears below the
**CrowdStrike Falcon Data Replicator** configuration, along with the
amount of data received.

###### Ingest raw EDR events from Microsoft Defender for Endpoint

Cortex XSIAM enables ingestion of raw EDR event data from Microsoft
Defender for Endpoint Events, streamed to Azure Event Hubs. In addition
to all standard SIEM capabilities, this integration unlocks some
advanced Cortex XSIAM features, enabling comprehensive analysis of data
from all sources, enhanced detection and response, and deeper visibility
into Microsoft Defender for Endpoint data. 

Key benefits include:

- Querying all raw event data received from Microsoft Defender for
  Endpoint using XQL.

- Querying critical modeled and unified EDR data via the `xdr_data`
  dataset.

- Enriching case and issue investigations with relevant context.

- Grouping issues with issues from other sources to accelerate the
  scoping process of cases, and to cut investigation time.

- Leveraging the data for analytics-based detection.

- Utilizing the data for rule-based detection, including correlation
  rules, BIOC, and IOC.

- Leveraging the data within playbooks for case response.

When Cortex XSIAM begins receiving EDR events from Microsoft Defender
for Endpoint Events, it automatically creates a new dataset labeled
`msft_defender_raw`, allowing you to query all Microsoft Defender for
Endpoint Events using XQL. For example XQL queries, refer to the in-app
XQL Library.

In addition, Cortex XSIAM parses and maps critical data into the
`xdr_data` dataset and XDM data model, enabling unified querying and
investigation across all supported EDR vendors\' data, and unlocking key
benefits like stitching and advanced analytics. While mapped data from
all supported EDR vendors, including Microsoft Defender for Endpoint
Events, will be available in the `xdr_data` dataset, it\'s important to
note that third-party EDR data present some limitations.

Third-party agents, including Microsoft Defender for Endpoint Events,
typically provide less data compared to our native agents, and do not
include the same level of optimization for causality analysis and
cloud-based analytics. Furthermore, external EDR rate limits and filters
might restrict the availability of critical data required for
comprehensive analytics. As a result, only a subset of our
analytics-based detectors will function with third-party EDR data.

We are continuously enhancing our support and using advanced techniques
to enrich missing third-party data, while somehow replicating some
proprietary functionalities available with our agents. This approach
maximizes value for our customers using third-party EDRs within existing
constraints. However, it's important to recognize that the level of
comprehensiveness achieved with our native agents cannot be matched, as
much of the logic happens on the agent itself. These capabilities are
unique, and are not found in typical SIEMs. Many of them, along with
their underlying logic, are patented by Palo Alto Networks. Therefore,
they should be regarded as added value beyond standard SIEM
functionalities for customers who are not using our agents.

> **Note**
>
> The generic Cortex XSIAM Azure Event Hub collector does not offer full
> functionality for EDR data (such as stitching), and is therefore not
> suitable for EDR data ingestion.

####### Task 1: Configure Microsoft Defender for Endpoint Events to stream raw data to Microsoft Azure Event Hub

> **Prerequisite**
>
> Ensure that you do the following tasks before you begin configuring
> data collection.

- > Create an Azure Event Hub. For more information, see [Quickstart:
  > Create an event hub using Azure
  > portal](https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-create).

  1.  > Create a resource group (optional if you already have a resource
      > group configured).

  2.  > Create an Event Hubs namespace.

  3.  > Create an event hub within the namespace. On the Settings \>
      > Networking page \> Public Access tab, ensure that you add Palo
      > Alto Networks IP addresses to the **Firewall** allow list. Set
      > **Exception** to **Yes**.

  4.  > Ensure that you keep a copy of the Event Hub resource ID and the
      > Event Hub name for use in the following procedures. To get
      > your Event Hubs resource ID, go to your Azure Event Hub
      > namespace page on Azure\'s **Properties** tab, and copy the text
      > under **Resource ID**.

  5.  > Create a storage account.

- > Ensure that you have Microsoft Defender user credentials to sign in
  > as a Security Administrator.

1.  Enable raw data streaming:

    a.  Sign in to the Microsoft Defender portal as a Security
        Administrator.

    b.  Go to the data export settings page in the Microsoft Defender
        portal: System \> Settings \> Windows Defender XDR \> Streaming
        API.

    c.  Click **+Add**.

    d.  In the **Name** box, enter a name for your new data streaming
        settings.

    e.  Select **Forward events to Event Hub**.

    f.  In the **Event-Hub Resource ID** box, enter the Event Hub
        resource ID that you prepared in advance.

    g.  In the **Event-Hub** box, enter the Event Hub name that you
        prepared in advance.

    h.  For **Event Types**, select the event types that you want to
        stream.

    - > **Note**

      > If you select all event types and leave **Event-Hub name**
      > empty, an event hub will be created for each category in the
      > selected namespace. If you are not using a Dedicated Event Hubs
      > ClusterEvent Hub, namespaces have a limit of 10 Event Hubs.

    i.  Click **Submit**.

    j.  Verify that the events that you selected are streaming by going
        to your Event Hubs namespace, Settings \> Networking. Select the
        **Event Hub** name and the **Consumer group**, and then under
        **Advanced properties**, click **View events**. Check the
        **Event body**.

2.  In the Microsoft Azure console, open the **Event Hubs** page, and
    select the Azure Event Hub that you created for collection of
    Microsoft Defender logs.

3.  Save a copy of the following parameters from your configured event
    hub, because you will need them when configuring data collection in
    Cortex XSIAM:

    - Your **event hub's consumer** group:

      1.  Select Entities \> Event Hubs, and select your event hub.

      2.  Select Entities \> Consumer groups, and select your event hub.

      3.  In the **Consumer group** table, copy the applicable value
          listed in the **Name** column for your Cortex XSIAM data
          collection configuration.

    - Your **event hub's connection string** for the designated policy:

      1.  Select Settings \> Shared access policies.

      2.  In the **Shared access policies** table, select the applicable
          policy.

      3.  Copy the **Connection string-primary key**.

    - Your **storage account connection string** required for partitions
      lease management and checkpointing in Cortex XSIAM:

      1.  Open the **Storage accounts** page, and either create a new
          storage account or select an existing one, which will contain
          the storage account connection string.

      2.  Select Security + networking \> Access keys, and click
          **Show keys**.

      3.  Copy the applicable **Connection string**.

####### Task 2: Configure the Microsoft Defender for Endpoint Events collector in Cortex XSIAM

1.  Select Settings \> Data Sources.

2.  On the **Data Sources** page, click **Add Data Source**, search for
    and select **Microsoft Defender for Endpoint**, and click
    **Connect**.

3.  Set these parameters:

    - **Name**: Specify a unique descriptive name for your log
      collection configuration. You cannot change this name later.

    - **Event Hub Connection String**: Specify your event hub's
      connection string for the designated policy.

    - **Storage Account Connection String**: Specify your storage
      account's connection string for the designated policy.

    - **Consumer Group**: Specify your event hub's consumer group.

4.  Click **Test** to validate access, and then click **Save**.

- When events start to come in, a green check mark appears beneath the
  **Microsoft Defender for Endpoint** configuration, with the amount of
  data received.

###### Ingest raw EDR events from SentinelOne DeepVisibility

Cortex XSIAM enables ingestion of raw EDR event data from SentinelOne
DeepVisibility, streamed via Cloud Funnel to Amazon S3. In addition to
all standard SIEM capabilities, this integration unlocks some advanced
Cortex XSIAM features, enabling comprehensive analysis of data from all
sources, enhanced detection and response, and deeper visibility into
SentinelOne data.

Key benefits include:

- Querying all raw event data received from SentinelOne using XQL.

- Querying critical modeled and unified EDR data via the `xdr_data`
  dataset.

- Enriching case and issue investigations with relevant context.

- Grouping issues with issues from other sources to accelerate the
  scoping process of cases, and to cut investigation time.

- Leveraging the data for analytics-based detection.

- Utilizing the data for rule-based detection, including correlation
  rules, BIOC, and IOC.

- Leveraging the data within playbooks for case response.

When Cortex XSIAM begins receiving EDR events from SentinelOne, it
automatically creates a new dataset labeled
`sentinelone_deep_visibility_raw`, allowing you to query all SentinelOne
events using XQL. For example XQL queries, refer to the in-app XQL
Library.

In addition, Cortex XSIAM parses and maps critical data into the
`xdr_data` dataset and XDM data model, enabling unified querying and
investigation across all supported EDR vendors\' data and unlocking key
benefits like stitching and advanced analytics. While mapped data from
all supported EDR vendors, including SentinelOne DeepVisibility, will be
available in the `xdr_data` dataset, it\'s important to note that
third-party EDR data present some limitations.

Third-party agents, including SentinelOne, typically provide less data
compared to our native agents, and do not include the same level of
optimization for causality analysis and cloud-based analytics.
Furthermore, external EDR rate limits and filters might restrict the
availability of critical data required for comprehensive analytics. As a
result, only a subset of our analytics-based detectors will function
with third-party EDR data.

We are continuously enhancing our support and using advanced techniques
to enrich missing third-party data, while somehow replicating some
proprietary functionalities available with our agents. This approach
maximizes value for our customers using third-party EDRs within existing
constraints. However, it's important to recognize that the level of
comprehensiveness achieved with our native agents cannot be matched, as
much of the logic happens on the agent itself. These capabilities are
unique, and are not found in typical SIEMs. Many of them, along with
their underlying logic, are patented by Palo Alto Networks. Therefore,
they should be regarded as added value beyond standard SIEM
functionalities for customers who are not using our agents.

> **Prerequisite**

- > The SentinelOne DeepVisibility logs that will be collected by your
  > dedicated Amazon S3 bucket must adhere to the following guidelines:

  - > Each log file must use the 1 log per line format as multi-line
    > format is not supported.

  - > The log format must be compressed as gzip or uncompressed.

  - > For best performance, we recommend limiting each file size to up
    > to 50 MB (compressed).

- > The minimum AWS permissions required for an Amazon S3 bucket and
  > Amazon Simple Queue Service (SQS) are:

  - > **Amazon S3 bucket**: `GetObject`

  - > **SQS**: `ChangeMessageVisibility`, `ReceiveMessage`, and
    > `DeleteMessage`

- > Determine how you want to provide access to Cortex XSIAM to your
  > logs and to perform API operations. You have the following options:

  - > Designate an AWS IAM user, where you will need to know the Account
    > ID for the user and have the relevant permissions to create an
    > access key/id for the relevant IAM user. If you do not have a
    > designated AWS IAM user configured yet, instructions for this are
    > included in the following procedures.

  - > Create an assumed role in AWS to delegate permissions to a Cortex
    > XSIAM AWS service. This role grants Cortex XSIAM access to your
    > flow logs. This is the **Assumed Role** option mentioned later in
    > the procedures that follow. To create an assumed role for Cortex
    > XSIAM, see [Create an assumed
    > role](#UUIDb11b8a8d194ebbe449559e38629b008d).

  <!-- -->

  - > For more information about assumed roles, see [Creating a role to
    > delegate permissions to an AWS
    > service](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-service.html).

- > To collect Amazon S3 logs that use server-side encryption (SSE), the
  > user role must have an IAM policy that states that Cortex XSIAM has
  > kms:Decrypt permissions. With this permission, Amazon S3
  > automatically detects if a bucket is encrypted and decrypts it. If
  > you want to collect encrypted logs from different accounts, you must
  > have the decrypt permissions for the user role also in the key
  > policy for the master account Key Management Service (KMS). For more
  > information, see [Allowing users in other accounts to use a KMS
  > key](https://docs.aws.amazon.com/kms/latest/developerguide/key-policy-modifying-external-accounts.html).

####### Task 1: Configure an Amazon S3 bucket

**Task A: Create a dedicated Amazon S3 bucket to store SentinelOne
DeepVisibility EDR data**

This step provides general guidelines. For more information, see
[Creating a bucket using the Amazon S3
Console](https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html).

> **Note**
>
> It is your responsibility to define a retention policy for your Amazon
> S3 bucket by creating a **Lifecycle rule** on the **Management** tab.
> We recommend setting the retention policy to at least 7 days to ensure
> that the data is retrieved under all circumstances.

1.  Log in to the AWS Management Console and navigate to the S3 Service.

2.  Create a new S3 bucket:

    a.  Click **Create bucket**.

    b.  For **Bucket Name**, enter a unique name for the bucket (for
        example, `xsiam-s1-edr-data`).

    c.  Choose an appropriate **AWS Region**.

    d.  Set **Block all public access** to **Enabled**.

    e.  Click **Create bucket**.

3.  Set up the **Bucket policy**:

    a.  Click the **Permissions** tab of your new bucket.

    b.  Under **Bucket policy**, click **Edit** and add the following
        policy to allow SentinelOne DeepVisibility to write data there.

    - Replace `your-sentinelone-account-id` with the relevant value for
      your environment; replace `xsiam-s1-edr-data` with the name of
      your new bucket.

          {
              "Version": "2012-10-17",
              "Statement": [
                  {
                      "Effect": "Allow",
                      "Principal": {
                          "AWS": "your-sentinelone-account-id"
                      },
                      "Action": "s3:PutObject",
                      "Resource": "arn:aws:s3:::xsiam-s1-edr-data/*"
                  }
              ]
          }

**Task B: Configure an Amazon Simple Queue Service (SQS) and grant it
permission to receive messages from S3**

> **Note**
>
> Ensure that you create your Amazon S3 bucket and Amazon SQS queue in
> the same region.

1.  In the [Amazon SQS Console](https://console.aws.amazon.com/sqs/),
    click **Create Queue**.

2.  Configure the following settings, where the default settings should
    be configured unless otherwise indicated.

    - **Type**: Select **Standard** queue (default).

    - **Name**: Specify a descriptive name for your SQS queue.

    - **Configuration** section: Keep the default settings for the
      various fields.

    - Access policy \> Choose method: Select **Advanced** and update the
      Access policy code in the editor window to enable your Amazon S3
      bucket to publish event notification messages to your SQS queue.
      Use this sample code as a guide for defining the `“Statement”`
      with the following definitions.

    <!-- -->

    - `“Resource”`: Keep the automatically generated ARN for the SQS
      queue that is set in the code, which uses the format
      `“arn:sns:Region:account-id:topic-name”`.

      You can retrieve your bucket's ARN by opening the [Amazon S3
      Console](https://console.aws.amazon.com/s3/) in a browser window.
      In the **Buckets** section, select the bucket that you created for
      collecting the Amazon S3 flow logs, click **Copy ARN**, and paste
      the ARN in the field.

      ![](media/rId5412.png){width="5.833333333333333in"
      height="1.5677077865266842in"}

      > **Note**

      > For more information on granting permissions to publish messages
      > to an SQS queue, see [Granting permissions to publish event
      > notification messages to a
      > destination](https://docs.aws.amazon.com/AmazonS3/latest/userguide/grant-destinations-permissions-to-s3.html).

          {
            "Version": "2012-10-17",
            "Statement": [
              {
                "Effect": "Allow",
                "Principal": {
                  "Service": "s3.amazonaws.com"
                },
                "Action": "SQS:SendMessage",
                "Resource": "[Leave automatically generated ARN for the SQS queue defined by AWS]",
                "Condition": {
                  "ArnLike": {
                    "aws:SourceArn": "[ARN of your Amazon S3 bucket]"
                  }
                }
              }
            ]
          }

    <!-- -->

    - **Dead-letter queue** section: We recommend that you configure a
      queue for sending undeliverable messages by selecting **Enabled**,
      and then in the **Choose queue** field selecting the queue to send
      the messages. You may need to create a new queue for this, if you
      do not already have one set up. For more information, see [Amazon
      SQS dead-letter
      queues](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html).

3.  Click **Create queue**.

- When the SQS is created, a message indicating that the queue was
  successfully configured is displayed at the top of the page.

**Task C: Configure an event notification to your Amazon SQS whenever a
file is written to your Amazon S3 bucket**

1.  Open the [Amazon S3 Console](https://console.aws.amazon.com/s3/) and
    in the **Properties** tab of your Amazon S3 bucket, scroll down to
    the **Event notifications** section, and click
    **Create event notification**.

2.  Configure the following settings:

    - **Event name**: Specify a descriptive name for your event
      notification containing up to 255 characters.

    - **Prefix**: Do not set a prefix, because the Amazon S3 bucket is
      meant to be a dedicated bucket for collecting only network flow
      logs.

    - **Event types**: Select **All object create events** for the type
      of event notifications that you want to receive.

    - **Destination**: Select **SQS queue** to send notifications to an
      SQS queue to be read by a server.

    - **Specify SQS queue**: You can either select
      **Choose from your SQS queues** and then select the **SQS queue**,
      or select **Enter SQS queue ARN** and specify the ARN in the
      **SQS queue** field.

    <!-- -->

    - You can retrieve your SQS queue ARN by opening another instance of
      the AWS Management Console in a browser window, opening the
      [Amazon SQS Console](https://console.aws.amazon.com/sqs/), and
      selecting the Amazon SQS that you created. In the **Details**
      section, under **ARN**, click the copy icon
      (![](media/rId5488.png){width="0.16666666666666666in"
      height="0.20833333333333334in"})), and paste the ARN in the field.

      ![](media/rId5491.png){width="5.833333333333333in"
      height="2.216666666666667in"}

3.  Click **Save changes**.

- When the event notification is created, a message indicating that the
  event notification was successfully created is displayed at the top of
  the page.

  > **Note**

  > If you receive an error when trying to save your changes, check that
  > the permissions are set up correctly, and fix them if necessary.

**Task D: Configure authentication\\authorization if you have not done
so yet**

For **Assumed Role**, follow these instructions: [Create an assumed
role](#UUIDb11b8a8d194ebbe449559e38629b008d), and then return to this
page to [Configure SentinelOne
DeepVisibility](#X72910328af135361f890e320fa4c50f26133921).

For **IAM access key**:

1.  Create an IAM Policy that grants permissions for SQS and S3:

    a.  In the **AWS Console**, navigate to the **IAM service**, and
        click **Policies**.

    b.  Click **Create policy**.

    c.  Select the **JSON** policy editor.

    d.  Use this sample code as a guide for defining the "Statement"
        with the following definitions:

    - {
              "Version": "2012-10-17",
              "Statement": [
                  {
                      "Sid": "S3ReadAccess",
                      "Effect": "Allow",
                      "Action": [
                          "s3:GetObject",
                          "s3:ListBucket"
                      ],
                      "Resource": [
                          "[ARN for the S3 Bucket Name defined by AWS]", #example: "arn:aws:s3:::bucketname/xsiam-s1-edr-data/"
                          "[ARN for the S3 Bucket path defined by AWS]" #example: "arn:aws:s3:::bucketname/xsiam-s1-edr-data/*"
                      ]
                  },
                  {
                      "Sid": "SQSReceiveAccess",
                      "Effect": "Allow",
                      "Action": [
                          "sqs:ReceiveMessage",
                          "sqs:GetQueueAttributes"
                      ],
                      "Resource": "[ARN for the SQS queue defined by AWS]"
                  }
              ]
          }

    e.  Click **Next**.

    f.  For **Policy name**, enter a name.

    g.  Click **Create policy**.

2.  Create an IAM User:

    a.  In the **AWS Console**, navigate to the **IAM service**, and
        click **Users**.

    b.  Click **Create user**.

    c.  For **User name**, enter a name (for example,
        `cortex-xsiam-s3`).

    d.  Attach the IAM Policy that you created in Step 1.

    e.  Click **Next**.

    f.  Click **Create user**.

3.  Configure access keys for the AWS IAM User:

- > **Note**

  > It is the responsibility of your organization to ensure that the
  > user who creates the access key is assigned the relevant
  > permissions. Otherwise, this can cause the process to fail with
  > errors.

  a.  Open the [AWS IAM Console](https://console.aws.amazon.com/iam/),
      and in the navigation pane, select Access management \> Users.

  b.  Select the **User name** of the AWS IAM user.

  c.  Select the **Security credentials** tab, scroll down to the
      **Access keys** section, and click **Create access key**.

  d.  Click the copy icon next to the **Access key ID** and
      **Secret access key** keys, where you must click
      **Show secret access key** to see the secret key, and save a copy
      of them somewhere safe before closing the window. You will need to
      provide these keys when you edit the Access policy of the SQS
      queue, and when setting the **AWS Client ID** and
      **AWS Client Secret** in Cortex XSIAM. If you forget to record the
      keys and close the window, you will need to generate new keys and
      repeat this process.

  - > **Note**

    > For more information, see [Managing access keys for IAM
    > users](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html).

4.  Update the **Access policy** of your Amazon SQS queue:

    a.  In the [Amazon SQS
        Console](https://console.aws.amazon.com/sqs/), select the SQS
        queue that you created when you configured an Amazon Simple
        Queue Service (SQS).

    b.  Select the **Access policy** tab, and click **Edit** to edit the
        Access policy code in the editor window, to enable the IAM user
        to perform operations on the Amazon SQS with the permissions
        `SQS:ChangeMessageVisibility`, `SQS:DeleteMessage`, and
        `SQS:ReceiveMessage`. Use this sample code as a guide for
        defining the `“Sid”: “__receiver_statement”` with the following
        definitions.

        - `“aws:SourceArn”`: Specify the ARN of the AWS IAM user. You
          can retrieve the **User ARN** from the
          **Security credentials** tab, which you accessed when you
          configured access keys for the AWS API user.

        - `“Resource”`: Keep the automatically generated ARN for the SQS
          queue that is set in the code, which uses the format
          `“arn:sns:Region:account-id:topic-name”`.

        <!-- -->

        - > **Note**

          > For more information on granting permissions to publish
          > messages to an SQS queue, see [Granting permissions to
          > publish event notification messages to a
          > destination](https://docs.aws.amazon.com/AmazonS3/latest/userguide/grant-destinations-permissions-to-s3.html).

              {
                "Version": "2012-10-17",
                "Statement": [
                  {
                    "Effect": "Allow",
                    "Principal": {
                      "Service": "s3.amazonaws.com"
                    },
                    "Action": "SQS:SendMessage",
                    "Resource": "[Leave automatically generated ARN for the SQS queue defined by AWS]",
                    "Condition": {
                      "ArnLike": {
                        "aws:SourceArn": "[ARN of your Amazon S3 bucket]"
                      }
                    }
                  },
                 {
                    "Sid": "__receiver_statement",
                    "Effect": "Allow",
                    "Principal": {
                      "AWS": "[Add the ARN for the AWS IAM user]"
                    },
                    "Action": [
                      "SQS:ChangeMessageVisibility",
                      "SQS:DeleteMessage",
                      "SQS:ReceiveMessage"
                    ],
                    "Resource": "[Leave automatically generated ARN for the SQS queue defined by AWS]"
                  }
                ]
              }

    c.  Click **Save**.

####### Task 2: Configure SentinelOne DeepVisibility

1.  In SentinelOne DeepVisibility, select Configure \> Policy & Settings
    and in the **Singularity Data Lake** section, click
    **Cloud Funnel**.

2.  For Cloud Provider, select AWS (Amazon Web Services).

3.  For **S3 Bucket Name**, enter the name of the Amazon S3 bucket that
    you created for SentinelOne DeepVisibility log ingestion.

4.  For **Telemetry Streaming**, select **Enable**.

5.  In the **Query Filters** box, create a query that includes the
    agents that should send data to the S3 bucket.

6.  To validate the query, click **Validate**.

7.  For **Fields to include**, ensure that all fields are selected.

8.  Click **Save**.

####### Task 3: Configure ingestion into Cortex XSIAM

1.  In Cortex XSIAM, select Settings \> Data Sources.

2.  On the **Data Sources** page, click **Add Data Source**, search for
    and select `SentinelOne - Deep Visibility`, and click **Connect**.

3.  Use the toggle to select either **Access Key** or **Assumed Role**.

4.  Set these parameters, depending on your choice in the previous step:

    - For the **Access Key** option:

      - **Name**: Specify a descriptive name for your log collection
        configuration. This name must be unique in your environment.

      - **SQS URL**: Specify the **SQS URL** that you received for the
        AWS S3 queue when you configured the Amazon Simple Queue Service
        (SQS), as explained above.

      - **AWS Client ID**: Specify the **Client ID** that you received
        when you configured the AWS IAM user, as explained above.

      - **AWS Client Secret**: Specify the **Secret** that you received
        when you configured the AWS IAM user, as explained above.

    - For the **Assumed Role** option:

      - **Name**: Specify a descriptive name for your log collection
        configuration. This name must be unique in your environment.

      - **SQS URL**: Specify the **SQS URL** that you received for the
        AWS S3 queue when you configured the Amazon Simple Queue Service
        (SQS), as explained above.

      - **Role ARN**: Specify the role ARN that you received when you
        created the assumed role.

      - **External Id**: Specify the External ID that you received when
        you created the assumed role.

5.  Click **Test** to validate access, and then click **Enable**.

- After events start to come in, a green check mark appears below the
  **SentinelOne - DeepVisibility** configuration, along with the amount
  of data received.

##### Ingest cloud assets

Cortex XSIAM provides a unified, normalized asset inventory for cloud
assets. This capability provides deeper visibility to all the assets and
superior context for incident investigation.

The cloud service provider (CSP) onboarding wizard is designed to
facilitate the seamless setup of CSP data into Cortex XSIAM. The guided
experience requires minimal user input; simply define the scope of your
CSP accounts and specify the scan mode. For full control of the CSP
setup, you can use the advanced settings. Based on the onboarding
settings, Cortex XSIAM generates an authentication template to establish
trust to the CSP and grant permissions to Cortex XSIAM. The template
must be executed in the CSP to complete the onboarding process.
Execution of the template grants the permissions and includes a
component that notifies Cortex XSIAM of the execution details and a new
cloud instance is created.

> **Note**
>
> The cloud accounts being onboarded must be owned by the customer
> performing the onboarding process.

You can leverage your CSP hierarchy and choose whether to onboard
individual accounts one at a time or collection of accounts (such as
organization in AWS and GCP or management group in Azure). Various
options are available for each CSP to allow you to customize your data
collection.

Cortex XSIAM supports two scan modes:

- **Cloud scan:** (Recommended) The scanning takes place within the
  Cortex XSIAM cloud environment. No additional setup is needed.

- **Outpost scan:** The scanning is performed on infrastructure deployed
  to a CSP account owned by you. The CSP account should be a dedicated
  account for the outpost, free from other resources. Each CSP account
  can host only one outpost. This mode requires additional cloud
  provider permissions and may incur additional cloud costs.

To allow you to fine tune your CSP data collection, you can modify the
scope of data collection by including or excluding specific regions. If
you selected to collect data from an organizational unit that is not the
lowest on the CSP hierarchy (such as organization or organizational unit
in AWS, organization or folder in GCP, and tenant or management group in
Azure), you can also modify the scope by including or excluding specific
accounts, projects, or subscriptions. If you choose to include specific
accounts, only those specified accounts will be included, even if
additional accounts are added to the CSP after onboarding. If you choose
to exclude specific accounts, any new accounts added to the CSP after
onboarding will be included in the scope. Excluded accounts are not
visible in Cortex XSIAM.

The advanced settings allow you to select which Cortex XSIAM modules you
want to enable for this CSP. By default, the following security
capabilities are enabled:

- Discovery engine

- Cloud security posture management

- Cloud infrastructure entitlement management

- Agentless disk scanning

- AI security posture management

The additional security capabilities you can enable include:

- XSIAM analytics: Analyzes your endpoint data to develop a baseline and
  raise Analytics and Analytics BIOC alerts when anomalies and malicious
  behaviors are detected.

- Data security posture management: An agentless
  multi-cloud data security solution that discovers, classifies,
  protects, and governs sensitive data.

- Registry scanning: Scan container registry images for vulnerabilities.
  malware, and secrets. You can configure your initial preference for
  scanning your registry. Any newly discovered registry, repository or
  image in the account will be scanned by default.

###### Onboard Amazon Web Services

> **Note**
>
> Requires the Cortex Cloud Posture Management add-on.

Follow this wizard to onboard your Amazon Web Services (AWS)
environment. The AWS onboarding wizard is designed to facilitate the
seamless setup of AWS data into Cortex XSIAM. The guided experience
requires minimal user input; simply define the scope of your AWS
accounts and specify the scan mode. For full control of the setup, you
can use the advanced settings. Based on the onboarding settings, Cortex
XSIAM generates an authentication template to establish trust to AWS and
grant permissions to Cortex XSIAM. The template must be executed in AWS
to complete the onboarding process. Execution of the template grants the
permissions and includes a component that notifies Cortex XSIAM of the
execution details and a new cloud instance is created.

> **Prerequisite**
>
> Before you begin, ensure you have:

- > Access to AWS Management Console

- > [Required AWS
  > permissions](#Xd0666e906d7d0cc92c3797ec89afb49315a3b05)

**How to onboard AWS:**

1.  Select Settings \> Data Sources.

2.  Select **Add Data Source**.

3.  On the **Add Data Sources** page, search for and select
    **Amazon Web Services (AWS)** and click **Connect**.

- ![](media/rId5627.png){width="4.333333333333333in" height="3.125in"}

4.  In the onboarding wizard, select the scope for this data source:

    - **Organization:** (Default) A collection of AWS accounts that are
      managed centrally.

    - **Organizational Unit:** A group of AWS accounts within an
      organization. It can also contain other organizational units.

    - **Account:** A specific AWS member account.

5.  Choose the **Scan Mode**:

    - **Cloud Scan:** (Recommended) Security scanning is performed in
      the Cortex cloud environment.

    - **Scan with Outpost:** Security scanning is performed on
      infrastructure deployed to a cloud account owned by you. If you
      select this option, choose the outpost account to use for this
      instance.

    <!-- -->

    - > **Note**

      > Scanning with an outpost may require additional CSP permissions
      > and may incur additional CSP costs.

6.  (Optional) Click **Show advanced settings** to define advanced
    settings:

    - **Instance Name:** Enter a unique instance name or leave empty to
      be automatically populated. The automatic naming convention is the
      CSP name followed by the ID of the scope unit selected in the
      onboarding wizard. For example, when onboarding an Amazon Web
      Services account, the automatic name would be `AWS-<accountID>`
      where `<accountID>` is the ID of the account onboarded.

    - **Scope Modifications:** To allow you to fine tune your AWS scope,
      you can modify the scope by including or excluding specific
      regions. Additionally, if you selected organization or
      organizational unit as the scope, you can modify the scope by
      including or excluding specific accounts. If you choose to include
      specific accounts, only those specified accounts will be included,
      even if additional accounts are added to your AWS environment
      after onboarding. If you choose to exclude specific accounts, any
      new accounts added to your AWS environment after onboarding will
      be included in the scope.

    <!-- -->

    - > **Note**

      > When onboarding an AWS organization or organizational unit (OU),
      > Cortex XSIAM creates IAM resources in every account within that
      > organization or OU. This occurs even if you choose to exclude
      > specific accounts from being scanned. While excluded accounts
      > will not be scanned and will not appear in the asset inventory,
      > the IAM resources may still be present.

    <!-- -->

    - **Additional Security Capabilities:** Choose from which security
      capabilities you want to benefit. Some security capabilities are
      enabled by default and can be modified. Adding security capability
      typically requires additional cloud provider permissions. For
      detailed information on the permissions required, see [Cloud
      service provider
      permissions](#UUID18e6f3f1a1c6b0c2ea3b86c571cdf3cd). The
      additional security capabilities you can enable include:

      - XSIAM analytics: Analyzes your endpoint data to develop a
        baseline and raise Analytics and Analytics BIOC alerts when
        anomalies and malicious behaviors are detected.

      - Data security posture management: An agentless
        data security scanner that discovers, classifies, protects, and
        governs sensitive data.

      - Registry scanning: A container registry scanner that scans
        registry images for vulnerabilities. malware, and secrets. For
        more details, see
        [/document/preview/1301719#UUID-3883835b-4520-cbaf-b90b-ba1d170cc153](/document/preview/1301719#UUID-3883835b-4520-cbaf-b90b-ba1d170cc153)

      - Serverless functions scanning: Implement serverless scanning to
        detect and remediate vulnerabilities within serverless functions
        during the development lifecycle. Seamless integration into
        CI/CD pipelines enables automated security scans for a
        continuously secure pre-production environment.

      <!-- -->

      - > **Note**

        > See the prerequisites above for specific permissions required
        > for serverless functions.

      <!-- -->

      - Automation: Pre-configures a list of integrations and associated
        commands to automate security issue responses. Commands can be
        utilized individually or as part of custom playbooks for issue
        remediation.

      - Agentless disk scanning: (Recommended) Implement agentless disk
        scanning to remotely detect and remediate vulnerabilities during
        the development lifecycle.

    - **Log Level:** (Optional - for **Automation** only) Configure the
      automation integration logging level. Possible values are:

      - Off (Default)

      - Debug

      - Verbose

    - **Cloud Tags:** Define tags and tag values to be added to any new
      resource created by Cortex in the cloud environment.

    - **Log Collection Configuration:** To maximize security coverage,
      include collection of audit logs using CloudTrail. This may
      require additional cloud service provider permissions. For
      detailed information on the permissions required, see [Cloud
      service provider
      permissions](#UUID18e6f3f1a1c6b0c2ea3b86c571cdf3cd).

    <!-- -->

    - For the purpose of collecting audit logs, the AWS onboarding
      wizard automatically provisions dedicated AWS resources in your
      AWS environment, specifically an AWS CloudTrail trail, Amazon SQS
      queue, and an Amazon S3 bucket. As a result, you may incur
      increased AWS costs, primarily due to CloudTrail event logging.
      While the trail defaults to capturing both read and write
      management events, the majority of these costs are typically
      associated specifically with read management events.

      To help manage these costs, you may manually modify the trail
      (`cortex-trail-<aws_account_id>`) configuration in the AWS
      Management Console to disable read events. While this reduces
      detection coverage, it should significantly lower
      CloudTrail-related charges. It is important to note that these
      manual changes will be overwritten during future Cortex XSIAM
      updates, but they can serve as a temporary measure for cost
      control.

7.  Cortex XSIAM creates an instance in pending state.

8.  To complete the process, execute the template in AWS using one of
    the following methods:

    - **Automated:** (Recommended) Click **Execute in AWS** to connect
      to AWS CloudFormation and create the stack.

    <!-- -->

    - > **Note**

      > If you select **Automated**, you must already be logged in to
      > AWS.

    <!-- -->

    - **Manual:** Click **Download CloudFormation** and follow the
      [instructions](#UUID51c6aaf074adc6c4c985d5e5ff4f1955) to manually
      execute the CloudFormation template file in AWS CloudFormation.

- > **Note**

  > The template is reusable and can be executed as many times as you
  > want to create new instances with the settings you defined in the
  > wizard. The template is valid for seven days from when it was
  > created.

9.  Click **Close**.

- When the template is successfully uploaded to AWS and the stack
  creation is complete, a new instance is created and the initial
  discovery scan is started. When the scan is complete, you can view the
  discovered assets in **Asset Inventory**.

  > **Note**

  > You can see the automatically created automation instance in the
  > **Automation & Feed Integrations** page under the **Cloud Services**
  > section, and it will have the same name as the cloud integration
  > instance. The instance is read-only, you can only edit the instance
  > from the **Data Sources** page.

  > (AWS only) instances that were previously created in the
  > **Automation & Feed Integration** page can also be edited or deleted
  > in the **Automation & Feed Integration** page.

####### Required AWS permissions

To onboard AWS to Cortex XSIAM, the following IAM policy actions are
required:

- iam:GetRole

- iam:UpdateAssumeRolePolicy

- iam:GetPolicyVersion

- iam:GetPolicy

- iam:UpdateRoleDescription

- iam:DeletePolicy

- iam:ListRoles

- iam:CreateRole

- iam:DeleteRole

- iam:AttachRolePolicy

- iam:PutRolePolicy

- iam:CreatePolicy

- iam:PassRole

- iam:CreateServiceLinkedRole

- iam:DetachRolePolicy

- iam:ListPolicyVersions

- iam:DeleteRolePolicy

- iam:UpdateRole

- iam:DeleteServiceLinkedRole

- iam:ListRolePolicies

- iam:GetRolePolicy

- iam:DeletePolicyVersion

- iam:SetDefaultPolicyVersion

- lambda:CreateFunction

- lambda:UpdateFunctionCode

- lambda:UpdateFunctionConfiguration

- lambda:GetFunction

Additional permissions are required for collecting audit logs and could
be scoped to Cortex XSIAM created resources. These include the following
types of permissions: KMS, S3, SQS , SNS , and Cloudtrail.

To enable serverless function scanning, grant the following permissions
in your AWS account for scanning outposts and accessing logs:

    {
     "Version": "2012-10-17",
     "Statement": [
       {
         "Effect": "Allow",
         "Action": [
           "lambda:GetFunction",
           "lambda:GetFunctionConfiguration",
           "lambda:GetLayerVersion",
           "iam:GetRole"
         ],
         "Resource": "*"
       }
     ]
    }

####### Manually upload template to AWS

When you have downloaded the CloudFormation template file in the
onboarding wizard, you must connect to AWS Management Console to create
a stack using the template file.

> **Prerequisite**
>
> Before you begin, ensure you have:

- > An AWS account

- > Access to AWS Management Console

- > Permission to create a stack and its resources in AWS CloudFormation

1.  In AWS Management Console, navigate to
    [**CloudFormation**](https://console.aws.amazon.com/cloudformation/).

2.  On the **Stacks** page, click **Create stack**, and then select
    **With new resources (standard)**.

3.  On the **Create stack** page, in
    **Prerequisite - Prepare template**, select
    **Choose an existing template**.

4.  In **Specify template**, select **Upload a template file**, then
    click **Choose file** and upload the template downloaded from your
    Cortex Platform. Click **Next**.

5.  In the **Specify stack details** page, enter a Stack name.

6.  In **Parameters**, enter a unique Amazon Resource Name (ARN) for the
    custom **CortexPrismaRoleName** role, and an **ExternalID**. Click
    **Next** and **Next** again.

7.  In **Review**, acknowledge that CloudFormation might create IAM
    resources with custom names and click **Submit**. The stack is
    complete when it appears in the Stacks list with status of
    **CREATE_COMPLETE**.

When the template is successfully uploaded to AWS and the stack creation
is complete, the initial discovery scan is started. When the scan is
complete, you can view the discovered assets in **Asset Inventory**.

###### Onboard Google Cloud Platform

> **Note**
>
> Requires the Cortex Cloud Posture Management add-on or the Cortex
> Cloud Runtime Security add-on.

Follow this wizard to onboard your Google Cloud Platform (GCP)
environment. The GCP onboarding wizard is designed to facilitate the
seamless setup of GCP data into Cortex XSIAM. The guided experience
requires minimal user input; simply define the scope of your GCP
accounts and specify the scan mode. For full control of the setup, you
can use the advanced settings. Based on the onboarding settings, Cortex
XSIAM generates an authentication template to establish trust to GCP and
grant permissions to Cortex XSIAM. The template must be executed in GCP
to complete the onboarding process. Execution of the template grants the
permissions and includes a component that notifies Cortex XSIAM of the
execution details and a new cloud instance is created.

> **Prerequisite**
>
> Before you begin, ensure you have:

- > Access to Google Cloud Console

- > Admin user with [required admin GCP
  > permissions](#X6bd0789b1ae00d6194b8a54cf8fd307f8f689e1)

- > Enabled the following APIs in the GCP project you are onboarding:

  - > [Cloud Resource Manager
    > API](https://console.cloud.google.com/apis/api/cloudresourcemanager.googleapis.com)

  - > [Identity and Access Management (IAM)
    > API](https://console.cloud.google.com/apis/api/iam.googleapis.com)

  - > [Cloud Pub/Sub
    > API](https://console.cloud.google.com/apis/api/pubsub.googleapis.com)
    > (if audit logs are enabled)

**How to onboard GCP:**

1.  Select Settings \> Data Sources.

2.  Select **Add Data Source**.

3.  On the **Add Data Sources** page, search for and select
    **Google Cloud Platform** and click **Connect**.

- ![](media/rId5637.png){width="4.347222222222222in"
  height="3.0694444444444446in"}

4.  In the onboarding wizard, choose the scope for this data source:

    - **Organization:** (Default) A collection of GCP projects that are
      managed centrally.

    - **Folder:** A GCP folder can contain projects, folders, or a
      combination of both.

    - **Project:** A specific GCP project.

5.  Choose the **Scan Mode**:

    - **Cloud Scan:** (Recommended) Security scanning is performed in
      the Cortex cloud environment.

    - **Scan with Outpost:** Security scanning is performed on
      infrastructure deployed to a cloud account owned by you. If you
      select this option, choose the outpost account to use for this
      instance.

    <!-- -->

    - > **Note**

      > Scanning with an outpost may require additional CSP permissions
      > and may incur additional CSP costs.

6.  If you selected **Organization** or **Project** as the scope, enter
    its ID.

7.  (Optional) Click **Show advanced settings** to define advanced
    settings:

    - **Instance Name:** Enter a unique instance name or leave empty to
      be automatically populated. The automatic naming convention is the
      CSP name followed by the ID of the scope unit selected in the
      onboarding wizard. For example, when onboarding a Google Cloud
      Platform project, the automatic name would be `GCP-<projectID>`
      where `<projectID>` is the ID of the project onboarded.

    - **Scope Modifications:** To allow you to fine tune your GCP data
      collection, you can modify the scope by including or excluding
      specific regions. Additionally, if you selected organization or
      folder as the scope, you can modify the scope by including or
      excluding specific projects. If you choose to include specific
      projects, only those specified projects will be included, even if
      additional projects are added to your GCP environment after
      onboarding. If you choose to exclude specific projects, any new
      projects added to your GCP environment after onboarding will be
      included in the scope. Excluded projects are not visible in Cortex
      XSIAM.

    - **Additional Security Capabilities:** Enable additional Cortex
      security add-ons, if available. This may require additional cloud
      provider permissions. For detailed information on the permissions
      required, see [Cloud service provider
      permissions](#UUID18e6f3f1a1c6b0c2ea3b86c571cdf3cd). The
      additional security capabilities you can enable include:

      - XSIAM analytics: Analyzes your endpoint data to develop a
        baseline and raise Analytics and Analytics BIOC alerts when
        anomalies and malicious behaviors are detected.

      - Data security posture management: An agentless
        multi-cloud data security solution that discovers, classifies,
        protects, and governs sensitive data.

      - Registry scanning: Scan container registry images for
        vulnerabilities. malware, and secrets. You can configure your
        initial preference for scanning your registry. Any newly
        discovered registry, repository or image in the account will be
        scanned by default. For more details, see
        [/document/preview/1301719#UUID-3883835b-4520-cbaf-b90b-ba1d170cc153](/document/preview/1301719#UUID-3883835b-4520-cbaf-b90b-ba1d170cc153)

      - Serverless functions scanning (Gen 1 only): Implement serverless
        scanning to detect and remediate vulnerabilities within
        serverless functions during the development lifecycle. Seamless
        integration into CI/CD pipelines enables automated security
        scans for a continuously secure pre-production environment.

      - Automation: Pre-configures a list of integrations and associated
        commands to automate security issue responses. Commands can be
        utilized individually or as part of custom playbooks for issue
        remediation.

      - Agentless disk scanning: (Recommended) Implement agentless disk
        scanning to remotely detect and remediate vulnerabilities during
        the development lifecycle.

    - **Log Level:** (Optional - for **Automation** only) Configure the
      automation integration logging level. Possible values are:

      - Off (Default)

      - Debug

      - Verbose

    - **Cloud Tags:** Define tags and tag values to be added to any new
      resource created by Cortex in the cloud environment.

    - **Log Collection Configuration:** To maximize security coverage,
      include collection of audit logs (GCP Pub/Sub). This may require
      additional cloud service provider permissions.For detailed
      information on the permissions required, see [Cloud service
      provider permissions](#UUID18e6f3f1a1c6b0c2ea3b86c571cdf3cd).

8.  Click **Save**.

9.  Download the template file by clicking **Download Terraform** and
    then click **Close**.

- > **Note**

  > The template is reusable and can be executed as many times as you
  > want to create new instances with the settings you defined in the
  > wizard. The template is valid for seven days from when it was
  > created.

  When the file has downloaded, proceed to [manually upload the template
  to GCP](#UUID758b43d38ba07bda30215cff22f0487e).

  > **Note**

  > You can see the automatically created automation instance in the
  > **Automation & Feed Integrations** page under the **Cloud Services**
  > section, and it will have the same name as the cloud integration
  > instance. The instance is read-only, you can only modify the
  > instance from the **Data Sources** page.

####### Required admin GCP permissions

To onboard a GCP organization to Cortex XSIAM, the following permissions
are required:

- iam.roles.create

- iam.roles.delete

- iam.roles.get

- iam.roles.list

- iam.roles.update

- iam.serviceAccounts.create

- iam.serviceAccounts.delete

- iam.serviceAccounts.get

- iam.serviceAccounts.getIamPolicy

- iam.serviceAccounts.list

- iam.serviceAccounts.setIamPolicy

- iam.serviceAccounts.update

- logging.sinks.create

- logging.sinks.delete

- logging.sinks.get

- logging.sinks.update

- pubsub.subscriptions.create

- pubsub.subscriptions.delete

- pubsub.subscriptions.getIamPolicy

- pubsub.subscriptions.setIamPolicy

- pubsub.subscriptions.update

- pubsub.topics.create

- pubsub.topics.delete

- pubsub.topics.getIamPolicy

- pubsub.topics.setIamPolicy

- pubsub.topics.update

- resourcemanager.folders.get

- resourcemanager.folders.getIamPolicy

- resourcemanager.folders.list

- resourcemanager.folders.setIamPolicy

- resourcemanager.organizations.get

- resourcemanager.organizations.getIamPolicy

- resourcemanager.organizations.setIamPolicy

- resourcemanager.projects.get

- resourcemanager.projects.getIamPolicy

- resourcemanager.projects.list

- resourcemanager.projects.setIamPolicy

####### Manually upload template to GCP

When you have downloaded the Terraform template file in the onboarding
wizard, you must connect to Google Cloud Console to create a stack using
the template file.

> **Prerequisite**
>
> Before you begin, ensure you have:

- > A GCP account

- > Permission to create the required resources in Google Cloud
  > Deployment Manager

- > Installed Terraform on your local machine. You can download
  > Terraform from the official [Terraform
  > website](https://www.terraform.io/downloads.html) and follow the
  > installation instructions for your operating system.

- > Installed the GCP gcloud CLI tool

1.  Open your local terminal (Command prompt, PowerShell, or Terminal).

2.  Log in to your GCP account using the gcloud CLI:

- gcloud auth login

3.  Create a directory on your local machine to store and run the
    Terraform code. If you have more than one GCP connector, you need a
    separate directory for each one:

- > **Note**

  > The directory you create must be a subdirectory of the home
  > directory.

      mkdir -p ~/terraform/gcp-connector-1

4.  Navigate to the directory you created and extract the Terraform
    files. Ensure all necessary Terraform files are present (`main.tf`,
    `template_params.tfvars`, etc).

- cd ~/terraform/gcp-connector-1
      tar -xzvf <your_template>.tar.gz

5.  Initialize Terraform in your project directory:

- terraform init

6.  Apply your Terraform configuration using the downloaded parameter
    file. When prompted, enter the project ID if you configured one in
    the onboarding wizard:

- terraform apply --var-file=template_params.tfvars

  The Terraform template is deployed.

When the template is successfully uploaded to GCP, the initial discovery
scan is started. When the scan is complete, you can view your cloud
assets in **Asset Inventory**.

####### Monitor GCP resources inside service perimeters

A service perimeter can provide an additional layer of security for your
GCP projects. It serves as a fortified boundary around your Google Cloud
resources. While resources inside the perimeter can communicate freely,
the perimeter is designed to prevent unauthorized communication to
Google Cloud services beyond its confines.

To enable Cortex XSIAM to scan assets and resources within your GCP
perimeter, you must authorize Cortex XSIAM\'s identities to access the
perimeter from within GCP. If you have a perimeter set up in your GCP
project and you have not authorized Cortex XSIAM\'s identities to scan
the perimeter, you will receive the following error:

    Request is prohibited by organization's policy. vpcServiceControlsUniqueIdentifier: {{<GCP-perimeter-ID>}}

> **Note**
>
> Each GCP cloud instance is assigned a scope within GCP. If the scope,
> whether it be organization, folder, or project, includes any projects
> with a service perimeter, this procedure must be performed for that
> cloud instance to authorize Cortex XSIAM to scan the resources in the
> perimeter.

**Obtain Cortex XSIAM identity details**

1.  In your Cortex XSIAM tenant, select Settings \> Data Sources.

2.  Hover over the Google Cloud Platform (GCP) row and select
    **View Details**.

3.  In the **Cloud Instances** page, identify the GCP instance with the
    perimeter, right-click it and select **Details**.

4.  In the details pane, click the more options icon and select
    **Authorization Details**.

5.  The authorization values that you need to add as approved identities
    in GCP are listed in the **Authorization Details** dialog box.

**Add Cortex XSIAM authorization values to GCP perimeter**

1.  Log into [Google Cloud Platform
    Console](https://console.cloud.google.com/).

2.  Navigate to **VPC Service Controls**.

3.  In the list of perimeters, select the perimeter to which you want to
    grant access to Cortex XSIAM.

4.  In the **Service perimeter details** screen, click **Edit**.

5.  In the **Edit service perimeter** screen, select **Ingress policy**.

6.  In the **Ingress rules** pane, click **Add an ingress rule**.

7.  Enter a **Title** for the ingress rule.

8.  In the **From** section, under **Identities**, select
    **Select identities & groups**.

9.  Click **Add identities**. In the **Add identities** pane, under
    **Search identities**, paste **Authorized value #1** from Cortex
    XSIAM\'s **Authorization Details** dialog box. If there are more
    authorized values, paste each of them under **Search identities**.
    Click **Add identities**.

10. In the **To** section, under **Resources**, select
    **Select projects**.

11. Click **Add projects**. In the **Add projects** pane, select the
    relevant projects.

12. Under **Operations or IAM roles**, select **All operations**.

13. Click **Next** to add an egress rule.

14. In the **Egress rules** pane, click **Add an egress rule**.

15. Enter a **Title** for the egress rule.

16. In the **From** section, under **Identities**, select
    **Select identities & groups**.

17. Click **Add identities**. In the **Add identities** pane, under
    **Search identities**, paste **Authorized value #1** from Cortex
    XSIAM\'s **Authorization Details** dialog box. If there are more
    authorized values, paste each of them under **Search identities**.
    Click **Add identities**.

18. In the **To** section, under **Resources**, select
    **Select projects**.

19. Click **Add projects**. In the **Add projects** pane, select the
    relevant projects.

20. Click **Save**. Confirm the changes and click **Confirm**.

The Cortex XSIAM authorization values have been added as approved
identities in GCP.

###### Onboard Microsoft Azure

> **Note**
>
> Requires the Cortex Cloud Posture Management add-on.

Follow this wizard to onboard your Microsoft Azure environment. The
Azure onboarding wizard is designed to facilitate the seamless setup of
Azure data into Cortex XSIAM. The guided experience requires minimal
user input; simply define the scope of your Azure accounts and specify
the scan mode. For full control of the setup, you can use the advanced
settings. Based on the onboarding settings, Cortex XSIAM generates an
authentication template to establish trust to Azure and grant
permissions to Cortex XSIAM. The template must be executed to complete
the onboarding process. Execution of the template grants the permissions
and includes a component that notifies Cortex XSIAM of the execution
details and a new cloud instance is created.

Azure private resources are not currently discoverable.

> **Prerequisite**
>
> Before you begin, ensure you have:

- > An Azure subscription

- > Admin [permissions
  > required](#Xa1eaada4983dc1c91e1adb6af0c81e6d4a1a553) to onboard
  > Azure or the built-in Security Administrator role.

- > Tenant ID and subscription ID. You can view these in Microsoft Azure
  > Portal in **Management groups**.

**How to onboard Azure:**

1.  Select Settings \> Data Sources.

2.  Select **Add Data Source**.

3.  On the **Add Data Sources** page, search for and select
    **Microsoft Azure** and click **Connect**.

- ![](media/rId5645.png){width="4.319444444444445in"
  height="3.0833333333333335in"}

4.  In the onboarding wizard, choose the scope for this data source.
    Onboarding of Azure tenants and management groups is performed using
    a bash script in the Microsoft Azure Resource Manager. Onboarding of
    Azure subscriptions can be performed using a Terraform script or in
    Microsoft Azure Resource Manager.

    - **Tenant:** (Default) A specific instance of Azure Active
      Directory, which can contain several subscriptions.

    - **Management Group:** A collection of Azure subscriptions.

    - **Subscription:** A collection of Azure resources associated with
      a specific Azure tenant.

5.  Choose the **Scan Mode**:

    - **Cloud Scan:** (Recommended) Security scanning is performed in
      the Cortex cloud environment.

    - **Scan with Outpost:** Security scanning is performed on
      infrastructure deployed to a cloud account owned by you. If you
      select this option, choose the outpost account to use for this
      instance or create a new outpost. For more information on
      outposts, see [Outposts](#UUIDf5f2211e19bc881c17c4c8de6e584185).

    <!-- -->

    - > **Note**

      > Scanning with an outpost may require additional CSP permissions
      > and may incur additional CSP costs.

6.  Select an approved tenant ID from the Tenant ID list. If no tenant
    IDs have been approved, enter the tenant ID. Click
    **Approve in Azure** to add Cortex as an approved application on
    this tenant. When the tenant ID is approved, it appears with a green
    check next to it.

7.  (Optional) Click **Show advanced settings** to define advanced
    settings:

    - **Instance Name:** Enter a unique instance name or leave empty to
      be automatically populated. The automatic naming convention is the
      CSP name followed by the ID of the scope unit selected in the
      onboarding wizard. For example, when onboarding an Azure tenant,
      the automatic name would be `AZURE-<tenantID>` where `<tenantID>`
      is the ID of the tenant onboarded.

    - **Scope Modifications:** To allow you to fine tune your Azure data
      collection, you can modify the scope by including or excluding
      specific regions. Additionally, if you selected tenant or
      management group as the scope, you can modify the scope by
      including or excluding specific subscriptions. If you choose to
      include specific subscriptions, only those specified subscriptions
      will be included, even if additional subscriptions are added to
      your Azure environment after onboarding. If you choose to exclude
      specific subscriptions, any new subscriptions added to your Azure
      environment after onboarding will be included in the scope.
      Excluded subscriptions are not visible in Cortex XSIAM.

    - **Additional Security Capabilities:** Enable additional Cortex
      security add-ons, if available. This may require additional cloud
      provider permissions. For detailed information on the permissions
      required, see [Cloud service provider
      permissions](#UUID18e6f3f1a1c6b0c2ea3b86c571cdf3cd). The
      additional security capabilities you can enable include:

      - - XSIAM analytics: Analyzes your endpoint data to develop a
          baseline and raise Analytics and Analytics BIOC alerts when
          anomalies and malicious behaviors are detected.

        - Data security posture management: An agentless
          multi-cloud data security solution that discovers, classifies,
          protects, and governs sensitive data.

        - Registry scanning: Scan container registry images for
          vulnerabilities. malware, and secrets. You can configure your
          initial preference for scanning your registry. Any newly
          discovered registry, repository or image in the account will
          be scanned by default. For more details, see
          [/document/preview/1301719#UUID-3883835b-4520-cbaf-b90b-ba1d170cc153](/document/preview/1301719#UUID-3883835b-4520-cbaf-b90b-ba1d170cc153)

        - Serverless functions scanning: Implement serverless scanning
          to detect and remediate vulnerabilities within serverless
          functions during the development lifecycle. Seamless
          integration into CI/CD pipelines enables automated security
          scans for a continuously secure pre-production environment.

        - Automation: Pre-configures a list of integrations and
          associated commands to automate security issue responses.
          Commands can be utilized individually or as part of custom
          playbooks for issue remediation.

        - Agentless disk scanning: (Recommended) Implement agentless
          disk scanning to remotely detect and remediate vulnerabilities
          during the development lifecycle.

    - **Log Level:** (Optional - for **Automation** only) Configure the
      automation integration logging level. Possible values are:

      - Off (Default)

      - Debug

      - Verbose

    - **Cloud Tags:** Define tags and tag values to be added to any new
      resource created by Cortex in the cloud environment.

    - **Log Collection Configuration:** To maximize security coverage,
      include collection of audit logs (Event Hub). This may require
      additional cloud service provider permissions. For detailed
      information on the permissions required, see [Cloud service
      provider permissions](#UUID18e6f3f1a1c6b0c2ea3b86c571cdf3cd).

8.  Click **Save**.

9.  To complete the process, download the authentication template:

    - For onboarding Azure tenants and management groups, click
      **Azure Resource Manager** to download a tar.gz file and proceed
      to [execute the Azure Resource Manager authentication
      template](#UUIDc3cd7c2b0e3a58bdb4fe9c8a72432105).

    - For onboarding Azure subscriptions, click:

      - **Download Terraform** to download a Terraform file and proceed
        to [execute the Terraform authentication
        template](#UUIDb7943492bee7f66ff68807355196843f).

      - **Azure Resource Manager** to download a JSON file and proceed
        to [deploy the authentication template in Azure Resource
        Manager](#UUIDb7943492bee7f66ff68807355196843f).

- > **Note**

  > The template is reusable and can be executed as many times as you
  > want to create new instances with the settings you defined in the
  > wizard. The template is valid for seven days from when it was
  > created.

10. Click **Close**.

- When the template is successfully executed, the initial discovery scan
  is started. When the scan is complete, you can view the discovered
  assets in **Asset Inventory**.

  > **Note**

  > You can see the automatically created automation instance in the
  > **Automation & Feed Integrations** page under the **Cloud Services**
  > section, and it will have the same name as the cloud integration
  > instance. The instance is read-only, you can only modify the
  > instance from the **Data Sources** page.

####### Required Azure permissions

To onboard an Azure subscription to Cortex XSIAM, the following
permissions are required:

- Microsoft.Resources/subscriptions/resourceGroups/moveResources/action

- Microsoft.Resources/deploymentScripts/write

- Microsoft.Resources/deploymentScripts/delete

- Microsoft.Resources/deploymentScripts/logs/read

- Microsoft.Resources/subscriptions/resourceGroups/read

- Microsoft.Resources/subscriptions/resourceGroups/write

- Microsoft.Resources/subscriptions/resourceGroups/delete

- Microsoft.Resources/deploymentScripts/read

- Microsoft.Resources/subscriptions/resourceGroups/validateMoveResources/action

- Microsoft.Resources/subscriptions/resourcegroups/deployments/read

- Microsoft.Resources/subscriptions/resourcegroups/deployments/write

- Microsoft.Resources/subscriptions/resourcegroups/deployments/operations/read

- Microsoft.Resources/subscriptions/resourcegroups/deployments/operationstatuses/read

- Microsoft.Resources/subscriptions/resourcegroups/resources/read

- Microsoft.Authorization/roleAssignments/read

- Microsoft.Authorization/roleAssignments/write

- Microsoft.Authorization/roleAssignments/delete

- Microsoft.Authorization/roleDefinitions/read

- Microsoft.Authorization/roleDefinitions/write

- Microsoft.Authorization/roleDefinitions/delete

- Microsoft.Authorization/roleManagementPolicies/read

- Microsoft.Authorization/roleManagementPolicies/write

- Microsoft.Authorization/roleManagementPolicyAssignments/read

- Microsoft.PolicyInsights/remediations/read

- Microsoft.PolicyInsights/remediations/write

- Microsoft.PolicyInsights/remediations/delete

- Microsoft.PolicyInsights/remediations/cancel/action

- Microsoft.PolicyInsights/remediations/listDeployments/read

- Microsoft.adiam/diagnosticsettings/write

- Microsoft.aadiam/diagnosticsettings/read

- Microsoft.aadiam/diagnosticsettings/delete

- Microsoft.Resources/deployments/validate/action

- Microsoft.aadiam/azureADMetrics/providers/Microsoft.Insights/diagnosticSettings/write

- Microsoft.aadiam/tenants/providers/Microsoft.Insights/diagnosticSettings/write

- Microsoft.Insights/DiagnosticSettings/Write

- Microsoft.Resources/deployments/read

- Microsoft.Resources/deployments/write

- Microsoft.Resources/deployments/delete

- Microsoft.Resources/deployments/cancel/action

- Microsoft.Resources/deployments/whatIf/action

- Microsoft.Resources/deployments/operations/read

- Microsoft.Resources/deployments/exportTemplate/action

- Microsoft.Resources/deployments/operationstatuses/read

To onboard an Azure management group or tenant, in addition to the above
Azure permissions, the following Azure permissions are required:

- Microsoft.PolicyInsights/remediations/read

- Microsoft.PolicyInsights/remediations/write

- Microsoft.PolicyInsights/remediations/delete

- Microsoft.PolicyInsights/remediations/cancel/action

- Microsoft.PolicyInsights/remediations/listDeployments/read

####### Manually upload template to Microsoft Azure Resource Manager using the CLI

When you select the **Azure Resource Manager** option in the Azure
onboarding wizard, you must then execute the Microsoft Azure Resource
Manager (ARM) template in ARM using the CLI. This procedure is used for
onboarding Microsoft Azure tenants or management groups. For onboarding
Microsoft Azure subscriptions, see [Manually upload template for
Microsoft Azure subscriptions](#UUIDb7943492bee7f66ff68807355196843f).

> **Prerequisite**
>
> Ensure the Azure CLI tool is installed and you are authorized to
> create management group policies.

1.  Log in to the Azure portal. Select **Cloud Shell** from the top
    navigation and then select **Bash**.

2.  Create a directory on your local machine to store the tar file. If
    you have more than one Azure connector, you need a separate
    directory for each one:

- mkdir -p ~/azure-connector-1

3.  Navigate to the directory you created and extract the files.

- cd ~/azure-connector-1
      tar -xzvf <your_template>.tar.gz.

4.  In Cloud Shell, run the `onboard.sh` file:

- bash onboard.sh

5.  When prompted, enter the following values:

    - The Azure region where you want the resources to be created. (For
      example, eastus or westus.)

    - The ID of the management group or tenant that you want to onboard

    - The ID of the subscription where the deployment script will run

- When the template is successfully executed, the initial discovery scan
  is started. When the scan is complete, you can view your cloud assets
  in **Asset Inventory**.

####### Manually upload template for Microsoft Azure subscriptions

You can choose one of the two following methods for executing the
authentication template in Microsoft Azure when onboarding Microsoft
Azure subscriptions:

######## Execute the Terraform authentication template

After you have downloaded the Terraform template file in the onboarding
wizard, you must log in to Microsoft Azure to execute the template file.

> **Prerequisite**
>
> Before you begin, ensure you have:

- > An Azure subscription.

- > A user with the required permissions for the subscription scope. We
  > recommend you create a dedicated role.

- > Tenant ID and subscription ID. You can view these in Microsoft Azure
  > Portal in **Management groups**.

- > Installed Terraform on your local machine. You can download
  > Terraform from the official [Terraform
  > website](https://www.terraform.io/downloads.html) and follow the
  > installation instructions for your operating system.

- > Installed the Azure CLI tool.

1.  Open your local terminal (Command prompt, PowerShell, or Terminal).

2.  Log in to your Azure account using the Azure CLI:

- az login

3.  Create a directory on your local machine to store and run the
    Terraform code. If you have more than one Azure connector, you need
    a separate directory for each one:

- mkdir -p ~/terraform/azure-connector-1

4.  Navigate to the directory you created and extract the Terraform
    files. Ensure all necessary Terraform files are present (`main.tf`,
    `template_params.tfvars`, etc).

- cd ~/terraform/azure-connector-1
      tar -xzvf <your_template>.tar.gz.

5.  Initialize Terraform in your project directory:

- terraform init

6.  Apply your Terraform configuration using the downloaded parameter
    file. When prompted, enter the subscription ID:

- terraform apply --var-file=template_params.tfvars

7.  When prompted, review the actions the Terraform will perform and
    approve them by entering `yes`.

- The Terraform template is executed.

######## Deploy the authentication template in Azure Resource Manager

When you select the **Azure Resource Manager** option in the Azure
onboarding wizard, you must then execute the Microsoft Azure Resource
Manager (ARM) template in ARM. This procedure is used for onboarding
Microsoft Azure subscriptions. For onboarding Microsoft Azure tenants or
management groups, see [Manually upload template to Microsoft Azure
Resource Manager using the CLI](#UUIDc3cd7c2b0e3a58bdb4fe9c8a72432105).

> **Prerequisite**
>
> Ensure the Azure CLI tool is installed and you are authorized to
> create management group policies.

1.  Open your local terminal.

2.  Log in to your Azure account using the Azure CLI:

- az login

3.  Deploy the template file:

- az deployment sub create  --location <LOCATION>  --subscription <SUBSCRIPTION_ID> --template-file <JSON_TEMPLATE> 

  where:

  - `<LOCATION>` is the location of the resource group. (For
    example, eastus or westus.)

  - `<SUBSCRIPTION_ID>` is the ID of the subscription you want to
    onboard.

  - `<JSON_TEMPLATE>` is the JSON template file that you downloaded at
    the end of the onboarding wizard.

  The template is deployed.

When the template is successfully executed, the initial discovery scan
is started. When the scan is complete, you can view your cloud assets in
**Asset Inventory**.

###### Onboard Oracle Cloud Infrastructure

> **Note**
>
> Requires the Cortex Cloud Posture Management add-on.

Follow this wizard to onboard your Oracle Cloud Infrastructure (OCI)
environment. The OCI onboarding wizard is designed to facilitate the
seamless setup of OCI data into Cortex XSIAM. This guided experience
requires minimal user input. For full control of the setup, you can use
the advanced settings. Based on the onboarding settings, Cortex XSIAM
generates an authentication template to establish trust to OCI and grant
permissions to Cortex XSIAM. Execution of the template completes the
onboarding process. The template grants the permissions, includes a
component that notifies Cortex XSIAM of the execution details, and a new
cloud instance is created.

> **Prerequisite**
>
> Before you begin, ensure you have:

- > Access to Oracle Cloud Infrastructure console

- > Permissions for all of the following are required:

  - > Creation of identity groups (for more information, refer to
    > [Managing
    > Groups](https://docs.oracle.com/en-us/iaas/Content/Identity/Tasks/managinggroups.htm))

  - > Policies (for more information, refer to [How Policies
    > Work](https://docs.oracle.com/en-us/iaas/Content/Identity/Concepts/policies.htm#How_Policies_Work))

  - > Tag namespaces in the root compartment (for more information,
    > refer to [Tags and Tag Namespace
    > Concepts](https://docs.oracle.com/en-us/iaas/Content/Tagging/Tasks/managingtagsandtagnamespaces.htm))

**How to onboard OCI:**

1.  Select Settings \> Data Sources.

2.  Select **Add Data Source**.

3.  On the **Add Data Sources** page, search for and select
    **Oracle Cloud Infrastructure (OCI)** and click **Connect**. It
    might take about 15 minutes until the wizard moves to the next
    phase.

- The scope for this data source type is always set to Tenancy.

4.  When the scanning infrastructure has been created, click **Close**.

5.  Return to the **Data Sources** page, and click the
    **Oracle Cloud Infrastructure** instance that you are adding.

6.  Optionally, enter a unique instance name.

- If you don\'t enter a name, the wizard will apply the default name,
  `OCI-<TENANCY_OCID>`.

7.  (Optional) To define advanced settings, click
    **Show advanced settings** and configure as needed:

    - **Scope Modifications**: You can modify the scope by including or
      excluding specific **Compartments**. If you choose to include
      specific compartments, only the specified compartments and their
      sub-compartments will be included. This setting will affect future
      sub-compartments added to your OCI environment after onboarding.
      If you choose to exclude specific compartments, this setting will
      also affect their sub-compartments.

    <!-- -->

    - **Note**:

      The root compartment is always onboarded, and only sub-compartment
      scope can be modified.

      Excluded compartments are not visible in Cortex XSIAM.

8.  Click **Save**.

9.  Download the OCI authentication template by clicking
    **Download Terraform**.

- > **Note**

  > The template is reusable and can be executed as many times as you
  > want to create new instances with the settings you defined in the
  > wizard. The template is valid for seven days from when it was
  > created.

10. Click **Close**, and then follow the instructions to [manually
    upload the template to OCI](#UUID482d8ef0e5ce3ff3327ab73e9a7ef488).

- > **Note**

  > You can see the automatically created automation instance in the
  > **Automation & Feed Integrations** page under the **Cloud Services**
  > section, and it will have the same name as the cloud integration
  > instance. The instance is read-only; you can only modify the
  > instance from the **Data Sources** page.

####### Manually upload template to OCI

When you have downloaded the Terraform template files in the onboarding
wizard, you must log in to the Oracle Cloud Infrastructure (OCI) CLI
tool to deploy the template file. For more information about the OCI CLI
tool, refer [Oracle
documentation](https://docs.oracle.com/en-us/iaas/Content/API/Concepts/cliconcepts.htm).

> **Prerequisite**
>
> Before you begin, ensure you have:

- > An Oracle Cloud Infrastructure account and the tenancy OCID.

- > Permission to deploy a custom template and create its resources in
  > OCI.

- > Installed Terraform on your local machine. You can download
  > Terraform from the official [Terraform
  > website](https://www.terraform.io/downloads.html) and follow the
  > installation instructions for your operating system.

- > Installed the OCI CLI tool, and authenticated with a key pair or
  > token-based credentials.

1.  Open your local terminal (command prompt).

2.  Create a directory on your local machine to store and run the
    Terraform code. If you have more than one OCI connector, you need a
    separate directory for each one. For example:

- mkdir -p ~/terraform/oci-connector-1

3.  Navigate to the directory you created and extract the Terraform
    files. Ensure all necessary Terraform files are present (`main.tf`,
    `template_params.tfvars`, and so on). For example:

- cd ~/terraform/oci-connector-1
      tar -xzvf <your_template>.tar.gz.

4.  Initialize Terraform in your project directory:

- terraform init

  It might take several seconds until the initialization is complete.

5.  Log in to OCI:

- oci session authenticate --profile-name DEFAULT

  An **Oracle Cloud** window is displayed.

6.  Click your account.

7.  Log in to your account, then close the window and return to your
    local terminal.

8.  Apply your Terraform configuration using the downloaded parameter
    file. When prompted to enter a value, enter the tenancy OCID.

- terraform apply --var-file=template_params.tfvars

9.  When prompted, review the actions the Terraform will perform, and
    approve them by entering `yes`.

- The Terraform template is deployed.

When the template is successfully uploaded to OCI, the initial discovery
scan starts. When the scan is complete, you can view your cloud assets
in **Asset Inventory**. You can also view details about the instance by
hovering over the instance on the **Data Sources** page, and then
clicking **View Details**.

###### Configure AWS integration instances and monitor integration instance health

You can streamline and simplify configuring AWS integration instances
within the **Data Sources** page. This includes granting the necessary
permissions for the platform to execute commands, scripts, and playbooks
as part of issue response. All automation permissions are added to the
Terraform as part of the setup process.

**Configure a new or existing AWS integration instance**

> **Note**
>
> If you have not yet onboarded your cloud integration, see [Ingest
> cloud assets](#UUIDd0ef91c2f98d2b5fbd7108a453bb69fb).

You can configure a new AWS integration instance or edit an existing AWS
integration instance, for example to enable automations.

1.  Navigate to Settings \> Data Sources.

2.  In the AWS integration row:

    - To configure a new AWS integration instance: Click
      ![](media/rId2196.png){width="0.18055555555555555in"
      height="0.20833333333333334in"} and then click
      **Add New Instance** or click **View Details** and from the
      **New Instance** drop down select the AWS cloud service provider.

    - To edit an existing AWS integration instance: Click
      **View Details** and then click the configuration pencil icon.

3.  (Optional) Under **Show advanced settings**, select **Automation**
    and select a log level for the automation integration logs.

4.  If the instance is not enabled, in the row for the AWS integration
    instance, right-click and select **Enable**. Alternatively, click
    the more options icon and select **Enable**.

5.  Manually upload the template (Terraform) to the relevant cloud
    provider.

- An automation integration instance with the same name as the cloud
  integration instance is automatically created and automation
  permissions are automatically updated in the system. For more
  information, see [Ingest cloud
  assets](#UUIDd0ef91c2f98d2b5fbd7108a453bb69fb).

**Monitor AWS integration instance health**

Monitoring AWS integration instance health ensures continuous, reliable
operation, facilitating issue response and improving overall security
posture.

1.  Navigate to Settings \> Data Sources.

2.  In the AWS integration instance row, click the **View Details** link
    and then click a specific **Instance Name**.

- From the list of health statuses, you can click the following to see
  automation instance health status:

  - **Permissions:** Shows any permission issues or missing permissions
    for the instance.

  - **Automation:** Indicates any errors during automation instance
    creation or configuration.

  <!-- -->

  - > **Note**

    > Currently, automation permission errors or missing automation
    > permissions do not affect the **Automation** health status. You
    > can view any permission errors or missing permissions in the the
    > **Permissions** health status.

###### Manually connect a cloud instance

When onboarding your cloud instance using the onboarding wizard, after
you download the authentication template and execute it in your cloud
environment, notification is sent to Cortex XSIAM and a cloud instance
is created. This connection between your cloud environment and the
Cortex XSIAM cloud instance typically occurs automatically.

There are several scenarios when the instance should be connected
manually:

- You executed the template in your cloud environment and your
  environment is an air-gapped network. In this case, the notification
  to create the instance in Cortex XSIAM does not happen.

- You have executed the template, but the instance has not appeared in
  **Cloud Instances**. This is often due to connectivity or firewall
  issues.

- You have a specific need to connect the instance manually.

To manually connect a cloud instance, you need to identify the pending
instance you want to connect. In **Cloud Instances**, remove the default
filter that excludes pending instances. Right-click on a pending
instance and select **View Details** to see the configuration details of
that specific pending instance. After you have identified the pending
instance you want to connect manually, right-click and select
**Manually connect an instance**. For more information on pending
instances, see [Pending cloud
instances](#UUID217ada995a3927ec497e6bf383d9160c).

####### AWS

In AWS Management Console, navigate
to [CloudFormation](https://console.aws.amazon.com/cloudformation/). Use
the following table to guide you on where to obtain the necessary input
for the manual onboarding. Not every field appears in every manual
onboarding instance.

  -----------------------------------------------------------------------
  Connect Instance input field        Value
  ----------------------------------- -----------------------------------
  **Organization ID**                 Onboarded organization ID.

  **Organizational Unit ID**          Onboarded organizational unit ID.

  **Account ID**                      Onboarded account ID.

  **Role ARN**                        The value of Outputs \>
                                      CORTEXXDRARN.

  **External ID**                     The value of Parameters \>
                                      ExternalID.

  **Audit Logs SQS URL**              The value of Resources \>
                                      CloudTrailLogsQueue.

  **Audit Logs Role ARN**             The value of Resources \>
                                      CloudTrailReadRole \> ARN.

  **Audit Logs Audience**             Automatically populated.

  **Outpost Scanner Role ARN**        The value of Resources \>
                                      CortexPlatformScannerRole \> ARN.
  -----------------------------------------------------------------------

####### GCP

1.  Open your local terminal (Command prompt, PowerShell, or Terminal).

2.  Log in to your GCP account using the gcloud CLI:

- gcloud auth login

3.  Display the values of all defined output variables in your Terraform
    configuration, formatted as a JSON object:

- terraform output -json

Use the following table to guide you on which values in the output map
to the necessary input for the manual onboarding. Not every field
appears in every manual onboarding instance.

  --------------------------------------------------------------------------------------------------------------------------
  Connect Instance input field                  Value
  --------------------------------------------- ----------------------------------------------------------------------------
  **Organization ID**                           organization_id.value

  **Project ID**                                project_id.value

  **Folder ID**                                 folder_id.value

  **Service Account Email**                     service_account_email.value

  **Audit Logs Audit Pubsub Subscription ID**   resources_data.value.AUDIT_LOGS.audit_pubsub_subscription_id

  **Audit Logs Service Account Email**          resources_data.value.AUDIT_LOGS.audit_service_account_email

  **Outpost Scanner Service Account Email**     resources_data.value.OUTPOST_SCANNER.outpost_scanner_service_account_email
  --------------------------------------------------------------------------------------------------------------------------

####### Azure with Terraform

1.  Open your local terminal (Command prompt, PowerShell, or Terminal).

2.  Log in to your Azure account using the Azure CLI:

- az login

3.  Display the values of all defined output variables in your Terraform
    configuration, formatted as a JSON object:

- terraform output -json

Use the following table to guide you on which values in the output map
to the necessary input for the manual onboarding. Not every field
appears in every manual onboarding instance.

  ------------------------------------------------------------------------------------------------------------------------------------
  Connect Instance input field                              Value
  --------------------------------------------------------- --------------------------------------------------------------------------
  **Resource Group Location** (only for subscription scope) Onboarded resource group location

  **Resource Group Name**                                   Automatically populated

  **Audit Logs Audience**                                   Automatically populated

  **Audit Logs Storage Account Name**                       resources_data.value.AUDIT_LOGS.storage_account_name

  **Audit Logs Tenant ID**                                  Automatically populated

  **Audit Logs Client ID**                                  resources_data.value.AUDIT_LOGS.client_id

  **Audit Logs Namespace**                                  resources_data.value.AUDIT_LOGS.namespace

  **Audit Logs Eventhub Name**                              resources_data.value.AUDIT_LOGS.eventhub_name

  **Audit Logs Azure Audit Eventhub Consumer Group Name**   resources_data.value.AUDIT_LOGS.azure_audit_eventhub_consumer_group_name
  ------------------------------------------------------------------------------------------------------------------------------------

####### Azure Portal

1.  Navigate to the [Microsoft Azure Portal](http://portal.azure.com)
    and log in.

Use the following table to guide you on which values in the output map
to the necessary input for the manual onboarding. Not every field
appears in every manual onboarding instance.

  ---------------------------------------------------------------------------------------------
  Connect Instance input field                              Value
  --------------------------------------------------------- -----------------------------------
  **Resource Group Location** (only for subscription scope) Onboarded resource group location

  **Resource Group Name**                                   Automatically populated

  **Audit Logs Audience**                                   Automatically populated

  **Audit Logs Storage Account Name**                       Navigate to **Storage accounts**
                                                            and filter by resource group.

  **Audit Logs Tenant ID**                                  Automatically populated

  **Audit Logs Client ID**                                  Navigate to **App registrations**
                                                            and sort by time. The default name
                                                            starts with \"auditlogsapp\".

  **Audit Logs Namespace**                                  Navigate to **Event Hubs** and
                                                            filter by resource group.

  **Audit Logs Eventhub Name**                              Navigate to **Event Hubs** and
                                                            select the Event Hub Namespace.
                                                            Under **Event Hubs**, take the
                                                            value in the **Name** column.

  **Audit Logs Azure Audit Eventhub Consumer Group Name**   Navigate to **Event Hubs** -and
                                                            select the Event Hub Namespace and
                                                            then the Event Hub. Under
                                                            **Consumer Groups**, use the value
                                                            in the **Name** column, but not
                                                            '\$Default'.
  ---------------------------------------------------------------------------------------------

###### Outposts

Cortex XSIAM allows you to have security scans performed in an outpost,
on infrastructure deployed to a cloud account owned by you. The outpost
is a set of dedicated cloud services for Cortex XSIAM for the purpose of
scanning your workloads while respecting your data security and
residence requirements.

The cloud account should be a dedicated account for the outpost, free
from other resources. Each cloud account can host only one outpost.
Using an outpost requires additional cloud provider permissions and may
incur additional cloud costs.

To use an outpost for your cloud workload scanning, under **Scan Mode**
in the CSP onboarding wizard, select **Scan with Outpost**.

To view all outposts and their details, navigate to Settings \> Data
Collection \> Outposts. In the **Outposts** page, you can edit and
delete outposts. You can also view the outpost instance.

####### Create an outpost

Follow this wizard to create a CSP outpost. Cortex XSIAM creates an
authentication template to establish trust to the CSP and grant the
necessary permissions to Cortex XSIAM. The template must be executed in
the CSP to complete the outpost creation process.

You can create a new outpost by navigating to Settings \> Data
Collection \> Outposts and clicking **New Outpost**. Alternatively, you
can create a new outpost in the cloud onboarding wizard after choosing
the **Scan with Outpost** option. Under **Choose Outpost**, click
**Select outpost account**, and then click **Create a new outpost**.

> **Note**

- > We recommend you use a dedicated account for the outpost, free from
  > other resources.

- > You can only onboard one outpost for each account.

######## AWS

1.  (Optional) Define tags and tag values to be added to any new
    resource created by Cortex in the cloud environment. Click **Next**.

2.  Click **Download Terraform** to download the Terraform template
    file.

- Execute the Terraform template in the CSP to create the outpost.

######## GCP

1.  Enter the project ID of the GCP project.

2.  (Optional) Define tags and tag values to be added to any new
    resource created by Cortex in the cloud environment. Click **Next**.

3.  Click **Download Terraform** to download the Terraform template
    file.

- Execute the Terraform template in the CSP to create the outpost.

######## Azure

> **Note**
>
> When creating an outpost for a specific Azure subscription, the
> outpost account must be in the same Azure organization as the
> monitored subscriptions.

1.  Enter the tenant ID of the Azure tenant in which you want to
    establish the outpost.

2.  (Optional) Define tags and tag values to be added to any new
    resource created by Cortex in the cloud environment. Click **Next**.

3.  Click **Download Terraform** to download the Terraform template
    file.

- Execute the Terraform template in the CSP to create the outpost.

####### Execute the template in the CSP to finalize the outpost

When you have downloaded the Terraform template file in the onboarding
wizard, log in to the CSP and execute the template file.

######## AWS

> **Prerequisite**
>
> Before you begin, ensure you have:

- > An AWS account

- > Permission to create a stack and its resources in AWS

- > Installed Terraform on your local machine. You can download
  > Terraform from the official [Terraform
  > website](https://www.terraform.io/downloads.html) and follow the
  > installation instructions for your operating system.

- > Installed the AWS CLI tool and configured your profile with the
  > `aws configure sso` wizard.

1.  Open your local terminal (Command prompt, PowerShell, or Terminal).

2.  Log in to your AWS account using the AWS CLI:

- aws sso login --profile <my-profile>

  Where `<my-profile>` is the profile you configured with the
  `aws configure sso` wizard.

3.  Create a directory on your local machine to store and run the
    Terraform code. If you are creating more than one outpost, you need
    a separate directory for each one:

- mkdir -p ~/terraform/aws-outpost-1

4.  Navigate to the directory you created and extract the Terraform
    files.

- cd ~/terraform/aws-outpost-1
      tar -xzvf <your_template>.tar.gz

5.  Initialize Terraform in your project directory:

- terraform init

6.  Apply your Terraform configuration using the downloaded parameter
    file. When prompted, enter the subscription ID:

- terraform apply --var-file=template_params.tfvars

7.  When prompted, review the actions the Terraform will perform and
    approve them by entering `yes`.

- The Terraform template is deployed and your outpost is created. To
  view all outposts and their details, navigate to Settings \> Data
  Collection \> Outposts.

######## GCP

> **Warning**
>
> Before you begin, ensure you have:

- > A GCP account

- > Permission to create the required resources in Google Cloud
  > Deployment Manager

- > Installed Terraform on your local machine. You can download
  > Terraform from the official [Terraform
  > website](https://www.terraform.io/downloads.html) and follow the
  > installation instructions for your operating system.

- > Installed the GCP gcloud CLI tool

1.  Open your local terminal (Command prompt, PowerShell, or Terminal).

2.  Log in to your GCP account using the gcloud CLI:

- gcloud auth login

3.  Create a directory on your local machine to store and run the
    Terraform code. If you are creating more than one outpost, you need
    a separate directory for each one:

- mkdir -p ~/terraform/gcp-outpost-1

4.  Navigate to the directory you created and extract the Terraform
    files.

- cd ~/terraform/gcp-outpost-1
      tar -xzvf <your_template>.tar.gz

5.  Initialize Terraform in your project directory:

- terraform init

6.  Apply your Terraform configuration using the downloaded parameter
    file. When prompted, enter the project ID:

- terraform apply --var-file=template_params.tfvars

7.  When prompted, review the actions the Terraform will perform and
    approve them by entering `yes`.

- The Terraform template is deployed and your outpost is created. To
  view all outposts and their details, navigate to Settings \> Data
  Collection \> Outposts.

######## Azure

> **Warning**
>
> Before you begin, ensure you have:

- > An active Azure subscription.

- > Installed the Azure CLI tool.

- > Permission to deploy a custom template and create its resources in
  > Microsoft Azure (\"Owner\" or \"Contributor\" on the designated
  > outpost subscription scope, and Active Directory \"Cloud Application
  > Administrator\" or \"Application Administrator\" privileged roles).

- > Installed Terraform 1.9.4 or above on your local machine. You can
  > download Terraform from the official [Terraform
  > website](https://www.terraform.io/downloads.html) and follow the
  > installation instructions for your operating system.

- > A static egress IP assigned to the machine running this Terraform.
  > This is used to configure the Azure Storage IP whitelist
  > (Recommended). Without this, future runs of this Terraform may fail
  > on Azure storage configurations.

1.  Open your local terminal (Command prompt, PowerShell, or Terminal).

2.  Log in to your Azure account using the Azure CLI:

- az login

3.  If prompted, select the **subscription_id** of the designated
    subscription, or run:

- az account set --subscription <subscription_id>

  Where `<subscription_id>` is the subscription ID of the designated
  subscription.

4.  Create a directory on your local machine to store and run the
    Terraform code. If you are creating more than one outpost, you need
    a separate directory for each one:

- mkdir -p ~/terraform/azure-outpost-1

5.  Navigate to the directory you created and extract the Terraform
    files.

- cd ~/terraform/azure-outpost-1
      tar -xzvf <your_template>.tar.gz

6.  Initialize Terraform in your project directory:

- terraform init

7.  Apply your Terraform configuration using the downloaded parameter
    file. When prompted, enter the subscription ID:

- terraform apply --var-file=template_params.tfvars

8.  When prompted for `var.storaage_account_ip_whitelist`, you can leave
    it empty to enable access from any public IP to the storage
    accounts. We recommend you to limit access to selected IPs. To limit
    access, enter a comma-separated list of public IP addresses,
    including your local machine\'s egress IP (to enable the completion
    of the Terraform run). For example: `8.8.8.8, 8.8.4.4`

9.  Review the actions the Terraform will perform and approve them by
    entering `yes`.

10. It is important to create a backup of the Terraform state file using
    one of the following methods:

- Backup the `terraform.tfstate` and `terraform.tfstate.backup` files or
  use Terraform backend to save the state.

  - Create copies of the `terraform.tfstate` and
    `terraform.tfstate.backup` files. These can then be moved to the
    working folder to allow Terraform to upgrade or destroy the created
    resources as necessary.

  - Ensure you\'re using a backend block in your Terraform
    configuration. For more information, see [Backend block
    configuration
    overview](https://developer.hashicorp.com/terraform/language/backend).

  The Terraform template is deployed and your outpost is created. To
  view all outposts and their details, navigate to Settings \> Data
  Collection \> Outposts.

######## After creating the outpost

Once you have executed the template in your cloud service provider, the
necessary permissions are granted and a notification is sent to Cortex
XSIAM with the execution details. A new outpost is created in pending
status and can be viewed in the **Outpost** page at Settings \> Data
Collection \> Outposts.

**Troubleshooting**

If you have successfully executed the template in your cloud service
provider and no new outpost has been created, verify that your internet
connection is active. An active internet connection is necessary for the
notification to be sent to Cortex XSIAM to create the new outpost. If
you are unable to establish an internet connection, contact customer
support for a manual workaround.

###### Container Registry Scanning

####### Overview of container registry scanning

Registry scanning identifies vulnerabilities, malware, and secrets,
ensuring comprehensive protection for containerized applications across
various cloud environments without requiring manual intervention.

Container Registry Scanning automatically detects and scans container
registries within your onboarded cloud accounts, including Amazon
Elastic Container Registry (ECR), Azure Container Registry (ACR), and
Google Artifact Registry (GAR). Also, Runtime Security supports registry
scanning for JFrog Artifactory and Docker V2 registries where you can
manually onboard them as new data connectors.

After you onboard your container registries, Runtime Security ensures
that all containers and images are scanned at regular intervals and that
you are notified about any deviation from your security policies and
best practices.

######## Registry Components

To understand how container registry scanning works, it\'s essential to
understand its core components:

- **Container registry:** A container registry is a service for
  publishing, maintaining, and securely distributing container images,
  providing a centralized hub for managing and accessing containerized
  application components across your organization. This scanning helps
  to enable proactive identification and remediation of security risks
  before deployment which means you will be using only trusted and
  compliant images in production environments.

- **Container image repository:** Within a container registry, container
  images are organized into multiple repositories to improve management,
  access control, collaboration, and security isolation. Each repository
  should ideally contain images related to a specific application,
  service, or project, allowing for granular permissioning and security
  policies. Images within a repository often share a common base image
  or purpose, making it easier to apply consistent security controls
  across related components.

- **Image Tags:** Image tags are essential for identifying and managing
  container image versions within a repository, enabling the selection
  and deployment of appropriate builds. From a security perspective,
  tags facilitate tracking vulnerable images, deploying patched
  versions, and maintaining image provenance for auditing. There are two
  common formats for referencing image tags:

  - image:tag -- A human-readable label that can be reassigned to
    different versions. For example, myapp:latest or myapp:v1.0.0.

  - image@sha -- A cryptographic hash that provides an immutable
    reference to a specific image version. For example,
    myapp@sha256:abc123.

While human-readable tags like myapp:latest (reassignable) and
myapp:v1.0.0 are common, using immutable tags such as
myapp@sha256:abc123 provides a cryptographically secure and verifiable
reference, crucial for ensuring the integrity and trustworthiness of
deployed images.

- **Image Digest:** A cryptographic digest (SHA-256 hash) uniquely
  identifies a container image\'s content, providing a strong guarantee
  of immutability. Unlike user-defined image tags, which can be
  reassigned, using the digest as a tag ensures that even if an image is
  renamed or retagged, its content remains verifiably identical, making
  it a critical element for security auditing and ensuring the integrity
  of deployed applications. Relying on image digests helps prevent
  potential supply chain attacks where malicious actors might attempt to
  replace images with compromised versions.

######## How Container Registry Scanning Works

The process of container registry scanning consists of three key phases:
discovery, scanning, and evaluation.

1.  **Discovery** involves detecting registries, repositories, and image
    tags within an environment. This step ensures that all container
    images, regardless of their source, are accounted for and available
    for analysis.

2.  **Scanning** detects vulnerabilities, malware, and secrets in the
    container images.

3.  **Evaluation** creates compliance findings based on the scan
    results. These findings identify vulnerabilities, compliance
    violations, or potential threats that require remediation before an
    image is deployed. Scan results are evaluated for vulnerabilities,
    malware and secrets, creating findings accordingly

####### Configure registry scanning

You can enable and configure container registry for managed registries,
such as Amazon Elastic Container Registry (ECR), Azure Container
Registry (ACR), and Google Artifact Registry (GAR) when you onboard the
respective cloud accounts.

You can also modify an already onboarded account to enable and configure
registry scanning for that account as an additional security capability
to scan images for vulnerabilities, malware, and secrets. 

Configuring registry scanning ensures that only verified and compliant
images are deployed across your cloud environments.

> **Prerequisite:**
>
> Ensure that you have performed the all steps till
> **Additional Security Capabilities** as listed in the onboarding
> wizard for the required CSP:

- > [Onboard Amazon Web Services](#UUID97a3a7dbb0ac7889ce2bae8d96784e09)

- > [Onboard Google Cloud
  > Platform](#UUID6ca58a018ee91fe08e96d50a2047795e)

- > [Onboard Microsoft Azure](#UUID54de27b4d4a6fb323a04f77b412de740)

To configure registry scanning, do the following:

1.  Under **Additional Security Capabilities**, select
    **Registry Scanning**, then click **Edit Preferences**.

- ![](media/rId5683.png){width="5.833333333333333in" height="2.3625in"}

2.  In **Initial Scan Configuration**, set your scanning process to
    focus on recently added or modified container images and exclude
    older ones that do not align with your current scanning objectives.
    This setting helps avoid unnecessary scans. Choose one of the
    following options:

    - **All**: Scans all container images, including all versions
      (tags), in all discovered repositories.

    - **Latest Tags**: Scans only images tagged \'latest\' in all
      discovered repositories.

    - **Days Modified**: Scans container images created or modified in
      the last few days. You can select a range of up to **90** days for
      the scan.

3.  Select **Save**.

- After configuring your container registers, a new discovery scan is
  started. When the scan is complete, you can view the scanned images in
  **Container Image** page. For more details, see [Container Image
  assets](#UUID0362c43ee0fb00c33370b18e009f4440).

####### Modify the container registry scanning scope

Using the Modify Scanning Scope option, you can define conditions to
automatically exclude selected scopes from scanning. These conditions
can be based on the registry, repository, or tag. After you set the
scope, the exclusion conditions are automatically applied to newly
discovered images in the account.

To modify the scanning scope, do the following:

1.  Navigate to Settings \> Data Sources.

2.  In the **Cloud Provider** section, locate the provider where your
    assets are stored and click **View Details**.

3.  On the **Cloud Instances** page, click the instance name for which
    you want to modify the scope.

4.  Under the **Accounts** section, select the account, right-click, and
    choose **Edit**.

5.  In the **Registry Scanning Scope** section, click
    **Modify Scanning Scope**.

6.  From the list of images, select the image you want to modify.

7.  Alternatively, you can also filter for a specific image by clicking
    the **Filter** icon and selecting **Registry**, **Repository** ,or
    **Tags** option and then adding the desired value to refine your
    search.

- The search results are applied automatically, even if you do not
  select **Save**.

8.  Click **Save** to confirm your modifications.

This ensures that the specified scanning scope is customized based on
your needs.

####### Scan re-evaluation process

After the initial scan has been completed, the scan re-evaluation
process ensures that container images remain secure over time without
requiring a full re-scan.

Instead of manually triggering new scans, the scan re-evaluation process
process automatically reassesses existing scan results every
**24 hours** using the latest threat intelligence feeds. This approach
reduces the need for resource-intensive re-scans, while maintaining
up-to-date security assessments.

By continuously monitoring container images for emerging threats,
organizations can proactively mitigate risks and ensure compliance with
security best practices.

####### Connect Docker V2 compliant container registry

A Docker V2-compliant registry is a registry service that complies with
the specifications and requirements outlined in the Docker Registry HTTP
API V2. This API defines the protocol for interacting with a Docker
registry, a repository where Docker images are stored and from which
they can be pulled or pushed.

Follow the wizard to use the Docker V2 connector in Cortex Cloud to scan
and secure container images from any container registry that supports
the Docker V2 protocol, ensuring comprehensive security.

> **Note**
>
> Scanning is supported only for private repositories on Docker Hub.

**How to connect Docker V2**

1.  Select Settings \> Data Sources.

2.  Select **Add Data Source**.

3.  On the **Add Data Sources** page, search for and select
    **Docker V2**, and then select **Connect**.

- ![](media/rId5689.png){width="5.833333333333333in" height="3.54375in"}

4.  The **Instance Name** is automatically populated. You can change it
    to a more meaningful name.

5.  Choose the **Scan Mode**, and then follow the steps for that mode to
    configure the connection.

- Cloud Scan
  Security scanning is done in the Cortex cloud environment when you
  select this mode.

  1.  Select a **Cloud Provider** to initialize registry scanning.

  2.  Select the **Region** where the registry is hosted.

  3.  (Optional) Enable **Allow access by IP's** to specify a static IP
      address for the scanner to use. Make sure the static IP is allowed
      through your firewall so the scanner can access the registry
      during the scanning process.

  4.  Enter the **Registry URL**. This must match the URL you use with
      the `docker login` command.

  - Equivalent URL: `https://docker.io/`

  5.  Under **Authentication Method**, enter the **Username** and
      **Password** of the registry that you want to connect.

  - Use your **Docker ID** as the username (for example, john0907) and
    **not** your email address.

  6.  Select **Next**.

  Scan with Outpost
  Security scanning is done on infrastructure deployed to a cloud
  account that you own. This mode requires additional cloud provider
  permissions and may incur extra costs.

  > **Prerequisite:**

  > Ensure an [Outpost](#UUIDf5f2211e19bc881c17c4c8de6e584185) is
  > connected to your tenant. Outposts

  1.  Choose a **Cloud Provider** to initialize registry scanning.

  - > **Note**

    > If you choose **Azure** as the **Cloud Provider**, you must also
    > select the **Tenant Id**. The Tenant Id is required to approve
    > Cortex as an enterprise application in your Azure tenant.

  2.  **Choose Outpost** account to use for this instance. If no
      Outposts are shown, you can **Create a new one**. For more
      details, see [Outposts](#UUIDf5f2211e19bc881c17c4c8de6e584185).

  - > **Note**

    > If you choose **Azure** as the cloud provider, only **Outposts**
    > associated with the selected tenant ID are displayed.

  3.  Select the **Region** where the registry is hosted.

  4.  (Optional) Enable **Allow access by IP's** if you want to specify
      a static IP address for the scanner to use. Make sure the static
      IP is allowed through your firewall so that the scanner can access
      the registry during the scanning process.

  5.  Enter the **Registry URL**. This must match the URL you use with
      the `docker login` command.

  - Equivalent URL: `https://docker.io/`

  6.  Under **Authentication Method**, enter the **Username** and
      **Password** of the registry that you want to connect.

  - Use your **Docker ID** as the username (for example, john0907) and
    **not** your email address.

  7.  Select **Next**.

  Scan with Broker VM
  Security scanning in private networks is done using broker VM
  infrastructure when you select this mode.

  > **Note**

  > FedRAMP is not supported for this mode.

  > **Prerequisite:**

  > Ensure one of the following is configured:

  - > [Set up and configure Broker
    > VM](#UUIDa2b1b832d74850d81f427e175514c501).

  - > [Configure High Availability
    > Cluster](#UUIDcdafbc5b0f072deba4bdea1f522d867d).

  1.  Choose a **Scan with Broker VM** mode to initiate registry
      scanning. You can select either a standalone **Broker VM** or a
      High Availability (HA) **Cluster**.

  2.  Select **Applicable Broker VMs**.

  - Choose the appropriate **Broker VM** or **Cluster** from the list
    configured in your tenant.

    > **Note**

    - > The list of **Broker VMs** displays only VMs that support
      > registry scanning.

    - > The list of high-availability **Clusters** displays only
      > clusters that contain at least one VM supporting registry
      > scanning.

    - > The registry scanning status for each VM appears in brackets if
      > it was previously activated for that specific VM.

    If the list does not display any Broker VMs or clusters, Add New
    Broker VM or Add New Cluster. For more details, see [Set up and
    configure Broker VM](#UUIDa2b1b832d74850d81f427e175514c501).

  3.  Enter the **Registry URL**. This must match the URL you use with
      the `docker login` command.

  - Equivalent URL: `https://docker.io/`

  4.  Under **Authentication Method**, enter the **Username** and
      **Password** of the registry that you want to connect.

  - Use your **Docker ID** as the username (for example, john0907) and
    **not** your email address.

  5.  Select **Next**.

6.  In the **Initial Scan Configuration**, set your scanning process to
    focus on recently added or modified container images and exclude
    older ones that do not align with your current scanning objectives.
    This setting helps avoid unnecessary scans. Choose one of the
    following options:

    - **All**: Scans all container images, including all versions
      (tags), in all discovered repositories.

    - **Latest Tag**: Scans only images tagged **\'latest\'** in all
      discovered repositories.

    - **Days Modified**: Scans container images that have been created
      in the last few days. You can select a range of up to **90** days
      for the scan.

7.  Select **Save**.

- When the Docker V2 data source is saved successfully, a new data
  connector is created, and the initial discovery scan begins. The
  connection process may take up to 15 minutes. After the scan is
  complete, you can view the scanned details on the
  **Container Images Inventory** page. For more details, see [Container
  Images assets](#UUID0362c43ee0fb00c33370b18e009f4440).

  If you have selected the **Scan with Broker VM** option, then a
  **Registry Scanner** applet is created on the selected **Broker VM**
  or **Cluster**. For details, see [Verify Registry Scanner
  connection](#UUIDe68b481249fd32fe4b4b05ea4142941a).

  ![](media/rId5695.png){width="5.833333333333333in"
  height="1.3197911198600174in"}

######## Manage a Docker V2 connector

After successfully adding a connector, go to the **Docker V2 Instances**
page to check the connector status and repository scan details.

You can also modify the connector settings and configure the scanning
scope for images in the connected registry on this page.

To manage the connector, follow these steps:

1.  Select Settings \> Data Sources.

2.  Find the **Docker V2** instance from the list of
    **3rd Party Data Sources** connectors, or use **Search**.

3.  In the **Docker V2** instance row, select **View Details**. The
    **Docker V2 Instances** page appears.

4.  On the **Docker V2 Instances** page, you can filter the results by
    any heading and value. You can also create a new instance by
    selecting **+ Add Instance** and following the [onboarding
    wizard](#UUID6eaac2b964fdffd94a949aaaa5e994fe) to define the
    settings.

5.  Select an instance name to open the details pane. The details pane
    contains the following granular information:

  -----------------------------------------------------------------------
  Instance Details                    Description
  ----------------------------------- -----------------------------------
  **Status**                          Shows the status of the connector:
                                      **Connected**, **Error**,
                                      **Warning**, **Disabled**, or
                                      **Pending**.

  **Applet Status on Broker VM**      Shows the status of the
                                      **Registry Scanner** applet on the
                                      **Broker VM** page. This status is
                                      visible only when the
                                      **Scan with Broker VM** mode is
                                      selected.

  **Repositories**                    Shows the number of scanned
                                      repositories in the registry.

  **Scan Mode**                       Shows the selected scan mode for
                                      the data connector, such as
                                      **Cloud Scan**,
                                      **Scan with Outpost**, or
                                      **Scan with Broker VM**.

  **Security Capabilities**           Shows a breakdown of the security
                                      capabilities enabled on the
                                      instance and their individual
                                      statuses. For example, select
                                      **Registry Scanning** when it shows
                                      a warning or error status to see
                                      the open errors and issues that
                                      contributed to the status.
  -----------------------------------------------------------------------

6.  You can also perform actions on each Docker V2 instance: For
    example, select the
    ![](media/rId5698.png){width="0.20089238845144358in"
    height="0.20833333333333334in"} (pencil) icon to **Edit** the
    instance, or select the
    ![](media/rId5701.png){width="0.1900579615048119in"
    height="0.20833333333333334in"} (three dots) icon to
    **Add image scope**, **Delete**, or **Disable** the instance as
    follows:

+-----------------------------------+--------------------------------------------+
| Action                            | Instructions                               |
+===================================+============================================+
| **Edit**                          | Edit the Docker V2 instance.               |
|                                   |                                            |
|                                   | > **Note**                                 |
|                                   |                                            |
|                                   | - > If you                                 |
|                                   |   > selected **Scan with Broker VM** mode, |
|                                   |   > you can\'t change to a different scan  |
|                                   |   > mode (such as Cloud Discovery or Scan  |
|                                   |   > with Outpost) when you edit the        |
|                                   |   > instance.                              |
|                                   |                                            |
|                                   | - > When editing an instance configured    |
|                                   |   > for **Scan with Broker VM**, you must  |
|                                   |   > re-enter your authentication           |
|                                   |   > credentials,                           |
|                                   |   > including **Username**, **Password**,  |
|                                   |   > and **CA certificate**.                |
+-----------------------------------+--------------------------------------------+
| **Add image scope**               | Define conditions to automatically include |
|                                   | specific images in scanning. Conditions    |
|                                   | can be based on **Repository** or          |
|                                   | **Tags**. These conditions apply           |
|                                   | automatically to newly discovered images   |
|                                   | in the account.                            |
+-----------------------------------+--------------------------------------------+
| **Delete**                        | Removes the connector.                     |
+-----------------------------------+--------------------------------------------+
| **Disable**                       | Stops image scanning for the connector     |
|                                   | without deleting it.                       |
+-----------------------------------+--------------------------------------------+

####### Connect JFrog container registry

Follow the wizard to connect your JFrog Container Registry with Cortex
Cloud.

Cortex Cloud allows you to scan and secure your container images from
vulnerabilities, malware, and secrets after you authenticate and connect
your JFrog account. This process ensures robust artifact management and
enhanced security.

**How to connect JFrog**

1.  Select Settings \> Data Sources.

2.  Select **Add Data Source**.

3.  On the **Add Data Sources** page, search for and select **JFrog**,
    and then select **Connect**.

- ![](media/rId5706.png){width="5.833333333333333in"
  height="3.689583333333333in"}

4.  The **Instance Name** is automatically populated. You can change it
    to a more meaningful name.

5.  Choose the **Scan Mode**, and then follow the steps provided for
    that mode to configure the connection.

- Cloud Scan
  Security scanning is done in the Cortex cloud environment when you
  select this mode.

  1.  Select a **Cloud Provider** to initialize registry scanning.

  2.  Select the **Region** where the registry is hosted.

  3.  (Optional) Enable **Allow access by IPs** to specify a static IP
      address for the scanner to use. Make sure the static IP is allowed
      through your firewall so the scanner can access the registry
      during the scanning process.

  4.  Choose the relevant **Account Type** for JFrog deployments:

  - JFrog Cloud (Saas)
    1.  Enter your JFrog **Account Name**.

    - For example, the scanner connects to `https://myaccount.jfrog.io`,
      where `<myaccount>` is your actual account name.

    2.  Under **Authentication Method**, enter your JFrog account
        credentials (**Username** and **Password**) for authentication.

    JFrog Self-Hosted
    1.  Enter the JFrog Artifactory URL as the **Registry URL**.

    - For example, `https://artifactory.example.com/artifactory`, where
      `<artifactory.example.com>` is your server\'s domain or IP
      address.

    2.  Under **Authentication Method**, enter your JFrog user
        credentials (**Username** and **Password**) for authentication.

    3.  (Optional) Expand **Show Advanced Settings**, and then enter the
        **CA certificate** if your JFrog Artifactory server uses a
        custom CA to sign the server certificate.

  5.  Select **Next**.

  Scan with Outpost
  Security scanning is done on infrastructure deployed to a cloud
  account that you own. This mode requires additional cloud provider
  permissions and may incur extra costs.

  > **Prerequisite:**

  > Ensure an [Outpost](#UUIDf5f2211e19bc881c17c4c8de6e584185) is
  > connected to your tenant.

  1.  Choose a **Cloud Provider** to initialize registry scanning.

  - > **Note**

    > If you choose **Azure** as the **Cloud Provider**, you must also
    > select the **Tenant Id**. The Tenant Id is required to approve
    > Cortex as an enterprise application in your Azure tenant.

  2.  **Choose Outpost** account to use for this instance. If no
      **Outposts** are shown, you can **Create a new one**. For more
      details, see [Outposts](#UUIDf5f2211e19bc881c17c4c8de6e584185).

  - > **Note**

    > If you choose **Azure** as the cloud provider, only **Outposts**
    > associated with the selected tenant ID are displayed.

  3.  Select the **Region** where the registry is hosted.

  4.  (Optional) Enable **Allow access by IPs** if you want to specify a
      static IP address for the scanner to use. Make sure the static IP
      is allowed through your firewall so that the scanner can access
      the registry during the scanning process.

  5.  Choose the relevant **Account Type** for JFrog deployments:

  - JFrog Cloud (Saas)
    1.  Enter your JFrog **Account Name**.

    - For example, the scanner connects to `https://myaccount.jfrog.io`,
      where `<myaccount>` is your actual account name.

    2.  Under **Authentication Method**, enter your JFrog account
        credentials (**Username** and **Password**) for authentication.

    JFrog Self-Hosted
    1.  Enter the JFrog Artifactory URL as the **Registry URL**.

    - For example, `https://artifactory.example.com/artifactory`, where
      `<artifactory.example.com>` is your server\'s domain or IP
      address.

    2.  Under **Authentication Method**, enter your JFrog user
        credentials (**Username** and **Password**) for authentication.

    3.  (Optional) Expand **Show Advanced Settings**, and then enter the
        CA certificate if your JFrog Artifactory server uses a custom CA
        to sign the server certificate.

  6.  Select **Next**.

  Scan with Broker VM
  Security scanning in private networks is done using broker VM
  infrastructure when you select this mode.

  > **Prerequisite:**

  - > [Set up and configure Broker
    > VM](#UUIDa2b1b832d74850d81f427e175514c501)

  - > [Configure High Availability
    > Cluster](#UUIDcdafbc5b0f072deba4bdea1f522d867d)

  1.  Choose a **Scan with Broker VM** mode to initiate registry
      scanning. You can select either a standalone **Broker VM** or a
      High Availability (HA) **Cluster**.

  2.  Select **Applicable Broker VMs**.

  - Choose the appropriate **Broker VM** or **Cluster** from the list
    configured in your tenant.

    > **Note**

    - > The list of **Broker VMs** displays only VMs that support
      > registry scanning.

    - > The list of high-availability **Clusters** displays only
      > clusters that contain at least one VM supporting registry
      > scanning.

    - > The registry scanning status for each VM appears in brackets if
      > it was previously activated for that specific VM.

    If the list does not display any **Broker VMs** or **Clusters**,
    **Add New Broker VM** or **Add New Cluster**. For more details, see
    [Set up and configure Broker
    VM](#UUIDa2b1b832d74850d81f427e175514c501).

  3.  Choose the relevant **Account Type** for JFrog deployments:

  - JFrog Cloud (Saas)
    1.  Enter your JFrog **Account Name**.

    - For example, the scanner connects to `https://myaccount.jfrog.io`,
      where `<myaccount>` is your actual account name.

    2.  Under **Authentication Method**, enter your JFrog account
        credentials (**Username** and **Password**) for authentication.

    JFrog Self-Hosted
    1.  Enter the JFrog Artifactory URL as the **Registry URL**.

    - For example, `https://artifactory.example.com/artifactory`, where
      `<artifactory.example.com>` is your server\'s domain or IP
      address.

    2.  Under **Authentication Method**, enter your JFrog user
        credentials (**Username** and **Password**) for authentication.

    3.  (Optional) Expand **Show Advanced Settings**.

        1.  Select **Use insecure connection** to pull images if you
            want to allow image pull from the registry over an HTTP
            connection instead of HTTPS.

        2.  Enter the **CA certificate** if your JFrog Artifactory
            server uses a custom CA to sign the server certificate.

  4.  Select **Next**.

6.  In **Initial Scan Configuration**, set your scanning process to
    focus on recently added or modified container images and exclude
    older ones that do not align with your current scanning objectives.
    This setting helps avoid unnecessary scans. Choose one of the
    following options:

    - **All**: Scans all container images, including all versions
      (tags), in all discovered repositories.

    - **Latest Tag**: Scans only images tagged \'latest\' in all
      discovered repositories.

    - **Days Modified**: Scans container images created or modified in
      the last few days. You can select a range of up to **90** days for
      the scan.

7.  Select **Save**.

- When the JFrog data source is saved successfully, a new data connector
  is created, and the initial discovery scan is started. The connection
  process may take up to 15 minutes. After the scan is complete, you can
  view the scanned details on the **Container Images Inventory** page.
  For more details, see [Container Image
  assets](#UUID0362c43ee0fb00c33370b18e009f4440).

  If you have selected the **Scan with Broker VM** option, then a
  **Registry Scanner** applet is created on the selected **Broker VM**
  or **Cluster**. For details, see [Verify Registry Scanner
  connection](#UUIDe68b481249fd32fe4b4b05ea4142941a).

  ![](media/rId5695.png){width="5.833333333333333in"
  height="1.3197911198600174in"}

######## Manage a JFrog connector

After successfully adding a connector, go to the
**JFrog Artifactory Instances** page to check the connector status and
repository scan details.

You can also modify the connector settings and configure the scanning
scope for images in the connected registry on this page.

To manage the connector, follow these steps:

1.  Select Settings \> Data Sources.

2.  Find the **JFrog** instance from the list of
    **3rd Party Data Sources** connectors, or use **Search**.

3.  In the **JFrog** instance row, select **View Details**. The
    **JFrog Artifactory Instances** page appears.

4.  On the **JFrog Artifactory Instances** page, you can filter results
    by any heading and value. You can also create a new instance by
    selecting **+ Add Instance** and following the [onboarding
    wizard](#UUID681088abe7ed7ca3aad6629a03a1098a) to define the
    settings.

5.  Select an instance name to open the details pane. The details pane
    contains the following granular information:

  -----------------------------------------------------------------------
  Instance Details                    Description
  ----------------------------------- -----------------------------------
  **Status**                          Shows the status of the connector:
                                      **Connected**, **Error**,
                                      **Warning**, **Disabled**, or
                                      **Pending**.

  **Applet Status on Broker VM**      Shows the status of the
                                      **Registry Scanner** applet on the
                                      Broker VM page. This status is
                                      visible only when the
                                      **Scan with Broker VM** mode is
                                      selected.

  **Repositories**                    Shows the number of scanned
                                      repositories in the registry.

  **Scan Mode**                       Shows the selected scan mode for
                                      the data connector, such as
                                      **Cloud Scan**,
                                      **Scan with Outpost**, or
                                      **Scan with Broker VM**.

  **Security Capabilities**           Shows a breakdown of the security
                                      capabilities enabled on the
                                      instance and their individual
                                      statuses. For example, select
                                      Registry Scanning when it shows a
                                      warning or error status to see the
                                      open errors and issues that
                                      contributed to the status.
  -----------------------------------------------------------------------

6.  You can also perform actions on each **JFrog Artifactory** instance:
    For example, select the
    ![](media/rId5698.png){width="0.20089238845144358in"
    height="0.20833333333333334in"} (pencil) icon to **Edit** the
    instance, or select the
    ![](media/rId5701.png){width="0.1900579615048119in"
    height="0.20833333333333334in"} (three dots) icon to
    **Add image scope**, **Delete**, or **Disable** the instance as
    follows:

+-----------------------------------+--------------------------------------------+
| Action                            | Instructions                               |
+===================================+============================================+
| **Edit**                          | Edit the JFrog Artifactory instance.       |
|                                   |                                            |
|                                   | > **Note**                                 |
|                                   |                                            |
|                                   | - > If you                                 |
|                                   |   > selected **Scan with Broker VM** mode, |
|                                   |   > you can\'t change to a different scan  |
|                                   |   > mode (such as Cloud Discovery or Scan  |
|                                   |   > with Outpost) when you edit the        |
|                                   |   > instance.                              |
|                                   |                                            |
|                                   | - > When editing an instance configured    |
|                                   |   > for **Scan with Broker VM**, you must  |
|                                   |   > re-enter your authentication           |
|                                   |   > credentials,                           |
|                                   |   > including **Username**, **Password**,  |
|                                   |   > and **CA certificate**.                |
+-----------------------------------+--------------------------------------------+
| **Add image scope**               | Define conditions to automatically include |
|                                   | specific images in scanning. Conditions    |
|                                   | can be based on **Repository** or          |
|                                   | **Tags**. These conditions apply           |
|                                   | automatically to newly discovered images   |
|                                   | in the account.                            |
+-----------------------------------+--------------------------------------------+
| **Delete**                        | Removes the connector.                     |
+-----------------------------------+--------------------------------------------+
| **Disable**                       | Stops image scanning for the connector     |
|                                   | without deleting it.                       |
+-----------------------------------+--------------------------------------------+

###### Cloud service provider permissions

When you set up Cortex XSIAM to collect data from your cloud
environments, the onboarding wizard will ensure that the correct
permissions are granted for Cortex XSIAM. The following tables list the
permissions required for each of the options available in the onboarding
wizards.

Review the permissions required for each cloud service provider:

- [Amazon Web Services](#UUID64221d5367815f02277decdbc841e1a9)

- [Google Cloud Platform](#UUID424144339eaaead35299c441135b736e)

- [Microsoft Azure](#UUID3482aefacaee2c3d5d2cbeb591ff394f)

- [Oracle Cloud Infrastructure](#UUID31528075eb5ca24460a8d35cd8f4cf81)

####### Amazon Web Services provider permissions

ADS

+-------------------------------------+-----------------------+-----------------------+
| Permission                          | Scope                 | Purpose               |
+=====================================+=======================+=======================+
| ec2:ModifySnapshotAttribute         | - Snapshots with      | Share snapshot with   |
|                                     |   managed_by:         | the outpost account   |
|                                     |   paloaltonetworks    |                       |
|                                     |   tag                 |                       |
|                                     |                       |                       |
|                                     | - The snapshots can   |                       |
|                                     |   be shared only with |                       |
|                                     |   the outpost account |                       |
+-------------------------------------+-----------------------+-----------------------+
| ec2:DeleteSnapshot                  | Snapshots with        | Delete scanned        |
|                                     | managed_by:           | snapshot              |
|                                     | paloaltonetworks tag  |                       |
+-------------------------------------+-----------------------+-----------------------+
| ec2:CreateTags                      | Only as part of       | Add tags for          |
|                                     | CreateSnapshot and    | permission scoping    |
|                                     | CopySnapshot          | and cost visibility   |
|                                     | operations            |                       |
+-------------------------------------+-----------------------+-----------------------+
| ec2:DescribeSnapshots               | Snapshots with        | Retrieve snapshot     |
|                                     | managed_by:           | creation status       |
|                                     | paloaltonetworks tag  |                       |
+-------------------------------------+-----------------------+-----------------------+
| ec2:CreateSnapshot                  | Snapshots created     | Create disk snapshot  |
|                                     | with managed_by:      |                       |
|                                     | paloaltonetworks tag  |                       |
+-------------------------------------+-----------------------+-----------------------+
| ec2:CopySnapshot                    | Snapshots copied with | Re-encrypt snapshot   |
|                                     | managed_by:           | with PANW\'s KMS key  |
|                                     | paloaltonetworks tag  |                       |
+-------------------------------------+-----------------------+-----------------------+
| kms:DescribeKey                     | - PANW\'s KMS key     | To allow the          |
|                                     |                       | re-encypt operation   |
|                                     | - Only EC2 Service    |                       |
|                                     |   can use this        |                       |
|                                     |   permission          |                       |
+-------------------------------------+-----------------------+-----------------------+
| kms:GenerateDataKeyWithoutPlaintext | - PANW\'s KMS key     | To allow the          |
|                                     |                       | re-encypt operation   |
|                                     | - Only EC2 Service    |                       |
|                                     |   can use this        |                       |
|                                     |   permission          |                       |
+-------------------------------------+-----------------------+-----------------------+
| kms:CreateGrant                     | - PANW\'s KMS key     | To allow the          |
|                                     |                       | re-encypt operation   |
|                                     | - Only EC2 Service    |                       |
|                                     |   can use this        |                       |
|                                     |   permission          |                       |
+-------------------------------------+-----------------------+-----------------------+

DSPM

  ----------------------------------------------------------------------------------------------------------
  Permission                                             Scope                   Purpose
  ------------------------------------------------------ ----------------------- ---------------------------
  s3:List\*                                              All S3 buckets          To allow the listing of all
                                                                                 S3 objects

  rds:DeleteDBSnapshot                                   PANW created snapshots  To delete snapshots created
                                                                                 as part of the
                                                                                 classification process

  rds:AddTagsToResource                                  RDS resources in the    Enables creating a unique
                                                         account                 tag for the created RDS
                                                                                 resourceCreateDBSnapshots
                                                                                 in order to find them at a
                                                                                 later stage

  rds:CancelExportTask                                   RDS resources in the    Enables cancelling export
                                                         account                 tasks in case of failure or
                                                                                 termination of the
                                                                                 classification process

  rds:CreateDBClusterSnapshot                            RDS resources in the    Enables creating a snapshot
                                                         account                 for the RDS clusters that
                                                                                 need to be scanned at a
                                                                                 later stage

  rds:CreateDBSnapshot                                   RDS resources in the    Enables creating a snapshot
                                                         account                 for the RDS instances that
                                                                                 need to be scanned at a
                                                                                 later stage

  rds:Describe\*                                         RDS resources in the    Describe permissions enable
                                                         account                 PANW to get metadata
                                                                                 information on the RDS
                                                                                 instance

  rds:List\*                                             RDS resources in the    List permissions enable
                                                         account                 PANW to understand which
                                                                                 instances and snapshots
                                                                                 exist in the account

  rds:StartExportTask                                    RDS resources in the    Enables to export data from
                                                         account                 the snapshots to an S3
                                                                                 bucket

  s3:PutObject\*                                         PANW created buckets    Enables writing data to an
                                                                                 object in PANW's bucket to
                                                                                 export data from the RDS
                                                                                 instances

  s3:DeleteObject\*                                      PANW created buckets    Enables deleting stale
                                                                                 objects that were created

  s3:Get\*                                               All S3 buckets          Enable PANW to read data
                                                                                 within S3 buckets

  kms:DescribeKey                                        KMS keys in the account Enables getting information
                                                                                 about the KMS keys in the
                                                                                 account

  kms:GenerateDataKeyWithoutPlaintext                    AWS account             Enables getting information
                                                                                 about the KMS keys in the
                                                                                 account

  kms:CreateGrant                                        KMS keys in the account The created EC2 instance
                                                                                 sends a CreateGrant request
                                                                                 to AWS KMS so that it can
                                                                                 share the encrypted
                                                                                 snapshot with the outpost
                                                                                 account

  iam:PassRole                                           PANW scanner role       Enables creating export
                                                                                 tasks for RDS snapshots

  arn:aws:iam::aws:policy/AmazonMemoryDBReadOnlyAccess   DynamoDB resource in    Read-only access to the
                                                         the account             MemoryDB resources

  dynamodb:DescribeTable                                 All DynamoDB tables     Enables getting information
                                                                                 about DynamoDB tables in
                                                                                 the account

  dynamodb:Scan                                          All DynamoDB tables     Enables accessing data in
                                                                                 DynamoDB tables in the
                                                                                 account

  cloudwatch:GetMetricStatistics                         All DynamoDB tables     Enables getting usage
                                                                                 statistics, which is used
                                                                                 to ensure that
                                                                                 classification processes do
                                                                                 not interfere with
                                                                                 production environments
  ----------------------------------------------------------------------------------------------------------

Discovery Engine

  -------------------------------------------------------------------------------------
  Permission                                        Purpose
  ------------------------------------------------- -----------------------------------
  arn:aws:iam::aws:policy/AmazonSQSReadOnlyAccess   Grants read-only access to Amazon
                                                    Simple Queue Service (SQS). Allows
                                                    the retrieval of SQS queue
                                                    attributes, messages, and
                                                    configurations.

  arn:aws:iam::aws:policy/ReadOnlyAccess            Grants read-only access to AWS
                                                    services and resources. Enables the
                                                    ability to list and view
                                                    configurations, metadata, and logs
                                                    across AWS resources.

  arn:aws:iam::aws:policy/SecurityAudit             Grants access to read security
                                                    configuration metadata. Allows
                                                    users to inspect IAM
                                                    configurations, security policies,
                                                    CloudTrail logs, and other
                                                    security-relevant settings.

  DS:DescribeDirectories                            Grants read access to directory
                                                    details in AWS Directory Service.

  DS:ListTagsForResource                            Lists tags associated with a
                                                    specific AWS Directory Service
                                                    resource

  DirectConnect:DescribeConnections                 Lists Direct Connect connections
                                                    and their attributes

  DirectConnect:DescribeDirectConnectGateways       Retrieves details about Direct
                                                    Connect gateways

  DirectConnect:DescribeVirtualInterfaces           Displays all virtual interfaces for
                                                    an AWS account

  Glue:GetSecurityConfigurations                    Retrieves security configurations
                                                    for AWS Glue

  WorkSpaces:DescribeTags                           Lists tags associated with
                                                    WorkSpaces resources

  WorkSpaces:DescribeWorkspaceDirectories           Retrieves details about WorkSpaces
                                                    directories

  WorkSpaces:DescribeWorkspaces                     Lists and describes WorkSpaces
                                                    instances

  apigateway:GetDomainNames                         Retrieves API Gateway custom domain
                                                    names

  bedrock-agent:GetAgents                           Retrieves details of Bedrock agents

  bedrock-agent:GetDataSource                       Retrieves details of a specific
                                                    data source

  bedrock-agent:GetKnowledgeBases                   Retrieves details of knowledge
                                                    bases

  bedrock-agent:ListAgentAliases                    Lists aliases associated with an
                                                    agent

  bedrock-agent:ListAgentKnowledgeBases             Lists knowledge bases linked to
                                                    agents

  bedrock-agent:ListAgents                          Lists all Bedrock agents

  bedrock-agent:ListDataSource                      Lists available data sources

  bedrock:ListCustomModel                           Lists custom AI models in Amazon
                                                    Bedrock, enabling visibility into
                                                    custom AI model configurations

  cloudcontrolapi:GetResource                       Retrieves the state of an AWS
                                                    resource managed via Cloud Control
                                                    API

  cloudformation:AmazonCloudFormation               General permission related to
                                                    CloudFormation resource management

  cloudformation:StackStatus                        Retrieves the status of
                                                    CloudFormation stacks

  cloudformation:StackSummary                       Provides a summary of
                                                    CloudFormation stacks

  cloudwatch:describeAlarms                         Retrieves details about CloudWatch
                                                    alarms

  comprehendmedical:ListEntitiesDetectionV2Jobs     Lists entity detection jobs in
                                                    Comprehend Medical

  configservice:DescribeDeliveryChannels            Retrieves details of AWS Config
                                                    delivery channels

  elasticfilesysttem:DescribeFileSystemPolicy       Retrieves policies associated with
                                                    an EFS file system

  elasticloadbalancingv2:DescribeSSLPolicies        Retrieves details of ELB SSL
                                                    policies

  forecast:ListTagsForResource                      Lists tags associated with an
                                                    Amazon Forecast resource

  glue:GetConnections                               Lists connection configurations for
                                                    AWS Glue

  glue:GetResourcePolicies                          Retrieves Glue Data Catalog
                                                    policies

  iam:AmazonIdentityManagement                      General IAM access for identity and
                                                    access management

  iam:AttachedPolicy                                Retrieves policies attached to IAM
                                                    identities

  iam:PolicyRole                                    Lists IAM roles associated with a
                                                    policy

  iam:RoleDetail                                    Retrieves detailed information
                                                    about IAM roles

  opensearchserverless:ListCollections              Lists collections in OpenSearch
                                                    Serverless

  s3-control:GetAccessPointPolicy                   Retrieves an S3 access point policy

  s3-control:GetAccessPointPolicyStatus             Retrieves the status of an access
                                                    point policy

  s3-control:GetPublicAccessBlock                   Retrieves the public access block
                                                    configuration for an account

  s3-control:ListAccessPoints                       Lists S3 access points that are
                                                    owned by the current account
                                                    that\'s associated with the
                                                    specified bucket

  servicecatalog-appregistry:ListApplications       Lists applications in AWS
                                                    AppRegistry

  servicecatalog-appregistry:ListAttributeGroups    Lists attribute groups in
                                                    AppRegistry
  -------------------------------------------------------------------------------------

Registry Scan

  ----------------------------------------------------------------------------------
  Permission                         Scope                   Purpose
  ---------------------------------- ----------------------- -----------------------
  ecr:BatchGetImage                  All ECR images in the   Gets detailed
                                     account                 information for an
                                                             image, required in
                                                             order to pull the image

  ecr:GetDownloadUrlForLayer         All ECR images in the   Used in the process of
                                     account                 pulling images, to
                                                             fetch the URL for the
                                                             various layers that
                                                             make up the image

  ecr:GetAuthorizationToken          All ECR images in the   Used to create a login
                                     account                 toker for pulling
                                                             images from ECR

  ecr-public:GetAuthorizationToken   All public ECR images   Used to create a login
                                     in the account          token for pulling
                                                             images from public ECR
  ----------------------------------------------------------------------------------

Log Collection

  -----------------------------------------------------------------------
  Permission                          Purpose
  ----------------------------------- -----------------------------------
  s3:GetObject                        Grants permission to download
                                      objects from the configured S3
                                      bucket

  s3:ListBucket                       Grants permission to see the
                                      specific bucket

  sqs:ReceiveMessage                  Grants permission to consume
                                      messages from the SQS queue to
                                      receive bucket notification
                                      messages

  sqs:DeleteMessage                   Grants permission to delete
                                      consumed messages, preventing
                                      re-processing of the same message

  sqs:GetQueueAttributes              Grants permission to retrieve SQS
                                      queue attributes, used for metrics
                                      and monitoring
  -----------------------------------------------------------------------

####### Google Cloud Platform provider permissions

ADS

  -------------------------------------------------------------------------------
  Permission                      Scope                   Purpose
  ------------------------------- ----------------------- -----------------------
  compute.snapshots.get           Snapshots with          Retrieve snapshot
                                  \"cortex-scan-\" prefix creation status

  compute.snapshots.create        Snapshots with          Create disk snapshot
                                  \"cortex-scan-\" prefix 

  compute.snapshots.delete        Snapshots with          Delete scanned snapshot
                                  \"cortex-scan-\" prefix 

  compute.snapshots.setLabels     Snapshots with          Add snapshot labels for
                                  \"cortex-scan-\" prefix a cost visibility

  compute.snapshots.useReadOnly   Snapshots with          Attach snapshot to a
                                  \"cortex-scan-\" prefix scanner VM
  -------------------------------------------------------------------------------

DSPM

  -----------------------------------------------------------------------------------------
  Permission                          Scope             Purpose           Notes
  ----------------------------------- ----------------- ----------------- -----------------
  bigquery.bireservations.get         All BigQuery      Enables           
                                      instances         classification of 
                                                        BigQuery by       
                                                        allowing access   
                                                        to data and usage 

  bigquery.capacityCommitments.get    All BigQuery      Enables           
                                      instances         classification of 
                                                        BigQuery by       
                                                        allowing access   
                                                        to data and usage 

  bigquery.capacityCommitments.list   All BigQuery      Enables           
                                      instances         classification of 
                                                        BigQuery by       
                                                        allowing access   
                                                        to data and usage 

  bigquery.config.get                 All BigQuery      Enables           
                                      instances         classification of 
                                                        BigQuery by       
                                                        allowing access   
                                                        to data and usage 

  bigquery.datasets.get               All BigQuery      Enables           
                                      instances         classification of 
                                                        BigQuery by       
                                                        allowing access   
                                                        to data and usage 

  bigquery.datasets.getIamPolicy      All BigQuery      Enables           
                                      instances         classification of 
                                                        BigQuery by       
                                                        allowing access   
                                                        to data and usage 

  bigquery.models.getData             All BigQuery      Enables           
                                      instances         classification of 
                                                        BigQuery by       
                                                        allowing access   
                                                        to data and usage 

  bigquery.models.getMetadata         All BigQuery      Enables           
                                      instances         classification of 
                                                        BigQuery by       
                                                        allowing access   
                                                        to data and usage 

  bigquery.models.list                All BigQuery      Enables           
                                      instances         classification of 
                                                        BigQuery by       
                                                        allowing access   
                                                        to data and usage 

  bigquery.routines.get               All BigQuery      Enables           
                                      instances         classification of 
                                                        BigQuery by       
                                                        allowing access   
                                                        to data and usage 

  bigquery.routines.list              All BigQuery      Enables           
                                      instances         classification of 
                                                        BigQuery by       
                                                        allowing access   
                                                        to data and usage 

  bigquery.tables.export              All BigQuery      Enables           
                                      instances         classification of 
                                                        BigQuery by       
                                                        allowing access   
                                                        to data and usage 

  bigquery.tables.get                 All BigQuery      Enables           
                                      instances         classification of 
                                                        BigQuery by       
                                                        allowing access   
                                                        to data and usage 

  bigquery.tables.getData             All BigQuery      Enables           
                                      instances         classification of 
                                                        BigQuery by       
                                                        allowing access   
                                                        to data and usage 

  bigquery.tables.getIamPolicy        All BigQuery      Enables           
                                      instances         classification of 
                                                        BigQuery by       
                                                        allowing access   
                                                        to data and usage 

  bigquery.tables.list                All BigQuery      Enables           
                                      instances         classification of 
                                                        BigQuery by       
                                                        allowing access   
                                                        to data and usage 

  cloudsql.backupRuns.get             All CloudSQL      Enables           
                                      instances         classification of 
                                                        CloudSQL by       
                                                        allowing access   
                                                        to data and       
                                                        backups           

  cloudsql.backupRuns.create          All CloudSQL      Enables           
                                      instances         classification of 
                                                        CloudSQL by       
                                                        allowing access   
                                                        to data and       
                                                        backups           

  cloudsql.backupRuns.delete          All CloudSQL      Enables           
                                      instances         classification of 
                                                        CloudSQL by       
                                                        allowing access   
                                                        to data and       
                                                        backups           

  cloudsql.backupRuns.get             All CloudSQL      Enables           
                                      instances         classification of 
                                                        CloudSQL by       
                                                        allowing access   
                                                        to data and       
                                                        backups           

  cloudsql.backupRuns.list            All CloudSQL      Enables           
                                      instances         classification of 
                                                        CloudSQL by       
                                                        allowing access   
                                                        to data and       
                                                        backups           

  roles/cloudfunctions.viewer                                             Built-in role

  roles/container.clusterViewer                                           Built-in role

  roles/storage.objectViewer                                              Built-in role

  roles/firebaserules.viewer                                              Built-in role
  -----------------------------------------------------------------------------------------

Discovery Engine

  -------------------------------------------------------------------------------------------------
  Permission                                        Purpose                 Notes
  ------------------------------------------------- ----------------------- -----------------------
  roles/viewer                                                              Built-in role

  roles/cloudfunctions.viewer                                               Built-in role

  roles/container.clusterViewer                                             Built-in role

  roles/firebaserules.viewer                                                Built-in role

  roles/storage.objectViewer                                                Built-in role

  serviceusage.services.use                         Use cloud services      

  storage.buckets.get                               Get metadata of a       
                                                    storage bucket          

  storage.buckets.getIamPolicy                      Get IAM policy of a     
                                                    storage bucket          

  storage.buckets.list                              List storage buckets    

  storage.buckets.listEffectiveTags                 List effective tags of  
                                                    storage buckets         

  storage.buckets.listTagBindings                   List tag bindings of    
                                                    storage buckets         

  storage.objects.getIamPolicy                      Get IAM policy of       
                                                    storage objects         

  run.services.list                                 List Cloud Run services 

  run.jobs.list                                     List Cloud Run jobs     

  run.jobs.getIamPolicy                             Get IAM policy of Cloud 
                                                    Run jobs                

  cloudscheduler.jobs.list                          List Cloud Scheduler    
                                                    jobs                    

  baremetalsolution.instances.list                  List Bare Metal         
                                                    Solution instances      

  baremetalsolution.networks.list                   List Bare Metal         
                                                    Solution networks       

  baremetalsolution.nfsshares.list                  List Bare Metal         
                                                    Solution NFS shares     

  baremetalsolution.volumes.list                    List Bare Metal         
                                                    Solution volumes        

  baremetalsolution.luns.list                       List Bare Metal         
                                                    Solution LUNs (Logical  
                                                    Unit Numbers)           

  analyticshub.dataExchanges.list                   List Analytics Hub data 
                                                    exchanges               

  analyticshub.listings.getIamPolicy                Get IAM policy for      
                                                    Analytics Hub listings  

  analyticshub.listings.list                        List Analytics Hub      
                                                    listings                

  notebooks.locations.list                          List notebook locations 

  notebooks.schedules.list                          List notebook schedules 

  composer.imageversions.list                       List Composer image     
                                                    versions                

  datamigration.connectionprofiles.list             List data migration     
                                                    connection profiles     

  datamigration.connectionprofiles.getIamPolicy     Get IAM policy for data 
                                                    migration connection    
                                                    profiles                

  datamigration.conversionworkspaces.list           List data migration     
                                                    conversion workspaces   

  datamigration.conversionworkspaces.getIamPolicy   Get IAM policy for data 
                                                    migration conversion    
                                                    workspaces              

  datamigration.migrationjobs.list                  List data migration     
                                                    jobs                    

  datamigration.migrationjobs.getIamPolicy          Get IAM policy for data 
                                                    migration jobs          

  datamigration.privateconnections.list             List data migration     
                                                    private connections     

  datamigration.privateconnections.getIamPolicy     Get IAM policy for data 
                                                    migration private       
                                                    connections             

  datamigration.migrationjobs.list                  List AI Platform batch  
                                                    prediction jobs         

  datamigration.migrationjobs.getIamPolicy          List AI Platform NAS    
                                                    jobs                    

  datamigration.privateconnections.list             List Cloud Security     
                                                    Scanner scans           

  datamigration.privateconnections.getIamPolicy     Allows viewing the      
                                                    access policy for a     
                                                    Database Migration      
                                                    Service private         
                                                    connection              

  aiplatform.batchPredictionJobs.list               Allows listing AI       
                                                    Platform batch          
                                                    prediction jobs         

  aiplatform.nasJobs.list                           Allows listing AI       
                                                    Platform Neural         
                                                    Architecture Search     
                                                    (NAS) jobs              

  cloudsecurityscanner.scans.list                   Allows listing Cloud    
                                                    Security Scanner scans  
  -------------------------------------------------------------------------------------------------

Log Collection

  -------------------------------------------------------------------------
  Permission                Purpose                 Notes
  ------------------------- ----------------------- -----------------------
  roles/pubsub.subscriber   Grants access to        Built-in role
                            consume messages from   
                            the subscription where  
                            audit logs are stored   

  -------------------------------------------------------------------------

Registry Scan

  -------------------------------------------------------------------------------------------------
  Permission                                        Scope                   Purpose
  ------------------------------------------------- ----------------------- -----------------------
  artifactregistry.repositories.downloadArtifacts   All artifacts listed in Needed in order to
                                                    the GAR of the          download images from
                                                    customer\'s account     GAR

  roles/iam.serviceAccountTokenCreator              Access to this          Allows impersonation to
                                                    permission is limited   a specfiic service
                                                    to a specific Service   account
                                                    Account defined within  
                                                    the Outpost. No account 
                                                    other than the defined  
                                                    Service Account can     
                                                    access the permission   
                                                    and access is limited   
                                                    to the permissions      
                                                    defined on the target   
                                                    SA                      
  -------------------------------------------------------------------------------------------------

####### Microsoft Azure provider permissions

ADS

  ----------------------------------------------------------------------------------------------
  Permission                               Module            Scope             Purpose
  ---------------------------------------- ----------------- ----------------- -----------------
  Microsoft.Compute/snapshots/write        ADS               No scoping        Create disk
                                                                               snapshot

  Microsoft.Compute/snapshots/delete       ADS               No scoping        Delete scanned
                                                                               snapshot

  Microsoft.Compute/virtualMachines/read   ADS               No scoping        Allow disk
                                                                               snapshot
                                                                               operation

  Microsoft.Compute/snapshots/read         ADS               No scoping        Convert snapshot
                                                                               to disk that will
                                                                               be attached to
                                                                               the scanner
  ----------------------------------------------------------------------------------------------

DSPM

  ---------------------------------------------------------------------------------------------------------------------------------
  Permission                                                                        Scope                   Purpose
  --------------------------------------------------------------------------------- ----------------------- -----------------------
  Microsoft.Storage/storageAccounts/PrivateEndpointConnectionsApproval/action       Entire subscription     Enabling a scan by
                                                                                                            assigning private
                                                                                                            endpoints to a storage
                                                                                                            account located in a
                                                                                                            private network

  Microsoft.Storage/storageAccounts/blobServices/containers/blobs/read              All blobs               Enables classification
                                                                                                            of data in storage
                                                                                                            blobs

  Microsoft.Storage/storageAccounts/fileServices/fileshares/files/read              All fileshares          Enables classification
                                                                                                            of data in storage
                                                                                                            fileshares

  Microsoft.Storage/storageAccounts/listKeys/action                                 Entire subscription     Getting access key to
                                                                                                            the storage account to
                                                                                                            scan file share
                                                                                                            instances using API

  Microsoft.Storage/storageAccounts/ListAccountSas/action                           Entire subscription     Getting access SAS
                                                                                                            token to the storage
                                                                                                            account to scan file
                                                                                                            share instances using
                                                                                                            API

  Microsoft.Storage/storageAccounts/PrivateEndpointConnectionsApproval/action       Entire subscription     Enabling a scan by
                                                                                                            assigning private
                                                                                                            endpoints to a storage
                                                                                                            account located in a
                                                                                                            private network

  Microsoft.Storage/\*/read                                                         Entire subscription     Reading blobs data for
                                                                                                            data classification

  Microsoft.Storage/storageAccounts/blobServices/generateUserDelegationKey/action   Entire subscription     Getting SAS token of
                                                                                                            blobServices to enable
                                                                                                            access

  Microsoft.DocumentDB/databaseAccounts/listKeys/                                   Entire subscription     Getting SAS token of
                                                                                                            CosmosDB to enable
                                                                                                            access

  Microsoft.Storage/storageAccounts/tableServices/tables/entities/read              All storage tables      Enables classification
                                                                                                            of data in storage
                                                                                                            tables

  Microsoft.CognitiveServices/\*/read                                               All deployments         Enables discovery of
                                                                                                            OpenAI resources and
                                                                                                            other Azure AI services

  Microsoft.CognitiveServices/\*/action                                             All deployments         Enables reading and
                                                                                                            scanning OpenAI files
                                                                                                            and other Azure AI data
                                                                                                            resources

  \*/read                                                                           Entire subscription     Read-only access, used
                                                                                                            to get metadata of all
                                                                                                            managed data assets in
                                                                                                            the subscription

  Microsoft.Network/routeTables/write                                               PANW resources          Enables routing and
                                                                                                            managing internal
                                                                                                            traffic for DB
                                                                                                            classification

  Microsoft.Network/routeTables/join/action                                         PANW resources          Enables routing and
                                                                                                            managing internal
                                                                                                            traffic for DB
                                                                                                            classification

  Microsoft.Network/routeTables/delete                                              PANW resources          Enables routing and
                                                                                                            managing internal
                                                                                                            traffic for DB
                                                                                                            classification

  Microsoft.Network/virtualNetworks/delete                                          PANW resources          Enables routing and
                                                                                                            managing internal
                                                                                                            traffic for DB
                                                                                                            classification

  Microsoft.Network/virtualNetworks/join/action                                     PANW resources          Enables routing and
                                                                                                            managing internal
                                                                                                            traffic for DB
                                                                                                            classification

  Microsoft.Network/virtualNetworks/subnets/delete                                  PANW resources          Enables routing and
                                                                                                            managing internal
                                                                                                            traffic for DB
                                                                                                            classification

  Microsoft.Network/virtualNetworks/subnets/join/action                             PANW resources          Enables routing and
                                                                                                            managing internal
                                                                                                            traffic for DB
                                                                                                            classification

  Microsoft.Network/virtualNetworks/subnets/write                                   PANW resources          Enables routing and
                                                                                                            managing internal
                                                                                                            traffic for DB
                                                                                                            classification

  Microsoft.Network/virtualNetworks/write                                           PANW resources          Enables routing and
                                                                                                            managing internal
                                                                                                            traffic for DB
                                                                                                            classification

  Microsoft.Network/networkSecurityGroups/securityRules/write                       PANW resources          Enables routing and
                                                                                                            managing internal
                                                                                                            traffic for DB
                                                                                                            classification

  Microsoft.Network/networkSecurityGroups/securityRules/delete                      PANW resources          Enables routing and
                                                                                                            managing internal
                                                                                                            traffic for DB
                                                                                                            classification

  Microsoft.Network/networkSecurityGroups/join/action                               PANW resources          Enables routing and
                                                                                                            managing internal
                                                                                                            traffic for DB
                                                                                                            classification

  Microsoft.Network/networkSecurityGroups/delete                                    PANW resources          Enables routing and
                                                                                                            managing internal
                                                                                                            traffic for DB
                                                                                                            classification

  Microsoft.Network/networkSecurityGroups/write                                     PANW resources          Enables routing and
                                                                                                            managing internal
                                                                                                            traffic for DB
                                                                                                            classification

  Microsoft.Sql/servers/databases/read                                              PANW resources          Enables getting
                                                                                                            configurations on Azure
                                                                                                            SQL databases

  Microsoft.Sql/servers/databases/write                                             PANW resources          Enables copying and
                                                                                                            managing SQL databases
                                                                                                            in Azure SQL server

  Microsoft.Sql/servers/databases/resume/action                                     PANW resources          Enables copying and
                                                                                                            managing SQL databases
                                                                                                            in Azure SQL server

  Microsoft.Sql/servers/databases/delete                                            PANW resources          Enable cleaning stale
                                                                                                            assets such as PANW's
                                                                                                            Azure SQL server
                                                                                                            databases

  Microsoft.Sql/servers/delete                                                      PANW resources          Enable cleaning stale
                                                                                                            assets such as PANW's
                                                                                                            Azure SQL server

  Microsoft.Sql/servers/write                                                       PANW resources          Enables creating and
                                                                                                            managing PANW's Azure
                                                                                                            SQL servers

  Microsoft.Sql/servers/virtualNetworkRules/write                                   PANW resources          Enables configuring
                                                                                                            network accessibility
                                                                                                            from the scanning VMs
                                                                                                            on PANW's Azure SQL
                                                                                                            servers

  Microsoft.Sql/servers/privateEndpointConnectionsApproval/action                   PANW resources          Enables connection
                                                                                                            using endpoints

  Microsoft.Sql/managedInstances/write                                              PANW resources          Enables creation of SQL
                                                                                                            Managed Instance for
                                                                                                            classification of
                                                                                                            managed instances

  Microsoft.Sql/managedInstances/databases/write                                    PANW resources          Used for copying PITR
                                                                                                            of SQL managed
                                                                                                            instances to PANW's
                                                                                                            resource group,
                                                                                                            enabling PANW to
                                                                                                            restore and scan it

  Microsoft.Sql/managedInstances/delete                                             PANW resources          Enable cleaning stale
                                                                                                            assets such as PANW's
                                                                                                            Azure SQL Managed
                                                                                                            Instance
  ---------------------------------------------------------------------------------------------------------------------------------

Discovery Engine

  ---------------------------------------------------------------------------------------------------------------------------------------------------------
  Permission                                                                                                            Purpose
  --------------------------------------------------------------------------------------------------------------------- -----------------------------------
  Domain.Read.All                                                                                                       

  EntitlementManagement.Read.All                                                                                        

  User.Read.All                                                                                                         

  Policy.ReadWrite.AuthenticationMethod                                                                                 

  GroupMember.Read.All                                                                                                  

  RoleManagement.Read.All                                                                                               

  Group.Read.All                                                                                                        

  AuditLog.Read.All                                                                                                     

  Policy.Read.All                                                                                                       

  IdentityProvider.Read.All                                                                                             

  Directory.Read.All                                                                                                    

  Organization.Read.All                                                                                                 

  Microsoft.ContainerInstance/containerGroups/containers/exec/action                                                    Execute commands in a container

  Microsoft.ContainerRegistry/registries/webhooks/getCallbackConfig/action                                              Get webhook callback config

  Microsoft.DocumentDB/databaseAccounts/listConnectionStrings/action                                                    List Cosmos DB connection strings

  Microsoft.DocumentDB/databaseAccounts/listKeys/action                                                                 List Cosmos DB access keys

  Microsoft.DocumentDB/databaseAccounts/readonlykeys/action                                                             Get Cosmos DB read-only keys

  Microsoft.Network/networkInterfaces/effectiveNetworkSecurityGroups/action                                             View effective NSGs on NICs

  Microsoft.Network/networkInterfaces/effectiveRouteTable/action                                                        View effective route table on NICs

  Microsoft.Network/networkWatchers/queryFlowLogStatus/\*                                                               Query NSG flow log status

  Microsoft.Network/networkWatchers/read                                                                                Read network watcher settings

  Microsoft.Network/networkWatchers/securityGroupView/action                                                            View effective security rules

  Microsoft.Network/virtualwans/vpnconfiguration/action                                                                 Download VPN config

  Microsoft.Storage/storageAccounts/listKeys/action                                                                     List storage account keys

  Microsoft.Web/sites/config/list/action                                                                                List web app configuration

  Microsoft.Advisor/configurations/read                                                                                 Read Advisor configuration

  Microsoft.AlertsManagement/prometheusRuleGroups/read                                                                  Read Prometheus rule groups

  Microsoft.AlertsManagement/smartDetectorAlertRules/read                                                               Read smart detector alert rules

  Microsoft.AnalysisServices/servers/read                                                                               Read Analysis Services servers

  Microsoft.ApiManagement/service/apis/diagnostics/read                                                                 Read diagnostics info of APIs

  Microsoft.ApiManagement/service/apis/policies/read                                                                    Read policies on APIs

  Microsoft.ApiManagement/service/apis/read                                                                             Read API details

  Microsoft.ApiManagement/service/identityProviders/read                                                                Read API Management identity
                                                                                                                        providers

  Microsoft.ApiManagement/service/portalsettings/read                                                                   Read developer portal settings

  Microsoft.ApiManagement/service/products/policies/read                                                                Read policies on API products

  Microsoft.ApiManagement/service/products/read                                                                         Read API products

  Microsoft.ApiManagement/service/read                                                                                  Read API Management service info

  Microsoft.ApiManagement/service/tenant/read                                                                           Read tenant info in API Management

  Microsoft.AppConfiguration/configurationStores/read                                                                   Read Azure App Configuration stores

  Microsoft.AppPlatform/Spring/apps/read                                                                                Read Spring apps in Azure App
                                                                                                                        Platform

  Microsoft.AppPlatform/Spring/read                                                                                     Read Azure App Platform Spring
                                                                                                                        resource info

  Microsoft.Attestation/attestationProviders/read                                                                       Read attestation providers

  Microsoft.Authorization/classicAdministrators/read                                                                    Read classic administrators info

  Microsoft.Authorization/locks/read                                                                                    Read resource locks

  Microsoft.Authorization/permissions/read                                                                              Read permissions

  Microsoft.Authorization/policyAssignments/read                                                                        Read policy assignments

  Microsoft.Authorization/policyDefinitions/read                                                                        Read policy definitions

  Microsoft.Authorization/roleAssignments/read                                                                          Read role assignments

  Microsoft.Authorization/roleDefinitions/read                                                                          Read role definitions

  Microsoft.Automanage/configurationProfiles/Read                                                                       Read Automanage configuration
                                                                                                                        profiles

  Microsoft.Automation/automationAccounts/credentials/read                                                              Read credentials in automation
                                                                                                                        accounts

  Microsoft.Automation/automationAccounts/hybridRunbookWorkerGroups/read                                                Read hybrid runbook worker groups

  Microsoft.Automation/automationAccounts/read                                                                          Read automation accounts

  Microsoft.Automation/automationAccounts/runbooks/read                                                                 Read runbooks

  Microsoft.Automation/automationAccounts/variables/read                                                                Read variables in automation
                                                                                                                        accounts

  Microsoft.AzureStackHCI/Clusters/Read                                                                                 Read Azure Stack HCI clusters

  Microsoft.Batch/batchAccounts/pools/read                                                                              Read batch account pools

  Microsoft.Batch/batchAccounts/read                                                                                    Read batch accounts

  Microsoft.Blueprint/blueprints/read                                                                                   Read blueprints

  Microsoft.BotService/botServices/read                                                                                 Read bot services

  Microsoft.Cache/redis/firewallRules/read                                                                              Read firewall rules on Redis cache

  Microsoft.Cache/redis/read                                                                                            Read Redis caches

  Microsoft.Cache/redisEnterprise/read                                                                                  Read Redis Enterprise caches

  Microsoft.Cdn/profiles/afdendpoints/read                                                                              Read CDN profile AFD endpoints

  Microsoft.Cdn/profiles/afdendpoints/routes/read                                                                       Read routes of CDN profile AFD
                                                                                                                        endpoints

  Microsoft.Cdn/profiles/customdomains/read                                                                             Read custom domains in CDN profiles

  Microsoft.Cdn/profiles/endpoints/customdomains/read                                                                   Read custom domains of CDN
                                                                                                                        endpoints

  Microsoft.Cdn/profiles/endpoints/read                                                                                 Read CDN profile endpoints

  Microsoft.Cdn/profiles/origingroups/read                                                                              Read origin groups in CDN profiles

  Microsoft.Cdn/profiles/read                                                                                           Read CDN profiles

  Microsoft.Cdn/profiles/securitypolicies/read                                                                          Read CDN profile security policies

  Microsoft.Chaos/experiments/read                                                                                      Read Chaos experiments

  Microsoft.ClassicCompute/VirtualMachines/read                                                                         Read classic compute virtual
                                                                                                                        machines

  Microsoft.ClassicNetwork/networkSecurityGroups/read                                                                   Read classic network security
                                                                                                                        groups

  Microsoft.ClassicNetwork/reservedIps/read                                                                             Read classic network reserved IPs

  Microsoft.ClassicNetwork/virtualNetworks/read                                                                         Read classic virtual networks

  Microsoft.ClassicStorage/StorageAccounts/read                                                                         Read classic storage accounts

  Microsoft.CognitiveServices/accounts/read                                                                             Read Cognitive Services accounts

  Microsoft.CognitiveServices/accounts/deployments/read                                                                 Read deployments in Cognitive
                                                                                                                        Services accounts

  Microsoft.CognitiveServices/accounts/raiPolicies/read                                                                 Read RAI policies in Cognitive
                                                                                                                        Services accounts

  Microsoft.CognitiveServices/models/read                                                                               Read Cognitive Services models

  Microsoft.CognitiveServices/accounts/models/read                                                                      Read models in Cognitive Services
                                                                                                                        accounts

  Microsoft.Communication/CommunicationServices/Read                                                                    Read Communication Services

  Microsoft.Compute/availabilitySets/read                                                                               Read availability sets

  Microsoft.Compute/cloudServices/read                                                                                  Read cloud services

  Microsoft.Compute/cloudServices/roleInstances/read                                                                    Read cloud service role instances

  Microsoft.Compute/diskEncryptionSets/read                                                                             Read disk encryption sets

  Microsoft.Compute/disks/beginGetAccess/action                                                                         Begin get access on disks (action)

  Microsoft.Compute/disks/read                                                                                          Read disks

  Microsoft.Compute/galleries/images/read                                                                               Read gallery images

  Microsoft.Compute/galleries/read                                                                                      Read galleries

  Microsoft.Compute/hostGroups/read                                                                                     Read host groups

  Microsoft.Compute/snapshots/read                                                                                      Read snapshots

  Microsoft.Compute/virtualMachineScaleSets/networkInterfaces/read                                                      Read network interfaces of VM scale
                                                                                                                        sets

  Microsoft.Compute/virtualMachineScaleSets/publicIPAddresses/read                                                      Read public IP addresses of VM
                                                                                                                        scale sets

  Microsoft.Compute/virtualMachineScaleSets/read                                                                        Read virtual machine scale sets

  Microsoft.Compute/virtualMachineScaleSets/virtualMachines/networkInterfaces/ipConfigurations/publicIPAddresses/read   Read public IPs of VM scale set VM
                                                                                                                        NICs IP configs

  Microsoft.Compute/virtualMachineScaleSets/virtualMachines/read                                                        Read virtual machines in VM scale
                                                                                                                        sets

  Microsoft.Compute/virtualMachineScaleSets/virtualmachines/instanceView/read                                           Read instance view of VM scale set
                                                                                                                        VMs

  Microsoft.Compute/virtualMachines/extensions/read                                                                     Read VM extensions

  Microsoft.Compute/virtualMachines/instanceView/read                                                                   Read VM instance view

  Microsoft.Compute/virtualMachines/read                                                                                Read virtual machines

  Microsoft.Confluent/organizations/Read                                                                                Read Confluent organizations

  Microsoft.ContainerInstance/containerGroups/containers/exec/action                                                    Execute commands in container
                                                                                                                        instances

  Microsoft.ContainerInstance/containerGroups/read                                                                      Read container groups

  Microsoft.ContainerRegistry/registries/metadata/read                                                                  Read container registry metadata

  Microsoft.ContainerRegistry/registries/pull/read                                                                      Read/pull from container registries

  Microsoft.ContainerRegistry/registries/read                                                                           Read container registries

  Microsoft.ContainerRegistry/registries/webhooks/getCallbackConfig/action                                              Get webhook callback config

  Microsoft.ContainerService/managedClusters/read                                                                       Read managed Kubernetes clusters

  Microsoft.DBforMariaDB/servers/firewallRules/read                                                                     Read MariaDB server firewall rules

  Microsoft.DBforMariaDB/servers/read                                                                                   Read MariaDB servers

  Microsoft.DBforMySQL/flexibleServers/configurations/read                                                              Read MySQL flexible server
                                                                                                                        configurations

  Microsoft.DBforMySQL/flexibleServers/databases/read                                                                   Read MySQL flexible server
                                                                                                                        databases

  Microsoft.DBforMySQL/flexibleServers/firewallRules/read                                                               Read MySQL flexible server firewall
                                                                                                                        rules

  Microsoft.DBforMySQL/flexibleServers/read                                                                             Read MySQL flexible servers

  Microsoft.DBforMySQL/servers/firewallRules/read                                                                       Read MySQL server firewall rules

  Microsoft.DBforMySQL/servers/read                                                                                     Read MySQL servers

  Microsoft.DBforMySQL/servers/virtualNetworkRules/read                                                                 Read MySQL server virtual network
                                                                                                                        rules

  Microsoft.DBforPostgreSQL/flexibleServers/configurations/read                                                         Read PostgreSQL flexible server
                                                                                                                        configurations

  Microsoft.DBforPostgreSQL/flexibleServers/databases/read                                                              Read PostgreSQL flexible server
                                                                                                                        databases

  Microsoft.DBforPostgreSQL/flexibleServers/firewallRules/read                                                          Read PostgreSQL flexible server
                                                                                                                        firewall rules

  Microsoft.DBforPostgreSQL/flexibleServers/read                                                                        Read PostgreSQL flexible servers

  Microsoft.DBforPostgreSQL/servers/configurations/read                                                                 Read PostgreSQL server
                                                                                                                        configurations

  Microsoft.DBforPostgreSQL/servers/firewallRules/read                                                                  Read PostgreSQL server firewall
                                                                                                                        rules

  Microsoft.DBforPostgreSQL/servers/read                                                                                Read PostgreSQL servers

  Microsoft.DBforPostgreSQL/serversv2/firewallRules/read                                                                Read PostgreSQL servers v2 firewall
                                                                                                                        rules

  Microsoft.Dashboard/grafana/read                                                                                      Read Grafana dashboards

  Microsoft.DataBoxEdge/dataBoxEdgeDevices/read                                                                         Read DataBox Edge devices

  Microsoft.DataFactory/datafactories/read                                                                              Read Data Factory data factories

  Microsoft.DataFactory/factories/integrationruntimes/read                                                              Read Data Factory integration
                                                                                                                        runtimes

  Microsoft.DataFactory/factories/linkedservices/read                                                                   Read Data Factory linked services

  Microsoft.DataFactory/factories/read                                                                                  Read Data Factories

  Microsoft.DataLakeAnalytics/accounts/dataLakeStoreAccounts/read                                                       Read Data Lake Analytics associated
                                                                                                                        Data Lake Store accounts

  Microsoft.DataLakeAnalytics/accounts/firewallRules/read                                                               Read Data Lake Analytics firewall
                                                                                                                        rules

  Microsoft.DataLakeAnalytics/accounts/read                                                                             Read Data Lake Analytics accounts

  Microsoft.DataLakeAnalytics/accounts/storageAccounts/read                                                             Read Data Lake Analytics storage
                                                                                                                        accounts

  Microsoft.DataLakeStore/accounts/firewallRules/read                                                                   Read Data Lake Store firewall rules

  Microsoft.DataLakeStore/accounts/read                                                                                 Read Data Lake Store accounts

  Microsoft.DataLakeStore/accounts/trustedIdProviders/read                                                              Read Data Lake Store trusted ID
                                                                                                                        providers

  Microsoft.DataLakeStore/accounts/virtualNetworkRules/read                                                             Read Data Lake Store virtual
                                                                                                                        network rules

  Microsoft.DataMigration/services/read                                                                                 Read Data Migration services

  Microsoft.DataShare/accounts/read                                                                                     Read Data Share accounts

  Microsoft.Databricks/accessConnectors/read                                                                            Read Databricks access connectors

  Microsoft.Databricks/workspaces/read                                                                                  Read Databricks workspaces

  Microsoft.Datadog/monitors/read                                                                                       Read Datadog monitors

  Microsoft.DesktopVirtualization/applicationgroups/read                                                                Read Desktop Virtualization
                                                                                                                        application groups

  Microsoft.DesktopVirtualization/hostpools/read                                                                        Read Desktop Virtualization host
                                                                                                                        pools

  Microsoft.DesktopVirtualization/hostpools/sessionhostconfigurations/read                                              Read Desktop Virtualization host
                                                                                                                        pool session host configs

  Microsoft.DesktopVirtualization/hostpools/sessionhosts/read                                                           Read Desktop Virtualization host
                                                                                                                        pool session hosts

  Microsoft.DesktopVirtualization/workspaces/providers/Microsoft.Insights/diagnosticSettings/read                       Read Desktop Virtualization
                                                                                                                        workspace diagnostic settings

  Microsoft.DesktopVirtualization/workspaces/read                                                                       Read Desktop Virtualization
                                                                                                                        workspaces

  Microsoft.DevCenter/devcenters/read                                                                                   Read DevCenter devcenters

  Microsoft.DevTestLab/schedules/read                                                                                   Read DevTestLab schedules

  Microsoft.Devices/iotHubs/Read                                                                                        Read IoT Hubs

  Microsoft.Devices/iotHubs/privateLinkResources/Read                                                                   Read IoT Hubs private link
                                                                                                                        resources

  Microsoft.DigitalTwins/digitalTwinsInstances/read                                                                     Read Digital Twins instances

  Microsoft.DocumentDB/cassandraClusters/read                                                                           Read DocumentDB Cassandra clusters

  Microsoft.DocumentDB/databaseAccounts/listConnectionStrings/action                                                    List connection strings of
                                                                                                                        DocumentDB accounts (action)

  Microsoft.DocumentDB/databaseAccounts/listKeys/action                                                                 List keys of DocumentDB accounts
                                                                                                                        (action)

  Microsoft.DocumentDB/databaseAccounts/read                                                                            Read DocumentDB database accounts

  Microsoft.DocumentDB/databaseAccounts/readonlykeys/action                                                             List readonly keys of DocumentDB
                                                                                                                        accounts (action)

  Microsoft.DomainRegistration/domains/Read                                                                             Read Domain registrations

  Microsoft.Easm/workspaces/read                                                                                        Read Easm workspaces

  Microsoft.Elastic/monitors/read                                                                                       Read Elastic monitors

  Microsoft.EventGrid/domains/privateLinkResources/read                                                                 Read Event Grid domains private
                                                                                                                        link resources

  Microsoft.EventGrid/domains/read                                                                                      Read Event Grid domains

  Microsoft.EventGrid/namespaces/read                                                                                   Read Event Grid namespaces

  Microsoft.EventGrid/partnerNamespaces/read                                                                            Read Event Grid partner namespaces

  Microsoft.EventGrid/topics/privateLinkResources/read                                                                  Read Event Grid topics private link
                                                                                                                        resources

  Microsoft.EventGrid/topics/read                                                                                       Read Event Grid topics

  Microsoft.EventHub/Namespaces/PrivateEndpointConnections/read                                                         Read EventHub Namespace private
                                                                                                                        endpoint connections

  Microsoft.EventHub/clusters/read                                                                                      Read EventHub clusters

  Microsoft.EventHub/namespaces/authorizationRules/read                                                                 Read EventHub namespaces
                                                                                                                        authorization rules

  Microsoft.EventHub/namespaces/eventhubs/authorizationRules/read                                                       Read EventHub event hub
                                                                                                                        authorization rules

  Microsoft.EventHub/namespaces/eventhubs/read                                                                          Read EventHub event hubs

  Microsoft.EventHub/namespaces/ipfilterrules/read                                                                      Read EventHub IP filter rules

  Microsoft.EventHub/namespaces/read                                                                                    Read EventHub namespaces

  Microsoft.EventHub/namespaces/virtualnetworkrules/read                                                                Read EventHub virtual network rules

  Microsoft.HDInsight/clusters/applications/read                                                                        Read HDInsight cluster applications

  Microsoft.HDInsight/clusters/read                                                                                     Read HDInsight clusters

  Microsoft.HealthBot/healthBots/Read                                                                                   Read HealthBot bots

  Microsoft.HealthcareApis/workspaces/read                                                                              Read Healthcare APIs workspaces

  Microsoft.HybridCompute/machines/read                                                                                 Read Hybrid Compute machines

  Microsoft.Insights/ActivityLogAlerts/read                                                                             Read Insights activity log alerts

  Microsoft.Insights/Components/read                                                                                    Read Insights components

  Microsoft.Insights/DataCollectionEndpoints/Read                                                                       Read Insights data collection
                                                                                                                        endpoints

  Microsoft.Insights/DataCollectionRules/Read                                                                           Read Insights data collection rules

  Microsoft.Insights/LogProfiles/read                                                                                   Read Insights log profiles

  Microsoft.Insights/MetricAlerts/Read                                                                                  Read Insights metric alerts

  Microsoft.Insights/actionGroups/read                                                                                  Read Insights action groups

  Microsoft.Insights/diagnosticSettings/read                                                                            Read Insights diagnostic settings

  Microsoft.Insights/eventtypes/values/read                                                                             Read Insights event type values

  Microsoft.IoTCentral/IoTApps/read                                                                                     Read IoT Central applications

  Microsoft.KeyVault/vaults/keys/read                                                                                   Read Key Vault keys

  Microsoft.KeyVault/vaults/privateLinkResources/read                                                                   Read Key Vault private link
                                                                                                                        resources

  Microsoft.KeyVault/vaults/read                                                                                        Read Key Vault vaults

  Microsoft.Kusto/Clusters/Databases/read                                                                               Read Kusto cluster databases

  Microsoft.Kusto/Clusters/read                                                                                         Read Kusto clusters

  Microsoft.Kusto/clusters/read                                                                                         Read Kusto clusters (alternative)

  Microsoft.LabServices/labs/read                                                                                       Read Lab Services labs

  Microsoft.LoadTestService/loadTests/read                                                                              Read Load Test Service tests

  Microsoft.Logic/integrationAccounts/read                                                                              Read Logic integration accounts

  Microsoft.Logic/workflows/read                                                                                        Read Logic workflows

  Microsoft.Logic/workflows/versions/read                                                                               Read Logic workflow versions

  Microsoft.MachineLearningServices/workspaces/computes/read                                                            Read Machine Learning Services
                                                                                                                        workspace computes

  Microsoft.MachineLearningServices/workspaces/outboundRules/read                                                       Read Machine Learning Services
                                                                                                                        workspace outbound rules

  Microsoft.MachineLearningServices/workspaces/read                                                                     Read Machine Learning Services
                                                                                                                        workspaces

  Microsoft.ManagedIdentity/userAssignedIdentities/read                                                                 Read Managed Identity user assigned
                                                                                                                        identities

  Microsoft.ManagedServices/marketplaceRegistrationDefinitions/read                                                     Read Managed Services marketplace
                                                                                                                        registration defs

  Microsoft.ManagedServices/registrationAssignments/read                                                                Read Managed Services registration
                                                                                                                        assignments

  Microsoft.Management/managementGroups/descendants/read                                                                Read Management Groups descendants

  Microsoft.Management/managementGroups/read                                                                            Read Management Groups

  Microsoft.Management/managementGroups/subscriptions/read                                                              Read Management Groups
                                                                                                                        subscriptions

  Microsoft.Maps/accounts/read                                                                                          Read Maps accounts

  Microsoft.Migrate/moveCollections/read                                                                                Read Migrate move collections

  Microsoft.MixedReality/ObjectAnchorsAccounts/read                                                                     Read Mixed Reality Object Anchors
                                                                                                                        accounts

  Microsoft.NetApp/netAppAccounts/capacityPools/read                                                                    Read NetApp capacity pools

  Microsoft.NetApp/netAppAccounts/capacityPools/volumes/read                                                            Read NetApp volumes

  Microsoft.NetApp/netAppAccounts/read                                                                                  Read NetApp accounts

  Microsoft.Network/ApplicationGatewayWebApplicationFirewallPolicies/read                                               Read Application Gateway Web
                                                                                                                        Application Firewall Policies

  Microsoft.Network/applicationGateways/read                                                                            Read Application Gateways

  Microsoft.Network/applicationSecurityGroups/read                                                                      Read Application Security Groups

  Microsoft.Network/azurefirewalls/read                                                                                 Read Azure Firewalls

  Microsoft.Network/bastionHosts/read                                                                                   Read Bastion Hosts

  Microsoft.Network/connections/read                                                                                    Read Network Connections

  Microsoft.Network/ddosProtectionPlans/read                                                                            Read DDoS Protection Plans

  Microsoft.Network/dnsZones/read                                                                                       Read DNS Zones

  Microsoft.Network/expressRouteCircuits/authorizations/read                                                            Read ExpressRoute Circuit
                                                                                                                        authorizations

  Microsoft.Network/expressRouteCircuits/peerings/connections/read                                                      Read ExpressRoute Circuit peerings
                                                                                                                        connections

  Microsoft.Network/expressRouteCircuits/peerings/peerConnections/read                                                  Read ExpressRoute Circuit peer
                                                                                                                        connections

  Microsoft.Network/expressRouteCircuits/peerings/read                                                                  Read ExpressRoute Circuit peerings

  Microsoft.Network/expressRouteCircuits/read                                                                           Read ExpressRoute Circuits

  Microsoft.Network/expressRouteCrossConnections/peerings/read                                                          Read ExpressRoute Cross Connections
                                                                                                                        peerings

  Microsoft.Network/expressRouteCrossConnections/read                                                                   Read ExpressRoute Cross Connections

  Microsoft.Network/expressRouteGateways/expressRouteConnections/read                                                   Read ExpressRoute Gateways
                                                                                                                        connections

  Microsoft.Network/expressRouteGateways/read                                                                           Read ExpressRoute Gateways

  Microsoft.Network/expressRoutePorts/authorizations/read                                                               Read ExpressRoute Ports
                                                                                                                        authorizations

  Microsoft.Network/expressRoutePorts/links/read                                                                        Read ExpressRoute Ports links

  Microsoft.Network/expressRoutePorts/read                                                                              Read ExpressRoute Ports

  Microsoft.Network/expressRoutePortsLocations/read                                                                     Read ExpressRoute Ports locations

  Microsoft.Network/firewallPolicies/read                                                                               Read Firewall Policies

  Microsoft.Network/frontDoorWebApplicationFirewallPolicies/read                                                        Read Front Door Web Application
                                                                                                                        Firewall Policies

  Microsoft.Network/frontDoors/backendPools/read                                                                        Read Front Door backend pools

  Microsoft.Network/frontDoors/frontendEndpoints/read                                                                   Read Front Door frontend endpoints

  Microsoft.Network/frontDoors/healthProbeSettings/read                                                                 Read Front Door health probe
                                                                                                                        settings

  Microsoft.Network/frontDoors/loadBalancingSettings/read                                                               Read Front Door load balancing
                                                                                                                        settings

  Microsoft.Network/frontDoors/read                                                                                     Read Front Doors

  Microsoft.Network/frontDoors/routingRules/read                                                                        Read Front Door routing rules

  Microsoft.Network/frontDoors/rulesEngines/read                                                                        Read Front Door rules engines

  Microsoft.Network/loadBalancers/read                                                                                  Read Load Balancers

  Microsoft.Network/localnetworkgateways/read                                                                           Read Local Network Gateways

  Microsoft.Network/locations/usages/read                                                                               Read Network locations usage

  Microsoft.Network/natGateways/read                                                                                    Read NAT Gateways

  Microsoft.Network/networkInterfaces/effectiveNetworkSecurityGroups/action                                             Execute effective network security
                                                                                                                        groups action

  Microsoft.Network/networkInterfaces/effectiveRouteTable/action                                                        Execute effective route table
                                                                                                                        action

  Microsoft.Network/networkInterfaces/read                                                                              Read Network Interfaces

  Microsoft.Network/networkSecurityGroups/defaultSecurityRules/read                                                     Read Network Security Groups
                                                                                                                        default security rules

  Microsoft.Network/networkSecurityGroups/read                                                                          Read Network Security Groups

  Microsoft.Network/networkSecurityGroups/securityRules/read                                                            Read Network Security Groups
                                                                                                                        security rules

  Microsoft.Network/networkWatchers/queryFlowLogStatus/\*                                                               Query network watcher flow log
                                                                                                                        status

  Microsoft.Network/networkWatchers/read                                                                                Read Network Watchers

  Microsoft.Network/networkWatchers/securityGroupView/action                                                            Execute security group view action

  Microsoft.Network/p2sVpnGateways/read                                                                                 Read P2S VPN Gateways

  Microsoft.Network/privateDnsZones/ALL/read                                                                            Read Private DNS Zones ALL

  Microsoft.Network/privateDnsZones/read                                                                                Read Private DNS Zones

  Microsoft.Network/privateEndpoints/privateDnsZoneGroups/read                                                          Read Private Endpoints DNS Zone
                                                                                                                        Groups

  Microsoft.Network/privateEndpoints/read                                                                               Read Private Endpoints

  Microsoft.Network/privateLinkServices/read                                                                            Read Private Link Services

  Microsoft.Network/publicIPAddresses/read                                                                              Read Public IP Addresses

  Microsoft.Network/publicIPPrefixes/read                                                                               Read Public IP Prefixes

  Microsoft.Network/routeFilters/read                                                                                   Read Route Filters

  Microsoft.Network/routeFilters/routeFilterRules/read                                                                  Read Route Filter Rules

  Microsoft.Network/routeTables/read                                                                                    Read Route Tables

  Microsoft.Network/routeTables/routes/read                                                                             Read Route Table Routes

  Microsoft.Network/serviceEndpointPolicies/read                                                                        Read Service Endpoint Policies

  Microsoft.Network/serviceEndpointPolicies/serviceEndpointPolicyDefinitions/read                                       Read Service Endpoint Policy
                                                                                                                        Definitions

  Microsoft.Network/trafficManagerProfiles/read                                                                         Read Traffic Manager Profiles

  Microsoft.Network/virtualNetworkGateways/read                                                                         Read Virtual Network Gateways

  Microsoft.Network/virtualNetworks/read                                                                                Read Virtual Networks

  Microsoft.Network/virtualNetworks/subnets/read                                                                        Read Virtual Network Subnets

  Microsoft.Network/virtualNetworks/virtualNetworkPeerings/read                                                         Read Virtual Network Peerings

  Microsoft.Network/virtualWans/read                                                                                    Read Virtual WANs

  Microsoft.Network/virtualwans/vpnconfiguration/action                                                                 Execute VPN configuration action

  Microsoft.Network/vpnServerConfigurations/read                                                                        Read VPN Server Configurations

  Microsoft.NetworkFunction/azureTrafficCollectors/read                                                                 Read Azure Traffic Collectors

  Microsoft.NotificationHubs/Namespaces/NotificationHubs/read                                                           Read Notification Hubs

  Microsoft.NotificationHubs/Namespaces/read                                                                            Read Notification Hub namespaces

  Microsoft.OperationalInsights/clusters/read                                                                           Read Operational Insights clusters

  Microsoft.OperationalInsights/querypacks/read                                                                         Read Operational Insights query
                                                                                                                        packs

  Microsoft.OperationalInsights/workspaces/read                                                                         Read Operational Insights
                                                                                                                        workspaces

  Microsoft.OperationalInsights/workspaces/tables/read                                                                  Read Operational Insights workspace
                                                                                                                        tables

  Microsoft.Orbital/spacecrafts/read                                                                                    Read Orbital spacecrafts

  Microsoft.PowerBIDedicated/capacities/read                                                                            Read Power BI Dedicated capacities

  Microsoft.PowerBIDedicated/servers/read                                                                               Read Power BI Dedicated servers

  Microsoft.Quantum/Workspaces/Read                                                                                     Read Quantum Workspaces

  Microsoft.RecoveryServices/Vaults/backupProtectedItems/read                                                           Read Recovery Services Vault backup
                                                                                                                        protected items

  Microsoft.RecoveryServices/Vaults/read                                                                                Read Recovery Services Vaults

  Microsoft.RecoveryServices/vaults/backupPolicies/read                                                                 Read Recovery Services Vault backup
                                                                                                                        policies

  Microsoft.RedHatOpenShift/openShiftClusters/read                                                                      Read Red Hat OpenShift clusters

  Microsoft.Relay/Namespaces/read                                                                                       Read Relay namespaces

  Microsoft.Resources/Resources/read                                                                                    Read generic resources

  Microsoft.Resources/subscriptions/providers/read                                                                      Read subscription providers

  Microsoft.Resources/subscriptions/read                                                                                Read subscriptions

  Microsoft.Resources/subscriptions/resourceGroups/read                                                                 Read resource groups

  Microsoft.Resources/subscriptions/resourceGroups/write                                                                Write resource groups

  Microsoft.Resources/templateSpecs/read                                                                                Read template specs

  Microsoft.SaaS/applications/read                                                                                      Read SaaS applications

  Microsoft.Search/searchServices/read                                                                                  Read Azure Search services

  Microsoft.Security/advancedThreatProtectionSettings/read                                                              Read Security advanced threat
                                                                                                                        protection settings

  Microsoft.Security/autoProvisioningSettings/read                                                                      Read Security auto provisioning
                                                                                                                        settings

  Microsoft.Security/automations/read                                                                                   Read Security automations

  Microsoft.Security/iotSecuritySolutions/read                                                                          Read IoT Security Solutions

  Microsoft.Security/locations/jitNetworkAccessPolicies/read                                                            Read Just-in-Time network access
                                                                                                                        policies

  Microsoft.Security/locations/read                                                                                     Read Security locations

  Microsoft.Security/pricings/read                                                                                      Read Security pricings

  Microsoft.Security/secureScores/read                                                                                  Read Security secure scores

  Microsoft.Security/securityContacts/read                                                                              Read Security contacts

  Microsoft.Security/settings/read                                                                                      Read Security settings

  Microsoft.Security/workspaceSettings/read                                                                             Read Security workspace settings

  Microsoft.ServiceBus/namespaces/authorizationRules/read                                                               Read Service Bus namespace
                                                                                                                        authorization rules

  Microsoft.ServiceBus/namespaces/networkrulesets/read                                                                  Read Service Bus namespace network
                                                                                                                        rule sets

  Microsoft.ServiceBus/namespaces/privateEndpointConnections/read                                                       Read Service Bus namespace private
                                                                                                                        endpoint connections

  Microsoft.ServiceBus/namespaces/providers/Microsoft.Insights/diagnosticSettings/read                                  Read Service Bus namespace
                                                                                                                        diagnostic settings

  Microsoft.ServiceBus/namespaces/queues/read                                                                           Read Service Bus queues

  Microsoft.ServiceBus/namespaces/read                                                                                  Read Service Bus namespaces

  Microsoft.ServiceBus/namespaces/topics/read                                                                           Read Service Bus topics

  Microsoft.ServiceBus/namespaces/topics/subscriptions/read                                                             Read Service Bus topic
                                                                                                                        subscriptions

  Microsoft.ServiceFabric/clusters/read                                                                                 Read Service Fabric clusters

  Microsoft.SignalRService/SignalR/read                                                                                 Read SignalR Service SignalR

  Microsoft.SignalRService/WebPubSub/read                                                                               Read SignalR Web PubSub

  Microsoft.Solutions/applications/read                                                                                 Read Solutions applications

  Microsoft.Sql/managedInstances/databases/read                                                                         Read SQL managed instances
                                                                                                                        databases

  Microsoft.Sql/managedInstances/databases/transparentDataEncryption/read                                               Read SQL managed instances
                                                                                                                        databases Transparent Data
                                                                                                                        Encryption

  Microsoft.Sql/managedInstances/encryptionProtector/Read                                                               Read SQL managed instances
                                                                                                                        encryption protector

  Microsoft.Sql/managedInstances/read                                                                                   Read SQL managed instances

  Microsoft.Sql/managedInstances/vulnerabilityAssessments/Read                                                          Read SQL managed instances
                                                                                                                        vulnerability assessments

  Microsoft.Sql/servers/administrators/read                                                                             Read SQL server administrators

  Microsoft.Sql/servers/auditingSettings/read                                                                           Read SQL server auditing settings

  Microsoft.Sql/servers/databases/auditingSettings/read                                                                 Read SQL server databases auditing
                                                                                                                        settings

  Microsoft.Sql/servers/databases/dataMaskingPolicies/read                                                              Read SQL server databases data
                                                                                                                        masking policies

  Microsoft.Sql/servers/databases/dataMaskingPolicies/rules/read                                                        Read SQL server databases data
                                                                                                                        masking policies rules

  Microsoft.Sql/servers/databases/read                                                                                  Read SQL server databases

  Microsoft.Sql/servers/databases/securityAlertPolicies/read                                                            Read SQL server databases security
                                                                                                                        alert policies

  Microsoft.Sql/servers/databases/transparentDataEncryption/read                                                        Read SQL server databases
                                                                                                                        Transparent Data Encryption

  Microsoft.Sql/servers/encryptionProtector/read                                                                        Read SQL server encryption
                                                                                                                        protector

  Microsoft.Sql/servers/firewallRules/read                                                                              Read SQL server firewall rules

  Microsoft.Sql/servers/read                                                                                            Read SQL servers

  Microsoft.Sql/servers/securityAlertPolicies/read                                                                      Read SQL server security alert
                                                                                                                        policies

  Microsoft.Sql/servers/vulnerabilityAssessments/read                                                                   Read SQL server vulnerability
                                                                                                                        assessments

  Microsoft.SqlVirtualMachine/sqlVirtualMachines/read                                                                   Read SQL Virtual Machines

  Microsoft.Storage/storageAccounts/blobServices/read                                                                   Read Storage blob services

  Microsoft.Storage/storageAccounts/fileServices/read                                                                   Read Storage file services

  Microsoft.Storage/storageAccounts/fileServices/shares/read                                                            Read Storage file shares

  Microsoft.Storage/storageAccounts/listKeys/action                                                                     List Storage account keys (action)

  Microsoft.Storage/storageAccounts/providers/Microsoft.Insights/diagnosticSettings/read                                Read Storage account diagnostic
                                                                                                                        settings

  Microsoft.Storage/storageAccounts/queueServices/read                                                                  Read Storage queue services

  Microsoft.Storage/storageAccounts/read                                                                                Read Storage accounts

  Microsoft.Storage/storageAccounts/tableServices/read                                                                  Read Storage table services

  Microsoft.StorageCache/Subscription/caches/read                                                                       Read Storage Cache subscription
                                                                                                                        caches

  Microsoft.StorageCache/caches/read                                                                                    Read Storage Cache caches

  Microsoft.StorageMover/storageMovers/read                                                                             Read Storage Mover storage movers

  Microsoft.StorageSync/storageSyncServices/privateLinkResources/read                                                   Read Storage Sync private link
                                                                                                                        resources

  Microsoft.StorageSync/storageSyncServices/read                                                                        Read Storage Sync services

  Microsoft.StreamAnalytics/clusters/Read                                                                               Read Stream Analytics clusters

  Microsoft.StreamAnalytics/streamingjobs/Read                                                                          Read Stream Analytics streaming
                                                                                                                        jobs

  Microsoft.Subscription/Policies/default/read                                                                          Read Subscription default policies

  Microsoft.Synapse/privateLinkHubs/privateLinkResources/read                                                           Read Synapse private link hubs
                                                                                                                        private link resources

  Microsoft.Synapse/privateLinkHubs/read                                                                                Read Synapse private link hubs

  Microsoft.Synapse/workspaces/privateLinkResources/read                                                                Read Synapse workspace private link
                                                                                                                        resources

  Microsoft.Synapse/workspaces/read                                                                                     Read Synapse workspaces

  Microsoft.Synapse/workspaces/sparkConfigurations/read                                                                 Read Synapse workspaces spark
                                                                                                                        configurations

  Microsoft.Synapse/workspaces/sqlPools/geoBackupPolicies/read                                                          Read Synapse workspaces SQL pools
                                                                                                                        geo backup policies

  Microsoft.Synapse/workspaces/sqlPools/read                                                                            Read Synapse workspaces SQL pools

  Microsoft.VideoIndexer/accounts/read                                                                                  Read Video Indexer accounts

  Microsoft.VisualStudio/Account/Read                                                                                   Read Visual Studio accounts

  Microsoft.Web/certificates/read                                                                                       Read Web certificates

  Microsoft.Web/customApis/read                                                                                         Read Web custom APIs

  Microsoft.Web/hostingEnvironments/Read                                                                                Read Web hosting environments

  Microsoft.Web/serverfarms/Read                                                                                        Read Web server farms

  Microsoft.Web/sites/Read                                                                                              Read Web sites

  Microsoft.Web/sites/basicPublishingCredentialsPolicies/Read                                                           Read Web sites basic publishing
                                                                                                                        credentials policies

  Microsoft.Web/sites/config/list/action                                                                                Execute action to list Web sites
                                                                                                                        config

  Microsoft.Web/sites/config/read                                                                                       Read Web sites config

  Microsoft.web/sites/config/appsettings/read                                                                           Read Web sites app settings

  Microsoft.Web/sites/privateEndpointConnections/Read                                                                   Read Web sites private endpoint
                                                                                                                        connections

  Microsoft.Web/sites/read                                                                                              Read Web sites

  Microsoft.Web/sites/slots/Read                                                                                        Read Web sites slots

  microsoft.web/serverfarms/sites/read                                                                                  Read Server farms sites

  Microsoft.Web/staticSites/Read                                                                                        Read Web static sites

  Microsoft.Workloads/monitors/read                                                                                     Read Workloads monitors

  Microsoft.classicCompute/domainNames/read                                                                             Read Classic Compute domain names

  microsoft.app/containerapps/read                                                                                      Read App container apps

  microsoft.monitor/accounts/read                                                                                       Read Monitor accounts

  microsoft.network/virtualnetworkgateways/connections/read                                                             Read Virtual network gateways
                                                                                                                        connections
  ---------------------------------------------------------------------------------------------------------------------------------------------------------

Log Collection

  -----------------------------------------------------------------------
  Permission              Scope                   Purpose
  ----------------------- ----------------------- -----------------------
  Azure Event Hubs Data   Event Hub that was      Allows receive access
  Receiver                created during the      to Azure Event Hubs
                          onboarding (containing  resources
                          the audit logs)         

  Storage Blob Data       Storage account that    Reads, writes, and
  Contributor             was created during the  deletes Azure Storage
                          onboarding              containers and blobs
  -----------------------------------------------------------------------

Registry Scan

  --------------------------------------------------------------------------
  Permission
  --------------------------------------------------------------------------
  Microsoft.ContainerRegistry/registries/metadata/read

  Microsoft.ContainerRegistry/registries/pull/read

  Microsoft.ContainerRegistry/registries/read

  Microsoft.ContainerRegistry/registries/webhooks/getCallbackConfig/action
  --------------------------------------------------------------------------

####### Oracle Cloud Infrastructure provider permissions

ADS

  --------------------------------------------------------------------------------------------------------------------
  Permission                                                     Module       Scope                  Purpose
  --------------------------------------------------------- ----------------- ---------------------- -----------------
  Admit group CortexOutpostGroup of tenancy CortexOutpost          ADS        In tenancy             Allow creation of
  to use volumes in tenancy                                                                          backups from
                                                                                                     volumes

  Admit group CortexOutpostGroup of tenancy CortexOutpost          ADS        In tenancy             Re-encrypt
  to use key-delegate in tenancy                                                                     backups during
                                                                                                     copy/restore
                                                                                                     operations

  Admit group CortexOutpostGroup of tenancy CortexOutpost          ADS        Volumes in tenancy     Associate
  to associate keys in tenancy with volumes in tenancy                                               encryption keys
  CortexOutpost                                                                                      with volumes
                                                                                                     during
                                                                                                     backup/restore

  Admit group CortexOutpostGroup of tenancy CortexOutpost          ADS        In tenancy             Enable tagging
  to use tag-namespaces in tenancy                                                                   for permission
                                                                                                     scoping, resource
                                                                                                     tracking, and
                                                                                                     cost visibility

  Admit group CortexOutpostGroup of tenancy CortexOutpost          ADS        Excludes delete        Allow full
  to manage boot-volume-backups in tenancy where                                                     management of
  request.operation != \'DeleteBootVolumeBackup\'                                                    boot volume
                                                                                                     backups except
                                                                                                     deletion

  Admit group CortexOutpostGroup of tenancy CortexOutpost          ADS        Only                   Restrict deletion
  to manage boot-volume-backups in tenancy where                              boot-volume-backups    to Cortex
  target.resource.tag.cortex_m-o-lcaas_id.panw_capability =                   tagged with            scan-related
  \'cortex-scan-platform\'                                                    panw_capability =      resources only
                                                                              cortex-scan-platform   

  Admit group CortexOutpostGroup of tenancy CortexOutpost          ADS        In tenancy             Read-only access
  to read all-resources in tenancy                                                                   to all resources
  --------------------------------------------------------------------------------------------------------------------

Discovery Engine

\"Discovery Engine\" read only access. Grants read-only access to OCI
tenancy and resources.

##### Ingest data for API security

Configure the settings in both Cortex XSIAM and your cloud service
provider to retrieve and collect API data for further analysis by
Cortex\'s comprehensive [API
security](#UUID6ca0d29dd0bbe838091e9df0176115f5) capabilities that
provides a transparent view of API traffic, helping to identify
potential security threats.

###### Ingest AWS API Gateway

Integrate AWS API Gateway with Cortex XSIAM to begin scanning the APIs
for potential threats and vulnerabilities.

Settings in Cortex XSIAM

In Cortex XSIAM, set up the **AWS API Gateway** data source to integrate
with the AWS API Gateway.

1.  From Settings \> Data Sources , click
    ![](media/rId4436.png){width="0.9668799212598426in"
    height="0.20833333333333334in"} and search for **AWS API Gateway**
    and then click **Connect** or **Connect Another Instance**.

2.  In the **AWS API Collector** wizard, enter a relevant name and click
    **Create and Proceed**.

3.  Copy the key and save it for later.

- > **Note**

  > You must generate a new key if you did not save.

4.  Click **Close**.

5.  An instance is created for the data source. Click
    ![](media/rId4439.png){width="0.2646391076115486in"
    height="0.20833333333333334in"} (Copy API URL) to establish a
    connection when setting up AWS API Gateway.

####### Settings in AWS Management Console

Configure the settings in the AWS Management Console to integrate with
Cortex XSIAM:

1.  Log in to the [AWS Management
    Console](https://aws.amazon.com/console/).

2.  In AWS Management Console, navigate to **API Gateway**.

    a.  Expand the left-hand menu of the API project.

    b.  Go to Settings \> Logging and click **Edit**. Verify that the
        **CloudWatch log role ARN** is filled.

    c.  Click **Stages** and from **Stages**, select the relevant stage.

    d.  From **Logs and Tracing**, click **Edit** and configure the
        following:

        - **CloudWatch Logs**: Select **Errors and info logs**

        - Select **Data tracing**

        - Select **Detailed metrics**

    e.  Click **Save**.

    - This creates a unique log group inside CloudWatch.

3.  Open CloudWatch in another window by typing **CloudWatch** in the
    search bar.

    a.  Go to Logs \> Log groups and search for the log group just
        created.

    - The group name follows the following format:
      `“API-Gateway-Execution-Logs_<gw ID>/<stage name>”`

    b.  Click the log group, and from the **Log group details**, copy
        the **ARN**.

4.  Return to **Edit logs and tracing**, go to
    **enable the custom access logging** , and paste the ARN without the
    \* in the **Access log destination ARN** field.

- ARN:
  `arn:aws:logs:us-east-1:123456789012:log-group:API-Gateway-Execution-Logs_153tp249k2/Prod:*`

  Paste in **Access log destination ARN**:
  `arn:aws:logs:us-east-1:123456789012:log-group:API-Gateway-Execution-Logs_153tp249k2/Prod`

5.  In **Log format**, type the following and click **Save**:

- ($context.requestId) accountId: $context.accountId; requestTime: $context.requestTime; path: $context.path

6.  Open Firehose in another window by typing **Firehose** in the search
    bar.

    a.  Configure the following:

        - **Source**: **Direct PUT**

        - **Destination**: **HTTP Endpoint**

        - **Firehose stream name**: Add a relevant name.

    b.  In **Destination settings**, configure the following:

        - **HTTP endpoint URL** : Add the API URL from Cortex XSIAM.

        - **Authentication**: Select **Use access key**.

        - **Access key**: Paste the generated token from
          **AWS API Gateway**.

        - **Content encoding**: Select **GZIP**.

    c.  In **Backup settings**, configure the following:

        - **Source record backup in Amazon S3**: select
          **Failed date only**.

        - **S3 backup bucket**: select a bucket or enter a bucket URI.

    d.  Click **Create**.

    - It takes up to 5 minutes for the stream to be activated.

7.  Refer to [Subscription filters with Amazon Data
    Firehose](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html#FirehoseExample).
    To create an IAM Role and provide CloudWatch with the appropriate
    permissions for the streaming, refer to steps 8-12.

- aws logs put-subscription-filter \
          --log-group-name "<YOUR_LOG_GROUP_NAME>" \
          --filter-name "<any_filter_name>" \
          --filter-pattern "" \
          --destination-arn "arn:aws:firehose:region:123456789012:deliverystream/<YOUR_DELIVERY_STREAM>" \
          --role-arn "arn:aws:iam::<ACCOUNT_ID>:role/<YOUR_IAM_ROLE>"

  > **Important**

  > Leave `–filter-pattern` empty as displayed above.

  After the Data Firehose delivery stream is active and you have created
  the IAM role, you can create the CloudWatch Logs subscription filter.
  The subscription filter immediately starts the flow of real-time log
  data from the chosen log group to your Amazon Data Firehose delivery
  stream.

  After you create the filter, go back to Data Sources \> AWS API
  Gateway to see the logs starting to come in.

  > **Note**

  > If no logs are showing, send some API requests on Postman or CURL.

###### Ingest Azure APIM

> **Note**
>
> Requires the Data Collection add-on.

Integrate Azure APIM with Cortex XSIAM to start scanning its APIs for
potential threats and vulnerabilities.

You need to set up a policy that enables you to customize the behavior
of managed APIs. You can configure the sending of HTTP request/response
data to Cortex XSIAM. The data is saved and analyzed by API security
modules, which provide information on the security risks associated with
the APIs.

> **Note**
>
> Microsoft Azure APIM service must be running before starting to
> configure the integration.

Settings in Cortex XSIAM

In Cortex XSIAM, set up the **Azure API Management** data source to
integrate with the Azure API Gateway.

1.  From Settings \> Data Sources , click
    ![](media/rId4436.png){width="0.9668799212598426in"
    height="0.20833333333333334in"} and search for
    **Azure API Management** and click **Connect** or
    **Connect Another Instance**.

2.  In the **APIM Collector** wizard, enter a relevant name and then
    click **Create and Proceed**.

3.  Copy the key and paste it somewhere so that you can access it for
    later.

- If you forget to record the key and close the window, you must
  generate a new key and repeat this process.

4.  Click **Close**.

5.  An instance is created for the data source. Click
    ![](media/rId4439.png){width="0.2646391076115486in"
    height="0.20833333333333334in"} (Copy API URL) to establish a
    connection when setting up Azure APIM.

####### Settings in Azure APIM policy

Configure an inbound and outbound policy to send HTTP traffic data of
the APIs to Cortex XSIAM. You can configure a policy for individual
operations (endpoints) or all operations of a single API.

Follow the steps to configure the policy.

1.  Log in to [Microsoft Azure](https://portal.azure.com/).

2.  Go to **API Management services** and select the relevant service.

3.  From the left-hand menu, select APIs \> Named values.

- > **Note**

  > From the URL, save the UUID and the resource group -
  > `/resource/subscriptions/<UUID>/resourceGroups/<ResourceGroup>`.

  > The UUID is the Azure account/subscription ID and the resource
  > group, which is the group where the APIM Service is defined.

4.  Configure the settings in each of the sections. Follow the steps in
    the order they are listed.

- > **Note**

  > Use the search to navigate to the specific section.

  **Named values**: Add the values:

  - cloud-account-id

    - **Type**: Plain

    - **Value**: The UUID you saved from the previous step.

  - cloud-resource-group

    - **Type**: Plain

    - **Value**: The resource group you saved from the previous step.

  - cortex-api-key

    - **Type**: Secret

    - **Value**: The token that you saved from data sources in Cortex.

  - cortex-api-url

    - **Type**: Plain

    - **Value**: The API URL from data sources in Cortex.

  - cortex-http-body-size-limit-bytes

    - **Type**: Plain

    - **Value**: 131072

    <!-- -->

    - > **Note**

      > 131072 bytes = 128 KB. This value determines the size (in bytes)
      > of request and response bodies to send to Cortex. Any bytes
      > beyond this limit are truncated.

  **APIs**: From the left-hand menu, go to APIs \> APIs.

  a.  You can create a policy on a specific API or choose to create a
      policy on all APIs.

  b.  From **Inbound Processing**, click
      ![](media/rId4454.png){width="0.2777777777777778in"
      height="0.20833333333333334in"}.

  - The Policies screen opens. There are three sections:

    - `<inbound>`

    - `<backend>`

    - `<outbound>`

    The `<inbound>` includes the request before it\'s sent to the
    `<outbound>`. The parameters are saved before they\'re sent.

    Add the following inside the `<inbound>`:

         <!-- Save the request body and headers to be sent to Cortex. This should always be placed at the very beginning of the inbound element. -->
                <set-variable name="requestBody" value="@((context.Request?.Body?.As<string>(preserveContent: true)) ?? string.Empty)" />
                <set-variable name="requestHeaders" value="@(JsonConvert.SerializeObject(context.Request.Headers))" />
                <!-- End of setting variables for sending to Cortex --><!-- Save the request body and headers to be sent to Cortex. This should always be placed at the very beginning of the inbound element. -->
                <set-variable name="requestBody" value="@((context.Request?.Body?.As<string>(preserveContent: true)) ?? string.Empty)" />
                <set-variable name="requestHeaders" value="@(JsonConvert.SerializeObject(context.Request.Headers))" />
                <!-- End of setting variables for sending to Cortex -->

    > **Note**

    > If any other inbound policies should be added, they must be added
    > after these elements.

    The `<outbound>` includes the request before it returns a response.

    Add the following inside the \<outbound\> element, at the end, after
    the other child elements:

        <!-- Send data to Cortex. This should always be placed at the very end of the outbound element. -->
                <send-request mode="new" response-variable-name="mirrorMessage">
                    <set-url>{{cortex-api-url}}</set-url>
                    <set-method>POST</set-method>
                    <set-header name="Content-Type" exists-action="override">
                        <value>application/json</value>
                    </set-header>
                    <set-header name="Authorization" exists-action="override">
                        <value>{{cortex-api-key}}</value>
                    </set-header>
                    <set-body>@{
                                string requestBody = context.Variables.GetValueOrDefault<string>("requestBody");
                                string responseBody = context.Response.Body.As<string>(preserveContent: true);
                                int bodySizeLimit = {{cortex-http-body-size-limit-bytes}};
                                bool requestBodySizeExceedsLimit = requestBody.Length > bodySizeLimit;
                                bool responseBodySizeExceedsLimit = responseBody.Length > bodySizeLimit;

                                return JsonConvert.SerializeObject(new {
                                    // Resource information
                                    subscriptionID          = "{{cloud-account-id}}",
                                    resourceGroup           = "{{cloud-resource-group}}",
                                    serviceID               = context.Deployment.ServiceId,
                                    region                  = context.Deployment.Region,
                                    apiID                   = context.Api.Id,
                                    apiRevision             = context.Api.Revision,
                                    // Request information
                                    requestID               = context.RequestId,
                                    url                     = context.Request.OriginalUrl,
                                    httpMethod              = context.Request.Method,
                                    requestBody             = requestBodySizeExceedsLimit ? requestBody.Substring(0, bodySizeLimit) : requestBody,
                                    requestBodyTruncated    = requestBodySizeExceedsLimit,
                                    requestHeaders          = JsonConvert.DeserializeObject(context.Variables.GetValueOrDefault<string>("requestHeaders")),
                                    timestamp               = new DateTimeOffset(context.Timestamp).ToUnixTimeMilliseconds(),
                                    requestIpAddress        = context.Request.IpAddress,
                                    statusCode              = context.Response.StatusCode,
                                    responseBody            = responseBodySizeExceedsLimit ? responseBody.Substring(0, bodySizeLimit) : responseBody,
                                    responseBodyTruncated   = responseBodySizeExceedsLimit,
                                    responseHeaders         = context.Response.Headers,
                                });
                            }
                    </set-body>
                </send-request>
                <!-- End of sending data to Cortex -->

    > **Important**

    > If you want to add additional data to the \<outbound\>, add it at
    > the start of the \<outbound\> code.

  c.  Click **Save**. Your APIM traffic collection is now configured.

  - Request and response data for the configured endpoints are sent to
    Cortex XSIAM for inspection by API security modules.

5.  Go to **Azure API Management** data source to validate that data is
    ingested from Azure APIM.

6.  Do the following to remove the integration of Azure APIM with Cortex
    XSIAM:

    - Remove the snippets you added to the policies.

    - Remove the named values from the API service.

    - Delete the HTTP log collector from Data Sources in Cortex.

###### Ingest Apigee Proxy

> **Note**
>
> Requires the Data Collection add-on.

Integrate Apigee Proxy with Cortex XSIAM to begin scanning the APIs for
potential threats and vulnerabilities.

The integration uses the Apigee's JavaScript (JS) policy, implemented
within a shared flow and deployed as a pre-proxy and post-proxy
flow-hook in selected environments. The JS policy is designed to capture
both request and response data from all traffic entering and exiting the
proxy.

Settings in Cortex XSIAM

In Cortex XSIAM, set up the **Apigee** data source to integrate with the
Apigee Gateway.

1.  From Settings \> Data Sources , click
    ![](media/rId4436.png){width="0.9668799212598426in"
    height="0.20833333333333334in"} and search for **Apigee** and click
    **Connect** or **Connect Another Instance**.

2.  In the **Apigee Collector** wizard, enter a relevant name and then
    click **Create and Proceed**.

3.  Copy the key and paste it somewhere so that you can access it for
    later.

- If you forget to record the key and close the window, you must
  generate a new key and repeat this process.

4.  Click the **Download Configuration Script** link to download the
    plugin, which you can then upload to the Apigee Gateway.

5.  Click **Close**.

6.  An instance is created for the data source. Click
    ![](media/rId4439.png){width="0.2646391076115486in"
    height="0.20833333333333334in"} (Copy API URL) to establish a
    connection when setting up Apigee Gateway.

First, download the resource file and then select the method to set up
the integration with Apigee.

####### Run an automated script to deploy configurations to Apigee

Use the script for full deployment (with or without connecting a flow
hook).

> **Note**
>
> The steps include the prerequisites that run the automated script that
> deploys files and configurations to Apigee. For manual configuration,
> refer to the section [Manual
> deployment](#bridgeheadidm113485995730202).

1.  Check that the GCP user running the script has `IAM` permissions.

- apigee.resourcefiles.list
      apigee.resourcefiles.create
      apigee.resourcefiles.update
      apigee.sharedflows.get
      apigee.sharedflows.create
      apigee.deployments.create
      apigee.sharedflowrevisions.deploy
      apigee.flowhooks.attachSharedFlow
      apigee.keyvaluemaps.create
      apigee.keyvaluemaps.delete
      apigee.keyvaluemapentries.create

2.  Run the `deploy.sh` script:

- chmod +x
      ./deploy.sh

3.  Verify that the JavaScript policies have been added to the shared
    flows:

- Go to Apigee \> Proxy development \> Shared Flows and check that the
  following policies have been added.

  - `sf-api-sec-extension-postflow`

  - `sf-api-sec-extension-preflow`

4.  Validate data ingestion:

- Send a request to the gateway and go to **Apigee** data source to
  validate that the data has been received from Apigee.

5.  (Optional) Exclude unwanted domains from being tracked by APIsec:

    a.  Uncomment: DOMAIN_EXCLUSION_LIST.

    b.  Add the domains to exclude.

    c.  Edit `deploy.sh` and set the following variables:

    - export DOMAIN_EXCLUSION_LIST="domain1,domain2"

6.  Discontinue the integration:

    a.  Edit `undeploy.sh`:

    - export PROJECT_ID=example-project-id
          export ORG=$PROJECT_ID
          export ENVIRONMENT=example-env

    b.  Run the undeploy.sh script:

    - chmod +x
          ./undeploy.sh

      Go to Apigee \> Proxy development \> Shared Flows and check that
      the following policies have been removed.

      - `sf-api-sec-extension-postflow`

      - `sf-api-sec-extension-preflow`

####### Configure Apigee\'s JavaScript for manual deployment

You can customize the shared flow and apply it to an existing flow hook
(pre-proxy, post-proxy).

Set up Apigee\'s JavaScript policy to send Apigee Collector\'s API data
to Cortex XSIAM.

> **Note**
>
> If you have an existing hookand would like to integrate with the
> shared flow, run the `deploy.sh` script, and select `n`\' and exit at
> the prompt to create a new hook. Refer to the section [Connect to
> existing hook](#bridgeheadidm234856423182936).

1.  Edit `panw-api-sec-extension-configuration.properties` file:

    - Enter the `targetUrl` and `projectID`.

    - You can update 127KB of `maxBodyInspectionSizeKB`.

    - For domain exclusion, uncomment the line and add the URL to
      exclude.

    <!-- -->

    - targetUrl=<Cortex collector url>
          projectID=<GCP project id of apigee>
          maxBodyInspectionSizeKB=127 // This is default 
          and can be modified if needed.
          commonBinaryContentType=audio/,video/,image/,
          application/octet-stream,application/ogg,application/
          pdf,application/zip,application/gzip,application/
          vnd.rar,application/x-7z-compressed
          #domainExclusionList=example.com,example2.com/shopping

2.  Upload the edited `property set`:

    a.  Get a token to upload updates via an API request. For more
        information, refer to [property
        sets](https://cloud.google.com/apigee/docs/api-platform/cache/property-sets).

    - Input:

          gcloud config config-helper --force-auth-refresh --format

      Output:

          configuration:
            active_configuration: 
            properties:
              compute:
                region: 
                zone:     
          core:
                account: 
                disable_usage_reporting: 
                project: 
          credential:
            access_token: <Copy this value>
            id_token: 
            token_expiry: 
          sentinels:
            config_sentinel: 

    b.  Copy the `<access_token>` value from the output.

3.  Upload the `property set` to Apigee:

- curl --silent -X GET 
      "https://apigee.googleapis.com/v1/organizations/
      <ORG>/environments/<ENVIRONMENT>/resourcefiles/
      properties" -H 
      "Authorization: Bearer <access_token from above>"

4.  Generate Key Value Map (KVM), which stores the Cortex API key
    that\'s encrypted

- curl --silent -X POST 
      "https://apigee.googleapis.com/v1/organizations/
      <ORG>/environments/<ENVIRONMENT>/keyvaluemaps" -H 
      "Authorization: Bearer <access_token from above>" 
      -H "Content-Type: application/json" --data-raw 
      '{"name": "'"APISec-KVM"'", "encrypted": true}'

  If there\'s an error when creating the KVM because of an existing
  name, delete the KVM and recreate.

      curl --silent -X DELETE 
      "https://apigee.googleapis.com/v1/organizations/
      <ORG>/environments/<ENVIRONMENT>/keyvaluemaps/
      $APISEC_KVM_NAME" -H "Authorization: Bearer 
      <access_token from above>"

  Add the Cortex API key entry to the created KVM.

      curl --silent -X POST "https://apigee.googleapis.com/
      v1/organizations/<ORG>/environments/<ENVIRONMENT>/
      keyvaluemaps/$APISEC_KVM_NAME/entries" -H 
      "Authorization: Bearer <access_token from above>" 
      -H "Content-Type: application/json" --data-raw 
      '{"name": "api-key","value": "'"<Generated key 
      from cortex env>"'"}'

5.  Upload the shared flows:

- **Shared flows**:

  - `sf-api-sec-extension-postflow`

  - `sf-api-sec-extension-preflow`

  **Upload**

  Replace the `<sf>` with the shared flows:

      curl --silent -X POST --data-binary "<sf>.zip" -H 
      "Content-Type: application/octet-stream" -H 
      "Authorization: Bearer <access_token from above>" 
      "https://apigee.googleapis.com/v1/organizations/$ORG/
      sharedflows?action=import&name=<sf>"

  **Deploy**

  Input:

      curl --silent -X GET "https://apigee.googleapis.com/
      v1/organizations/<ORG>/sharedflows/<sf>" -H 
      "Authorization: Bearer <access_token from above>"

  Output:

      {
        "metaData": {
          "createdAt": "1736952161610",
          "lastModifiedAt": "1736952161610",
          "subType": "SharedFlow"
        },
        "name": "sf-api-sec-extension-postflow",
        "revision": [
          "1" // This is the revision number
        ],
        "latestRevisionId": "1"
      }

6.  Deploy `<sf>`:

- curl --silent -X POST -H "Authorization: 
      Bearer <access_token from above>" 
      "https://apigee.googleapis.com/
      v1/organizations/$ORG/environments/<ENVIRONMENT>/
      sharedflows/$sf/revisions/<REVISION>/
      deployments?override=true"

7.  Verify API security shared flows were created:

- Go to Apigee \> Proxy development \> Shared Flows and check that the
  following policies have been added.

  - `sf-api-sec-extension-postflow`

  - `sf-api-sec-extension-preflow`

####### Connect to an existing hook

Follow the steps if you have an existing hook and would like to
integrate with a shared flow.

1.  Check for existing flow hooks.

    a.  Go to Apigee \> Management \> Environments and select the
        environment to hook the shared flow.

    b.  In the **Flow Hooks** tab, select the relevant flow hook.

2.  Configure policy for shared flow to the existing hook.

    a.  Go to Apigee \> Proxy development \> Shared Flows and select the
        flow hook from the relevant environment.

    - > **Note**

      > Start with the hook in pre-proxy.

    b.  From the **Develop** tab, expand **Policies** and select
        **Flow Callout**.

    c.  Enter a meaningful name and select the **Sharedflow**:
        `sf-api-sec-extension-preflow` , and then click **Create**.

    d.  From the **Develop** tab, select **Shared flows** and expand
        **Default**.

    e.  From **Select policy**, select **Select existing policy**, and
        select the policy just created and then click **Add**.

    f.  Repeat the previous steps for the post-proxy hook. Select the
        **Sharedflow**: `sf-api-sec-extension-postflow`.

    g.  Click **Save and Deploy**.

- The steps automatically run without linking to the hooks.

  > **Important**

  > This should only be done when there are already existing hooks, and
  > API security shared flows can\'t be hooked as a standalone. Run the
  > deployment script, but skip step 9 by passing `n`. This step
  > publishes API security shared flows to the desired Apigee
  > environment without setting them to flow hooks.

3.  Limitations:

    - The API security extension deployment scripts currently do not
      support archive-deployment Apigee environments. Refer to [Manage
      archive
      deployment](https://cloud.google.com/apigee/docs/api-platform/deploy/manage-archive-deployments)
      for more information.

    <!-- -->

    - > **Note**

      > Archive deployments are currently in preview and are subject to
      > change.

    <!-- -->

    - The API security extension for Apigee relies on flow-hooks, which
      are available only with Intermediate or Comprehensive Apigee
      Environment types. Refer to
      [Environments](https://cloud.google.com/apigee/docs/api-platform/fundamentals/environments-overview#environment-types)
      for more information.

    - For requests/responses with binary payloads, the binary payload is
      not sent to the collector for analysis; only the metadata (for
      example, HTTP headers, query parameters, etc.) is sent.

###### Ingest Kong

> **Note**
>
> Requires the Data Collection add-on.

Integrate Kong with Cortex XSIAM to start scanning its APIs for
potential threats and vulnerabilities.

You need to integrate a dedicated Kong HTTP log plugin. This plugin
enables seamless traffic ingestion from your Kong API gateway to Cortex
XSIAM, allowing for comprehensive security measures such as OWASP
Top-10, bot detection, access control, and more.

Settings in Cortex XSIAM

In Cortex XSIAM, set up the **Kong** data source to integrate with the
Kong API Gateway.

1.  From Settings \> Data Sources , click
    ![](media/rId4436.png){width="0.9668799212598426in"
    height="0.20833333333333334in"} and search for **Kong** , and then
    click **Connect** or **Connect Another Instance**.

2.  In the **Kong Collector** wizard, enter a relevant name and then
    click **Create and Proceed**.

3.  Copy the key and paste it somewhere so that you can access it later.

- If you forget to record the key and close the window, you must
  generate a new key and repeat this process.

4.  Click the **Download Custom Plugin** link to download the plugin,
    which you can then install on the Kong API Gateway.

5.  Click **Close**.

6.  An instance is created for the data source. Click
    ![](media/rId4439.png){width="0.2646391076115486in"
    height="0.20833333333333334in"} (Copy API URL) to establish a
    connection when setting up Kong API Gateway.

Follow the steps to integrate Kong\'s API gateway with Cortex XSIAM.

####### Provision Kong API gateway with the custom plugin

To deploy the custom plugin, refer to the Kong API documentation online:

- [Kong
  Gateway](https://docs.konghq.com/gateway/latest/plugin-development/distribution/)

- [Kong
  Konnect](https://docs.konghq.com/konnect/gateway-manager/plugins/add-custom-plugin/)

- [Kong Ingress
  Controller](https://docs.konghq.com/kubernetes-ingress-controller/latest/plugins/custom/)

Kong as docker container

In this example, we\'ll use Docker to deploy Kong with the custom
plugin.

1.  Extract the zip archive into some target directory TARGET_DIRECTORY.
    This directory should contain an extracted \"kong\" directory.

2.  Add the following arguments to the \`docker run\` command, replacing
    TARGET_DIRECTORY with the correct path:

- -v "TARGET_DIRECTORY/kong:/tmp/custom_plugins/kong" \
      -e "KONG_LUA_PACKAGE_PATH=/tmp/custom_plugins/?.lua;;" \
      -e "KONG_PLUGINS=bundled,panw-apisec-http-log"

  You may want to adjust the size of the nginx body buffer, which is
  used by Kong internally. This size sets the upper limit on the amount
  of HTTP body bytes that can be mirrored by the plugin. By default,
  this value is 8192 bytes (8 KB). To change it, another argument can be
  passed to the docker: for example, setting it to 128 KB:

      -e "KONG_NGINX_HTTP_CLIENT_BODY_BUFFER_SIZE=128k"

  See <https://nginx.org/en/docs/syntax.html> for information on the
  allowed values of this variable.

  > **Important**

  > The size of the buffer must be equal to or larger than the max body
  > size setting in the plugin configuration, on every data plane node.

3.  To verify that the plugin is installed, query Kong's Admin API using
    the following command:

- curl admin-api-hostname:8001 | jq .configuration.loaded_plugins.'"panw-apisec-http-log"'

  This prints true to the terminal if the plugin is loaded into the Kong
  instance.

####### Add and configure the custom plugin

Add and configure the plugin.

1.  From the Kong Manager menu, go to **Plugins**.

2.  From the **Plugins** page, scroll down to the **Custom Plugins**
    section.

3.  Select **panw-apisec-http-log** and click **Edit** to configure the
    panw-apisec-http-log plugin settings.

  -----------------------------------------------------------------------
  Configuration           Description             Example
  ----------------------- ----------------------- -----------------------
  Protocols               The request protocols   Either http, https, or
                          the plugin will be      both
                          applied to.             

  Cloud Context           Cloud context, such as  987654321000
                          AWS Account ID, GCP     
                          Project ID, Azure       
                          Subscription or an      
                          appropriate value for   
                          on-prem.                

  Cloud Provider          Cloud provider where    AWS.
                          Kong API Gateway is     
                          installed.              

  Cloud Region            Cloud region.           us-east-2

  Cloud API Key           The collector           
                          authorization key       
                          provided by the Cortex  
                          platform.               

  HTTP Endpoint           The Cortex collector\'s 
                          endpoint URL.           
  -----------------------------------------------------------------------

4.  Click the **View Advanced Parameters** to configure optional
    settings.

- > **Note**

  > The queue parameters can be updated to change when the plugin
  > mirrors data to Cortex.

+-----------------------+----------------------------------------+-----------------------+
| Configuration         | Description                            | Example               |
+=======================+========================================+=======================+
| Instance Name         | A custom name for this plugin          | Empty                 |
|                       | instance. This is useful when applying |                       |
|                       | different instances to different       |                       |
|                       | scopes.                                |                       |
+-----------------------+----------------------------------------+-----------------------+
| Tags                  | An optional set of strings for         | Empty                 |
|                       | grouping and filtering,                |                       |
|                       |                                        |                       |
|                       | > **Note**                             |                       |
|                       | >                                      |                       |
|                       | > Use commas to separate tags.         |                       |
+-----------------------+----------------------------------------+-----------------------+
| Keepalive             | An optional value in milliseconds that | 60000 (60 seconds)    |
|                       | defines how long an idle connection    |                       |
|                       | will live before being closed.         |                       |
+-----------------------+----------------------------------------+-----------------------+
| Timeout               | An optional timeout in milliseconds    | 10000 (10 seconds)    |
|                       | when sending data to Cortex.           |                       |
+-----------------------+----------------------------------------+-----------------------+
| Max body size         | The maximum body size to mirror in     | 131072 (128 KB), or   |
|                       | bytes (for example: 1024 is 1KB). Any  | the nginx body buffer |
|                       | bytes beyond this size are omitted     | size if it's smaller. |
|                       | from the request and response bodies.  |                       |
|                       | Must be \<= 4 MB and \<= the value of  |                       |
|                       | Kong\'s                                |                       |
|                       | **nginx_http_client_body_buffer_size** |                       |
|                       | setting.                               |                       |
+-----------------------+----------------------------------------+-----------------------+
| Queue Concurrency     | The number of queue delivery timers.   | 1                     |
| Limit                 | -1 indicates unlimited.                |                       |
+-----------------------+----------------------------------------+-----------------------+
| Queue.Initial Retry   | Time in seconds before the initial     | 0.01 (10              |
| Delay                 | retry is made for a failing batch.     | milliseconds)         |
+-----------------------+----------------------------------------+-----------------------+
| Queue.Max Batch Size  | Maximum number of entries that can be  | 1                     |
|                       | processed at a time.                   |                       |
+-----------------------+----------------------------------------+-----------------------+
| Queue.Max Bytes       | Maximum number of bytes that can be    | Unlimited             |
|                       | waiting in a queue, requires string    |                       |
|                       | content                                |                       |
+-----------------------+----------------------------------------+-----------------------+
| Queue.Max Coalescing  | Maximum number of (fractional) seconds | 1                     |
| Delay                 | to elapse after the first entry was    |                       |
|                       | queued before the queue starts calling |                       |
|                       | the handler.                           |                       |
+-----------------------+----------------------------------------+-----------------------+
| Queue.Max Entries     | Maximum number of entries that can be  | 10000                 |
|                       | waiting in the queue.                  |                       |
+-----------------------+----------------------------------------+-----------------------+
| Queue.Max Retry Delay | Maximum time in seconds between        | 60                    |
|                       | retries, caps exponential backoff.     |                       |
+-----------------------+----------------------------------------+-----------------------+
| Queue.Max Retry Time  | Time in seconds before the queue gives | 60                    |
|                       | up calling a failed handler for a      |                       |
|                       | batch.                                 |                       |
+-----------------------+----------------------------------------+-----------------------+

5.  Go to **Kong** data source to validate that data is ingested from
    the Kong API Gateway.

####### Limitations

- The plugin supports HTTP and HTTP/S protocols.

- The plugin supports Kong API Gateway version 3.4.x and above.

- The nginx body buffer size on each data plane node must be equal or
  larger than the **max body size** setting.

- Request and response bodies will not be mirrored if their size exceeds
  the nginx body buffer size. When this occurs, it is indicated in the
  metadata that is sent to Cortex along with the HTTP transaction data.

- The mirrored response body is the body returned from the upstream
  service. This means that changes made to the response body by other
  plugins, is not reflected in the mirrored data.

##### Ingest data from third-party pipeline solutions

Ingest data from third-party pipeline solutions into Cortex XSIAM.

###### Ingest data from Cribl

The Cribl data collector is an out-of-the-box native integration which
ingests data that Cribl collects from multiple data sources and streams
to Cortex XSIAM, while ensuring that all downstream capabilities,
including analytics, are available in Cortex XSIAM. 

The onboarding process in Cribl has an impact on the output that is sent
to Cortex XSIAM. Therefore, the onboarding process of some sources in
Cribl might have to be implemented in a certain way in order to adhere
to Cortex XSIAM requirements. These processes are described in more
detail in Tasks 1 and 3, below. 

Raw data must be collected by Cribl and streamed as-is from the passed
through source, because any changes made by Cribl might affect the way
that Cortex XSIAM handles the data.

For best results, we recommend ingesting data from Palo Alto Networks
products, such as Next-Generation Firewall (NGFW) using the dedicated
Cortex XSIAM data collectors, instead of source collectors provided by
Cribl. Although you can ingest FW data through Cribl, ingesting it that
way will omit a layer of data (EAL).

> **Note**
>
> We do not support email data collection via Cribl.

**Workflow high-level overview:**

1.  Task 1: In Cribl, onboard data collection from your data sources.

2.  Task 2: In Cortex XSIAM, create a Cribl data collector instance, and
    obtain the authorization token and the API URL.

3.  Task 3: In Cribl, for each source, configure the destination, using
    the Cortex XSIAM authorization token, the Cortex XSIAM API URL, and
    the source UUID.

4.  Task 4: Verify that data is streamed to Cortex XSIAM as expected,
    and perform ongoing maintenance.

Perform the following tasks in the order that they appear.

Task 1 (in Cribl, create new data sources)

> **Prerequisite**
>
> Ensure that you have the credentials and IDs for each data source,
> such as Tenant ID, App ID and Client secret.

General guidelines specifically for Cortex XSIAM (for more information,
refer to  [Cribl
documentation](https://docs.cribl.io/stream/destinations-xsiam/)):

- If you have not already done so, create source collectors to onboard
  the desired data sources.

- For some data sources, Cribl includes specific collectors in its
  catalog. If you can\'t find one in the catalog specifically for your
  source, use a generic collector.

- Although some native Cribl source collectors allow you to ingest
  several data types using the same source collector, we do not
  recommend this approach. To ensure optimal Cortex XSIAM performance,
  configure a separate Cribl source collector for each data type. For
  reference purposes, [this data source UUID
  list](#UUID760aec02216e15354e6cb14dddadab27) shows all the data types
  that can be onboarded.

<!-- -->

- For example, Microsoft 365 has several data types, such as users,
  groups, and contacts.

Task 2 (in Cortex XSIAM)

> **Note**
>
> Only one Cribl data collector instance can be configured in Cortex
> XSIAM.

1.  Go to Settings \> Configuration \> Data Collection \> Data Sources.

2.  Search for **Cribl**.

3.  Click the **Cribl** integration, and then click **Connect**.

4.  In the **Name** field, enter a meaningful name for the integration.

5.  Click **Save & generate token**.

6.  Click the **Copy** icon to copy the authorization token.

7.  Save the authorization token copy in a safe place for future use.
    You cannot access this token again, so take care to copy it and save
    it before you close the dialog box.

8.  Click **Close**.

9.  On the **Data Sources** page, in the row for the Cribl instance,
    click the link icon (**Copy API URL**). Save the API URL copy in a
    safe place for future use.

Task 3 (In Cribl, configure Cortex XSIAM as a destination for each
source)

> **Prerequisite**
>
> Ensure that you have the copies of the Cortex XSIAM authorization
> token and API URL obtained in Task 2.

The following table includes guidelines that are relevant specifically
for Cortex XSIAM. While you are configuring Cortex XSIAM destinations
for your sources, configure the items listed in the table below as
described.

For general information about configuring destinations, refer to  [Cribl
documentation.](https://docs.cribl.io/stream/destinations-xsiam/)

+-----------------------+-------------------------+-------------------------------------------------+
| Item                  | Setting                 | Details                                         |
+=======================+=========================+=================================================+
| Cortex XSIAM URL      | **XSIAM Endpoint**      | Paste the API URL obtained from Cortex XSIAM.   |
|                       | field                   |                                                 |
+-----------------------+-------------------------+-------------------------------------------------+
| Authorization token   | **Authorization Token** | Paste the authorization token obtained from     |
|                       | field                   | Cortex XSIAM.                                   |
+-----------------------+-------------------------+-------------------------------------------------+
| Advanced Settings     | **Compress** toggle     | Ensure that **Compress** is disabled.           |
+-----------------------+-------------------------+-------------------------------------------------+
| HTTP headers          | **Extra HTTP headers**  | Add extra HTTP headers for each data source:    |
|                       |                         |                                                 |
|                       |                         | - **Source-identifier:** Search the table       |
|                       |                         |   supplied in this topic for the vendor and     |
|                       |                         |   product.                                      |
|                       |                         |                                                 |
|                       |                         | <!-- -->                                        |
|                       |                         |                                                 |
|                       |                         | - The [Data source UUIDs                        |
|                       |                         |   table](#UUID760aec02216e15354e6cb14dddadab27) |
|                       |                         |   lists the data sources that can be identified |
|                       |                         |   by Cortex XSIAM, using their corresponding    |
|                       |                         |   UUIDs. These UUIDs are required to map data   |
|                       |                         |   collected by Cribl to the Cortex XSIAM        |
|                       |                         |   destination.                                  |
|                       |                         |                                                 |
|                       |                         |   - If you find the desired vendor and product, |
|                       |                         |     copy the corresponding UUID from the table  |
|                       |                         |     and paste here. This UUID will allow Cortex |
|                       |                         |     XSIAM to leverage all the data ingested     |
|                       |                         |     from the data source, such as identifiers,  |
|                       |                         |     pipeline sources such as IP addresses,      |
|                       |                         |     devices, and so on. Data from sources known |
|                       |                         |     to Cortex XSIAM are saved in the            |
|                       |                         |     appropriate datasets--the same datasets as  |
|                       |                         |     those used by dedicated data collectors in  |
|                       |                         |     Cortex XSIAM.                               |
|                       |                         |                                                 |
|                       |                         |   <!-- -->                                      |
|                       |                         |                                                 |
|                       |                         |   - **Note:** Do not use the generic UUID for a |
|                       |                         |     data source that is known to Cortex XSIAM   |
|                       |                         |     and appears in the table.                   |
|                       |                         |                                                 |
|                       |                         |   <!-- -->                                      |
|                       |                         |                                                 |
|                       |                         |   - If you can't find the desired vendor and    |
|                       |                         |     product source in the table, copy the       |
|                       |                         |     generic UUID provided in the first row of   |
|                       |                         |     the table, and paste here. Data from        |
|                       |                         |     unknown sources are saved in a separate     |
|                       |                         |     searchable dataset. The dataset name will   |
|                       |                         |     reflect the Vendor and Product names that   |
|                       |                         |     you enter next.                             |
|                       |                         |                                                 |
|                       |                         | <!-- -->                                        |
|                       |                         |                                                 |
|                       |                         | - **Format:** json                              |
|                       |                         |                                                 |
|                       |                         | - **Vendor:** When using the UUID for unknown   |
|                       |                         |   data sources, you must enter the vendor name. |
|                       |                         |                                                 |
|                       |                         | - **Product:** When using the UUID for unknown  |
|                       |                         |   data sources, you must enter the product      |
|                       |                         |   name.                                         |
+-----------------------+-------------------------+-------------------------------------------------+
| Mapping               | **Passthru** option     | Map the data source(s) that you created in Task |
|                       |                         | 1 to the XSIAM data destination created in this |
|                       |                         | task. Ensure that you select the **Passthru**   |
|                       |                         | option.                                         |
+-----------------------+-------------------------+-------------------------------------------------+
| Deployment            | **Commit & Deploy**     | When mapping is complete, click                 |
|                       |                         | **Commit & Deploy**, and then click **Deploy**. |
|                       | **Deploy**              |                                                 |
+-----------------------+-------------------------+-------------------------------------------------+

Task 4 (in Cribl and in Cortex XSIAM)

Verify that data is streaming as expected from Cribl to Cortex XSIAM.

- In the Cribl user interface, click the **Source** collector, click
  **Configure**, and then the **Charts** tab. Check the charts to verify
  that streaming is in progress.

- In Cortex XSIAM, on the **Data Sources** page, when streaming begins,
  a green check mark appears below the Cribl configuration, along with
  the amount of data received.

Other optional tasks

Use the Disable and Delete options with extreme caution.

- Disabling the integration will cease streaming from Cribl.

- Deleting the integration will erase the integration completely and
  will require reconfiguration, because the original authorization token
  will be lost.

**Disable the integration with Cribl**

1.  To disable the integration, in Cortex XSIAM, search for the Cribl
    integration on the **Data Sources** page, and clear the **Enable**
    checkbox.

2.  In the **Are you sure?** dialog box, type `disable`, and then click
    **Disable**.

**Delete the integration with Cribl**

1.  To delete the Cribl integration, in Cortex XSIAM, search for the
    integration on the **Data Sources** page, and click the
    integration\'s **Delete** icon.

2.  In the **Are you sure?** dialog box, type `delete`, and then click
    **Delete**.

####### Data source UUIDs

+-----------------------+-----------------------+----------------------------------+
| Vendor                | Product               | UUID                             |
+=======================+=======================+:=================================+
|                       |                       | af01292940d7426594d3d3e55ae17ee0 |
|                       |                       |                                  |
|                       |                       | > **Note**                       |
|                       |                       | >                                |
|                       |                       | > Do not use this generic UUID   |
|                       |                       | > when your data source is       |
|                       |                       | > listed in this table.          |
+-----------------------+-----------------------+----------------------------------+
| Salesforce            | Salesforce logs       | ab109687acd24978aabcb7ad8b5742e3 |
+-----------------------+-----------------------+----------------------------------+
| Salesforce            | Salesforce snapshots  | addbf31a6372491e88d45934dff5b5b0 |
+-----------------------+-----------------------+----------------------------------+
| Dropbox               | Events                | a6322b2fd9e545e0a4223ba754c48fb9 |
+-----------------------+-----------------------+----------------------------------+
| Dropbox               | Directory             | e8d2c52bc9594621924fab0507264586 |
+-----------------------+-----------------------+----------------------------------+
| Workday               | Workday               | 00d4e740702d4eb2939a87c2318513dd |
+-----------------------+-----------------------+----------------------------------+
| Google                | Cloud Logging         | 00a8322c85e14beabfa7ad5f3d62db73 |
+-----------------------+-----------------------+----------------------------------+
| Okta                  | SSO                   | 5faf4c1fdb8443d9920d6a54815432c1 |
+-----------------------+-----------------------+----------------------------------+
| Microsoft             | Azure AD              | c00d6d52e5b141a8baa8db9d9345423d |
+-----------------------+-----------------------+----------------------------------+
| PingID                | PingONE               | 924951a8394b4605b1725f943292ab4f |
+-----------------------+-----------------------+----------------------------------+
| CrowdStrike           | Falcon incident       | 230b2b0233bf4327806af72e6e5769f3 |
+-----------------------+-----------------------+----------------------------------+
| CrowdStrike           | Hosts                 | 8b673ac8e2f34b4a8dc14c22f0e6063b |
+-----------------------+-----------------------+----------------------------------+
| CrowdStrike           | Falcon Data           | 6cd7d60f0ff5497baecf6b9073c8000e |
|                       | Replicator            |                                  |
+-----------------------+-----------------------+----------------------------------+
| SentinelOne           | Deep Visibility       | b9fa55e6fa564c709358425ce0f61517 |
+-----------------------+-----------------------+----------------------------------+
| Microsoft             | Azure                 | fce13a1d51294f84bae4a37851503060 |
+-----------------------+-----------------------+----------------------------------+
| Microsoft             | Defender              | ce9e8cf36e0742c38aa89787a256855f |
+-----------------------+-----------------------+----------------------------------+
| ServiceNow            | CMDB                  | 8b3e767247e44471a95e563378d0b9be |
+-----------------------+-----------------------+----------------------------------+
| Prisma                | Cloud                 | f8c3403a02fd4147862ee293bf4e74e2 |
+-----------------------+-----------------------+----------------------------------+
| Prisma                | Assets                | 6a61c1cba1b64cd2a977c76c41f7950d |
+-----------------------+-----------------------+----------------------------------+
| Proofpoint            | TAP                   | 3eefce0f791e4391a3643b8cf860a361 |
+-----------------------+-----------------------+----------------------------------+
| Microsoft             | Office 365 Exchange   | dee8e85ce7db4573a8bc21b807e1d73a |
|                       | Online                |                                  |
+-----------------------+-----------------------+----------------------------------+
| Microsoft             | Azure AD audit        | 0e076d5abe864bf78e8145ea9e0d749e |
+-----------------------+-----------------------+----------------------------------+
| Microsoft             | Azure AD sign-ins     | f56dcfdf6bca43e793a4b6e9290e7b12 |
+-----------------------+-----------------------+----------------------------------+
| Microsoft             | Graph security alerts | 5619f2f691fc46c4b202587fdaa031c3 |
+-----------------------+-----------------------+----------------------------------+
| Palo Alto Networks    | IOT Security devices  | 80cee50bfc6e4ac5b34b19794b767acd |
+-----------------------+-----------------------+----------------------------------+
| Palo Alto Networks    | IOT Security alerts   | e772949c88ec4107ad81ec38061d35c0 |
+-----------------------+-----------------------+----------------------------------+
| Google                | Gsuite reports        | 3ddd43030db142839568943e0a2fe785 |
+-----------------------+-----------------------+----------------------------------+
| Google                | Gmail                 | 8607490288d1407ba82b5c5ad9dc64a0 |
+-----------------------+-----------------------+----------------------------------+
| Box                   | Box                   | 3ef05d14ae9349f8bbd48c8a4797334a |
+-----------------------+-----------------------+----------------------------------+
| OneLogin              | Events                | 22b23a3f9f1e49998645b683d5dc3a6f |
+-----------------------+-----------------------+----------------------------------+
| OneLogin              | OneLogin              | 88cfbd3e7b974d999b10edac83995b8a |
+-----------------------+-----------------------+----------------------------------+
| Google                | Workspace alerts      | 4f263650cd29475c81f2ff953cf19827 |
+-----------------------+-----------------------+----------------------------------+
| Microsoft             | Office 365 devices    | de229685f708413fad46289657ea09de |
+-----------------------+-----------------------+----------------------------------+
| Microsoft             | Office 365 rules      | 6b925df8923d4038bf78998d1ffde77c |
+-----------------------+-----------------------+----------------------------------+
| Microsoft             | Office 365 users      | dcfb7a412e654efd868de0b8cf81766a |
+-----------------------+-----------------------+----------------------------------+
| Microsoft             | Office 365 groups     | 0b0499ac0d984145b201c6d674771dbf |
+-----------------------+-----------------------+----------------------------------+
| Microsoft             | Office 365 contacts   | de1b694a6c8341958bc08c4b7c140874 |
+-----------------------+-----------------------+----------------------------------+
| Microsoft             | Office 365 domains    | cae29fd87b554bd9a5694afb225e8dc9 |
+-----------------------+-----------------------+----------------------------------+
| Microsoft             | Office 365 mailboxes  | 9855a03559ce4263b568671e695d1fa8 |
+-----------------------+-----------------------+----------------------------------+
| Google                | Workspace ChromeOS    | e82ae276e6b9442fa80920a03d2a38d6 |
|                       | devices               |                                  |
+-----------------------+-----------------------+----------------------------------+
| Google                | Workspace groups      | 689ae8ef14e848e3855b81e91d8af9bc |
+-----------------------+-----------------------+----------------------------------+
| Google                | Workspace domains     | 2738e963ac3141afaad05885e060a73c |
+-----------------------+-----------------------+----------------------------------+
| Google                | Workspace profiles    | f2aed57ff13c439eb93153ba7309fe87 |
+-----------------------+-----------------------+----------------------------------+
| Google                | Workspace rules       | 2621aaf3334a4147ae727afe84db31a9 |
+-----------------------+-----------------------+----------------------------------+
| Google                | Workspace             | 4e342367057d46c7b38ce7d40682fd1e |
|                       | organization          |                                  |
+-----------------------+-----------------------+----------------------------------+
| Google                | Workspace group       | 8a0140fc47b643838d0fcf096773c0a1 |
|                       | members               |                                  |
+-----------------------+-----------------------+----------------------------------+
| Google                | Workspace contacts    | d20d6cfea3e943d5a5a6bc005c429ef4 |
+-----------------------+-----------------------+----------------------------------+
| Google                | Workspace users       | 359ecd845fa54caab6ddb4b7c7a2764d |
+-----------------------+-----------------------+----------------------------------+
| Google                | Workspace mailboxes   | 328796d692f343c38f07351e8c783f80 |
+-----------------------+-----------------------+----------------------------------+
| Google                | Workspace users send  | 3b8f9e65f8ed43f4a4e5679236691fe2 |
|                       | as aliases            |                                  |
+-----------------------+-----------------------+----------------------------------+
| Google                | Workspace schemas     | c978986a6b3846c7b6fdbe15bef14f69 |
+-----------------------+-----------------------+----------------------------------+
| Google                | Workspace mailbox     | 3c4beffadfac40a18aaf4d143a19dc27 |
|                       | settings              |                                  |
+-----------------------+-----------------------+----------------------------------+
| Google                | Workspace privileges  | 462ebcbfce9341ac8c006e5aa45ccf44 |
+-----------------------+-----------------------+----------------------------------+
| Google                | Workspace mobile      | 149b58ec938d4d1a8568359483e50800 |
|                       | devices               |                                  |
+-----------------------+-----------------------+----------------------------------+
| Google                | Workspace roles       | 82170b42b9684b79bda124c712bcbdc0 |
+-----------------------+-----------------------+----------------------------------+
| Google                | Workspace contact     | 5a42004787064021a462bb2120160514 |
|                       | groups                |                                  |
+-----------------------+-----------------------+----------------------------------+
| Google                | Workspace apps        | 5a617df8827d461db66a10d084c7b39f |
+-----------------------+-----------------------+----------------------------------+
| Amazon                | AWS EKS               | fb8a9d4922cb4095b76d71e921d2d999 |
+-----------------------+-----------------------+----------------------------------+
| Microsoft             | DHCP                  | b55819e8959c49728d5d98a6d87eafb6 |
+-----------------------+-----------------------+----------------------------------+
| Amazon                | AWS flow logs         | 667083aa68544eee8b67cdd2d4cc327b |
+-----------------------+-----------------------+----------------------------------+
| Amazon                | AWS audit logs        | c19f87b6262f48259b3d5d2a2c691802 |
+-----------------------+-----------------------+----------------------------------+
| Amazon                | AWS generic logs      | 0498f8a24de04b3e85102e742f6783f8 |
+-----------------------+-----------------------+----------------------------------+
| Amazon                | AWS Route 53 logs     | d57ae82c1e2a4d138fc34084d159b09e |
+-----------------------+-----------------------+----------------------------------+
| Amazon                | AWS prompt logs       | a53edad7ef0c46ffb5037fb2e21520cb |
+-----------------------+-----------------------+----------------------------------+
| Microsoft             | Office 365 Sharepoint | 3a37f519e9094a3f8c4185fa572cd111 |
|                       | Online                |                                  |
+-----------------------+-----------------------+----------------------------------+
| Microsoft             | Office 365 Azure AD   | e1f109f886ea42fbb96be6ec0cc597a9 |
+-----------------------+-----------------------+----------------------------------+
| Microsoft             | Office 365 DLP        | 8f052782739d4b8389644cca23b994ac |
+-----------------------+-----------------------+----------------------------------+
| Microsoft             | Office 365 General    | c7655e83805b4a058e66043a6715156c |
+-----------------------+-----------------------+----------------------------------+

##### Additional log ingestion methods

In addition to native log ingestion support, Cortex XSIAM also supports
a number of custom log ingestion methods.

###### Ingest logs from a Syslog receiver

Cortex XSIAM can receive Syslog from a variety of supported vendors (see
[External data ingestion vendor
support](#UUID09e9f44b9e75774aa5cb905d1ecebf28)). In addition, Cortex
XSIAM can receive Syslog from additional vendors that use CEF, LEEF,
CISCO, CORELIGHT, or RAW formatted over Syslog.

After Cortex XSIAM begins receiving logs from the third-party source,
Cortex XSIAM automatically parses the logs in CEF, LEEF, CISCO,
CORELIGHT, or RAW format and creates a dataset with the name
`<vendor>_<product>_raw`. You can then use XQL Search queries to view
logs and create new IOC, BIOC, and Correlation Rules.

To receive Syslog from an external source:

1.  Set up your Syslog receiver to forward logs.

2.  Activate the Syslog collector applet on a Broker VM within your
    network. For more information, see [Activate the Syslog
    Collector](#UUIDd7d85d4a97df7ec2a052d22376f5a52e).

3.  Use the XQL Search to search your logs.

###### Ingest Apache Kafka events as datasets

Cortex XSIAM can receive events from Apache Kafka clusters directly to
your log repository for query and visualization purposes. After you
activate the Kafka Collector applet on a Broker VM in your network,
which includes defining the connection details and settings related to
the list of subscribed topics to monitor and upload to Cortex XSIAM, you
can collect events as datasets.

After Cortex XSIAM begins receiving topic events from the Kafka
clusters, Cortex XSIAM automatically parses the events and creates a
dataset with the specific name you set as the target dataset when you
configured the Kafka Collector, and adds the data in these files to the
dataset. You can then use XQL Search queries to view events and create
new Correlation Rules.

Configure Cortex XSIAM to receive events as datasets from topics in
Kafka clusters.

1.  Activate the Kafka collector applet on a Broker VM within your
    network. For more information, see [Activate the Kafka
    Collector](#UUIDcea5b166351b073287f4c3c3624287c2).

2.  Use the XQL Search to query and review logs.

###### Ingest CSV files as datasets

Cortex XSIAM can receive CSV log files from a shared Windows directory
directly to your log repository for query and visualization purposes.
After you activate the CSV Collector applet on a Broker VM in your
network, which includes defining the list of folders mounted to the
Broker VM and setting the list of CSV files to monitor and upload to
Cortex XSIAM (using a username and password), you can ingest CSV files
as datasets.

The ingested CSV log files must conform to the following guidelines:

- Header field names must contain only letters (a-z, A-Z) or numbers
  (0-9) and must start with a letter. Spaces are converted to
  underscores (\_).

- Date values can be in either of the following formats:

  - YYYY-MM-DD (optionally including HH:MM:SS)

  - Unix Epoch time. For example, 1614858795.

After Cortex XSIAM begins receiving logs from the shared Windows
directory, Cortex XSIAM automatically parses the logs and creates a
dataset with the specific name you set as the target dataset when you
configured the CSV Collector. The CSV Collector checks for any changes
in the configured CSV files, as well as any new CSV files added to the
configuration folders, in the Windows directory every 10 minutes and
replaces the data in the dataset with the data from those files. You can
then use XQL Search queries to view logs and create new Correlation
Rules.

Configure Cortex XSIAM to receive CSV files as datasets from a shared
Windows directory.

1.  Ensure that you configure Windows to share the applicable CSV files
    in your Windows directory.

2.  Activate the CSV collector applet on a Broker VM within your
    network. For more information, see [Activate the CSV
    Collector](#UUID9549d7acf76fed0ee5998431177a1e9a).

3.  Use the XQL Search to locate and review logs.

###### Ingest database data as datasets

Cortex XSIAM can receive data from a client relational database directly
to your log repository for query and visualization purposes. After you
activate the Database Collector applet on a Broker VM in your network,
which includes defining the database connection details and settings
related to the query details for collecting the data from the database
to monitor and upload to Cortex XSIAM, you can collect data as datasets.
For more information about activating this collector applet, see
[Activate the Database
Collector](#UUID2480f49e18ca37b5443080cbc348ef7a).

After Cortex XSIAM begins receiving data from a client relational
database, Cortex XSIAM automatically parses the logs and creates a
dataset with the specific name you set as the target dataset when you
configured the Database Collector using the format
`<Vendor>_<Product>_raw`. The Database Collector checks for any changes
in the configured database based on the SQL Query defined in the
database connection according to the execution frequency of collection
that you configured and appends the data to the dataset. You can then
use XQL Search queries to view data and create new Correlation Rules.

Configure Cortex XSIAM to receive data as datasets data from a client
relational database.

1.  Activate the Database Collector applet on a Broker VM within your
    network.

2.  Use the XQL Search to query and review logs.

###### Ingest logs in a network share as datasets

Cortex XSIAM can receive logs from files and folders in a network share
directly to your log repository for query and visualization purposes.
After you activate the Files and Folders Collector applet on a Broker VM
in your network, which includes defining the connection details and
settings related to the list of files to monitor and upload to Cortex
XSIAM, you can collect files as datasets.

After Cortex XSIAM begins receiving logs from files and folders in a
network share, Cortex XSIAM automatically parses the logs and creates a
dataset with the specific name you set as the target dataset when you
configured the Files and Folders Collector using the format
`<Vendor>_<Product>_raw`. The Files and Folders Collector reads and
processes the configured files one by one, as well as any new files
added to the configured files and folders, in the network share
according to the execution frequency of collection that you configured
and adds the data in these files to the dataset. You can then use XQL
Search queries to view logs and create new Correlation Rules.

> **Note**
>
> The Files and Folders Collector applet only starts to collect files
> that are more than 256 bytes.

Configure Cortex XSIAM to receive logs as datasets from files and
folders in a network share.

1.  Activate the Files and Folders collector applet on a Broker VM
    within your network. For more information, see [Activate the Files
    and Folders Collector](#UUIDddd09123bfe9bf7a4a72aec8fb890d9e).

2.  Use the XQL Search to query and review logs.

###### Ingest FTP files as datasets

Cortex XSIAM can receive logs from files and folders via FTP, FTPS, or
SFTP directly to your log repository for query and visualization
purposes. After you activate the FTP Collector applet on a Broker VM in
your network, which includes defining the connection details and
settings related to the list of files to monitor and upload to Cortex
XSIAM, you can collect files as datasets.

After Cortex XSIAM begins receiving logs from files and folders via FTP,
FTPS, or SFTP, Cortex XSIAM automatically parses the logs and creates a
dataset with the specific name you set as the target dataset when you
configured the FTP Collector using the format `<Vendor>_<Product>_raw`.
The FTP Collector reads and processes the configured FTP files one by
one, as well as any new FTP files added to the configured files and
folders, in the FTP directory according to the execution frequency of
collection that you configured, and adds the data in these files to the
dataset. You can then use XQL Search queries to view logs and create new
Correlation Rules.

Configure Cortex XSIAM to receive logs as datasets from files and
folders via FTP, FTPS, or SFTP.

1.  Activate the FTP collector applet on a Broker VM within your
    network. For more information, see [Activate the FTP
    Collector](#UUID46b300416ed7fb186b42648c3b91447a).

2.  Use the XQL Search to query and review logs.

###### Ingest NetFlow flow records as datasets

Cortex XSIAM can receive NetFlow flow records and IPFIX from a UDP port
directly to your log repository for query and visualization purposes.
After you activate the NetFlow Collector applet on a Broker VM in your
network, which includes configuring your NetFlow Collector settings, you
can ingest NetFlow flow records and IPFIX as datasets.

The ingested NetFlow flow record format must include, at the very least:

- Source and Destination IP addresses

- TCP/UDP source and destination port numbers

When Cortex XSIAM begins receiving flow records from the UDP port,
Cortex XSIAM automatically parses the flow records and creates a dataset
with the specific name you set as the target dataset when you configured
the NetFlow Collector. The NetFlow Collector adds the flow records to
the dataset. You can then use XQL Search queries to view those flow
records and create new IOC, BIOC, and Correlation Rules. Cortex XSIAM
can also analyze your logs to generate Analytics issues.

Configure Cortex XSIAM to receive NetFlow flow records as datasets from
the routers and switches that support NetFlow.

1.  Set up your NetFlow exporter to forward flow records to the IP
    address of the Broker VM that runs the NetFlow collector applet.

2.  Activate the NetFlow collector applet on a Broker VM within your
    network. For more information, see [Activate the NetFlow
    Collector](#UUIDaf885e898b9338abd11f504660650131).

3.  Use the XQL Search to query your flow records, using your designated
    dataset.

###### Set up an HTTP log collector to receive logs

In addition to logs from supported vendors, you can set up a custom HTTP
log collector to receive logs in Raw, JSON, CEF, or LEEF format. The
HTTP Log Collector can ingest up to 80,000 events per sec.

When Cortex XSIAM begins receiving logs from the third-party source,
Cortex XSIAM automatically parses the logs and creates a dataset with
the name `<Vendor>_< Product>_raw`. You can then use XQL Search queries
to view logs and create new Correlation rules.

To set up an HTTP log collector to receive logs from an external source.

1.  Create an HTTP Log collector in Cortex XSIAM.

    a.  Select Settings \> Data Sources.

    b.  On the **Data Sources** page, click **Add Data Source**, search
        for and select **HTTP**, and click **Connect**.

    c.  Specify a descriptive **Name** for your HTTP log collection
        configuration.

    d.  Select the data object **Compression**, either gzip or
        uncompressed.

    e.  Select the **Log Format** as **Raw**, **JSON**, **CEF**, or
        **LEEF**.

    - Cortex XSIAM supports logs in single line format or multiline
      format. For a **JSON** format, multiline logs are collected
      automatically when the **Log Format** is configured as **JSON**.
      When configuring a **Raw** format, you must also define the
      **Multiline Parsing Regex** as explained below.

      > **Note**

      > -The **Vendor** and **Product** defaults to **Auto-Detect** when
      > the **Log Format** is set to **CEF** or **LEEF**.

      > -For a **Log Format** set to **CEF** or **LEEF**, Cortex XSIAM
      > reads events row by row to look for the **Vendor** and
      > **Product** configured in the logs. When the values are
      > populated in the event log row, Cortex XSIAM uses these values
      > even if you specified a value in the **Vendor** and **Product**
      > fields in the HTTP collector settings. However, when the values
      > are blank in the event log row, Cortex XSIAM uses the **Vendor**
      > and **Product** that you specified in the HTTP collector
      > settings. If you did not specify a **Vendor** or **Product** in
      > the HTTP collector settings, and the values are blank in the
      > event log row, the values for both fields are set to
      > **unknown**.

    f.  Specify the **Vendor** and **Product** for the type of logs you
        are ingesting.

    g.  (*Optional*) Specify the **Multiline Parsing Regex** for logs
        with multilines.

    - This option is only displayed when the **Log Format** is set to
      **Raw**, so you can set the regular expression that identifies
      when the multiline event starts in logs with multilines. It is
      assumed that when a new event begins, the previous one has ended.

    h.  **Save & Generate Token**.

    - Click the copy icon next to the key and record it somewhere safe.
      You will need to provide this key when you configure your HTTP
      POST request and define the **api_key**. If you forget to record
      the key and close the window you will need to generate a new key
      and repeat this process.

      Click **Done** when finished.

2.  Send data to your Cortex XSIAM HTTP log collector.

    a.  Send an HTTP POST request to the URL for your HTTP Log
        Collector.

    - You can view a sample curl or python request on an HTTP collector
      instance by selecting
      ![](media/rId2824.png){width="0.13690398075240595in"
      height="0.20833333333333334in"}View Example.

      Here is a CURL example:

          curl -X POST https://api-{tenant external URL}/logs/v1/event -H 'Authorization: {api_key}' -H 'Content-Type: text/plain' -d '{"example1": "test", "timestamp": 1609100113039}
          {"example2": [12321,546456,45687,1]}'

      Python 3 example:

          import requests
          def test_http_collector(api_key):
              headers = {
                  "Authorization": api_key,
                  "Content-Type": "text/plain"
              }
              # Note: the logs must be separated by a new line
              body = "{'example1': 'test', 'timestamp': 1609100113039}" \
                     "{'example2': [12321,546456,45687,1]}"
              res = requests.post(url="https://api-{tenant external URL}/logs/v1/event",
                                  headers=headers,
                                  data=body)
              return res

    b.  Substitute the values specific to your configuration.

        - `url`: You can copy the URL for your HTTP log collector from
          the **Custom Collectors** page. For example:
          `https://api-{tenant external URL}/logs/v1/event`.

        - `Authorization`: Paste the `api_key` you previously recorded
          for your HTTP log collector, which is defined in the header.

        - `Content-Type`: Depending on the data object format you
          selected during setup, this will be `application/json` for
          JSON format or `text/plain` for Text format. This is defined
          as part of the header.

        - `Body`: The body contains the records you want to send to
          Cortex XSIAM. Separate records with a `\n` (new line)
          delimiter. The request body can contain up to 10 Mib records,
          but 1 Mib is recommended. In the case of a curl command, the
          records are contained in the `-d ‘<records>’` parameter.

        <!-- -->

        - > **Note**

          > Each record cannot exceed 5 MB in size.

    c.  Review the possible success and failure code responses to your
        HTTP Post requests.

    - The following table provides the various success and failure code
      responses to your HTTP Post requests, which can help you
      troubleshoot any problems with your HTTP Collector configuration.

+-----------------------+-----------------------+-------------------------------------------------------------------------------------+
| Success/failure       | Description           | Output code displayed (if applicable)                                               |
| response code         |                       |                                                                                     |
+=======================+=======================+=====================================================================================+
| 200                   | Success code that     |     {    "error": "false"}                                                          |
|                       | indicates there are   |                                                                                     |
|                       | no errors and the     |                                                                                     |
|                       | request was           |                                                                                     |
|                       | successful.           |                                                                                     |
+-----------------------+-----------------------+-------------------------------------------------------------------------------------+
| 401                   | Unauthorized error    |                                                                                     |
|                       | code that indicates   |                                                                                     |
|                       | either an incorrect   |                                                                                     |
|                       | authorization token   |                                                                                     |
|                       | is being used or that |                                                                                     |
|                       | the HTTP Collector is |                                                                                     |
|                       | deleted/disabled.     |                                                                                     |
+-----------------------+-----------------------+-------------------------------------------------------------------------------------+
| 404                   | Error code 404 page   |                                                                                     |
|                       | not found that        |                                                                                     |
|                       | indicates a wrong     |                                                                                     |
|                       | URL.                  |                                                                                     |
+-----------------------+-----------------------+-------------------------------------------------------------------------------------+
| 413                   | Error code indicating |                                                                                     |
|                       | the payload is too    |                                                                                     |
|                       | large as the request  |                                                                                     |
|                       | size limit is 10 MB.  |                                                                                     |
+-----------------------+-----------------------+-------------------------------------------------------------------------------------+
| 500                   | Error code indicating |     {    "error": "error processing request, error: failed to process the request"} |
|                       | the request was not   |                                                                                     |
|                       | able to be processed  |                                                                                     |
|                       | due to an incorrect   |                                                                                     |
|                       | log format between    |                                                                                     |
|                       | the request and the   |                                                                                     |
|                       | HTTP collector        |                                                                                     |
|                       | configuration.        |                                                                                     |
+-----------------------+-----------------------+-------------------------------------------------------------------------------------+
| 429                   | Error code indicating |                                                                                     |
|                       | too many requests as  |                                                                                     |
|                       | the rate limit is 400 |                                                                                     |
|                       | requests per second   |                                                                                     |
|                       | per customer per      |                                                                                     |
|                       | endpoint.             |                                                                                     |
+-----------------------+-----------------------+-------------------------------------------------------------------------------------+

3.  Monitor your HTTP Log Collection integration.

- You can return to the Settings \> Data Sources page to monitor the
  status of your HTTP Log Collection configuration. For each instance,
  Cortex XSIAM displays the number of logs received in the last hour,
  day, and week. You can also use the Data Ingestion Dashboard to view
  general statistics about your data ingestion configurations.

4.  After Cortex XSIAM begins receiving logs, use the XQL Search to
    search your logs.

###### Ingest logs from BeyondTrust Privilege Management Cloud

If you use BeyondTrust Privilege Management Cloud, you can take
advantage of Cortex XSIAM investigation and detection capabilities by
forwarding your logs to Cortex XSIAM. This enables Cortex XSIAM to help
you expand visibility into computer, activity, and authorization
requests in the organization, correlate and detect access violations,
and query BeyondTrust Endpoint Privilege Management logs using XQL
Search.

When Cortex XSIAM starts to receive logs, Cortex XSIAM can analyze your
logs in XQL Search and you can create new Correlation Rules.

To integrate your logs, you first need to configure SIEM settings and an
AWS S3 Bucket according to the specific requirements provided by
BeyondTrust. You can then configure data collection in Cortex XSIAM by
configuring an Amazon S3 data collector for a generic log type using the
**Beyondtrust Cloud ECS** log format.

Before you begin configuring data collection verify that you are using
BeyondTrust Privilege Management Cloud version 21.6.339 or later.

Configure BeyondTrust Privilege Management Cloud collection in Cortex
XSIAM.

1.  Configure SIEM settings and an AWS S3 Bucket according to the
    requirements provided in the [BeyondTrust
    documentation](https://docs.beyondtrust.com/epm-wm/docs/configure-siem-settings).

- Ensure that when you add the AWS S3 bucket in the PMC and set the SIEM
  settings, you select **ECS - Elastic Common Schema** as the
  **SIEM Format**.

2.  Configure BeyondTrust logs collection with Cortex XSIAM using an
    [Amazon S3 data collector for generic
    data](/document/preview/1039055#UUID-d5ace3ce-5a93-86f4-55f2-4943fbf09cbe).

- Ensure your Amazon S3 data collector is configured with the following
  settings.

  - **Log Type**: Select **Generic** to configure your log collection to
    receive generic logs from Amazon S3.

  - **Log Format**: Select the log format type as
    **Beyondtrust Cloud ECS**.

  <!-- -->

  - > **Note**

    > For a **Log Format** set to **Beyondtrust Cloud ECS**, the
    > following fields are automatically set and not configurable.

    - > **Vendor**: **Beyondtrust**

    - > **Product**: **Privilege Management**

    - > **Compression**: **Uncompressed**

3.  After Cortex XSIAM begins receiving data from BeyondTrust Privilege
    Management Cloud, you can use XQL Search to search your logs using
    the `beyondtrust_privilege_management_raw` dataset that you
    configured when setting up your Amazon S3 data collector.

###### Ingest logs and data from Box

Cortex XSIAM can ingest different types of data from Box enterprise
accounts using the **Box** data collector. To receive logs and data from
Box enterprise accounts via the Box REST APIs, you must configure the
Data Sources settings in Cortex XSIAM based on your Box enterprise
account credentials. After you set up data collection, Cortex XSIAM
begins receiving new logs and data from the source.

When Cortex XSIAM begins receiving logs, the app creates a new dataset
for the different types of data that you are collecting, which you can
use to initiate XQL Search queries. For example queries, refer to the
in-app XQL Library. For all logs, Cortex XSIAM can generate Cortex XSIAM
issues (Analytics, Correlation Rules, IOC, and BIOC), when relevant,
from Box logs. While Correlation Rules issues are generated on
non-normalized and normalized logs, Analytics, IOC, and BIOC issues are
only generated on normalized logs.

The following table provides a brief description of the different types
of data you can collect, the collection method and fetch interval for
new data collected, the name of the dataset to use in Cortex XSIAM to
query the data using XQL Search, and whether the data is normalized.

> **Note**
>
> The Fetch Intervals are non-configurable.

+--------------------------------+----------------------------------------+------------+-----------+-------------------------+---------------------------+
| Type of data                   | Description                            | Collection | Fetch     | Dataset name            | Normalized data           |
|                                |                                        | method     | interval  |                         |                           |
+================================+========================================+============+===========+=========================+===========================+
| **Events and security alerts** |                                        |            |           |                         |                           |
+--------------------------------+----------------------------------------+------------+-----------+-------------------------+---------------------------+
| Events (admin_logs)            | Retrieves events related to            | Appends    | 60        | `box_admin_logs_raw`    | When relevant, Cortex     |
|                                | file/folder management, permission     | data       | seconds   |                         | XSIAM normalizes SaaS     |
|                                | changes, access and login activities,  |            |           |                         | audit event logs into     |
|                                | user/groups management, folder         |            |           |                         | stories, which are        |
|                                | collaboration, file/folder sharing,    |            |           |                         | collected in a dataset    |
|                                | security settings changes, tasks,      |            |           |                         | called `saas_audit_logs`. |
|                                | permission changes on folders, storage |            |           |                         |                           |
|                                | expiration and data retention, and     |            |           |                         |                           |
|                                | workflows.                             |            |           |                         |                           |
+--------------------------------+----------------------------------------+------------+-----------+-------------------------+---------------------------+
| Box Shield Alerts              | Retrieves security alerts related to   | Appends    | 60        | `box_shield_alerts_raw` | ---                       |
|                                | suspicious locations, suspicious       | data       | seconds   |                         |                           |
|                                | sessions, anomalous download, and      |            |           |                         |                           |
|                                | malicious content.                     |            |           |                         |                           |
|                                |                                        |            |           |                         |                           |
|                                | > **Note**                             |            |           |                         |                           |
|                                | >                                      |            |           |                         |                           |
|                                | > Collecting Box Shield Alerts         |            |           |                         |                           |
|                                | > requires implementing [Box           |            |           |                         |                           |
|                                | > Shield](https://www.box.com/shield), |            |           |                         |                           |
+--------------------------------+----------------------------------------+------------+-----------+-------------------------+---------------------------+
| **Directory and metadata**     |                                        |            |           |                         |                           |
+--------------------------------+----------------------------------------+------------+-----------+-------------------------+---------------------------+
| Users                          | Lists user data.                       | Overwrites | 10        | `box_users_raw`         | ---                       |
|                                |                                        | data       | minutes   |                         |                           |
+--------------------------------+----------------------------------------+------------+-----------+-------------------------+---------------------------+
| Groups                         | Lists user group data.                 | Overwrites | 10        | `box_groups_raw`        | ---                       |
|                                |                                        | data       | minutes   |                         |                           |
+--------------------------------+----------------------------------------+------------+-----------+-------------------------+---------------------------+

> **Prerequisite**

1.  > Set up an [Enterprise](https://www.box.com/pricing) Box plan.

- > **Important**

  > To collect Box Shield Alerts, you must purchase [Box
  > Shield](https://www.box.com/shield) and it must be enabled on Box
  > enterprise.

2.  > Create a valid Box account that is assigned to a role with
    > sufficient permissions for the data you want to collect. For
    > example, create an account assigned to an Admin role to enable
    > Cortex XSIAM to collect all metadata for all files, folders, and
    > enterprise events for the entire organization.

3.  > Enable two-factor authentication for the Box account. For more
    > information, see the [Box
    > documentation](https://support.box.com/hc/en-us/articles/360043697154-Two-Factor-Authentication-Set-Up-for-Your-Account).

Configure Cortex XSIAM to receive logs and data from Box.

1.  Complete the prerequisites mentioned above for your Box enterprise
    account.

2.  Create a new app in your Box account.

    a.  Log in to your Box account, and in the [Dev
        Console](https://account.box.com/login?redirect_url=%2Fdevelopers%2Fconsole),
        click **Create New App**.

    b.  Select **Custom App**.

    c.  Set these settings in the **Custom App** dialog:

        - Select **Server Authentication (Client Credentials Grant)**.

        - Specify an **App Name**.

        - Click **Create App**.

    - The new app is created and the opened in the **Configuration**
      tab.

    d.  In the **Configuration** tab of the new app, scroll down to the
        following sections and configure the app.

        - In the **App Access Level** section, select
          **App + Enterprise Access**.

        - In the **Application Scopes** section, set the following
          **Administrative Action** permissions depending on the type of
          data you want to collect.

+-----------------------------------+-----------------------------------+
| Administrative action             | Data type                         |
+===================================+===================================+
| **Manage users**                  | Users                             |
+-----------------------------------+-----------------------------------+
| **Manage groups**                 | Groups                            |
|                                   |                                   |
|                                   | > **Note**                        |
|                                   | >                                 |
|                                   | > There is a current bug with the |
|                                   | > Groups API from Box. If you     |
|                                   | > don\'t configure the Box app    |
|                                   | > with the proper permissions for |
|                                   | > managing groups data, the       |
|                                   | > Groups API from Box won\'t      |
|                                   | > return an error message to      |
|                                   | > Cortex XSIAM indicating that    |
|                                   | > the API failed to receive the   |
|                                   | > data, and the Groups data will  |
|                                   | > not be collected.               |
+-----------------------------------+-----------------------------------+
| **Manage enterprise properties**  | - Events (admin_logs)             |
|                                   |                                   |
|                                   | - Box Shield Alerts               |
+-----------------------------------+-----------------------------------+

- Once completed, scroll up in the tab to **Save Changes**.

a.  In the **Authorization** tab, click **Review and Submit** to send
    your changes to the administrator for approval.

- In the **Review App Authorization Submission** dialog that is
  displayed, you can add a **Description** of the app changes, and then
  click **Submit**.

3.  Ensure the new app changes are approved by an administrator in the
    **Admin Console** of the Box account.

    a.  Select Apps \> Customer Apps Manager \> Server Authentication
        Apps.

    b.  In the table, look for the **Name** of the Box app with the
        changes, where the **Authorization Status** is set to
        **Pending Authorization**, and select the options menu \>
        Authorize App.

    c.  Click **Authorize**.

- > **Note**

  > For any future change that you make to your Box app, ensure that you
  > send the changes for approval to the administrator, who will need to
  > approve them as explained above.

4.  In Cortex XSIAM, select Settings \> Data Sources.

5.  On the **Data Sources** page, click **Add Data Source**, search for
    and select **Box**, and click **Connect**.

6.  Set the following parameters, where some values require you to log
    in to your Box account to copy and paste the values to the
    applicable fields:

    - **Name**: Specify a descriptive name for this Box instance.

    - **Enterprise ID**: Specify the unique identifier for your
      organization\'s Box instance, which is used to access the token
      request. This field can\'t be edited once the Box data collector
      instance is created.

    <!-- -->

    - You can retrieve this value from your Box account in the the
      **General Settings** tab, and scrolling to the **App Info**
      section. Copy the **Enterprise ID** and paste it in this field in
      Cortex XSIAM.

    <!-- -->

    - **Client ID**: Specify the client ID or API key for the Box app
      you created.

    <!-- -->

    - You can retrieve this value from your Box account in the
      **Configuration** tab, and scrolling down to the
      **OAuth 2.0 Credentials** section. **COPY** the **Client ID** and
      paste it into this field in Cortex XSIAM.

    <!-- -->

    - **Client Secret**: The client secret or API secret fort he Box app
      you created.

    <!-- -->

    - You can retrieve this value from your Box account in the
      **Configuration** tab, and scrolling down to the
      **OAuth 2.0 Credentials** section. Click **Fetch Client Secret**,
      where you will need to authenticate yourself according to the
      [two-factor authentication
      method](#Xc07c5672aafe3a741379d0407220ff8dd8bf743) defined in your
      Box app before the **Client Secret** is displayed. Copy this value
      and paste it in this field in Cortex XSIAM.

    <!-- -->

    - **Collect**: Select the types of data you want to collect from
      Box. All the options are selected by default.

      - Events and security alerts

        - **Events (admin_logs)**: Collects events related to
          file/folder management, permission changes, access and login
          activities, user/groups management, folder collaboration,
          file/folder sharing, security settings changes, tasks,
          permission changes on folders, storage expiration and data
          retention, and workflows.

        - **Box Shield Alerts**: Collects security alerts related to
          suspicious locations, suspicious sessions, anomalous download,
          and malicious content.

      - Directory and metadata

      <!-- -->

      - > **Note**

        > Inventory data snapshots are collected every 10 minutes.

        - **Users**: Collects user data.

        - **Groups**: Collects user group data.

7.  To test the connection settings, click **Test**.

8.  If the test is successful, click **Enable** to enable Box log
    collection.

- When events start to come in, a green check mark appears underneath
  the Box configuration.

###### Ingest logs and data from Dropbox

Cortex XSIAM can ingest different types of data from Dropbox Business
accounts using the Dropbox data collector. To receive logs and data from
Dropbox Business accounts via the Dropbox Business API, you must
configure the Data Sources settings in Cortex XSIAM based on your
Dropbox Business Account credentials. After you set up data collection,
Cortex XSIAM begins receiving new logs and data from the source.

When Cortex XSIAM begins receiving logs, the app creates a new dataset
for the different types of data that you are collecting, which you can
use to initiate XQL Search queries. For example queries, refer to the
in-app XQL Library. For all logs, Cortex XSIAM can generate Cortex XSIAM
issues (Analytics, Correlation Rules, IOC, and BIOC), when relevant,
from Dropbox Business logs. While Correlation Rules issues are generated
on non-normalized and normalized logs, Analytics, IOC, and BIOC issues
are only generated on normalized logs.

The following table provides a brief description of the different types
of data you can collect, the collection method and fetch interval for
new data collected, the name of the dataset to use in Cortex XSIAM to
query the data using XQL Search, and whether the data is normalized.

> **Note**
>
> The Fetch Interval is non-configurable.

+----------------------------+------------------------------------------------------------------------------------------------------------------------------------+------------+-----------+-------------------------------+---------------------------+
| Type of data               | Description                                                                                                                        | Collection | Fetch     | Dataset name                  | Normalized data           |
|                            |                                                                                                                                    | method     | interval  |                               |                           |
+============================+====================================================================================================================================+============+===========+===============================+===========================+
| **Log collection**         |                                                                                                                                    |            |           |                               |                           |
+----------------------------+------------------------------------------------------------------------------------------------------------------------------------+------------+-----------+-------------------------------+---------------------------+
| Events                     | Retrieves team events, including access events, administrative events, file/folders events, security settings events, and more.    | Appends    | 60        | `dropbox_events_raw`          | When relevant, Cortex     |
|                            |                                                                                                                                    | data       | seconds   |                               | XSIAM normalizes SaaS     |
|                            | [team_log/get_events](https://www.dropbox.com/developers/documentation/http/teams#team_log-get_events)                             |            |           |                               | audit event logs into     |
|                            |                                                                                                                                    |            |           |                               | stories, which are        |
|                            |                                                                                                                                    |            |           |                               | collected in a dataset    |
|                            |                                                                                                                                    |            |           |                               | called `saas_audit_logs`. |
+----------------------------+------------------------------------------------------------------------------------------------------------------------------------+------------+-----------+-------------------------------+---------------------------+
| **Directory and metadata** |                                                                                                                                    |            |           |                               |                           |
+----------------------------+------------------------------------------------------------------------------------------------------------------------------------+------------+-----------+-------------------------------+---------------------------+
| Member Devices             | Lists all device sessions of a team.                                                                                               | Overwrites | 10        | `dropbox_members_devices_raw` | ---                       |
|                            |                                                                                                                                    | data       | minutes   |                               |                           |
|                            | [team/devices/list_members_devices](https://www.dropbox.com/developers/documentation/http/teams#team-devices-list_members_devices) |            |           |                               |                           |
+----------------------------+------------------------------------------------------------------------------------------------------------------------------------+------------+-----------+-------------------------------+---------------------------+
| Users                      | Lists members of a group.                                                                                                          | Overwrites | 10        | `dropbox_users_raw`           | ---                       |
|                            |                                                                                                                                    | data       | minutes   |                               |                           |
|                            | [team/members/list_v2](https://www.dropbox.com/developers/documentation/http/teams#team-members-list)                              |            |           |                               |                           |
+----------------------------+------------------------------------------------------------------------------------------------------------------------------------+------------+-----------+-------------------------------+---------------------------+
| Groups                     | Lists groups on a team.                                                                                                            | Overwrites | 10        | `dropbox_groups_raw`          | ---                       |
|                            |                                                                                                                                    | data       | minutes   |                               |                           |
|                            | [team/groups/list](https://www.dropbox.com/developers/documentation/http/teams#team-groups-list)                                   |            |           |                               |                           |
+----------------------------+------------------------------------------------------------------------------------------------------------------------------------+------------+-----------+-------------------------------+---------------------------+

> **Prerequisite**

1.  > Set up an [Advanced](https://www.dropbox.com/plans) Dropbox plan.

2.  > Create a Dropbox Business [admin
    > account](https://help.dropbox.com/account-access) with
    > **Security admin** permissions, which is required to authorize
    > Cortex XSIAM to access the Dropbox Business account and generate
    > the OAuth 2.0 access token.

Configure Cortex XSIAM to receive logs and data from Dropbox.

1.  Complete the prerequisite steps mentioned above for your Dropbox
    Business account.

2.  Log in to Dropbox using an admin account designated with
    **Security admin** level permissions.

3.  In the Dropbox **App console**, ensure that you either create a new
    app, or your existing app is created, with the following settings:

    - **Choose an API**: Select **Scoped access**.

    - **Choose the type of access you need**: Select **Full dropbox**
      for access to all files and folders in a user\'s Dropbox.

4.  In the **Permissions** tab of your app, ensure that the applicable
    permissions are selected under the relevant section heading for the
    type of data you want to collect:

  -----------------------------------------------------------------------
  Section heading         Permission              Data to collect
  ----------------------- ----------------------- -----------------------
  **Account Info**        **account_info.read**   All types of data

  **Team Data**           **team_data.member**    All types of data

  **Members**             **members.read**        Users

  **groups.read**         Groups                  

  **Sessions**            **sessions.list**       Member Devices

  **events.read**         Events                  
  -----------------------------------------------------------------------

5.  In the **Settings** tab of your app, copy the **App key** and
    **App secret** , where you must click **Show** to see the App secret
    and record them somewhere safe. You will need to provide these keys
    when you configure the Dropbox data collector in Cortex XSIAM.

6.  In Cortex XSIAM, select Settings \> Data Sources.

7.  On the **Data Sources** page, click **Add Data Source**, search for
    and select **Dropbox** and click **Connect**.

8.  Set the following parameters:

    - **Name**: Specify a descriptive name for this Dropbox instance.

    - **App Key**: Specify the **App key**, which is taken from the
      [Settings tab](#X79f9bf5718b1bfbc52ab3ac1bc931ea7cda85f6) of your
      Dropbox app.

    - **App Secret**: Specify the **App secret**, which is taken from
      the [Settings tab](#X79f9bf5718b1bfbc52ab3ac1bc931ea7cda85f6) of
      your Dropbox app.

    - **Access Code**: After specifying an **App Key**, you can obtain
      the access code by hovering over the **Access Code** tooltip,
      clicking the **here** link, and signing in with your Dropbox
      Business account credentials. The URL link is
      `https://www.dropbox.com/oauth2/authorize?client_id=%APP_KEY%&amp;token_access_type=offline&amp;response_type=code`,
      where the `%APP_KEY%` is replaced with the **App Key** value
      specified.

    <!-- -->

    - > **Note**

      > When the **App Key** field is empty, the **here** link in the
      > tooltip is disabled. When an incorrect **App Key** is entered,
      > clicking the link results in a 404 error.

      To obtain the Access Code complete the following steps in the page
      that opens in your browser:

      1.  Read the disclaimer and click **Continue**.

      2.  Review the permissions listed, which should match the
          permissions you configured in your Dropbox app in the
          [Permissions tab](#Xafda62f1b4839c3820d6b12165f12056ad510c2)
          according to the type of data you want to collect, and click
          **Allow**.

      3.  Copy the **Access Code Generated** and paste it in the
          **Access Code** field in Cortex XSIAM. The access code is
          valid for around four minutes from when it is generated.

      > **Note**

      > Whenever you change the permissions of the Dropbox app, we
      > recommend that you generate a new **Access Code** for the
      > **Dropbox** data collector instance so that the permissions
      > match the updates.

    <!-- -->

    - **Collect**: Select the types of data you want to collect from
      Dropbox. All the options are selected by default.

      - Log collection

        - **Events (get_events}**: Retrieves team events, including
          access events, administrative events, file/folders events,
          security settings events and more.

      <!-- -->

      - > **Note**

        > Event data is collected every 60 seconds with a 10 minute lag
        > time.

      <!-- -->

      - Directory and metadata

        - **Member Devices**: Collects all device sessions of a team.

        - **Users**: Collects all members of a group.

        - **Groups**: Collects all groups on a team.

      <!-- -->

      - > **Note**

        > Inventory data snapshots are collected every 10 minutes.

9.  To test the connection settings, click **Test**.

10. If the test is successful, click **Enable** to enable Dropbox log
    collection.

- After events start to come in, a green check mark appears underneath
  the Dropbox configuration.

###### Ingest logs from Elasticsearch Filebeat

If you want to ingest logs about file activity on your endpoints and
servers and do not use the Cortex XDR agent, you can install
Elasticsearch Filebeat as a system logger and then forward those logs to
Cortex XSIAM. To facilitate log ingestion, Cortex XSIAM supports the
same protocols that Filebeat and Elasticsearch use to communicate.
Cortex XSIAM supports using Filebeat up to version 8.2 with the Filebeat
data collector. Cortex XSIAM also supports logs in single line format or
multiline format. For more information on handling messages that span
multiple lines of text in Elasticsearch Filebeat, see [Manage Multiline
Messages](https://www.elastic.co/guide/en/beats/filebeat/current/multiline-examples.html).

Cortex XSIAM supports all sections in the `filebeat.yml` configuration
file, such as support for Filebeat fields and tags. As a result, this
enables you to use the
[add_fields](https://www.elastic.co/guide/en/beats/filebeat/current/add-fields.html)
processor to identify the product/vendor for the data collected by
Filebeat so the collected events go through the ingestion flow (Parsing
Rules). To configure the product/vendor ensure that you use the default
`fields` attribute, as opposed to the `target` attribute, as shown in
the following example.

    processors:
      - add_fields:
          fields:
            vendor: <Vendor>
            product: <Product>

To provide additional context during investigations, Cortex XSIAM
automatically creates a new Cortex Query Language (XQL) dataset from
your Filebeat logs. You can then use the XQL dataset to search across
the logs Cortex XSIAM received from Filebeat.

To receive logs, you configure collection settings for Filebeat in
Cortex XSIAM and output settings in your Filebeat installations. As soon
as Cortex XSIAM begins receiving logs, the data is visible in XQL Search
queries.

1.  In Cortex XSIAM, set up Data Collection.

    a.  Select Settings \> Data Sources.

    b.  On the **Data Sources** page, click **Add Data Source**, search
        for and select **Filebeat**, and click **Connect**.

    c.  Specify a descriptive **Name** for your Filebeat log collection
        configuration.

    d.  Specify the **Vendor** and **Product** for the type of logs you
        are ingesting.

    - The vendor and product are used to define the name of your XQL
      dataset (`<vendor>_<product>_raw`). If you do not define a vendor
      or product, Cortex XSIAM examines the log header to identify the
      type and uses that to define the vendor and product in the
      dataset. For example, if the type is Acme and you opt to let
      Cortex XSIAM determine the values, the dataset name would be
      `acme_acme_raw`.

    e.  **Save & Generate Token**.

    - Click the copy icon next to the key and record it somewhere safe.
      You will need to provide this key when you set up output settings
      on your Filebeat instance. If you forget to record the key and
      close the window you will need to generate a new key and repeat
      this process.

2.  Set up Filebeat to forward logs.

- After installing the Filebeat agent, configure an Elasticsearch
  output:

  a.  Under the **output.elasticsearch** section, configure the
      following entities:

  - ![](media/rId5821.png){width="5.833333333333333in"
    height="1.5968744531933508in"}

    - `hosts`: Copy the API URL from your Filebeat configuration and
      paste it in this field.

    - `compression_level`: 5 (recommended)

    - `bulk_max_size`: 1000 (recommended)

    - `api_key`: Paste the key you created in when you configured
      Filebeat Log Collection in Cortex XSIAM.

    - `proxy_url`: (*Optional*) `<server_ip>:<port_number>`. You can
      specify your own `<server_ip>` or use the Broker VM to proxy
      Filebeat communication using the format
      `<Broker_VM_ip>:<port_number>`. When using the Broker VM, ensure
      that you activate the Local Agent Settings applet with the
      **Agent Proxy** enabled.

  b.  Save the changes to your output file.

  After Cortex XSIAM begins receiving logs from Filebeat, they will be
  available in XQL Search queries.

3.  (*Optional*) Monitor your Filebeat integration.

- You can return to the Settings \> Configurations \> Data Collection \>
  Data Sources page to monitor the status of your Filebeat
  configuration. For each instance, Cortex XSIAM displays the number of
  logs received in the last hour, day, and week. You can also use the
  Data Ingestion Dashboard to view general statistics about your data
  ingestion configurations.

4.  (*Optional*) Set up issue notifications to monitor the following
    events.

    - A Filebeat agent status changes to disconnected.

    - A Filebeat module has stopped sending logs.

###### Ingest logs from Forcepoint DLP

If you use Forcepoint DLP to prevent data loss over endpoint channels,
you can take advantage of Cortex XSIAM investigation and detection
capabilities by forwarding your logs to Cortex XSIAM. This enables
Cortex XSIAM to help you expand visibility into data violation by users
and hosts in the organization, correlate and detect DLP incidents, and
query Forcepoint DLP logs using XQL Search.

When Cortex XSIAM starts to receive logs, Cortex XSIAM can analyze your
logs in XQL Search and you can create new Correlation Rules.

To integrate your logs, you first need to set up an applet in a Broker
VM within your network to act as a Syslog Collector. You then configure
forwarding on your log devices to send logs to the Syslog Collector in a
CEF or LEEF format.

Configure Forcepoint DLP collection in Cortex XSIAM.

1.  Verify that your Forcepoint DLP meet the following requirements.

    - Must use version 8.8.0.347 or a later release.

    - On premise installation only.

2.  [Activate the Syslog
    Collector](#UUIDd7d85d4a97df7ec2a052d22376f5a52e) applet on a Broker
    VM in your network.

- Ensure the Broker VM is configured with the following settings.

  - **Format**: Select either a **CEF** or **LEF** Syslog format.

  - **Vendor**: Specify the **Vendor** as `forcepoint`.

  - **Product**: Specify the **Product** as `dlp_endpoint`.

3.  Increase log storage for Forcepoint DLP logs.

- As an estimate for initial sizing, note the average Forcepoint DLP log
  size. For proper sizing calculations, test the log sizes and log rates
  produced by your Forcepoint DLP. For more information, see Manage Your
  Log Storage.

4.  Configure the log device that receives Forcepoint DLP logs to
    forward syslog events to the Syslog Collector in a CEF or LEEF
    format.

- For more information, see the [Forcepoint DLP
  documentation](https://www.websense.com/content/support/library/web/v85/siem/siem.pdf).

5.  After Cortex XSIAM begins receiving data from Forcepoint DLP, you
    can use XQL Search to search your logs using the
    `forcepoint_dlp_endpoint` dataset.

###### Ingest logs from Proofpoint Targeted Attack Protection

To receive logs from Proofpoint Targeted Attack Protection (TAP), you
must first configure TAP service credentials in the TAP dashboard, and
then the Collection Integrations settings in Cortex XSIAM based on your
Proofpoint TAP configuration. After you set up data collection, Cortex
XSIAM begins receiving new logs and data from the source.

When Cortex XSIAM begins receiving logs, the app creates a new dataset
(`proofpoint_tap_raw`) that you can use to initiate XQL Search queries.
For example queries, refer to the in-app XQL Library.

Configure the Proofpoint TAP collection in Cortex XSIAM.

1.  Generate TAP Service Credentials in Proofpoint TAP.

- TAP service credentials can be generated in the TAP Dashboard, where
  you will receive a Proofpoint Service Principal for authentication and
  Proofpoint API Secret for authentication. Record these credentials as
  you will need to provide them when configuring the
  **Proofpoint Targeted Attack Protection** data collector in Cortex
  XSIAM. For more information on generating TAP service credentials, see
  [Generate TAP Service
  Credentials](https://ptr-docs.proofpoint.com/ptr-guides/integrations-files/ptr-tap/).

2.  Configure the Proofpoint TAP collection in Cortex XSIAM.

    a.  Select Settings \> Data Sources.

    b.  On the **Data Sources** page, click **Add Data Source**, search
        for and select **Proofpoint Targeted Attack Protection**, and
        click **Connect**.

    c.  Set these parameters:

        - **Name**: Specify a descriptive name for your log collection
          configuration.

        - **Proofpoint Endpoint**: All Proofpoint endpoints are
          available on the `tap-api-v2.proofpoint.com` host. You can
          leave the default configuration or specify another host.

        - **Service Principal**: Specify the Proofpoint Service
          Principal for authentication. TAP service credentials can be
          generated in the TAP Dashboard.

        - **API Secret**: Specify the Proofpoint API Secret for
          authentication. TAP service credentials can be generated in
          the TAP Dashboard.

    d.  Click **Test** to validate access, and then click **Enable**.

    - Once events start to come in, a green check mark appears
      underneath the **Proofpoint Targeted Attack Protection**
      configuration with the amount of data received.

3.  (*Optional*) Manage your **Proofpoint Targeted Attack Protection**
    data collector.

- After you enable the **Proofpoint Targeted Attack Protection** data
  collector, you can make additional changes as needed.

  You can perform any of the following:

  - **Edit** the **Proofpoint Targeted Attack Protection** data
    collector settings.

  - **Disable** the **Proofpoint Targeted Attack Protection** data
    collector.

  - **Delete** the **Proofpoint Targeted Attack Protection** data
    collector.

###### Ingest logs and data from Salesforce.com

The Cortex XSIAM data collector can collect Audit Trail and Security
Monitoring event logs from Salesforce.com. During setup of this data
collector, you can choose to accept the default collection settings, or
exclude the collection of content metadata and accounts.

The Salesforce.com data collector fetches events, and objects and
metadata, including:

- Login history

- Setup audit trail

- Flow Execution events

- Transaction Security events

- Content Distribution events

- Package Install events

You can create multiple Salesforce.com data collector instances in
Cortex XSIAM, for different parts of your organization.

Logs are collected from Salesforce.com every 30 seconds. When Cortex
XSIAM begins receiving logs, it creates new datasets for them, called
`salesforce_<object>_raw`.  Examples of `<object>` include:

- connectedapplication

- permissionset

- profile

- groupmember

- group

- user

- userrole

- document

- contentfolder

- attachment

- contentdistribution

- tenantsecuritylogin

- useraccountteammember

- tenantsecurityuserperm

- account

- audit

- login

- eventlogfile

You can use these datasets to perform XQL search queries. For example
queries, refer to the in-app XQL Library.

> **Prerequisite**

- > Cortex XSIAM:

  - > To manage collection integration in Cortex XSIAM, ensure that you
    > have the privilege to View/Edit Log Collections (for example,
    > Instance Administrator).

- > Salesforce.com:

  - > The minimum required Salesforce.com editions are Professional
    > Edition with API access enabled, or Enterprise Edition, or higher.

  - > To use the client credentials flow required for
    > Salesforce.com--Cortex XSIAM integration, you must create a
    > connected app for Cortex XSIAM in Salesforce.com, and configure
    > its OAuth settings and access policies, as described in this
    > procedure. The connected app must be created by a Full System
    > Admin.

  - > Ensure that your organization has a Salesforce Shield license. 
    > For more information, refer to:

  <!-- -->

  - > <https://help.salesforce.com/s/articleView?id=xcloud.salesforce_shield.htm&amp;type=5>

    > <https://trailhead.salesforce.com/content/learn/modules/event_monitoring/event_monitoring_intro>

    > **Note**

    > Ensure that you have the required licenses. If these prerequisites
    > are not met, fetching of security data and event data will be
    > severely limited, and errors will be generated.

  <!-- -->

  - > In **Setup**, **Event Monitoring Settings**, ensure that
    > **Generate event log files** is enabled.

    - > In **Setup**, verify that there are event log files in the
      > **Event Log File Browser**.

    - > In **Setup**, **Permission Sets**, verify that there is a
      > permission set called **Event Monitoring**.

> **Note**
>
> For more detailed reference information, see [Configure a Connected
> App for the OAuth 2.0 Client Credentials
> Flow](https://help.salesforce.com/s/articleView?id=sf.connected_app_client_credentials_setup.htm&type=5).
>
> Unlike other data collector setups, in this case, the setup includes
> obtaining an OAuth 2.0 code from Salesforce.com, and this code is only
> valid for 15 minutes. Therefore, make sure that you enable the data
> collector within 15 minutes of obtaining the authorization code.

Perform the following procedures in the order that they appear, below.

####### Task 1. Configure Salesforce Connected App

1.  On the **Setup** page, in **Quick Find**, type `App Manager`.

2.  Click **New Connected App**.

3.  Enter a meaningful name for the connected application and for the
    API. For example, you could name it panw_cortex_integration.

4.  Enter your email address. This address will be used to retrieve the
    Consumer Key and Consumer Secret.

5.  Select the **Enable OAuth Settings** checkbox.

6.  In **Callback URL**, type

- `https://login.salesforce.com/services/oauth2/callback`

  and

  `https://{tenant external URL}.paloaltonetworks.com/configuration/data-sources`

  on separate lines, where `{tenant external URL}` is the name of your
  tenant as it appears in the URL of your Cortex XSIAM tenant.

7.  For **OAuth Scopes**, select **Full access (full)** and
    **Perform requests at any time (refresh_token, offline_access)**.

8.  In the next options after **OAuth Scopes**, ensure that only the
    following checkboxes are selected:

    - **Require Secret for Web Server Flow**

    - **Require Secret for Refresh Token Flow**

    - **Enable Credentials Flow**

9.  Click **Save**, and then **Continue**.

####### Task 2. Retrieve the Consumer Key and Consumer Secret

Consumer Key will be used for client_id, and Consumer Secret will be
used for client_secret in OAuth 2.0.

1.  On the **Setup** page, in **Quick Find**, type `App Manager`.

2.  Find your connected application (the one that you defined for Cortex
    XSIAM). In the last column, click the arrow button and then click
    **View**.

3.  In the **API (Enable OAuth Settings)** area, click
    **Manage Consumer Details**.

4.  When you are asked to verify your identity, open the email that
    Salesforce sent to you, and copy the verification code. Go back to
    the Salesforce **Verify Your Identity** page, paste the code in the
    **Verification Code** box, and click **Verify**. One of the
    following will happen:

    - The Consumer Key and Consumer Secret will be sent to the email
      address that you configured earlier for the Cortex XSIAM connected
      app.

    - On the Salesforce **Connected App Name** page, the
      **Consumer Details** area will display the **Consumer Key** and
      **Consumer Secret**, and you will be able to copy them from here
      when required in the following procedures.

####### Task 3. Configure the Refresh Token expiration policy

1.  On the **Setup** page, in **Quick Find**, type `App Manager`.

2.  Find your connected application (the one that you defined for Cortex
    XSIAM). In the last column, click the arrow button and then click
    **Manage**.

3.  Click **Edit Policies**.

4.  In the **OAuth Policies** area:

    - Under **Permitted Users**, select
      **All users may self-authorize**.

    - Choose your refresh token policy. We recommend:
      **Expire refresh token if not used for \_ Day(s)**. For example,
      select this option and set it for 7 days.

####### Task 4. Configure OAuth 2.0

- Configure the OAuth 2.0 application to call the Salesforce.com API
  using client_id and client_secret.

<!-- -->

- References:
  <https://help.salesforce.com/s/articleView?id=sf.remoteaccess_oauth_client_credentials_flow>

####### Task 5. Configure Cortex XSIAM

1.  In Cortex XSIAM, create a Salesforce.com data collector instance:

    - Select Settings \> Data Sources.

    - On the **Data Sources** page, click **Add Data Source**, search
      for and select **Salesforce.com**, and click **Connect**.

2.  Enter a unique **Name** for the instance, enter the
    **Salesforce Domain Name**, and the **Consumer Key** and the
    **Consumer Secret** credentials obtained earlier in this workflow.
    For example, the domain could be the API URL from which logs are
    received, such as
    `https://MyDomainName.my.salesforce.com/services/data/vXX.X/resource/`

3.  (Optional) Clear options that you do not require:

    - **Content metadata**: when selected (default), collects documents'
      metadata. 

    - **Accounts**: when selected (default), collects account objects.

- > **Note**

  > When these options are cleared, only these data types will be
  > omitted from collection. All other data will be collected as usual.

4.  Click **Enable**. A popup which redirects you to your Salesforce
    instance appears, to get OAuth 2.0 authorization credentials and
    access.

5.  Click **OK**.

- In Salesforce.com, a new tab appears.

6.  Enter your **username** and **password**, and **Log In**. 

7.  When you are asked to allow access, select **Allow**.

- A Salesforce data collection instance is created, and an authorization
  token is created and returned to Cortex XSIAM. Data collection begins.

####### Task 6. (Optional) Edit or test existing Salesforce.com collector settings

You can edit and test an existing collector instance after a successful
initial connection between Salesforce.com and Cortex XSIAM. Do this by
clicking **Edit** (pencil icon) for the collector instance. The log
collection window will be displayed, where you can make changes or test,
by clicking **Test**.

####### Troubleshooting

If for any reason, the token is not created and sent to Cortex XSIAM,
after a timeout period, an authorization failure error will be returned
for the collector instance. In this case, try again by clicking **Edit**
(pencil icon) for the collector instance. The log collection window will
be displayed again, where you can edit settings and retry getting the
authorization code.

###### Ingest data from ServiceNow CMDB

To receive data from the ServiceNow CMDB database, you must first
configure data collection from ServiceNow CMDB. ServiceNow CMDB is a
logical representations of assets, services, and the relationships
between them that comprise the infrastructure of an organization. It is
built as a series of connected tables that contain all the assets and
business services controlled by a company and its configurations. You
can configure the Collection Integration settings in Cortex XSIAM for
the ServiceNow CMDB database, which includes selecting the specific
tables containing the data that you want to collect, in the ServiceNow
CMDB Collector. You can select from the list of default tables and also
specify custom tables. By default, the ServiceNow CMDB Collector is
configured to collect data from the following tables, which you can
always change depending on your system requirements.

- `cmdb_ci`

- `cmdb_ci_computer`

- `cmdb_rel_ci`

- `cmdb_ci_application_software`

When Cortex XSIAM begins receiving data, the app automatically creates a
ServiceNow CMDB dataset for each table using the format
`servicenow_cmdb_<table name>_raw`. You can then use XQL Search queries
to view the data and create new Correlation Rules.

You can only configure a single ServiceNow CMDB Collector, which is
automatically configured every 6 hours, to reload the data from the
configured tables and replace the existing data. You can always use the
**Sync Now** option to reload the data and replace the existing data
whenever you want.

Complete the following task before you begin configuring Cortex XSIAM to
receive data from ServiceNow CMDB.

- Create a ServiceNow CMDB user with SNOW credentials, who is designated
  to access the tables from ServiceNow CMDB for data collection in
  Cortex XSIAM. Record the credentials for this user as you will need
  them when configuring the ServiceNow CMDB Collector in Cortex XSIAM.

Configure Cortex XSIAM to receive data from ServiceNow CMDB:

1.  Select Settings \> Data Sources.

2.  On the **Data Sources** page, click **Add Data Source**, search for
    and select **ServiceNow CMDB**, and click **Connect**.

3.  Set the following parameters.

    - **Domain**: Specify your ServiceNow CMDB domain URL.

    - **User Name**: Specify the username for your ServiceNow CMDB user
      designated in Cortex XSIAM.

    - **Password**: Specify the password for your ServiceNow CMDB user
      designated in Cortex XSIAM.

    - **Tables**: You can do any of the following actions to configure
      the tables whose data is collected from ServiceNow CMDB.

      - Select the tables from the list of default ServiceNow CMDB
        tables that you want to collect from. After each table
        selection, select
        ![](media/rId2150.png){width="0.14583333333333334in"
        height="0.20833333333333334in"} to add the table to the tables
        already listed below for data collection.

      - Specify any custom tables that you want to configure for data
        collection.

      - From the default list of tables already configured, you can
        delete any of them by hovering over the table and selecting the
        **X** icon.

4.  Click **Test** to validate access, and then click **Enable**.

- After events start to come in, a green check mark appears underneath
  the **ServiceNow CMDB** Collector configuration with the data and time
  that the data was last synced.

5.  (*Optional*) Manage your ServiceNow CMDB Collector.

- After you enable the ServiceNow CMDB Collector, you can make
  additional changes as needed. To modify a configuration, select any of
  the following options:

  - **Edit** the ServiceNow CMDB Collector settings.

  - **Disable** the ServiceNow CMDB Collector.

  - **Delete** the ServiceNow CMDB Collector.

  - **Sync Now** to get the latest data from the tables configured. The
    data is replaced automatically every 6 hours, but you can always get
    the latest data as needed.

6.  After Cortex XSIAM begins receiving data from ServiceNow CMDB, you
    can use the XQL Search to search for logs in the new datasets, where
    each dataset name is based on the table name using the format
    `servicenow_cmdb_<table name>_raw`.

###### Ingest report data from Workday

To receive Workday report data, you must first configure data collection
from Workday using a Workday custom report to ingest the appropriate
data. This is configured by setting up a Workday Collector in Cortex
XSIAM and configuring report data collection via this Workday custom
report that you set up.

As soon as Cortex XSIAM begins receiving data, the app automatically
creates a Workday Cortex Query Language (XQL) dataset
(`workday_workday_raw`). You can then use XQL Search queries to view the
data and create new Correlation Rules. In addition, Cortex XSIAM adds
the Workday fields next to each user in the Key Assets list on the
**Cases** page, and in the User node in the Causality View of Identity
Analytics issues.

> **Note**
>
> Any user with permissions to view issues and cases can view the
> Workday data.

You can only configure a single Workday Collector, which is
automatically configured to run the report every 6 hours. You can always
use the **Sync Now** option to run the report whenever you want.

> **Prerequisite**

1.  > Create an Integration System User that is designated to access the
    > custom report from Workday for data collection in Cortex XSIAM.

2.  > Create an Integration System Security Group for the Integration
    > System User created in Step 1 for accessing the report. When
    > setting this group ensure to define the following:

    - > **Type of Tenanted Security Group**: Select either
      > **Integration System Security Group (Constrained)** or
      > **Integration System Security Group (Unconstrained)** depending
      > on how your data is configured. For more information, see the
      > Workday documentation.

    - > **Integration System User**: Select the user that you defined in
      > step 1 for accessing the custom report.

3.  > Create the Workday credentials for the Integration System User
    > created in Step 1 so that the username and password can be used to
    > access the report in Cortex XSIAM. Record these credentials as you
    > will need them when configuring the Workday Collector in Cortex
    > XSIAM.

> **Note**
>
> For more information on completing any of the prerequisite steps, see
> the Workday documentation.

Configure Cortex XSIAM to receive report data from Workday:

1.  Configure a Workday custom report to use for data collection.

    a.  Login to the [Workday Resource
        Center](https://signin.resourcecenter.workday.com/).

    b.  In the search field, specify **Create Custom Report** to open
        the wizard.

    c.  Configure the following **Create Custom Report** settings:

    - ![](media/rId5845.png){width="5.833333333333333in"
      height="4.5980391513560805in"}

      - **Report Name**: Specify the name of the report.

      - **Report Details** section:

        - **Report Type**: Select **Advanced**. When you select this
          option, the **Enable As Web Service** checkbox is displayed.

        - **Enable As Web Service**: Select this checkbox, so that you
          will be able to generate a URL of the report to configure in
          Cortex XSIAM.

      - **Data Source** section:

        - **Optimized for Performance**: Select whether the data should
          be optimized for performance. The way this checkbox is
          configured determines the **Data Source** options available to
          choose from.

        - **Date Source**: Select the applicable data source containing
          the data that is used to configure data collection from
          Workday to Cortex XSIAM.

    d.  Click **OK**, and configure the following **Additional Info**
        settings.

    - The **Additional Info** table in the **Columns** tab is where you
      can perform the following.

      - For the incident and card views in Cortex XSIAM, map the
        required fields from the **Data Source** configured by selecting
        the applicable **Field** that you want to map to the Cortex
        XSIAM field name required for data collection in the
        **Column Heading Override XML Alias** column.

      - (*Optional*) You can map any additional fields from the
        **Data Source** configured that you want to be able to query in
        XQL Search using the `workday_workday_raw` dataset. This is
        configured by selecting the applicable **Field** and leaving the
        default field name that is displayed in the
        **Column Heading Override XML Alias** column. This default field
        name is what is used in XQL Search and the dataset to view and
        query the data.

      ![](media/rId5848.png){width="5.833333333333333in"
      height="1.96875in"}

      > **Note**

      > The **Business Object** changes depending on the **Data Source**
      > selected.

      For the incident and card views in Cortex XSIAM, map the following
      fields in the table by selecting the applicable **Field** that
      contains the data representing the Cortex XSIAM field name as
      provided below that should be added to the
      **Column Heading Override XML Alias**. For example, for
      `full_name`, select the applicable **Field** from the
      **Business Object** defined that contains the full name of the
      user and in the **Column Heading Override XML Alias** specify
      `full_name` to map the set **Field** to the Cortex XSIAM field
      name.

      > **Note**

      > Cortex XSIAM uses a structured schema when integrating Workday
      > data. To get the best Analytics results, specify all the fields
      > marked with an asterisk from the recommended schema.

      - > `workday_user_id*`

      - > `full_name*`

      - > `workday_manager_user_id*`

      - > `manager*`

      - > `worker_type*`

      - > `position_title*`

      - > `department*`

      - > `private_email_address*`

      - > `business_email_address*`

      - > `employment_start_date*`

      - > `employment_end_date`

      - > `phone_number`

      - > `mailing_address`

    e.  (*Optional*) Filter out any employees that you do not want
        included in the **Filter** tab.

    f.  Share access to the report with the designated Integration
        System User that you created by setting the following settings
        in the **Share** tab:

        - **Report Definition Sharing Options**: Select
          **Share with specific authorized groups and users**.

        - **Authorized Users**: Select the designated Integration System
          User that you created for accessing the custom report.

    g.  Ensure that the following **Web Services Options** settings in
        the **Advanced** tab are configured.

    - Here is an example of the configured settings, where the
      **Web Service API Version** and **Namespace** are automatically
      populated and dependent on your report.

      ![](media/rId5851.png){width="5.833333333333333in"
      height="1.436457786526684in"}

    h.  (*Optional*) **Test** the report to ensure all the fields are
        populated.

    i.  Get the URL for the report.

        1.  In the related actions menu, select Actions \> Web Service
            \> View URLs.

        2.  Click **OK**.

        3.  Scroll down to the **JSON** section.

        4.  Hover over the **JSON** link and click the icon, which open
            a new tab in your browser with the URL for the report. You
            need to use the designated user credentials to open the
            report.

        5.  Copy the URL for the report and record them somewhere as
            this URL needs to be provided when setting up the Workday
            Collector in Cortex XSIAM.

    j.  Complete the report by clicking **Done**.

2.  Configure the Workday collection in Cortex XSIAM.

    a.  Select Settings \> Data Sources.

    b.  On the **Data Sources** page, click **Add Data Source**, search
        for and select **Workday**, and click **Connect**.

    c.  Set the following parameters.

        - **Name**: Specify the name for the Workday Collector that is
          displayed in Cortex XSIAM.

        - **URL**: Specify the URL of the custom report you configured
          in Workday.

        - **User Name**: Specify the username for the designated
          Integration System User that you created for accessing the
          custom report in Workday.

        - **Password**: Specify the password for the designated
          Integration System User that you created for accessing the
          custom report in Workday.

    d.  Click **Test** to validate access, and then click **Enable**.

    - A notification appears confirming that the Workday Collector was
      saved successfully, and closes on its own after a few seconds.

      Once report data starts to come in, a green check mark appears
      underneath the **Workday** Collector configuration with the data
      and time that the data was last synced.

3.  (*Optional*) Manage your Workday Collector.

- After you enable the Workday Collector, you can make additional
  changes as needed. To modify a configuration, select any of the
  following options.

  - **Edit** the Workday Collector settings.

  - **Disable** the Workday Collector.

  - **Delete** the Workday Collector.

  - **Sync Now** to run the report to get the latest report data. The
    report is run automatically every 6 hours, but you can always get
    the latest data as needed.

4.  After Cortex XSIAM begins receiving report data from Workday, you
    can use the XQL Search to search for logs in the new dataset
    (`workday_workday_raw`).

###### Ingest external alerts

For a more complete and detailed picture of the activity involved in a
case, Cortex XSIAM can ingest alerts from any external source. Cortex
XSIAM stitches the external alerts together with relevant endpoint data
and displays alerts from external sources in relevant cases and issues
tables. You can also see external alerts and related artifacts and
assets in causality views. For example, in the **Issues** table,
right-click an issue and select **Investigate Causality Chain**.

To ingest alerts from an external source, you configure your alert
source to forward alerts (in **Auto-Detect** (default), **CEF**,
**LEEF**, **CISCO**, or **CORELIGHT** format) to the Syslog collector.
You can also ingest alerts from external sources using the Cortex XSIAM
APIs.

After Cortex XSIAM begins receiving external alerts, you must map the
following required fields to the Cortex XSIAM format.

- TIMESTAMP

- SEVERITY

- ALERT NAME

In addition, these optional fields are available, if you want to map
them to the Cortex XSIAM format.

- SOURCE IP

- SOURCE PORT

- DESTINATION IP

- DESTINATION PORT

- DESCRIPTION

- DIRECTION

- EXTERNAL ID

- CATEGORY

- ACTION

- PROCESS COMMAND LINE

- PROCESS SHA256

- DOMAIN

- PROCESS FILE PATH

- HOSTNAME

- USERNAME

> **Note**
>
> If you send pre-parsed alerts using the Cortex XSIAM API, additional
> mapping is not required.

Storage of external alerts is determined by your Cortex XSIAM tenant
retention policy. For more information, see [Dataset
Management](/document/preview/952274#UUID-ae82030e-2493-a33b-9a9e-a9834e993e93).

1.  Send alerts from an external source to Cortex XSIAM.

- There are two ways to send alerts:

  - API: Use the **Insert CEF Alerts API** to send the raw Syslog alerts
    or use the **Insert Parsed Alerts API** to convert the Syslog alerts
    to the Cortex XSIAM format before sending them to Cortex XSIAM. If
    you use the API to send logs, you do not need to perform the
    additional mapping step in Cortex XSIAM.

  - Activate the Syslog collector (see [Activate the Syslog
    collector](#UUIDd7d85d4a97df7ec2a052d22376f5a52e)) and then
    configure the alert source to forward alerts to the Syslog
    collector. Then configure an alert/issue mapping rule as follows.

2.  In Cortex XSIAM, select Settings \> Configurations \> Data
    Collection \> External Issue Mapping.

3.  Right-click the **Vendor Product** for your issues and select
    **Filter and Map**.

4.  Use the filters at the top of the table to narrow the results to
    only the alerts you want to map.

- Cortex XSIAM displays a limited sample of results during the mapping
  rule creation. As you define your filters, Cortex XSIAM applies the
  filter to the limited sample but does not apply the filters across all
  alerts. As a result, you might not see any results from the alert
  sample during the rule creation.

5.  Click **Next** to begin a new mapping rule.

- On the left, configure the following:

  a.  **Rule Information**: Define the **NAME** and optional
      **DESCRIPTION** to identify your mapping rule.

  b.  **Issues Field**: Map each required and any optional Cortex XSIAM
      field to a field in your alert source.

  - If needed, use the field converter
    (![](media/rId5856.png){width="0.14583333333333334in"
    height="0.20833333333333334in"}) to translate the source field to
    the Cortex XSIAM syntax.

    For example, if you use a different severity system, you need to use
    the converter to map your severities fields to the Cortex XSIAM
    risks of Critical, High, Medium, and Low.

    You can also use regex to convert the fields to extract the data to
    facilitate matching with the Cortex XSIAM format. For example, if
    you need to map the port, but your source field contains both the IP
    address and port (`192.168.1.200:8080`), to extract everything after
    the `:`, use the following regex:

    `^[^:]*_`

    For additional context when you are investigating a case, you can
    also map additional optional fields to fields in your alert source.

6.  To submit your alert filter and mapping rule when finished, click
    **Submit**.

#### Onboard the Kubernetes Connector

> **Note**
>
> Requires the Cortex Cloud Posture Management add-on.

Follow this wizard to deploy your Kubernetes Connector. The Kubernetes
onboarding wizard is designed to facilitate the seamless setup of
Kubernetes data into Cortex XSIAM. The guided experience requires
minimal user input; simply enter a name for the installer file and
select the type of connector you want to install. For full control of
the setup, you can use the advanced settings. Based on the onboarding
settings, Cortex XSIAM then creates a custom installer file for running
in your Kubernetes environment. This file, once executed in your
Kubernetes environment, grants Cortex XSIAM the necessary permissions to
collect the data. The installer file must be executed in your Kubernetes
environment to complete the onboarding process. The connector then
appears in **Kubernetes Connectors**.

1.  Select Settings \> Data Sources.

2.  Select **Add Data Source**.

3.  On the **Add Data Sources** page, search for and select
    **Kubernetes** and click **Connect**.

4.  In the **Connect Kubernetes** onboarding wizard, enter a name for
    the installer file, the deployment YAML script that is generated by
    the selections you choose in this wizard.

5.  Select the **Connector**:

    - **Connector:** A lightweight solution that provides additional
      Kubernetes related capabilities, such as enhanced inventory with
      relations mapping and policy enforcement.

6.  (Optional) Click **Show advanced settings** to define advanced
    settings:

    - **Connector Namespace:** Specify the Kubernetes Connector
      namespace.

    - **Scan Cadence:** Define how often to scan (from every one to 24
      hours). Default is 12 hours. To optimize performance and minimize
      I/O impact in production environments, we recommend configuring
      the scan cadence to every 12 hours. This ensures comprehensive
      file system scanning while efficiently managing system resources.

    - **Admission Controller:** Select to allow enforcement policies to
      be configured, ensuring that only compliant resources are admitted
      into the cluster.

    - **Version:** Select which version of the Kubernetes Connector to
      install. For detailed information on each version, see [What\'s
      new in Kubernetes
      Connector?](#UUID4ceca3ba1aa748690d2133e298bbd4a0).

    - **Auto Upgrade:** Select whether to have the Kubernetes Connector
      automatically upgraded when new versions become available.

- > **Note**

  > For GKE or EKS clusters with the metadata service disabled and for
  > AKS clusters with non-default nodes managed resource group, the
  > cluster resource identifier must be specified.

7.  Click **Next**.

8.  To complete the onboarding of the Kubernetes Connector, you must
    download the Helm chart values `values.yaml` and run it in your
    Kubernetes environment:
    `helm repo add cortex https://paloaltonetworks.github.io/cortex-cloud --force-update`

9.  Install the Helm charts in your Kubernetes environment:
    `helm install konnector cortex/konnector --create-namespace --namespace pan --values test.values.yaml`

10. Verify the deployment succeeded when you see \"Status: Deployed\"

- When the Kubernetes Connector is deployed, the initial discovery scan
  is started and the connector appears in Data Sources \> Kubernetes \>
  Kubernetes Connectors.

##### What\'s new in Kubernetes Connector?

This topic describes the changes, additions, known issues, and fixes for
each version of the Kubernetes Connector.

> **Note**
>
> Requires the Cortex Cloud Posture Management add-on.

###### Kubernetes Connector version 1.0

**New features**

The following section describes the new features introduced in
Kubernetes Connector version 1.0.

+-----------------------------------+-----------------------------------+
| Feature                           | Description                       |
+===================================+===================================+
| Kubernetes Connector Onboarding   | Supports AKS, EKS, and GKE        |
|                                   | clusters, with a Kubernetes       |
|                                   | Native installation method of     |
|                                   | Helm Installer.                   |
+-----------------------------------+-----------------------------------+
| Kubernetes Resource Discovery     | Automatically discovers           |
|                                   | in-cluster resources such as      |
|                                   | namespaces, workloads, and more.  |
+-----------------------------------+-----------------------------------+
| Kubernetes Policy Management &    | Define and enforce policies at    |
| Prevention                        | the Admission Controller level    |
|                                   | with prevention capabilities.     |
+-----------------------------------+-----------------------------------+
| Kubernetes Compliance &           | Leverages hundreds of             |
| Misconfiguration Detection        | out-of-the-box KSPM rules. (More  |
|                                   | pre-defined rules are being added |
|                                   | continuously.)                    |
+-----------------------------------+-----------------------------------+
| Kubernetes Custom                 | - Create custom                   |
| Compliance/Misconfiguration Rules |   compliance/misconfiguration     |
| Support                           |   rules using Rego (for           |
|                                   |   ADS/Kubernetes Connector).      |
|                                   |                                   |
|                                   | - Create custom                   |
|                                   |   compliance/misconfiguration     |
|                                   |   rules using Python (for XDR for |
|                                   |   Cloud Agent-managed Kubernetes  |
|                                   |   endpoints).                     |
+-----------------------------------+-----------------------------------+
| KSPM Dashboard v1                 | A visual overview of your         |
|                                   | Kubernetes security posture. It   |
|                                   | includes inventory insights,      |
|                                   | protection coverage, riskiest     |
|                                   | clusters, and more.               |
+-----------------------------------+-----------------------------------+

**Known limitations**

The following table describes known limitations in the Kubernetes
Connector release.

+-----------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Feature                           | Description                                                                                                              |
+===================================+==========================================================================================================================+
| Connector onboarding and cluster  | The Kubernetes Connector automatically calculates the Kubernetes cluster cloud identifier by using the metadata service  |
| identifier                        | (for EKS and GKE) and cluster resources (for AKS).                                                                       |
|                                   |                                                                                                                          |
|                                   | - For EKS and GKE, the metadata service must be enabled.                                                                 |
|                                   |                                                                                                                          |
|                                   | - For AKS, the cluster\'s node pool resource group should be the default. For more information, see [Azure Docs \|       |
|                                   |   Create a node                                                                                                          |
|                                   |   pool](https://learn.microsoft.com/en-us/azure/architecture/aws-professional/eks-to-aks/node-pools#create-a-node-pool). |
+-----------------------------------+--------------------------------------------------------------------------------------------------------------------------+

#### Automation and feed integrations

Integrations are mechanisms through which Cortex XSIAM connects and
communicates with other products. These integrations can be executed
through REST APIs, webhooks, and other techniques. Integrations enable
you to orchestrate and automate SOC operations.

**Integrations installed from a content pack**

Integrations are included in content packs, which you download and
install from Marketplace. After you download and install a content pack
that includes an integration, you need to configure the integration by
adding an instance. You can have multiple instances of an integration,
for example, to connect to different environments. Additionally, if you
are an MSSP and have multiple tenants, you could configure a separate
instance for each tenant.

> **Note**

- > Some integrations can be downloaded directly without having to
  > initially download a content pack from Marketplace. For more
  > information, see [Define data
  > sources](#UUID00d964f3e91bf4f6d19a763269659fad).

- > In addition to content packs that you install from Marketplace,
  > related content packs are automatically downloaded when you adopt
  > playbooks or edit tasks that require content items such as scripts
  > or integrations.

Cortex XSIAM comes out-of-the-box with integrations to help you onboard,
such as:

- Mail Sender

<!-- -->

- Sends email notifications to users.

<!-- -->

- Generic Export Indicators Service

<!-- -->

- Provides an endpoint with a list of indicators as a service for the
  system indicators. For more information about how to set up the
  integration, see [Export indicators using the Generic Export
  Indicators Integration](#X1ed0d74913567e4ac43b9680ff39f339651f584).

<!-- -->

- Palo Alto Networks WildFire Reports

<!-- -->

- Generates a Palo Alto Networks WildFire PDF report. For more
  information, see [Palo Alto Networks WildFire
  Reports](https://xsoar.pan.dev/docs/reference/integrations/wild-fire-reports).

<!-- -->

- Rasterize

<!-- -->

- Converts URLs, PDF files, and emails to an image file or PDF file. For
  more information, see
  [Rasterize](https://xsoar.pan.dev/docs/reference/integrations/rasterize).

**Create an integration**

You can create an integration, by adding parameters, commands,
arguments, and outputs as well as writing the necessary integration
code. You should have a working Cortex XSIAM tenant and programming
experience with Python.

To create an integration, on the **Automation and feed integrations**
page, click **BYOI**.

![](media/rId387.png){width="5.833333333333333in"
height="0.5614577865266842in"}

The Cortex XSIAM IDE and the **HelloWorld** integration template are
loaded by default. For more information about how to create an
integration, including an example, see [Create an
Integration](https://xsoar.pan.dev/docs/tutorials/tut-integration-ui).

**Configure an integration**

On the **Automation and feed integrations** page, after you have either
downloaded the integration or created an integration, you can do the
following:

+-----------------------------------+------------------------------------------------------------------------+
| Option                            | Description                                                            |
+===================================+========================================================================+
| Add instance                      | Configure an integration instance to connect and communicate with      |
|                                   | other products. For more information, see [Add an integration          |
|                                   | instance](#UUIDd5353dd15235a4a7cf7a40eb66cc900e).                      |
|                                   |                                                                        |
|                                   | After configuring the instance, you can also enable/disable the        |
|                                   | integration instance, copy the instance, and view the integration      |
|                                   | fetch history.                                                         |
+-----------------------------------+------------------------------------------------------------------------+
| View integration\'s source        | View the integration settings and source code.                         |
+-----------------------------------+------------------------------------------------------------------------+
| Edit integration\'s source        | Edit the integration settings and source code. For more information    |
|                                   | about editing the integration\'s source code, see [Create an           |
|                                   | Integration](https://xsoar.pan.dev/docs/tutorials/tut-integration-ui). |
|                                   |                                                                        |
|                                   | > **Note**                                                             |
|                                   | >                                                                      |
|                                   | > If the integration was installed from a content pack, you need to    |
|                                   | > duplicate the integration before editing.                            |
+-----------------------------------+------------------------------------------------------------------------+
| Duplicate integration             | If you want to change the source code, and settings, or download the   |
|                                   | integration, you need to duplicate the integration.                    |
+-----------------------------------+------------------------------------------------------------------------+
| Delete                            | Although you can\'t delete an integration installed from a content     |
|                                   | pack (unless a duplicate), you can delete an integration instance.     |
+-----------------------------------+------------------------------------------------------------------------+
| Always / On Demand                | For each integration instance, you have the option of setting the      |
|                                   | instance to be used only **On Demand**, when it is specified with the  |
|                                   | `using` argument in a playbook or the CLI. By default, the settings is |
|                                   | **Always** and the integration instance is used whenever the           |
|                                   | integration is called.                                                 |
+-----------------------------------+------------------------------------------------------------------------+
| Download the integration          | Download the integration in YAML format. You can also upload an        |
|                                   | integration.                                                           |
|                                   |                                                                        |
|                                   | > **Note**                                                             |
|                                   | >                                                                      |
|                                   | > If the integration was installed from a content pack, you need to    |
|                                   | > duplicate the integration before downloading.                        |
+-----------------------------------+------------------------------------------------------------------------+
| Version History                   | If the integration is a duplicate or you create your own integration,  |
|                                   | you can see the changes in the integration.                            |
+-----------------------------------+------------------------------------------------------------------------+

You can view all the integration changes (the last 100 changes) by
clicking the **Version History** button.

##### Using integration commands

The command line interface (CLI) enables you to run system commands,
integration commands, scripts, etc from the Cases War Room, Issues War
Room, or Playground CLI. The CLI auto-complete feature allows you to
find relevant commands, scripts, and arguments.

Cortex XSIAM uses the \"`!`\" such as
`!ad-create-user username=[name of user]`

Under each integration, you can view a list of commands.

> **Note**
>
> Integration commands are only available when the integration instance
> is enabled. Some commands depend on a successful connection
> between Cortex XSIAM and third-party integrations.

You can run the CLI commands in the Playground or in a case/issue War
Room. The Playground is a non-production environment where you can
safely develop and test automation scripts, APIs, commands, etc. It is
an investigation area that is not connected to a live (active)
investigation.

When running the command, the results are returned in the War Room or
Playground and also in a JSON format in Context Data.

> **Tip**
>
> In the Playground, you can clear the context data, if needed, which
> deletes everything in the Playground context data, but does not affect
> the actual issue or case. To clear the context, run
> `!DeleteContext all=yes'` from the CLI or
> click **Clear Context Data** while viewing the context data.

##### Integration use cases

The following categories are common use cases for Cortex XSIAM
integrations. While this list is not meant to be exhaustive, it\'s a
starting point to understand what use cases are supported by Cortex
XSIAM and third-party integrations.

###### Analytics and SIEM

Top use cases:

- Fetch issues with relevant filters.

- Create, close, and delete issues/events/cases.

- Update issues - update status, assignees, severity, SLA, and more.

- Get events related to an issue/case for enrichment/investigation
  purposes.

- Query SIEM (consider aggregating logs).

These integrations usually include the Fetch Issues or Fetch Alerts
option for an integration instance configuration. The integration may
also include integration commands enabling you to list or retrieve
issues or related information.

Analytics & SIEM integration Example: ArcSight ESM

###### Authentication and Identity Management

Top use cases:

- Use credentials from the authentication vault to configure instances
  in Cortex XSIAM. (Save credentials in: Settings \> Configurations \>
  Integrations \> Credentials.) Integrations that use credentials from
  the vault should have the **Switch to credentials** option.

- Lock/Delete Account -- Use an integration to lock/unlock a third-party
  account.

- Reset Account - Perform a reset password command for a third-party
  account.

- Lock an external credentials vault - in case of an emergency (if the
  vault has been compromised), allow the option to lock/unlock the
  entire vault via an integration.

- Step-Up authentication - Enforce Multi-Factor Authentication for an
  account.

- Create, update, and delete users.

- Manage user groups.

- Block users, force a change of passwords.

- Manage access to resources and applications.

- Create, update, and delete roles.

Authentication integration example: CyberArk AIM v2 (Partner
Contribution)

###### Case Management

Top use cases:

- Create, get, edit, close a ticket or issue, and add and view comments.

- Assign a ticket/issue to a specified user.

- List all tickets, and filter by name, date, and assignee.

- Get details about a managed object, update, create, or delete.

- Add and manage users.

Case Management/Ticketing integration example: ServiceNow V2

###### Data Management and Threat Intelligence

Top use cases:

- Enrich information about different IOC types: Upload object for scan
  and get the scan results. (If there's an option to upload
  private/public, the default should be set to private.) Search for
  former scan results about an object to get information about a sample
  without uploading it yourself. Enrich information and scoring for the
  object.

- Add indicators to the system and search for existing indicators.

- Add indicators to the exclusion list.

- Calculate DBot Score for indicators.

- Enrich asset -- get vulnerability information for an asset (or a group
  of assets) in the organization.

- Generate/trigger a scan on specified assets.

- Get a scan report including vulnerability information for a specified
  scan and export it.

- Get details for a specified vulnerability.

- Scan assets for a specific vulnerability.

Data Enrichment & Threat Intelligence integration example: Unit 42
Objects Feed.

###### Email

Top use cases:

- Get message -- download the email itself, retrieve metadata, and body.

- Download attachments for a given message.

- Manage senders -- block/allow specified mail senders.

- Manage URLs -- block/allow the sending of specified URLs.

- Encode/decode URLs in messages

- Release a held message when a gateway has placed a suspicious message
  on hold.

Email Gateway integration example: MimeCast v2

###### Endpoint

Top use cases:

- Fetch issues and events

- Get event details (from a specified alert)

- Quarantine a file

- Isolate and contain endpoints

- Update indicators (for example, network and hashes) by policy (can be
  block, monitor) -- deny list

- Add indicators to the exclusion list

- Search for indicators in the system (see indicators and related
  issues/events)

- Download a file based on the hash and the path

- Trigger scans on specified hosts

- Update .DAT files for signatures and compare existing .DAT files to
  the newest one on the Cortex XSIAM tenant

- Get information for a specified host (OS, users, addresses, hostname)

- Get policy information and assign policies to endpoints

Endpoint integration example: Tanium V2

###### Forensics and Malware Analysis

Top use cases:

- Submit a file and get a report (detonation)

- Submit a URL and get a report (detonation)

- Search for past analysis (input being a hash/URL)

- Retrieve a PCAP file

- Retrieve screenshots taken during analysis

Forensic and Malware Analysis example: Cuckoo Sandbox

###### Network Security

Top use cases:

- Create block/accept policies (source, destination, port), for IP
  addresses and domains

- Add addresses and ports (services) to predefined groups, create
  groups, and more

- Support custom URL categories

- Fetch network logs for a specific address for a configurable time
  frame

- URL filtering categorization change request

- Built-in blocked rule command for fast blocking

- If there is a Management Firewall, allow the option to manage policy
  rules through it

- Get/fetch issues

- Get PCAP file, packet

- Get network logs filtered by time range, IP addresses, ports, and more

- Create/manage/delete policies and rules

- Update signatures from an online source/upload + get the last
  signature update information

- Install policy (if existing)

Network Security Firewall integration examples: Tufin (Partner
Contribution), Protectwise

###### Vulnerability Management

Top use cases:

- Enrich asset -- get vulnerability information for an asset (or a group
  of assets) in the organization.

- Generate/trigger a scan on specified assets

- Get a scan report including vulnerability information for a specified
  scan and export it

- Get details for a specified vulnerability

- Scan assets for a specific vulnerability

Vulnerability Management integration example: Tenable.sc

##### Add an integration instance

Configure an integration instance to connect and communicate with other
products.

When you define an integration instance for your third-party security
and incident management vendors, events triggered by this integration
instance can become cases in Cortex XSIAM. When cases are created, you
can run playbooks on them to enrich them with information from other
products in your system. For indicators, you can enrich those indicators
depending on the integration instance and add them to a case if
required.

Although you can view the integration documents when adding an instance,
the [Developer Hub](https://xsoar.pan.dev/docs/welcome) has more
detailed information about the integrations, including commands,
outputs, and recommended permissions. You can also see more information
about content packs, playbooks, scripts, and Marketplace documentation.

> **Note**
>
> This procedure describes how to add an integration instance from the
> **Automation and Feed Integration** page. Some integration instances
> can also be configured on the **Data Sources** page. For more
> information, see [Add a new data source or
> instance](#UUID4f226047ebe62516a23316de720884c4).

Before you begin

- From Marketplace, download and install the relevant content pack,
  which includes your integration. Content packs containing integrations
  are also downloaded, in some cases, when you adopt playbooks and
  configure playbook tasks.

- Consider whether you want to add credentials, which enable you to save
  login information without exposing usernames, passwords, certificates,
  and SSH keys. For more information, see
  [/document/preview/1306520#UUID-3fd6a049-4552-7a32-3341-59e1d7ad282d](/document/preview/1306520#UUID-3fd6a049-4552-7a32-3341-59e1d7ad282d).

1.  Go to Settings \> Configurations \> Data Collection \> Automation &
    Feed Integrations and search for the integration.

2.  In the integration you want to add, click **Add instance**.

3.  Add the parameters, as required.

4.  If you want to fetch issues, select the **Fetches alerts**.

- For more information, see [Fetch issues from an integration
  instance](#UUID88ccf818d08d74d81a26f9acbad8f5bf).

5.  (Optional) To check that the integration instance is working
    correctly, click **Test**.

6.  Save & Exit.

- Expand the integration to see more details such as the number of
  pulled issues/indicators or error messages.

  ![](media/rId5881.png){width="5.833333333333333in"
  height="1.042707786526684in"}

  You can also enable/disable the integration instance, copy the
  instance, and view the integration fetch history.

  If you encounter an error, see [Troubleshoot
  Integrations](#UUID06962aa9d9bad9adc3284cd807e17b75).

7.  By default, the integration instance is used whenever the
    integration is called. If you want to only use the integration
    instance when specified with the `using` argument in a playbook or
    the CLI, change the integration instance setting from **Always** to
    **On Demand**. For example, you might have two instances of an
    integration and want to use one instance as the default and the
    other instance only for manual testing on demand.

8.  (Optional) To manage access to specific commands, see [Configure
    integration permissions](#UUID57bc36143fe172c0eb6a6b652bc8cf65).

In this example, you will set up the **OnboardingIntegration**.

If you have not done so, download the **OnboardingIntegration** content
pack from Marketplace. Most integrations follow a similar configuration.

1.  Go to Settings \> Configurations \> Data Collection \> Automation &
    Feed Integrations and search for **OnboardingIntegration**.

2.  Click **Add Instance**.

3.  Add the number of issues to fetch per minute. By default, there is a
    maximum of 5 issues per minute.

4.  Add the maximum number of issues to create. By default, there is a
    maximum number 10 issues to create.

5.  Add the number of issues you want to create in minutes.

6.  Set the Alerts Fetch Interval. By default, the issues are fetched
    every minute.

7.  Select whether to run on an engine.

8.  When troubleshooting the instances, adjust the default setting from
    off to a higher debugging level.

9.  Select **Fetches alerts** to start ingesting issues.

- For all integrations, we recommend only fetching issues when
  everything is set up. When enabled, Cortex XSIAM searches for events
  that occurred within the time frame set for the integration, which is
  based on the specific integration. The default is 5 issues per minute.

  > **Note**

  > In some integrations, a classifier, an issue type, and mapper fields
  > are included.

10. Test and Save & Exit.

##### Configure integration permissions

You can use role-based access control (RBAC) to restrict running
commands to specific roles at the integration instance level. If you
have multiple instances of the same integration, you can assign
different roles (permission levels) for the same command in each
instance.

For example, you may want limit the roles that can run potentially
harmful commands, such as the ability to isolate endpoints.

Users who do not have permission to run a command cannot do the
following:

- Run the command from the CLI.

- Complete pending tasks in a Work Plan that uses the restricted
  command.

- Edit arguments for playbook tasks that use the restricted command.

- Select the command when editing a playbook.

- Leverage the restricted command when executing a reputation command,
  such as IP, Domain, and File.

If you have multiple instances of the same integration, you can assign
different roles (permission levels) for the same command in each
instance.

To view or edit integration permissions:

1.  Go to Settings \> Configurations \> Data Collection \> Integration
    Permissions.

- You can see a list of all enabled integrations.

2.  Select the integration.

- You can see the following:

  - **INSTANCE:** Lists all instances for the integration.

  - **COMMANDS:** Lists all commands for the integration.

  - **PERMITTED ROLES:** Lists the roles that have permission to run the
    command. Default is **No Restrictions**.

3.  For a specific command, restrict the roles that can run the command.

    1.  Go to the relevant command.

    2.  Click **Edit**.

    3.  In the **PERMITTED ROLES**, column, select the roles that you
        want to allow running the command.

4.  Save the integration permissions.

##### Fetch issues from an integration instance

You can poll third-party integration instances for events and turn them
into Cortex XSIAM issues (fetching). Many integrations support fetching,
but not all support this feature. You can view each integration in the
[Developer Hub](https://xsoar.pan.dev/docs/reference/index).

When setting up an instance, you can configure the integration instance
to fetch events. You can also set the interval for which to fetch new
issues by configuring the **Issue Fetch Interval** field. The fetch
interval default is 1 minute. This enables you to control the interval
in which an integration instance reaches out to third-party platforms to
fetch issues into Cortex XSIAM.

> **Note**

- > In some integrations, the **Issue Fetch interval** is called
  > **Feed Fetch Interval**.

- > If the integration instance does not have the
  > **Issue Fetch Interval** field, you need to add this field by
  > editing the integration settings. If the integration is from a
  > content pack, you need to create a copy of the integration. Any
  > future updates to this integration will not be applied to the copy
  > integration.

- > If you turn off fetching for a while and then turn it on or disable
  > the instance and enable it, the instance remembers the last run and
  > pulls all events that occurred while it was off. If you don\'t want
  > this to happen, verify that the instance is enabled and
  > click **Reset the "last run" timestamp** when editing the instance.
  > Also, note that \"last run\" is retained when an instance is
  > renamed.

After configuring the instance, you may need to set up a correlation
rule to generate issues in Cortex XSIAM.

Correlation rules are predefined logic or patterns that Cortex XSIAM
uses to identify relationships between disparate events occurring across
an organization\'s IT environment. If the conditions specified in the
rule are met, Cortex XSIAM generates an issue.

**How to fetch issues**

1.  Go to Settings \> Configuration \> Data Collection \> Automation and
    Feed Integrations, find the integration, and click
    **+ Add instance**.

2.  In the integration\'s dialog box, select **Fetch issues**.

- After this setting is enabled, Cortex XSIAM searches for events that
  occurred within the time frame set for the integration, which is based
  on the specific integration. The default is 10 minutes, but it can be
  changed in the integration script.

3.  (*Optional*) In the **Issue Fetch Interval** field, set the interval
    of hours and minutes to fetch alerts (default 1 minute).

4.  (*Optional*) If the **Issue Fetch Interval** field does not appear,
    add it to the integration.

- Relevant for any issue fetching integration:

  a.  For integrations installed from a content pack, select the
      duplicate integration button.

  - If you have already duplicated the integration, click the Edit
    integration's source button.

  b.  In the **Basic** section, select the **Fetch issues** checkbox.

  - In the **Parameters** section, you can see that the
    `IssueFetchInterval` parameter is added. Change the default value if
    necessary.

  c.  Click **Save** to save the changes.

5.  To generate issues in Cortex XSIAM, add a correlation rule, as
    required.

- > **Note**

  > Some content packs include preconfigured correlation rules, but you
  > should review them to see if they suit your use case and duplicate
  > them if required. Go to Threat Management \> Detection Rules \>
  > Correlations, search for the relevant rule, right-click, and select
  > **Preview Rule**. For example, the
  > **ServiceNow v2 Alerts (automatically generated)** correlation rule
  > uses the following XQL Query:

      dataset = servicenow_v2_generic_alert_raw
      | filter _alert_data != null
      | alter alert_severity = json_extract_scalar(_alert_data, "$.severity")
      | alter alert_category = json_extract_scalar(_alert_data, "$.alert_category")
      | alter alert_name = json_extract_scalar(_alert_data, "$.alert_name")
      | alter alert_description = json_extract_scalar(_alert_data, "$.alert_description")

  > You may want to update the query by defining complex, multi-source
  > detection logic or add filters, such as alert severity or assignee.

###### Map fields to issue types

Mappers enable you to map information from incoming events to the issue
fields that you have in your system. You can map to system issue fields
or custom issue fields.

Mapping event attributes or issue fields takes place in two stages.
First you map all of the fields that are common to all issues in the
default mapping. Second, you map the additional fields that are specific
for each issue indicator type, or overwrite the mapping that you used in
the default mapping.

> **Note**
>
> In the **Classification & Mapping** page, the mapping does not
> indicate for which issue types they are configured. Therefore, when
> creating a mapper, it is best practice to add to the mapper name, the
> issue types the mapper is for. For example, Mail Listener - Phishing.
>
> **Note**
>
> When mapping a list, we recommend you map to a multi select field.
> Short text fields do not support lists. If you do need to map a list
> to a short text field, add a transformer in the relevant playbook
> task, to split the data back into a list.

You can use this procedure for creating a classifier or duplicating an
existing mapper for issue types.

1.  Go to Settings \> Configurations \> Object Setup \> Issues \>
    Classification & Mapping.

2.  Click **New** and select **Issue Mapper (incoming)**. The Issue
    Mapper maps all of the fields you are pulling from the integrations
    to the issue fields in your layouts.

3.  Under **Get data**, select from where you want to pull the
    information based on where you want to map the issue types.

    - Pull from instance - select an existing integration instance.

    - Select schema - when supported by the integration, this pulls all
      of the fields for the integration from the database. This enables
      you to see all of the fields for each given event type that the
      integration supports.

    - Upload JSON - upload a formatted JSON file which includes the
      field you want to map.

4.  Under **Issue Type**, start by mapping out the **Common Mapping**.
    This mapping includes the fields that are common to all of the issue
    types and will save time having to define these fields individually
    in each issue type.

5.  Click the event attribute to which you want to map. You can further
    manipulate the field using filters and transformers.

- You can click **Auto Map** to automatically map fields with common or
  similar names to fields in Cortex XSIAM . For example, Severity to
  Importance or Description to Description.

6.  Repeat this process for the other issue types for which this mapping
    is relevant.

7.  Click **Save**.

8.  Go to Settings \> Configurations \> Data Collection \> Automation &
    Feed Integrations.

    a.  Select the integration instance to which you want to apply the
        mapper.

    b.  In the integration settings, under **Mapper (incoming)** select
        the mapper you created and click **Save**.

###### Classify events using a classifier for issue types

When an integration fetches issues, it populates the rawJSON object in
the issue object. The rawJSON object contains all of the attributes for
the event. For example, source, when the event was created, the priority
that was designated by the integration, etc. When classifying the event,
you want to select an attribute that can determine the event type.

You can use this procedure for creating a classifier or duplicating an
existing classifier.

1.  Go to Settings \> Configurations \> Object Setup \> Issues \>
    Classification & Mapping.

2.  Click **New** and select **Issue Classifier**.

- If you want to duplicate the classifier, select the relevant
  classifier and then duplicate it.

3.  Under **Get data**, select from where you want to pull the
    information based on which you will classify the issue types.

    - Pull from instance - select an existing integration instance.

    - Select schema - when supported by the integration, this will pull
      all the fields for the integration from the database from which
      you can select by which to classify the events.

    - Upload JSON - upload a formatted JSON file which includes the
      field by which you want to classify.

4.  In the **Select Instance** field, select the instance from where you
    want to choose the value.

5.  In the **Data fetched from** select the value by which you want to
    classify the events.

6.  Drag values from the **Unmapped Values** column to the relevant
    issue type on the right.

- You can optionally choose a default issue type for unclassified issues
  from **Direct unclassified events to: Select**.

  ![](media/rId5888.png){width="4.395603674540682in"
  height="3.0989009186351706in"}

7.  Click **Save**.

8.  Go to Settings \> Configurations \> Data Collection \> Automation &
    Feed Integrations.

    a.  Select the integration to which you want to apply the
        classifier.

    b.  In the integration settings, under **Classifier**, select the
        classifier you created and click **Save**.

##### Troubleshoot Integrations

When troubleshooting integrations, do the following:

- Use the **Test** button in the integration instance.

- Verify the integration settings. Check settings such as usernames,
  URLs, and passwords.

- Download the debug log file and review its contents.

<!-- -->

- In the following example, you receive a 401 unauthorized error code
  after testing the integration.

  ![](media/rId426.png){width="5.833333333333333in" height="3.92in"}

  Click **Run Test & Download Debug Log**, to download the debug file
  locally. You can verify what server the URL request is being forwarded
  to and any other reasons as to why you received this error code. The
  401 unauthorized error code usually relates to invalid error
  credentials, expired tokens, or incorrect API settings.

<!-- -->

- Enable verbose or debug-level logging on the integration.

If you are unable to fix the integration, contact Customer Support for
further assistance.

##### Forward Requests to Long-Running Integrations

Some long-running integrations provide internal data via API calls to
your third-party software, such as a firewall. You can set up Cortex
XSIAM to allow third-party software to access long-running integrations
installed either on the Cortex XSIAM tenant or on an engine.

Long-running integrations provide internal data via API calls such as:

+-----------------------+--------------------------------------------------+---------------------------------------------------------------------------------------------------+
| Integration           | Description                                      | See More                                                                                          |
+=======================+==================================================+===================================================================================================+
| O365 Teams (Using     | Get authorized access to a user\'s Teams app in  | [O365 Teams (Using Graph                                                                          |
| Graph API)            | a personal or organizational account.            | API)](https://xsoar.pan.dev/docs/reference/integrations/microsoft-graph-teams)                    |
+-----------------------+--------------------------------------------------+---------------------------------------------------------------------------------------------------+
| Generic Webhook       | Creates cases on event triggers. The trigger can | [Generic Webhook](https://xsoar.pan.dev/docs/reference/integrations/generic-webhook)              |
|                       | be any query posted to the integration.          |                                                                                                   |
+-----------------------+--------------------------------------------------+---------------------------------------------------------------------------------------------------+
| Generic Export        | Use the Generic Export Indicators Service        | [Generic Export Indicators](#UUID20234eeb7afffdf4985fa70052c0a876)                                |
| Indicators Service    | integration to provide an endpoint with a list   |                                                                                                   |
|                       | of indicators as a service for the system        |                                                                                                   |
|                       | indicators.                                      |                                                                                                   |
|                       |                                                  |                                                                                                   |
|                       | You can set up the tenant to export internal     |                                                                                                   |
|                       | data to an endpoint.                             |                                                                                                   |
|                       |                                                  |                                                                                                   |
|                       | > **Note**                                       |                                                                                                   |
|                       | >                                                |                                                                                                   |
|                       | > This integration replaces the External Dynamic |                                                                                                   |
|                       | > list integration, which is deprecated. For     |                                                                                                   |
|                       | > more information about how to set up the       |                                                                                                   |
|                       | > integration, see [Manage external dynamic      |                                                                                                   |
|                       | > lists](#UUID35fa653f80ceb4d7218a0b086bf8a454). |                                                                                                   |
+-----------------------+--------------------------------------------------+---------------------------------------------------------------------------------------------------+
| Microsoft Teams       | Send messages and notifications to team members. | [Microsoft Teams](https://xsoar.pan.dev/docs/reference/integrations/microsoft-teams)              |
+-----------------------+--------------------------------------------------+---------------------------------------------------------------------------------------------------+
| TAXII Server          | Provides TAXII Services for system indicators    | [TAXII Server](https://xsoar.pan.dev/docs/reference/integrations/taxii-server)                    |
|                       | (Outbound feed).                                 |                                                                                                   |
+-----------------------+--------------------------------------------------+---------------------------------------------------------------------------------------------------+
| TAXII2 Server         | Provides TAXII2 Services for system indicators   | [TAXII2 Server](https://xsoar.pan.dev/docs/reference/integrations/taxii2-server)                  |
|                       | (outbound feed). You can choose to use TAXII     |                                                                                                   |
|                       | v2.0 or TAXII v2.1.                              |                                                                                                   |
+-----------------------+--------------------------------------------------+---------------------------------------------------------------------------------------------------+
| PingCastle            | Listens for PingCastle XML reports.              | [PingCastle](https://xsoar.pan.dev/docs/reference/integrations/ping-castle)                       |
+-----------------------+--------------------------------------------------+---------------------------------------------------------------------------------------------------+
| Publish List          | Publishes Cortex XSIAM lists for external        | [Publish List](https://cortex.marketplace.pan.dev/marketplace/details/PublishList/)               |
|                       | consumption.                                     |                                                                                                   |
+-----------------------+--------------------------------------------------+---------------------------------------------------------------------------------------------------+
| Simple API Proxy      | Provides a simple API proxy to restrict          | [Simple API Proxy](https://cortex.marketplace.pan.dev/marketplace/details/SimpleAPIProxy/)        |
|                       | privileges or minimize the number of credentials |                                                                                                   |
|                       | issued at the API.                               |                                                                                                   |
+-----------------------+--------------------------------------------------+---------------------------------------------------------------------------------------------------+
| Syslog v2             | Opens cases automatically from Syslog clients.   | [Syslog v2](https://xsoar.pan.dev/docs/reference/integrations/syslog-v2)                          |
+-----------------------+--------------------------------------------------+---------------------------------------------------------------------------------------------------+
| Web File Repository   | Make your environment ready for testing purposes | [Web File                                                                                         |
|                       | for your playbooks or automations to download    | Repository](https://xsoar.pan.dev/docs/reference/integrations/web-file-repository#context-output) |
|                       | files from a web server.                         |                                                                                                   |
+-----------------------+--------------------------------------------------+---------------------------------------------------------------------------------------------------+

> **Note**

- > When running on the tenant, you can only use long-running
  > integrations provided by Cortex XSIAM, you cannot create custom
  > ones. Custom long-running integrations are supported only on engines
  > at this time.

- > Configuring custom certificates or private API Keys in the
  > long-running integration instance is supported only on engines, not
  > on the Cortex XSIAM tenant.

###### Credentials

For long-running integrations running on a tenant, you must set a
username and password. For long-running integrations running on an
engine, we strongly recommend setting a username and password, but it is
not required.

Users with sufficient permissions can set the username and password for
specific integration instances on the **Automation & Feed Integrations**
page.

###### Test the long-running integration connection

- **Integration instance running on a tenant**

<!-- -->

- You can use CURL commands from any terminal to access and test the
  long-running integration. The string `xdr` in the URL must be replaced
  by `crtx` and the data URL must always be prefixed by `ext-`.

  > **Note**

  > For the TAXII Server and TAXII2 Server integrations, the `xdr`
  > string is automatically replaced by `crtx`. For the Microsoft Teams
  > integration, you can use the
  > `microsoft-teams-create-messaging-endpoint` command to get the
  > correct messaging endpoint based on the server URL, the server
  > version, and the instance configurations. For more information, see
  > [Microsoft
  > Teams](https://xsoar.pan.dev/docs/reference/integrations/microsoft-teams).

  Example:

  **Tenant URL**: https://crtx-cnt-onr-xsiam-dran-9c0.xdr-qa2-uat.us.com

  **Request URL**:
  https://ext-crtx-cnt-onr-xsiam-dran-9c0.crtx-qa2-uat.us.com/xsoar/instance/execute/edl_instance_01\\?q\\=type:ip

  **CURL**: curl -v -u user:pass
  https://ext-crtx-cnt-onr-xsiam-dran-9c0.crtx-qa2-uat.us.com/xsoar/instance/execute/edl_instance_01\\?q\\=type:ip

<!-- -->

- **Integration instance running on an engine**

<!-- -->

- You can use CURL commands from any terminal to access and test the
  long-running integration at the engine URL:

  `http://<engine-address>:<integration listen port>/`

  For example,
  `curl -v -u user:pass http://<engine_address>:<listen_port>/?n=50`

**Curl request parameters**

When sending a curl request to the URL, use the following parameters:

+-----------------------+--------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+
| Argument              | Description                                            | Example                                                                                                                     |
+=======================+========================================================+=============================================================================================================================+
| `n`                   | The maximum number of entries in the output. If no     | `https://ext-<tenant-address>/instance/execute/<ExportIndicators_instance_name>?n=50`                                       |
|                       | value is provided, will use the value specified in     |                                                                                                                             |
|                       | the **List Size** parameter in the integration         |                                                                                                                             |
|                       | instance settings.                                     |                                                                                                                             |
+-----------------------+--------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+
| `s`                   | The starting entry index from which to export the      | `https://ext-<tenant-address>/instance/execute/<ExportIndicators_instance_name>?s=10&n=50`                                  |
|                       | indicators.                                            |                                                                                                                             |
+-----------------------+--------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+
| `v`                   | The output format. Supports PAN-OS                     | `https://ext-<tenant-address>/instance/execute/<ExportIndicators_instance_name>?v=json`                                     |
|                       | (text), CSV, JSON, mwg, and proxysg (alias: bluecoat). |                                                                                                                             |
+-----------------------+--------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+
| `q`                   | The query is used to retrieve indicators from the      | `https://ext-<tenant-address>/instance/execute/<ExportIndicators_instance_name>?q="type:ip and sourceBrand:my_source"`      |
|                       | system.                                                |                                                                                                                             |
+-----------------------+--------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+
| `t`                   | Only with mwg format. The type is indicated at the top | `https://ext-<tenant-address>/instance/execute/<ExportIndicators_instance_name>?v=mwg&t=ip`                                 |
|                       | of the exported list. Supports: string, applcontrol,   |                                                                                                                             |
|                       | dimension, category, ip, mediatype, number, and regex. |                                                                                                                             |
+-----------------------+--------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+
| `sp`                  | If set, will strip ports off URLs; otherwise, will     | `https://ext-<tenant-address>/instance/execute/<ExportIndicators_instance_name>?v=text&sp`                                  |
|                       | ignore URLs with ports.                                |                                                                                                                             |
+-----------------------+--------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+
| `di`                  | Only with PAN-OS (text) format. If set, will ignore    | `https://ext-<tenant-address>/instance/execute/<ExportIndicators_instance_name>?v=text&di`                                  |
|                       | URLs that are not compliant with PAN-OS URL format     |                                                                                                                             |
|                       | instead of being rewritten.                            |                                                                                                                             |
+-----------------------+--------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+
| `cr`                  | If set, will strip protocols off URLs.                 | `https://ext-<tenant-address>/instance/execute/<ExportIndicators_instance_name>?v=text&pr`                                  |
+-----------------------+--------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+
| `cd`                  | Only with proxysg format. The default category for the | `https://ext-<tenant-address>/instance/execute/<ExportIndicators_instance_name>?v=proxysg&cd=default_category`              |
|                       | exported indicators.                                   |                                                                                                                             |
+-----------------------+--------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+
| `ca`                  | Only with proxysg format. The categories that will be  | `https://ext-<tenant-address>/instance/execute/<ExportIndicators_instance_name>?v=proxysg&ca=category1,category2`           |
|                       | exported. Indicators not in these categories will be   |                                                                                                                             |
|                       | classified as the default category.                    |                                                                                                                             |
+-----------------------+--------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+
| `tr`                  | Only with PAN-OS (text) format. Whether to collapse    | `https://ext-<tenant-address>/instance/execute/<ExportIndicators_instance_name>?q="type:ip and sourceBrand:my_source"&tr=1` |
|                       | IPs.                                                   |                                                                                                                             |
|                       |                                                        |                                                                                                                             |
|                       | - 0 - Do not collapse.                                 |                                                                                                                             |
|                       |                                                        |                                                                                                                             |
|                       | - 1 - Collapse to ranges.                              |                                                                                                                             |
|                       |                                                        |                                                                                                                             |
|                       | - 2 - Collapse to CIDRs                                |                                                                                                                             |
+-----------------------+--------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+
| `tx`                  | Whether to output CSV formats as textual web pages.    | `https://ext-<tenant-address>/instance/execute/<ExportIndicators_instance_name>?v=csv&tx`                                   |
+-----------------------+--------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+

###### Define a listening port for long-running integrations

When configuring a long-running integration instance, you may need to
define a listening port.

- **Integration Instance Running on a Tenant**

<!-- -->

- If the long-running integration runs on the Cortex XSIAM tenant, you
  do not need to enter a **Listen Port** in the instance settings. The
  system auto-selects an unused port for the long-running integration
  when the instance is saved.

<!-- -->

- **Integration Instance Running on an Engine**

<!-- -->

- You must set the **Listen Port** for access when configuring a
  long-running integration instance on an engine. Use a unique port for
  each long-running integration instance. Do not use the same port for
  multiple instances.

#### Verify collector connectivity

You can verify the connectivity status of a collector instance on the
Data Sources page. Instances are grouped by integration, and a status
icon shows a summary of instance statuses for each integration. Expand
the integration section to see the status of each individual instance,
and hover over the status icons to see details about warning or error
statuses.

In addition, Cortex XSIAM creates Collection health issues if
connectivity disruptions occur in your collection integrations, custom
collectors, and Marketplace integrations. For more information, see
[About health issues](#UUIDdfd48b778b41de7955182409372663e9).

##### Troubleshooting collector errors

> **Note**
>
> For more information on troubleshooting data collector applet errors,
> see Troubleshoot Broker VM applet connectivity.

Where can I see if I have a connectivity error on a collector instance?

On the Data Sources page, instances in error status display an error
icon. Hover over the error icon next to the instance name to see the
error message as received from the API.

Where can I trace the connectivity changes of a collector instance?

Each status change of an instance is logged in the `collection_auditing`
dataset. Querying this dataset can help you see all the connectivity
changes of an instance over time, the escalation or recovery of the
connectivity status, and the error, warning, and informational messages
related to status changes.

This example searches for status changes on Strata IOT integrations:

    dataset = collection_auditing 
    |filter collector_type = "STRATA_IOT"

How can I set up correlation rules to trigger collection issues?

Cortex XSIAM provides OOTB Collection issues that are triggered when a
data collector instance is in error status, which means it is
disconnected or not sending data. In addition, you can set up your own
correlation rules that trigger collection issues for your specific
needs. For example, you might want to be notified if a high-profile
collector is in warning status so that you can fix the problem and
prevent the collector from disconnecting.

Example: Trigger collection issues for warning statuses on the
STRATA_IOT collector

In this example, a correlation rule triggers a Collection issue if an
integration of the Strata IOT collector changes to warning status. Any
issues will appear on the **Health Issues** page.

Example XQL:

    dataset = collection_auditing 
    |filter classification = "Warning" and collector_type = "STRATA_IOT"

Additional fields to specify in the correlation rule:

+-----------------------------------+--------------------------------------+
| Field                             | Value                                |
+===================================+======================================+
| Time Schedule                     | Hourly                               |
+-----------------------------------+--------------------------------------+
| Query time frame                  | 1 Hour                               |
+-----------------------------------+--------------------------------------+
| Issue Suppression                 | Select **Enable issue suppression**. |
+-----------------------------------+--------------------------------------+
| Action                            | Select **Generate issue**.           |
+-----------------------------------+--------------------------------------+
| Issue Domain                      | Health                               |
+-----------------------------------+--------------------------------------+
| Severity                          | Medium                               |
+-----------------------------------+--------------------------------------+
| Category                          | Collection                           |
|                                   |                                      |
|                                   | > **Note**                           |
|                                   | >                                    |
|                                   | > If an issue is triggered, the      |
|                                   | > investigation options in the       |
|                                   | > right-click menu of the            |
|                                   | > **Health Issues** pages are        |
|                                   | > context-specific. Make sure that   |
|                                   | > you specify the relevant issue     |
|                                   | > category.                          |
+-----------------------------------+--------------------------------------+

#### Overview of data ingestion metrics

> **Prerequisite**
>
> For Cortex XSIAM to monitor data ingestion health and create health
> issues, you must enable the following settings under
> **Configurations**:

- > **Cortex - Analytics**: Go to Configurations \> Cortex - Analytics.
  > For more information, see [Enable the Analytics Engine and Identity
  > Analytics](#UUIDefaf1a2cc1ea98a077d09b52d3ed5cc4).

- > **Data Ingestion Monitoring**: Go to Configurations \> General \>
  > Server Settings \> Data Ingestion Monitoring. For more information,
  > see [Set up your
  > environment](#UUIDe99e556efcc7391bff6fddf7e17625cf).

The data ingestion metrics are calculated in 5-minute aggregation
periods and saved to the `metrics_source` dataset and `metrics_view`
preset. These metrics measure the amount, size, and rate at which logs
are ingested by a data source:

  -----------------------------------------------------------------------
  Metric                              Description
  ----------------------------------- -----------------------------------
  total_size_bytes                    Total size (in bytes) of the logs
                                      collected during the aggregation
                                      period.

  total_size_rate                     Average size (in bytes per second)
                                      of the logs collected during the
                                      aggregation period.

  total_event_count                   Total number of logs collected
                                      during the aggregation period

  total_event_rate                    Average number (in count per
                                      second) of logs collected during
                                      the aggregation period.
  -----------------------------------------------------------------------

In the `metrics_source` dataset, the data ingestion metrics are saved
alongside additional fields that describe the data source associated
with the metrics. Only entries with ingestion metric values greater than
zero are saved in the dataset. Entries with zero values are not saved in
this dataset.

`metrics_view` is a preset for data in the `metrics_source` dataset. The
preset also simulates completion of entries with zero values in data
ingestion metrics at runtime, which allows effective use of metrics.
Therefore, when investigating disruptions in data collection, we
recommend using the `metrics_view` preset in XQL queries and correlation
rules.

Cortex XSIAM built-in data ingestion monitoring and issue mechanism uses
the data ingestion metrics to identify disruptions in the data ingestion
pipeline. Using analytical logic, Cortex XSIAM creates an ingestion
baseline for each data source that reflects the routine pattern of log
collection. If a data source isn\'t ingesting logs, or there is a
significant deviation from the baseline, ingestion issues are triggered.
You can see all ingestion issues on the **Health Issues** page. To
troubleshoot or investigate an issue, right-click an issue and click
**Investigate in XQL query**. For more information, see [Investigate and
resolve health issues](#UUIDbfa92d48c79e2e13f234bcc2a336b90c).

In addition, you can create your own custom logic for data ingestion
health monitoring by setting up correlation rules that monitor the data
ingestion metrics. For more information, see [Creating correlation rules
to monitor data ingestion
health](#UUIDb99c429af373626305b0fef509c35967).

The following table describes all the fields in the `metrics_source`
dataset and `metrics_view` preset:

  -------------------------------------------------------------------------------------
  Field                                 Type                    Description
  ------------------------------------- ----------------------- -----------------------
  total_size_bytes                      Integer                 Total size (in bytes)
                                                                of the logs collected
                                                                during the aggregation
                                                                period.

  total_size_rate                       Integer                 Average size (in bytes
                                                                per second) of the logs
                                                                collected during the
                                                                aggregation period.

  total_event_count                     Integer                 Total number of logs
                                                                collected during the
                                                                aggregation period.

  total_event_rate                      Integer                 Average number (in
                                                                count per second) of
                                                                logs collected during
                                                                the aggregation period.

  data_freshness_max_delay              Float                   Maximum delay value
                                                                from all log entries in
                                                                a record between log
                                                                creation at the source
                                                                and ingestion into
                                                                Cortex XSIAM (in
                                                                seconds).

  data_freshness_median                 Float                   Median delay value from
                                                                all log entries in a
                                                                record between log
                                                                creation at the source
                                                                and ingestion into
                                                                Cortex XSIAM (in
                                                                seconds).

  data_freshness_ninetieth_percentile   Float                   Ninetieth percentile of
                                                                delay values from all
                                                                log entries in a record
                                                                between log creation at
                                                                the source and
                                                                ingestion into Cortex
                                                                XSIAM (in seconds).

  last_seen                             Datetime                Time that the last logs
                                                                were collected.

  \_vendor                              String                  Vendor of the observing
                                                                data source.

  \_product                             String                  Product name of the
                                                                observing data source.

  \_device_id                           String                  (For firewall devices)
                                                                Device ID

  \_log_type                            String                  (For firewall devices)
                                                                Log type

  \_collector_type                      String                  (Event Metadata) Type
                                                                of collector that
                                                                provided the log.

  \_collector_name                      String                  (Event Metadata) Name
                                                                of the collector
                                                                instance.

  \_collector_id                        String                  (Event Metadata) ID of
                                                                the XDR Collector.

  \_collector_ip                        String                  (Event Metadata) IP
                                                                address of the XDR
                                                                Collector.

  \_reporting_device_name               String                  (Event Metadata) Host
                                                                name of the device
                                                                where the log
                                                                originated.

  \_reporting_device_ip                 String                  (Event Metadata) IP
                                                                Address of the device
                                                                where the log
                                                                originated.

  \_final_reporting_device_name         String                  (Event Metadata)
                                                                Hostname of the device
                                                                that the log was
                                                                extracted from.

  \_final_reporting_device_ip           String                  (Event Metadata) IP of
                                                                the device that the log
                                                                was extracted from.

  \_broker_device_name                  String                  (Event Metadata) Host
                                                                name of the Broker VM.

  \_broker_device_ip                    String                  (Event Metadata) IP
                                                                address of the Broker
                                                                VM.

  \_broker_device_id                    String                  (Event Metadata) ID of
                                                                the Broker VM.

  \_time                                Datetime                Timestamp of the
                                                                interval.

  \_insert_timestamp                    Datetime                Recorded time of the
                                                                entry.
  -------------------------------------------------------------------------------------

##### Creating correlation rules to monitor data ingestion health

In addition to the OOTB Ingestion health issues, you can build your
monitoring logic for ingestion by creating correlation rules that are
specific to your requirements. You can create rules that monitor the
data ingestion metrics for a specific source within a specific
timeframe, and trigger ingestion health issues if there is a deviation
from the regular pattern of log collection.

The following examples can help you set up your own correlation rules
with the data ingestion metrics:

Example 1: No logs collected from a data source for 1 hour

In this example, the correlation runs every hour and calculates the
number of logs that are collected for each data source over the previous
hour. If no logs are collected for a data source during an aggregation
period, a security issue is triggered.

Example XQL:

    preset = metrics_view  
    | comp sum(total_event_count) as total_event_count_sum by _collector_id, _collector_ip, 
    _collector_name, _collector_type, _final_reporting_device_ip, _final_reporting_device_name,
     _broker_device_id, _vendor, _product 
    | filter total_event_count_sum = 0

Addition fields to specify in the correlation rule:

  -----------------------------------------------------------------------
  Field                               Value
  ----------------------------------- -----------------------------------
  Time Schedule                       Hourly

  Query time frame                    1 Hour

  Issue Suppression                   Select
                                      **Enable issue suppression**.

  Fields                              Uncheck `total_event_rate_sum`,
                                      leave other fields checked.

  Action                              Select **Generate issue**.

  Issue Domain                        Health

  Severity                            High

  Type                                Ingestion

  Issue Fields Mapping                Select **Use preconfigured fields**
                                      to map the fields that are relevant
                                      to data ingestion health.
  -----------------------------------------------------------------------

Example 2: No logs received from a Firewall for 20 minutes

In this example, the correlation runs every 20 minutes and calculates
the number of logs that are received for each firewall in a lookup
dataset during the last 20 minutes. If no logs are received from a
device during an aggregation period, a security issue is triggered.

Example XQL:

    preset = metrics_view  
    | join conflict_strategy = left  type = inner (dataset = ngfw_device_Id_keepalive 
    | fields _device_id) as devices devices._device_id = _device_id  | comp sum(total_event_count)
     as total_event_count_sum by _device_id, _product,_vendor 
    | filter total_event_count_sum = 0

Addition fields to specify in the correlation rule:

  -----------------------------------------------------------------------
  Field                               Value
  ----------------------------------- -----------------------------------
  Time Schedule                       Every 20 minutes

  Query time frame                    20 minutes

  Issue Suppression                   Select
                                      **Enable issue suppression**.

  Fields                              Uncheck `total_event_rate_sum`,
                                      leave other fields checked.

  Action                              Select **Generate issue**.

  Issue Domain                        Health

  Severity                            High

  Type                                Collection

  Issues Fields Mapping               Select **Use preconfigured fields**
                                      to map the fields that are relevant
                                      to data ingestion health.
  -----------------------------------------------------------------------

##### Measuring data freshness

Cortex XSIAM provides metrics that calculate the freshness of your
ingested data and highlight delays in your data collection. The metrics
calculate the freshness delay value by measuring the difference between
log creation at the source (`_TIME`) and ingestion into Cortex XSIAM
(`_INSERT_TIME`).

Metrics are collected and calculated per data source during five-minute
aggregation periods and allocated into the following buckets. The
recorded freshness delay value is the top value in the range of the
bucket:

- 0 to 30 seconds → 30 seconds

- 30 to 60 seconds → 60 seconds

- 60 seconds to 5 minutes → 300 seconds

- 5 minutes to 1 hour → 3,600 seconds

- 1 hour to 24 hours→ 86,400 seconds

- 24 hours to week→ 604,800 seconds

+-------------------------------------+-----------------------------------+
| Metric                              | Description                       |
+=====================================+===================================+
| data_freshness_max_delay            | Maximum freshness delay value     |
|                                     | among all log entries in an       |
|                                     | aggregation period.               |
|                                     |                                   |
|                                     | This reflects the worst case.     |
+-------------------------------------+-----------------------------------+
| data_freshness_median               | Median freshness delay value      |
|                                     | among all log entries in an       |
|                                     | aggregation period.               |
|                                     |                                   |
|                                     | 50% of values are smaller than    |
|                                     | the median, and 50% of values are |
|                                     | higher or equal to the median.    |
+-------------------------------------+-----------------------------------+
| data_freshness_ninetieth_percentile | Ninetieth percentile of delay     |
|                                     | values among all log entries in   |
|                                     | an aggregation period.            |
|                                     |                                   |
|                                     | This delay value is 90% higher    |
|                                     | than other log entry differences. |
|                                     | It reflects the worst case, but   |
|                                     | eliminates the spikes.            |
+-------------------------------------+-----------------------------------+

The metrics are saved to the `metrics_source` dataset and are also
available in the `metrics_view` preset.

> **Note**

- > The max_delay metric is taken from the maximum bucket value with a
  > restricted limit; therefore, metrics show whole numbers.

- > The median and ninetieth_percentile metrics are statistical
  > calculations that give an approximation of the real value;
  > therefore, metrics show decimal numbers.

- > Time slots with a zero log count or zero byte count display records
  > with zero values. Subsequently, the data freshness metrics will also
  > have zero values.

- > Timezone differences between `_TIME` and `_INSERT_TIME` might cause
  > time skews with negative differences. Negative differences are
  > rounded to zero values.

#### About health issues

> **Prerequisite**
>
> For Cortex XSIAM to monitor data ingestion health and create health
> issues, you must enable the following settings under
> **Configurations**:

- > **Cortex - Analytics**: Go to Configurations \> Cortex - Analytics.
  > For more information, see [Enable the Analytics Engine and Identity
  > Analytics](#UUIDefaf1a2cc1ea98a077d09b52d3ed5cc4).

- > **Data Ingestion Monitoring**: Go to Configurations \> General \>
  > Server Settings \> Data Ingestion Monitoring. For more information,
  > see [Set up your
  > environment](#UUIDe99e556efcc7391bff6fddf7e17625cf).

Cortex XSIAM provides health issues to help you monitor the health and
integrity of supported Cortex XSIAM resources. Health issues provide
insights into health drifts, such as failure events or status changes.
The issues help you stay on top of your health related errors and ensure
optimal performance in Cortex XSIAM. In addition, you can set up
notifications on health issues.

Health issues are associated with the **Health Domain**. When setting up
notification forwarding or other configurations for health issues, use
the filter **Issue Domain = Health**.

To view health issues, go to Settings \> Health Issues, or on the
**Issues** page select the **Health Domain** table view. Click an issue
to see more details in the issue card, or right-click to take actions
and investigate an issue. For more information, see [Investigate and
resolve health issues](#UUIDbfa92d48c79e2e13f234bcc2a336b90c).

> **Note**
>
> The **Health Issues** page displays issues that were triggered after
> July 2024. To see health issues that were triggered before this date,
> click **Legacy Health Issues**.

**Types of health issues**

Cortex XSIAM provides the following types of OOTB health issues:

- **Ingestion issues:** Triggered by interruptions in data ingestion, or
  deviation from the calculated ingestion baseline

- **Collection issues:**: Triggered by connectivity errors in your
  collection integrations, custom collectors, and Marketplace
  integrations

- **Correlation issues:** Triggered by correlation rules that complete
  with an error status

> **Note**
>
> Cortex XSIAM enforces the dedup logic to health issues. This logic
> reduces the likelihood of identical health issues from flooding the
> issues dataset.

##### Query health issue data

Health issues are associated with the **Health** domain. To query health
issue data, use the following XQL:

    dataset = alerts | filter alert_domain = "DOMAIN_HEALTH"

##### Health issue field descriptions

The following table describes the health issue fields.

  -----------------------------------------------------------------------
  Field                               Description
  ----------------------------------- -----------------------------------
  Issue ID                            A unique identifier that Cortex
                                      XSIAM assigns to each issue.

  Issue Name                          Name of the issue.

  Issue Type                          Type of health issue.

  Issue Source                        Source of the issue.

  Broker VM ID                        ID of the Broker VM.

  Broker VM Name                      Host name of the Broker VM.

  Broker VM IP                        IP address of the Broker VM.

  Collector Name                      Name of the collector instance.

  Collector Type                      Type of the collector.

  Description                         Text summary of the event including
                                      the issue source, issue name, and
                                      severity.

  Device ID                           Firewall device ID.

  Excluded                            Whether the issue is excluded.

  External ID                         Issue ID as recorded in the
                                      detector from which this issue was
                                      sent.

  Final Reporting Device IP           IP of the device from which the log
                                      was extracted.

  Final Reporting Device Name         Hostname of the device from which
                                      the log was extracted.

  Ingestion Failure Duration          Amount of time that logs were not
                                      received or a drop in log ingestion
                                      was detected in minutes.

  Observation Time                    Time that the issue was observed in
                                      the system.

  Playbook                            Playbook that was run.

  Playbook run status                 Status of the playbook.

  Product                             Product name of the observing data
                                      source.

  Resolution Status                   Status that was assigned to this
                                      issue when it was triggered (or
                                      modified). Right-click an issue
                                      to change the status. If you set
                                      the status to **Resolved**, select
                                      a resolution reason.

  Reporting Device Name               Host name of the device where the
                                      log originated.

  Reporting Device IP                 IP Address of the device where the
                                      log originated.

  Severity                            Severity level that was assigned to
                                      this issue when it was triggered
                                      (or modified).

  Starred                             Whether the issue is starred by
                                      starring configuration.

  Vendor                              Vendor of the observing data
                                      source.

  XDR Collector ID                    ID of the XDR Collector.

  XDR Collector IP                    IP address of the XDR Collector.

  XDR Collector Name                  Host name of the XDR Collector.
  -----------------------------------------------------------------------

##### Investigate and resolve health issues

The following tasks explain how to investigate and resolve health
issues. You can see health issues on the following pages:

- Go to Settings \> Health Issues

- Go to Cases & Issues \> Issues and change the table view to
  **Health Domain**.

###### Investigate data ingestion errors

A data ingestion issue identifies disruption in the data ingestion
pipeline. For example, a data source is not sending logs, or there is a
significant drop in log collection compared to the calculated ingestion
baseline.

1.  Identify the error: **Type** = **Ingestion**.

2.  Right-click and select **Investigate in XQL query**.

- The **Query Builder** opens and runs a prefilled query to display
  related data ingestion metrics entries.

3.  Review the query results.

- The results provide context for the issue and the events leading up to
  it. For more information about data ingestion metrics and setting up
  correlation rules with your own data ingestion logic, see [Monitor
  data ingestion health](#UUID87fabbdc1cb768aac217c01716bb9832).

4.  Investigate data collector errors. Return to the **Health Issues**
    page, right-click the issue, and select Pivot to views \> View
    collector details.

- Depending on the type of collector in error, the relevant data
  collector settings page opens, filtered by data collector.

###### Investigate collection errors

A collection issue identifies connectivity disruption in your collection
integrations, custom collectors, and Marketplace integrations.

1.  Identify the error: **Type** = **Collection**.

2.  See the current status of the collector.

- Right-click and select Pivot to views \> View collector details.
  Depending on the type of collector in error, the relevant data
  collector settings page opens, filtered by data collector.

  If the data collector is still in error, you can update the collector
  settings as required.

3.  Investigate the collector error status.

- Run a query on the `collection_auditing` dataset to see all the
  connectivity changes of the collector over time, the escalation or
  recovery of the connectivity status, and the error, warning, and
  informational messages related to status changes.

  This example searches for status changes for the \"instance1\" data
  collector integration:

      dataset = collection_auditing 
      |filter collector_type = "STRATA_IOT" and instance = "instance1"

  For more information about troubleshooting collector errors and
  setting up correlation rules to trigger additional collection issues,
  see [Verify collector
  connectivity](#UUIDb7462bf72e1eb1f6c344359c5f7a23dd).

###### Investigate correlation errors

A correlation issue identifies errors in your correlation rules.

1.  Identify the error: **Type** = **Correlation**.

2.  Right-click and select **Investigate Correlation Auditing**.

- The **Query Builder** opens and runs a prefilled query to display
  related correlation execution records.

3.  Review the query results.

- Identify the correlation rule in error and take steps to resolve the
  error. For more information about how Cortex XSIAM identifies
  correlation rule errors, see [Monitor correlation
  rules](#UUIDd2907ab1cdc981c5632094dd1134849a).

##### Monitor data ingestion health

> **Prerequisite**
>
> For Cortex XSIAM to monitor data ingestion health and create health
> issues, you must enable **Data Ingestion Monitoring** in your
> **Server Settings**. For more information, see [Set up your
> environment](#UUIDe99e556efcc7391bff6fddf7e17625cf).

Cortex XSIAM collects granular data ingestion metrics that provide an
insight into the data ingestion pipeline, and identify disruptions in
data collection. With these metrics you can trace data collection from a
specific source, and see a breakdown by data source attributes such as
Collector Name and Final Reporting Device.

You can use these metrics in Cortex Query Language (XQL) queries to
investigate disruption and degradation in log collection. You can also
create correlation rules that use your own data ingestion logic to
trigger issues when disruption occurs for a specific data source within
a specific timeframe.

In addition, Cortex XSIAM has a built-in data ingestion monitoring and
issues mechanism that monitors the availability and overall health of
data ingestion in your environment, and triggers ingestion health issues
if disruptions occur.

**Related topics**

- [Overview of data ingestion
  metrics](#UUIDea34d22cafb5700f684dc6f52342e84d)

- [Creating correlation rules to monitor data ingestion
  health](#UUIDb99c429af373626305b0fef509c35967)

- [Measuring data freshness](#UUID4f09ceefaece55e33e0842b0de296007)

- [About health issues](#UUIDdfd48b778b41de7955182409372663e9)

##### Monitor correlation rules

Cortex XSIAM audits all correlation executions in the
`correlations_auditing` dataset. The dataset records the query
initiation times, end times, retry attempts, failure reasons, and other
useful metrics. You can use this dataset to monitor your correlation
executions. Cortex XSIAM also provides OOTB health issues that are
generated when a correlation rule completes with errors. For more
information, see [About health
issues](#UUIDdfd48b778b41de7955182409372663e9).

In the `correlations_auditing` dataset, audit entries are added as
follows:

- The rule starts executing. This is audited with the status of
  **Initiated** or **Initiated Manually**.

- The rule completes successfully. This is audited as **Completed**.

- The rule completes with errors. This is audited as **Error**.

> **Note**
>
> In the dataset, the **Query start time** and **Query end time**
> indicate the timeframe of the data that was queried. The actual start
> and end times of the correlation rule execution are recorded in the
> **\_time** field for the **Initiated** and **Completed** entries.

###### Field descriptions for the correlations_auditing dataset

The following table describes the fields in the correlations_auditing
dataset:

+-----------------------------------+-----------------------------------+
| Field                             | Description                       |
+===================================+===================================+
| \_time                            | Timestamp of the audit.           |
|                                   |                                   |
|                                   | For entries with an **Initiated** |
|                                   | or **Initiated Manually** status, |
|                                   | this is the start time of the     |
|                                   | correlation rule execution. For   |
|                                   | entries with a **Completed** or   |
|                                   | **Error** status, this is the end |
|                                   | time of the rule execution.       |
+-----------------------------------+-----------------------------------+
| \_id                              | Unique identifier of the audit    |
|                                   | entry.                            |
+-----------------------------------+-----------------------------------+
| Rule ID                           | Unique identification number for  |
|                                   | the correlation rule.             |
+-----------------------------------+-----------------------------------+
| Name                              | Correlation rule name.            |
+-----------------------------------+-----------------------------------+
| Status                            | The status of the correlation     |
|                                   | rule query.                       |
|                                   |                                   |
|                                   | Possible values are Initiated,    |
|                                   | Initiated Manually, Completed,    |
|                                   | and Error.                        |
+-----------------------------------+-----------------------------------+
| Query start time                  | The start time of the query       |
|                                   | timeframe.                        |
+-----------------------------------+-----------------------------------+
| Query end time                    | The end time of the query         |
|                                   | timeframe.                        |
+-----------------------------------+-----------------------------------+
| Time frame                        | Time frame for the query.         |
+-----------------------------------+-----------------------------------+
| Failure reason                    | For correlation rules with        |
|                                   | errors, this field displays the   |
|                                   | error message.                    |
+-----------------------------------+-----------------------------------+
| Retry attempts                    | Number of retry attempts before   |
|                                   | the query initiated or failed to  |
|                                   | run.                              |
+-----------------------------------+-----------------------------------+
| Schedule                          | Scheduled frequency to execute    |
|                                   | the correlation rule.             |
+-----------------------------------+-----------------------------------+
| Rule creation time                | Date and time that the            |
|                                   | correlation rule was created.     |
+-----------------------------------+-----------------------------------+
| Rule modification time            | Date and time that the            |
|                                   | correlation rule was last         |
|                                   | modified.                         |
+-----------------------------------+-----------------------------------+
| Description                       | Description of the correlation    |
|                                   | rule.                             |
+-----------------------------------+-----------------------------------+
| Severity                          | Defined severity of the           |
|                                   | correlation rule.                 |
+-----------------------------------+-----------------------------------+
| Dataset                           | Target data set, as defined in    |
|                                   | the correlation rule              |
+-----------------------------------+-----------------------------------+
| Suppression status                | Whether issue suppression is      |
|                                   | Enabled or Disabled.              |
+-----------------------------------+-----------------------------------+
| Suppression duration              | Duration for which to ignore      |
|                                   | additional events that match the  |
|                                   | issue suppression criteria.       |
+-----------------------------------+-----------------------------------+
| Suppression fields                | Fields on which the issue         |
|                                   | suppression is based.             |
+-----------------------------------+-----------------------------------+
| Timezone                          | Timezone on which the scheduled   |
|                                   | frequency is based.               |
+-----------------------------------+-----------------------------------+
| MITRE ATT&CK Tactic               | MITRE ATT&CK tactic that the      |
|                                   | correlation rule attempted to     |
|                                   | generate.                         |
+-----------------------------------+-----------------------------------+
| MITRE ATT&CK Technique            | MITRE ATT&CK technique that the   |
|                                   | correlation rule attempted to     |
|                                   | generate.                         |
+-----------------------------------+-----------------------------------+
| Issue category                    | Category of issue as configured   |
|                                   | when creating the rule.           |
+-----------------------------------+-----------------------------------+
| Source                            | Source of the correlation rule.   |
+-----------------------------------+-----------------------------------+
| XQL search                        | XQL query for the correlation     |
|                                   | rule.                             |
+-----------------------------------+-----------------------------------+
| Drill-down query                  | XQL query configured for further  |
|                                   | investigation.                    |
+-----------------------------------+-----------------------------------+
| Issue name                        | Name of the issue that the        |
|                                   | correlation rule will generate.   |
+-----------------------------------+-----------------------------------+

### Dataset management

The **Dataset Management** page enables you to manage your datasets and
understand your overall data storage duration for different retention
periods and datasets based on your hot and cold storage licenses, and
retention add-ons that extend your storage. You can view details about
your Cortex XSIAM licenses and retention add-ons by selecting Settings
\> Cortex XSIAM License. For more information on license retention and
the defaults provided per license, see [Data
retention](#UUID4236266de86558d967fd776625fd8b59).

> **Important**
>
> Cortex XSIAM enforces retention on all log-type datasets excluding
> Host Inventory, Vulnerability Assessment, Metrics, and Users.

#### Hot and cold storage

Your current hot and cold storage licenses, including the default
license retention and any additional retention add-ons to extend
storage, are listed within the **Hot Storage License** and
**Cold Storage License** sections of the **Dataset Management** page.
Whenever you extend your license retention, depending on your
requirements and license add-ons for both hot storage and cold storage,
the add-ons are listed.

> **Note**
>
> Cold storage, in addition to a cold storage license, requires compute
> units (CU) to run cold storage queries. For more information on CU,
> see [Manage compute units](#UUID6b9a9271d5433372d393491a1b72cacf).
>
> For information on the CU add-on license, see
> [/document/preview/859757#UUID-5baa60d9-cc4d-e656-4f53-4a3acb74a713](/document/preview/859757#UUID-5baa60d9-cc4d-e656-4f53-4a3acb74a713).

#### Additional hot storage

You can expand your license retention to include flexible Hot Storage
based retention to help accommodate varying storage requirements for
different retention periods and datasets. This add-on license is
available to purchase based on your storage requirements for a minimum
of 1,000 GB. If this license is purchased, an **Additional Storage**
subheading in the **Hot Storage License** section is displayed on the
**Dataset Management** page with a bar indicating how much of the
storage is used.

> **Note**
>
> Only datasets that are already handled as part of the GB license are
> supported for this license. In addition, the retention configuration
> is only available in Cortex XSIAM, as opposed to the public APIs.

#### Edit the retention plan

On any dataset configured to use Additional Hot Storage, you can edit
the retention period. This enables you to view the current retention
details for hot and cold storage and configure the retention. This
includes setting the amount of flexible hot storage-based retention
designated for a dataset and the priority for the dataset\'s hot
storage.

How to edit the retention plan

1.  Select Settings \> Configurations \> Data Management \> Dataset
    Management.

2.  In the **Datasets** table, right-click any dataset designated with
    flexible hot storage, and select **Edit Retention Plan**.

3.  Set the following parameters:

    - **Additional hot storage**: Set the amount of flexible hot
      storage-based retention designated for this dataset in months,
      where a month is calculated as 31 days.

    - **Hot Storage Priority**: Select the priority designated for this
      dataset\'s hot storage as either **Low**, **Medium**, or **High**.

4.  Click **Save**.

#### Datasets table

For each dataset listed in the table, the following information is
available:

> **Note**

- > Certain fields are exposed and hidden by default. An asterisk (\*)
  > is beside every field that is exposed by default.

- > Datasets include dataset permission enforcements in the Cortex Query
  > Language(XQL), Query Center, and XQL Widgets. For example, to view
  > or access any of the `endpoints` and `host_inventory` datasets, you
  > need role-based access control (RBAC) permissions to the
  > **Endpoint Administration** and **Host Inventory** views. Managed
  > Security Services Providers (MSSP) administration permissions are
  > not enforced on child tenants, but only on the MSSP tenant. 

+-----------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+
| Field                             | Description                                                                                                                               |
+===================================+===========================================================================================================================================+
| \*TYPE                            | Displays the type of dataset based on the method used to upload the data. The possible values include: Correlation, Lookup, Raw,          |
|                                   | Snapshot, System, and User. For more information on each dataset type, see                                                                |
|                                   | [/document/preview/952372#UUID-3ef0648c-1032-a887-27be-7c2264b6daf0](/document/preview/952372#UUID-3ef0648c-1032-a887-27be-7c2264b6daf0). |
+-----------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+
| \*LOG UPDATE TYPE                 | Event logs are updated either continuously (**Logs**) or the current state is updated periodically (**State**) as detailed in the         |
|                                   | **Last Updated** column.                                                                                                                  |
+-----------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+
| \*LAST UPDATED                    | Last time the data in the dataset logs were updated.                                                                                      |
|                                   |                                                                                                                                           |
|                                   | > **Important**                                                                                                                           |
|                                   | >                                                                                                                                         |
|                                   | > This column is updated once a day. Therefore, if the dataset was created or updated by the target or lookup flows, it\'s possible that  |
|                                   | > the **Last Updated** value is a day behind when the queries or reports were run as it was before this column was updated.               |
+-----------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+
| \*ADDITIONAL STORAGE              | Amount of flexible hot storage-based retention designated for this dataset in months, where a month is calculated as 31 days.             |
+-----------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+
| \*TOTAL DAYS STORED               | Actual number of days that the data is stored in the Cortex XSIAM tenant, which is comprised of the **HOT RANGE** + the **COLD RANGE**.   |
+-----------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+
| \*HOT RANGE                       | Details the exact period of the Hot Storage from the start date to the end date.                                                          |
+-----------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+
| \*COLD RANGE                      | Details the exact period of the Cold Storage from the start date to the end date.                                                         |
+-----------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+
| \*TOTAL SIZE STORED               | Actual size of the data that is stored in the Cortex XSIAM tenant. This number is dependent on the events stored in the hot storage. For  |
|                                   | the `xdr_data` dataset, where the first 31 days of storage are included with your license, the first 31 days are not included in the      |
|                                   | **TOTAL SIZE STORED** number.                                                                                                             |
+-----------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+
| \*ADDITIONAL SIZE STORED          | Actual size of the additional flexible hot storage data that is stored in the Cortex XSIAM tenant in GB. This number is dependent on the  |
|                                   | events stored in the hot storage.                                                                                                         |
+-----------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+
| \*AVERAGE DAILY SIZE              | Average daily amount stored in the Cortex XSIAM tenant. This number is dependent on the events stored in the hot storage.                 |
+-----------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+
| \*HOT STORAGE PRIORITY            | Indicates the priority set for the dataset\'s hot storage as either **Low**, **Medium**, or **High**.                                     |
+-----------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+
| \*TOTAL EVENTS                    | Number of total events/logs that are stored in the Cortex XSIAM tenant. This number is dependent on the events stored in the hot storage. |
+-----------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+
| \*AVERAGE EVENT SIZE              | Average size of a single event in the dataset (**TOTAL SIZE STORED** divided by the **TOTAL EVENTS**). This number is dependent on the    |
|                                   | events stored in the hot storage.                                                                                                         |
+-----------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+
| \*TTL                             | For lookup datasets, displays the value of the time to live (TTL) configured for when lookup entries expire and are removed automatically |
|                                   | from the dataset. The possible values are:                                                                                                |
|                                   |                                                                                                                                           |
|                                   | - **Forever**: Lookup entries never expire (default).                                                                                     |
|                                   |                                                                                                                                           |
|                                   | - **Custom**: Lookup entries expire according to a set number of days, hours, and minutes. The maximum number of days is 99999.           |
|                                   |                                                                                                                                           |
|                                   | For more information, see [Set time to live for lookup datasets](#UUID78b8201b3921c75be0ca73208d027395).                                  |
+-----------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+
| DEFAULT QUERY TARGET              | Details whether the dataset is configured to use as your default query target in XQL Search, so when you write your queries you do not    |
|                                   | need to define a dataset. By default, only the `xdr_data` dataset is configured as the **DEFAULT QUERY TARGET** and this field is set to  |
|                                   | **Yes**. All other datasets have this field set to **No**. When setting multiple default datasets, your query does not need to mention    |
|                                   | any of the dataset names, and Cortex XSIAM queries the default datasets using a `join`.                                                   |
+-----------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+
| TOTAL HOT RETENTION               | Total hot storage retention configured for the dataset in months, where a month is calculated as 31 days.                                 |
+-----------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+
| TOTAL COLD RETENTION              | Total cold storage retention configured for the dataset in months, where a month is calculated as 31 days.                                |
+-----------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+

#### Dataset views

Cortex XSIAM supports creating dataset views in the `Dataset Management`
page to enhance data efficiency and security. Dataset views provide a
virtual representation of data from one or more datasets, based on the
Cortex Query Language (XQL) query defined, and provide multiple
benefits, such as joining datasets into logical subsets through defined
queries, manipulating data without altering underlying datasets, and
segregating data for specific user needs or access privileges through
the Role-based access control (RBAC) settings.

Once a dataset view is created, you can edit or delete the dataset view
by right-clicking the dataset view in the **Dataset Views** table. A
dataset view can only be deleted if there are no other dependencies. For
example, if a Correlation Rule is based on a dataset view, you wouldn\'t
be able to delete the dataset view until you removed the dataset view
from the XQL query of the Correlation Rule.

Cortex XSIAM logs entries for events related to creating, editing, and
deleting datasets or dataset views. These monitored activities are
available to view in the datasets and dataset views audit logs in the
Management Audit Logs. For more information, see [Monitor datasets and
dataset views activity](#UUID1e95ea1a051e5bc15c03681575585b90).

Building XQL dataset view queries

When building an XQL query to define a dataset view, the query is built
in the same way as creating a query through the Query Builder. Yet,
it\'s important to be aware of the following points that are specific
for dataset view queries:

- The following features are unsupported in dataset view queries:

  - RT Correlation Rules

  - Cortex Data Model (XDM)

  - Query Library

  - Presets

  - Cold storage queries (`cold_dataset = <dataset name>`)

- Only the following XQL stages are supported when building a dataset
  view query:

  - `alter`

  - `dedup`

  - `fields`

  - `filter`

  - `join`

  - `replacenull`

  - `union`

- Once the dataset view is created, it is listed as an available
  `dataset` when building your XQL queries as long as you have the
  necessary permissions to access the dataset view in the Role-based
  access control (RBAC) settings.

How to create a dataset view

1.  Select Settings \> Configurations \> Data Management \> Dataset
    Management \> Dataset Views.

2.  Click **New Datset View**.

3.  Enter a **Name** and **Description** (optional) for the dataset
    view.

4.  Create your XQL query for the dataset view by typing in the query
    box.

5.  (Optional) Click **Run** to view the query results.

- The query must contain no errors, including using only supported
  commands, to run; otherwise, the **Run** button remain disabled.

6.  Click **Save**.

- > **Note**

  > You\'ll only be able to save the dataset view if the query contains
  > no errors; otherwise, the **Save** button is disabled.

  Once the dataset view is created, you can now control user access
  permissions through Role-based access control (RBAC).

Dataset views access permissions

> **Note**
>
> Managing Roles requires an Account Admin or Instance Administrator
> role. For more information, see [Predefined user
> roles](#UUIDc0966cb32b3c88e214d33131de93fa8a).

Access permissions for dataset views are configured in the same way that
you set dataset access permissions for any dataset through user roles in
Cortex XSIAM **Access Management**. Cortex XSIAM uses role-based access
control (RBAC) to manage roles with specific permissions for controlling
user access. RBAC helps manage access to Cortex XSIAM components and
datasets, so that users, based on their roles, are granted minimal
access required to accomplish their tasks. Once the user role is
configured to access these dataset views, you can now assign the user
role to the designated users or user groups, who you want to access
these dataset views.

**How to set access permissions for dataset views**

1.  Select Settings \> Configurations \> Access Management.

2.  Configure a user role with the dataset views that you want users to
    access.

    a.  Select **Roles**.

    b.  You can perform one of the following:

        - To create a new role to assign the dataset views, click
          **New Role**, and set a **Role Name** and **Description**
          (optional).

        - To edit an existing user role with these dataset views,
          right-click the relevant user role, and select **Edit Role**.

        - To create a new role based on an existing role, right-click
          the relevant user role, select **Save As New Role**, and set a
          **Role Name** and **Description** (optional).

    c.  Under **Datasets**, you have two options for setting the Cortex
        Query Language (XQL) dataset access permissions for the user
        role:

        - Set the user role with access to all XQL datasets by disabling
          the **Enable dataset access management** toggle.

        - Set the user role with limited access to certain XQL datasets
          by selecting the **Enable dataset access management** toggle
          and selecting the datasets under the different dataset
          category headings.

    d.  Scroll down to **Dataset View** and select the particular
        dataset views that you want assigned to this user role.

    e.  Click **Save**.

- > **Note**

  > For more information on user roles, see [Manage user
  > roles](#UUID751d26ed9390ddddd4f6bb1f20db3a1d).

3.  Assign the user role with the dataset views configured to the
    designated users or user groups. For more information, see [Assign a
    user to a role](#UUIDc0966cb32b3c88e214d33131de93fa8a).

Dataset Views table

For each dataset view listed in the table, information is available.
Here are descriptions on the columns that may require further
explanation:

  -----------------------------------------------------------------------
  Field                               Description
  ----------------------------------- -----------------------------------
  SOURCE QUERY                        Displays the query used to create
                                      the dataset view.

  IS VALID                            Details whether the query for the
                                      dataset view is still valid or not.

  RELATED TABLES                      Details the other datasets that are
                                      related to this dataset view.
  -----------------------------------------------------------------------

#### What are datasets?

Cortex XSIAM runs every Cortex Query Language (XQL) query against a
dataset. A dataset is a collection of column:value sets. If you do not
specify a dataset in your query, Cortex XSIAM runs the query against the
default datasets configured, which is by default `xdr_data` for a
dataset query. The `xdr_data` dataset contains all of the endpoint and
network data that Cortex XSIAM collects. For a Cortex Data Model (XDM)
query, unless specific datasets are specified, a query will run against
all mapped datasets. You can always change the default datasets using
the set to default option. You can also upload datasets as a CSV, TSV,
or JSON file that contains the data you are interested in querying.
These uploaded datasets are called lookup datasets.

It\'s also possible to create dataset views, which provide a virtual
representation of data from one or more datasets, based on the Cortex
Query Language (XQL) query defined. Dataset views enhance data
efficiency and security. For example, by segregating data for specific
user needs or access privileges through the Role-based access control
(RBAC) settings. For more information, see [Dataset
views](/document/preview/952274#UUID-ae82030e-2493-a33b-9a9e-a9834e993e93).

To query other datasets, you have the following options:

- Set a dataset as default, which enables you to query the datasets
  without specifying them in the query.

- Name a specific dataset at the beginning of your query with the
  `dataset` stage command.

Dataset types

The type of dataset is based on the method used to upload the data. The
possible types include:

- **Correlation**: A dataset containing data saved from a correlation
  rule.

- **Lookup**: A dataset containing key-value pairs that can be used as a
  reference to correlate to events. For example, a user list with
  corresponding access privileges. You can import or create a lookup
  dataset, and then reference the values for a certain key, run queries
  and take action. For more information, see [Lookup
  datasets](#UUID107aa61b95edf7a5a46e0e9521d891c4).

- **Raw**: Every dataset where PANW data is ingested out-of-the-box or
  third-party data is ingested using a configured dedicated collector.

- **Snapshot**: A dataset that contains only the last successful
  snapshot of the data, such as Workday or ServiceNow CMDB tables.

- **System**: Cortex XSIAM datasets that are created out-of-the-box.

- **User**: If saved by a query using the `target` command, the **Type**
  can be either **User** or **Lookup**.

Datasets in XQL

> **Important**
>
> By default, forensic datasets are not included in XQL query results,
> unless the dataset query is explicitly defined to use a forensic
> dataset.

Cortex Query Language (XQL) supports using different languages for
dataset and field names. In addition, when setting up your XQL query, it
is important to keep in mind the following:

- The dataset formats supported are dependent on the data retention
  offerings available in Cortex XSIAM according to whether you want to
  query hot storage or cold storage.

  - Hot Storage queries are performed on a dataset using the format
    `dataset = <dataset name>`. This is the default option.

  <!-- -->

  - dataset = xdr_data

  <!-- -->

  - Cold Storage queries are performed using the format
    `cold_dataset = <dataset name>`.

  <!-- -->

  - cold_dataset = xdr_data

- The refresh times for datasets, where all Cortex XSIAM system
  datasets, which are created out-of-the-box, are continuously ingested
  in near real-time as the data comes in, except for the following:

  - `endpoints`: Refreshed every hour.

  - `pan_dss_raw`: Refreshed daily.

  - Forensics datasets: The Forensics data is not configured to be
    updated by default. When you enable a collection in the
    **Agent Settings** profile, the data is collected only once unless
    you specify an interval. If you specify an interval, the data is
    collected every `<interval>` number of hours with the minimum being
    12.

- Query against a dataset by selecting it with the `dataset` command
  when you create an XQL query. For more information, see [Create XQL
  query](#UUID5f5b967337c3cb75a413d47e8b461681).

- After your query runs, you can always save your query results as a
  dataset. You can use the
  [target](#UUID2633c59ff09cbb66a58380019f45d292) stage command to save
  query results as a dataset.

- Schema changes to datasets may not be reflected in the autocomplete
  suggestions and deﬁnitions as you type in real time the XQL query and
  can appear with a slight delay.

##### Managing datasets and dataset views

You can manage your datasets and dataset views in Cortex XSIAM from the
Settings \> Configurations \> Data Management \> Dataset Management
page.

Below are some of the main tasks available for all dataset types by
right-clicking a particular dataset or dataset view listed in either the
**Datasets** or **Dataset Views** table. Only tasks that need further
explanation are explained below. Datasets and dataset views can only be
deleted if there are no other dependencies. For example, if a
Correlation Rule is based on a dataset or dataset view or dataset view,
you wouldn\'t be able to delete the dataset or dataset view until you
removed the dataset view from the XQL query of the Correlation Rule.

> **Note**
>
> For more information on tasks specific to lookup datasets, see [Lookup
> datasets](#UUID107aa61b95edf7a5a46e0e9521d891c4).

View Schema

Select **View Schema** to view the schema information for every field
found in the dataset or dataset view result set in the **Schema** tab
after running the query in XQL. Each system field in the schema is
written with an underscore (`_`) before the name of the field in the
**FIELD NAME** column in the table.

> **Note**
>
> Schema changes to datasets may not be reflected in the autocomplete
> suggestions and deﬁnitions as you type in real time the XQL query and
> can appear with a slight delay.

Set as default

Select **Set as default** to query the dataset without having to specify
it in your queries in XQL by typing `dataset = <name of dataset>`. Once
configured, the **DEFAULT QUERY TARGET** column entry for this dataset
is set to **Yes** in the **Datasets** table. By default, this option is
not available when right-clicking the `xdr_data` dataset as this dataset
is the only dataset configured as the **DEFAULT QUERY TARGET** as it
contains all of the endpoint and network data that Cortex XSIAM
collects. Once you **Set as default** another dataset, you can always
remove it by right-clicking the dataset and selecting
**Remove from defaults**. When setting multiple default datasets, your
query does not need to mention any of the dataset names, and Cortex
XSIAM queries the default datasets using a `join`. This option is only
relevant for datasets.

Copy text to clipboard

Select **Copy text to clipboard** to copy the name of the dataset or
dataset view to your clipboard.

#### Lookup datasets

Lookup datasets enable you to correlate data from a data source you
provide with the events in your environment. For example, you can create
a lookup with a list of high-value assets, terminated employees, or
service accounts in your environment. Use lookups in your search,
detection rules, threat hunting, and response playbooks. Lookups are
stored as name-value pairs and are cached for optimal query performance
and low latency.

Lookup tables support low frequency changes of up to 1200 modifications
per day. Changes are implemented whenever a lookup dataset is edited,
where only one person or user can edit the file at a given time.
Concurrent users editing the file are not supported.

Use case scenarios

- Investigate threats and respond to cases quickly with the rapid import
  of IP addresses, file hashes, and other data from CSV files. After you
  import the data, use lookup name-value pairs for joins and filters in
  threat hunting and general queries.

- Import business data as a lookup. For example, import user lists with
  privileged system access, or terminated employees. Then, use the
  lookup to create allow lists and blocklists to detect or prevent those
  users from logging in to the network.

- Create allow lists to suppress issues from a group of users, such as
  users from authorized IP addresses that perform tasks that would
  normally trigger the issue. Prevent benign events from becoming
  issues.

- Enrich event data. Use lookups to enrich your event data with
  name-value combinations derived from external data sources.

How are lookup datasets created?

You can import or create a lookup dataset, and then reference the values
for a certain key, run queries and take action. Lookup datasets are
created by any of the following methods:

- Manual upload from a CSV, TSV, or JSON file to Cortex XSIAM from the
  **Dataset Management** page. For more information, see [Import a
  lookup dataset](#UUID67d9ed4151a0dd37572f431131c526f5).

- Automatic upload by the Files and Folders Collector.

- Query results are saved to a lookup dataset. If saved using
  the `target` stage, the **Type** can be either **User** or **Lookup**.
  For more information, see the `target` stage in the XQL Language
  Reference Guide.

After a lookup a dataset is imported, you can always edit the dataset to
update the data manually by right-clicking the dataset and selecting
**Edit**.

> **Note**
>
> A lookup dataset can only be deleted if there are no other
> dependencies. For example, if a Correlation Rule is based on a lookup
> dataset, you wouldn\'t be able to delete the lookup dataset until you
> removed the dataset from the XQL query of the Correlation Rule.

##### Import a lookup dataset

You can import data from CSV, TSV, or JSON files into Cortex XSIAM to
create or update lookup datasets.

> **Prerequisite**
>
> When uploading a CSV, TSV, or JSON file, ensure that the file meets
> the following requirements:

- > The maximum size for the total data to be imported into a lookup
  > dataset is 30 MB from the **Dataset Management** page. Otherwise,
  > the limit is 50 MB using Cortex Query Language (XQL) or APIs.

- > Field names can contain characters from different languages, special
  > characters, numbers (`0-9`), and underscores (`_`).

- > Field names can\'t exceed 128 characters.

- > Field names can\'t contain duplicate names, white spaces, or
  > carriage returns.

- > The file doesn\'t contain a byte array (binary data) as it can\'t be
  > uploaded.

- > Each line in the JSON file must represent one JSON object. Ensure no
  > brackets enclose the objects at the top-level.

Here\'s an example of a JSON file in the correct format for upload:

    {"firstName": "NAME_1", "SurName": "NAME_11", "employeeID": {"id": "ID_AAAAA_2"}}
    {"firstName": "NAME_2", "SurName": "NAME_22", "employeeID": {"id": "ID_AAAAA_3"}}
    {"firstName": "NAME_3", "SurName": "NAME_32", "employeeID": {"id": "ID_AAAAA_4"}}

1.  Select Settings \> Configurations \> Data Management \> Dataset
    Management \> + Lookup.

2.  Browse to your CSV, TSV, or JSON file. You can only upload a TSV
    file if it contains a `.tsv` file extension.

3.  (Optional) Under **Name**, type a new name for the target dataset.

- By default, Cortex XSIAM uses the name of the original file as the
  dataset name. You can change this name to something that will be more
  meaningful for your users when they query the dataset. For example, if
  the original file name is mrkdptusrsnov23.json, you can save the
  dataset as marketing_dept_users_Nov_2023.

  Dataset names can contain special characters from different languages,
  numbers (`0-9`) and underscores (`_`). You can create dataset names
  using uppercase characters, but in queries, dataset names are always
  treated as if they are lowercase.

  > **Important**

  > The name of a dataset created from a TSV file must always include
  > the extension. For example, if the original file name is
  > `mrkdptusrsnov23.tsv`, you can save the dataset with the name
  > `marketing_dept_users_Nov_2023.tsv`.

4.  **Replace the existing data in the dataset** overwrites the data in
    an existing lookup dataset with the contents of the new file.

5.  Click **Add** to add the file as a lookup.

6.  After receiving a notification reporting that the upload succeeded,
    **Refresh** ![](media/rId5949.png){width="0.18382327209098862in"
    height="0.20833333333333334in"} to view it in your list of datasets.

- If the upload fails for any reason, you\'ll receive a notification in
  the Notification Center.

##### Download JSON file of lookup dataset

You can only download a JSON file for a lookup dataset, where the
**Type** set to **Lookup** on the **Dataset Management** page. This
option is not available for any other dataset type.

When you download a lookup dataset with field names in a foreign
language, the downloaded JSON file displays the fields as
`COL_<randomstring>` as opposed to returning the fields in the foreign
language as expected.

1.  Open the Settings \> Configurations \> Data Management \> Dataset
    Management page.

2.  In the **Datasets** table, right-click the lookup dataset that you
    want to download as a JSON file, and select **Download**.

##### Set time to live for lookup datasets

You can specify when lookup entries expire and are removed automatically
from the lookup dataset by configuring the time to live (TTL). The time
period of the TTL interval is based on when the data was last updated.
The default is forever and the entries never expire. You can also
configure a specific time according to the days, hours, and minutes.
Expired elements are removed from the lookup dataset by a scheduled job
that runs every five minutes.

1.  Open the Settings \> Configurations \> Data Management \> Dataset
    Management page.

2.  In the **Datasets** table, right-click the lookup dataset, and
    select **Set TTL**.

3.  Select one of the following to configure when lookup dataset entries
    expire and are removed:

    - **Forever**: Lookup entries never expire (default).

    - **Custom**: Lookup entries expire according to a set number of
      days, hours, and minutes. The maximum number of days is 99999.

4.  Click **Save**.

- The **TTL** column in the **Datasets** table is updated with the
  changes and these changes are applied immediately on all existing
  lookup entries.

#### Monitor datasets and dataset views activity

Cortex XSIAM logs entries for events related to datasets and dataset
views monitored activities. Cortex XSIAM stores the logs for 365 days.
To view the datasets and dataset views audit logs, select Settings \>
Management Audit Logs.

You can customize your view of the logs by adding or removing filters to
the **Management Audit Logs** table. You can also filter the page result
to narrow down your search. The following table describes the default
and optional fields that you can view in the Cortex XSIAM
**Management Audit Logs** table:

> **Note**
>
> Certain fields are exposed and hidden by default. An asterisk (\*) is
> beside every field that is exposed by default.

+-----------------------------------+-----------------------------------+
| Field                             | Description                       |
+===================================+===================================+
| Description\*                     | Log message that describes the    |
|                                   | action.                           |
+-----------------------------------+-----------------------------------+
| Email                             | Email of the user who performed   |
|                                   | the action.                       |
+-----------------------------------+-----------------------------------+
| Host Name\*                       | This field is not applicable for  |
|                                   | datasets and dataset views logs.  |
+-----------------------------------+-----------------------------------+
| ID                                | Unique ID of the action.          |
+-----------------------------------+-----------------------------------+
| Reason                            | This field is not applicable for  |
|                                   | datasets and dataset views logs.  |
+-----------------------------------+-----------------------------------+
| Result\*                          | The result of the action (        |
|                                   | `Success`, `Fail`, or `N/A`)      |
+-----------------------------------+-----------------------------------+
| Severity\*                        | Severity associated with the log: |
|                                   |                                   |
|                                   | - `Critical`                      |
|                                   |                                   |
|                                   | - `High`                          |
|                                   |                                   |
|                                   | - `Medium`                        |
|                                   |                                   |
|                                   | - `Low`                           |
|                                   |                                   |
|                                   | - `Informational`                 |
+-----------------------------------+-----------------------------------+
| Timestamp\*                       | Date and time when the action     |
|                                   | occurred.                         |
+-----------------------------------+-----------------------------------+
| Type\* and Sub-Type\*             | Additional classifications of     |
|                                   | dataset and dataset view logs     |
|                                   | (Type and Sub-Type):              |
|                                   |                                   |
|                                   | - **Datasets**:                   |
|                                   |                                   |
|                                   |   - Create Dataset                |
|                                   |                                   |
|                                   |   - Delete Dataset                |
|                                   |                                   |
|                                   |   - Update Dataset                |
|                                   |                                   |
|                                   | - **Dataset Views**:              |
|                                   |                                   |
|                                   |   - Create Dataset View           |
|                                   |                                   |
|                                   |   - Delete Dataset View           |
|                                   |                                   |
|                                   |   - Update Dataset View           |
+-----------------------------------+-----------------------------------+
| User Name\*                       | Name of the user who performed    |
|                                   | the action.                       |
+-----------------------------------+-----------------------------------+

### Archived data

Cortex XSIAM enables you to import historical data into cold storage as
archived data. Learn how to send the data in the recommended format, so
that you can import the data to access and search the data for analysis,
compliance, and audits.

#### Import historical data into cold storage

> **Note**
>
> Importing historical data into cold storage requires a Period-Based
> Retention - Cold Storage add-on license.
>
> **Prerequisite**
>
> Importing historical data into cold storage requires a **View/Edit**
> RBAC permission for **Data Management** (under **Configurations**).

Importing historical data into Cortex XSIAM cold storage is a detailed
process made up of different phases as illustrated in the image below.
This dedicated process is available for data migration to ensure secure,
long-term storage. Some phases, such as the Data Extraction phase and
Data Preparation phase, are not performed in Cortex XSIAM, and are the
customer's responsibility to complete before the data is ready to be
sent to Cortex XSIAM. It is critical that the data is sent in the
recommended format to be able to access and search the data for
analysis, compliance, and audits.

Each data source that is imported to Cortex XSIAM is available as a cold
storage dataset and can be accessed using the Cortex Query Language
(XQL). These datasets are a new type of archived dataset in cold
storage, and after the import, are renamed using the format
`archive_<dataset name>`.

The entire import process requires sufficient time and planning as it
takes time to extract data from your third party sources, prepare the
data according to your requirements, send the files to the HTTP
collector, and import the data into cold storage. There are also
additional limitations due to your retention licenses for cold and hot
storage, and how the HTTP collector is configured to work. We highly
recommend that you review carefully the different phases explained
below, so you can make the best decisions to access and analyze your
data from cold storage.

##### Initial planning and prerequisites before data is ready to import

The import process requires initial planning and data preparation before
the data is ready to import in Cortex XSIAM. This process can take time,
so ensure to factor this into your import timeline.

The sections below explain the various phases in the import process
along with the necessary preparation and prerequisite guidelines, so you
understand what is required at each phase before your data is ready to
be imported. Review these initial guidelines carefully, so that your
data is imported successfully, and meets your expectations when you
choose to query the data in cold storage.

These phases are listed in a suggested order for importing your data in
cold storage. Some phases can overlap or are interchangeable according
to your preferences. This is explained more in each phase.

![](media/rId5958.png){width="5.833333333333333in"
height="3.0697911198600174in"}

###### Phase 1

**Data extraction**

**Reason to perform**: Initial prerequisite task for you to import any
data into Cortex XSIAM.

**Where is this performed**: You must perform this task on a system.
This phase is not performed in Cortex XSIAM.

**Description**: To import data into Cortex XSIAM, you need to extract
the data from a system. It's your responsibility to perform this task
according to your vendor's specifications. Since extracting data can
involve multiple sources of different schemas (called datasets in Cortex
XSIAM), you'll need to bring each one individually into Cortex XSIAM.

**Understand the process**:

Typically there are two approaches to data extraction depending on the
system:

- Querying the system: Involves querying the system for the data and
  storing the output results.

- Bulk extraction: In some systems, there are bulk extraction
  capabilities that you can leverage to extract the data.

**Preparation and prerequisite guidelines**:

- **How to decide which data extraction method to use?** Both approaches
  to extraction require the data to be prepared before the data can be
  sent to the HTTP collector. It's important to carefully review the
  Data preparation phase to decide which of the two approaches to
  implement to extract your data. There are tradeoffs to each extraction
  method that you choose.

  - **Example 1**: If you need to extract a lot of data, it could be
    that bulk extraction is the more suitable method. In contrast, if
    your data is relatively small, querying the system may make more
    sense.

  - **Example 2**: It's possible that the bulk extraction capabilities
    outputs the data quicker. Yet, it may require more data engineering
    effort to prepare the data after it's extracted. In this case, you
    may decide to go with querying the system as it can be the more
    efficient option.

- Understand all Cortex XSIAM requirements in the context of preparing
  the data to influence your extraction decision.

- The Cortex XSIAM retention licenses for hot and cold storage limit the
  data that can be imported. You have to send data within the license
  retention period for hot and cold storage, so ensure to only extract
  data within this time period.

###### Phase 2

**Data preparation**

**Reason to perform**: Initial prerequisite task for you to import
historical data into Cortex XSIAM.

**Where is this performed**: You must perform this task on all data
extracted from the third party systems in phase 1. This phase is not
performed in Cortex XSIAM.

**Description**: Prepare the extracted data into manageable chunks that
can be sent to the HTTP collector and meets Cortex XSIAM requirements
for sending data to the HTTP collector. This phase requires you to make
decisions about your data by carefully reviewing the guidelines
provided, so you\'ll be able to access the data according to your
requirements in cold storage.

**Understand the output of the process**:

- Separate data according to the data source / schema (dataset).

- For each separated source, separate by time periods (date).

- For each separated time period, separate the data into files, where no
  file can be more than 25 MB.

- Separate records with a new line.

- Files sent to the HTTP collector must be uncompressed.

**Preparation and prerequisite guidelines**:

Before dividing up the data, consider the following points to determine
how best to prepare the data before sending it to the HTTP collector:

- **Data format**: The format of the data and how the data is prepared
  impacts your ability to query the data in Cortex XSIAM. You can send
  the data in any format to cold storage. Yet, querying some data
  formats from cold storage can be difficult. If you plan on querying
  the data and analyzing it in XQL, we recommend that you transform the
  data to a JSON format and parse it. If not, the data will be stored as
  is in cold storage. Within the JSON file, you can store the raw log
  and/or parse the data. When importing JSON files, they are parsed and
  brought into cold storage, so instead of having a table with a single
  column, you\'ll have a table of `n` columns according to the dataset
  schema.

- **Raw format**: You can prepare the data to include the raw format of
  the raw log if it\'s important for you to hold on to the original raw
  log format. The raw format of the raw log for data sent to cold
  storage is stored in a unique column called `RAW_FORMAT`. If you want
  to save on import capacity and don\'t want to send a lot of data, you
  don\'t have to leave the raw format. This data is only for you to
  query using cold storage. The data is not used for any other purposes,
  such as detection or analysis. We recommend converting the data into a
  JSON and parsing it. If not, the data will be stored as is in cold
  storage.

- **File size**: No file can be more than 25 MB.

- Files sent to the HTTP collector must be uncompressed.

###### Phase 3

**HTTP POST requests preparation**

**Reason to perform**: Prerequisite task for you to import data into
Cortex XSIAM.

**Where is this performed**: You must create the HTTP POST requests,
which is performed outside Cortex XSIAM.

**Description**: Prepare an HTTP POST request for each file that needs
to be sent (as explained in Phase 2). Each request sent to the HTTP
collector must contain the appropriate headers for the dataset and date
as well as conform to a specific format. You will need to use the API
URL and generated key that is available when you enable the HTTP
collector connection (as explained in Phase 4).

**Understand the process**:

For examples on how to prepare the HTTP POST requests, see [Task 2. Send
data to your Cortex
XSIAM HTTP collector](#X8ec5f0d79093ac5052c96e19e66f381aba6f0f9). Yet,
you can only retrieve all the information necessary to complete your
HTTP POST request, when you enable the HTTP collector connection as
explained in Phase 4.

**Preparation and prerequisite guidelines**:

- When sending files to the HTTP collector, the header of the HTTP
  request must include the following tags / headers to store the data
  correctly:

  - **Dataset name**: A valid dataset name is alphanumeric with an
    option to use underscores (`_`) to concatenate multiple names using
    the format `<dataset name1>_<dataset name2>_...`. This is added in
    the header using the `x-cortex-source-dataset` parameter.

  - **Date**: Use the format `YYYY-MM-DD`. This is added in the header
    using the `x-cortex-partition` parameter.

- **What determines whether data is found when you query it?** These
  headers determine how this data will be accessible through querying
  cold storage. For example, the date that you put in the header of the
  HTTP request and send to the HTTP collector impacts how you can query
  the data. If you put the date that you sent the data in the header,
  but the data is really for a different day, when you search cold
  storage you won\'t find the data. The date in the header is the date
  used for the data in cold storage.

- **How is the data sent?** The headers are critical to send data to
  cold storage and alignment of the data is important. If the headers in
  the HTTP POST request sent to the HTTP collector are valid, the data
  is accepted. As a result, you need to ensure the data is set up
  correctly. For example, if you send different data with the same
  headers or the same source with the same date, you\'ll create a
  problem that you\'ll have data of different schemas, potentially of
  different formats, for the same day and if you query the data in XQL
  it will be difficult to understand the output.

###### Phase 4

**Enable the HTTP collector connection**

**Reason to perform**: Prerequisite task for you to perform in Cortex
XSIAM to retrieve the required HTTP collector settings to define in the
HTTP POST requests (as explained in Phase 3), and send these requests
(as explained in Phase 5).

**Where is this performed**: You must perform this task in Cortex XSIAM
to prepare the HTTP POST requests and be able to send the requests to
the HTTP collector (as explained in Phase 5).

**Description**: Data is sent to Cortex XSIAM in an HTTP request using
the dedicated API of the HTTP collector when the HTTP connection is
enabled and a key is generated. You\'ll need to use the API URL and the
generated key to prepare your HTTP POST requests.

**Understand the process**:

See [Task 1. Enable the HTTP collector connection in Cortex
XSIAM](#X3c59b389f55fea119024805c269ada6814cf278).

Once the HTTP collector connection is enabled, you can finish defining
the HTTP POST requests as explained in [Task 2. Send data to your Cortex
XSIAM HTTP collector](#X8ec5f0d79093ac5052c96e19e66f381aba6f0f9).

**Preparation and prerequisite guidelines**:

The HTTP collector connection is automatically disabled if not used for
14 days. If disabled, you will need to generate a new key for your HTTP
POST requests.

###### Phase 5

**Send data to HTTP collector through the HTTP requests**

**Reason to perform**: Prerequisite task for you to import data in
Cortex XSIAM.

**Where is this performed**: You must send all the HTTP POST requests
that you\'ve created in phase 3.

**Description**: Send the files through the HTTP POST requests to the
HTTP collector. Every POST request sent is answered with a notification
to indicate if the request to upload the data was successful or not. If
there is an issue, the request can fail and there are different error
messages provided to help troubleshoot.

**Understand the process**:

For details on how to send the HTTP POST requests, see [Task 2. Send
data to your Cortex
XSIAM HTTP collector](#X8ec5f0d79093ac5052c96e19e66f381aba6f0f9).

**Preparation and prerequisite guidelines**:

- Files uploaded by the HTTP collector must be uncompressed.

- Maximum file size limit is 25 MB for uploading data by the HTTP
  collector.

- HTTP request headers must include the dataset name and date as
  explained in the step above.

- Daily upload limit of 100,000 files sent to the HTTP Collector.

- Total daily upload capacity for sending files to the HTTP collector is
  based on the formula: `100 * (Daily GB License)`.

- Total upload capacity for sending files to the HTTP Collector is
  related to the retention license using the formula:
  `(# of months of hot + cold storage) * 30 * (Daily ingest limit)`,
  where 30 represents the number of days in a month.

###### Phase 6

**Import the files in Cortex XSIAM**

**Reason to perform**: Final task performed in Cortex XSIAM.

**Where is this performed**: You must perform this task in Cortex XSIAM.

**Description**: After you validate that all the files sent to Cortex
XSIAM are listed in the **Remote Files** table in the **Archived Data**
page with a **Remote** status, you can now import the dataset files into
cold storage. Once imported, the files are no longer available in the
**Remote Files** table. They are now accessible in cold storage as
archived datasets and are listed in the **Dataset Management** page.

**Understand the process**:

See [Task 3. Import the dataset files to cold
storage](#Xc858a4c537feda4f653e232925280aba824ccf7).

**Preparation and prerequisite guidelines**:

- You can import multiple remote files at once, which can take time, up
  to several days, to complete. A notification is sent once the import
  is completed.

##### How to import data

Perform the following procedures in the order listed below.

Task 1. Enable the HTTP collector connection in Cortex XSIAM

1.  Select Settings \> Configurations \> Data Management \> Archived
    Data.

2.  Open the HTTP collector settings by clicking **HTTP Collector**.

3.  Select the **Enable HTTP connection** toggle.

4.  Copy the API URL.

- Click the copy icon beside the API URL displayed and record it
  somewhere safe. You\'ll use this URL when you configure your HTTP POST
  request to send your data to the HTTP collector.

5.  Click **Generate Key**.

- Next to the key displayed, in the **Generated Key** dialog box, click
  the copy icon and record it somewhere safe. You will need to provide
  this key when you configure your HTTP POST request and define
  the **Authorization** key. If you forget to record the key and close
  the window, you will need to generate a new key and repeat this
  process. The HTTP connection is only established after a key is
  generated.

  Click **Close** when finished.

Task 2. Send data to your Cortex XSIAM HTTP collector

1.  Send an HTTP POST request to the URL for your HTTP collector.

- Here is a CURL example:

      curl -X POST "https://api-{tenant external URL}/logs/v1/bulk_load" \
      -H "Authorization: {generated_key}" \
      -H "x-cortex-partition: {partition_date_of_the_data}" \
      -H "x-cortex-source-dataset: {dataset_name}" \
      -H "Content-Type: application/json" \
      -d '{"example1": "test", "timestamp": 1609100113039}
      {"example2": [12321,546456,45687,1]}'

  Python 3 example:

      import requests
      def test_http_collector(generated_key):
          headers = {
              "Authorization": generated_key,
              "x-cortex-partition": partition_date_of_the_data,
              "x-cortex-source-dataset": dataset_name,
              "Content-Type": "application/json"
          }
          # Note: the logs must be separated by a new line
          body = "{'example1': 'test', 'timestamp': 1609100113039}" \
                 "{'example2': [12321,546456,45687,1]}"
          res = requests.post(url="https://api-{tenant external URL}/logs/v1/event",
                              headers=headers,
                              data=body)
          return res

2.  Substitute the values specific to your configuration.

    - API URL: Paste the API URL that you copied when you enabled the
      HTTP collector from the **Archived Data** page. The format of the
      URL is `https://api-{tenant external URL}/logs/v1/bulk_load`.

    - Authorization: Paste the generated key you previously recorded
      when enabling the HTTP collector, which is defined in the header.

    - x-cortex-partition: Enter the name of the file/folder containing
      the data from the log source / schema that you want to send to the
      HTTP collector. The file/folder name must be a date in the format
      `YYYY-MM-DD`. This is defined as part of the header.

    - x-cortex-source-dataset: Enter the name of the dataset for the
      data you want to send to the HTTP collector. A valid dataset name
      is alphanumeric with an option to use underscores (`_`) to
      concatenate multiple names using the format
      `<dataset name1>_<dataset name2>_....`. This is defined as part of
      the header.

    - Content-Type: This setting is dependent on the data object format
      of your files. For example, use `application/json` for JSON format
      or `text/plain` for Text format. This is defined as part of the
      header.

    - Body: The body contains the records you want to send to Cortex
      XSIAM. Separate records with a \\n (new line) delimiter. The
      request body can contain up to 25 MB of records, and uncompressed.
      In the case of a CURL command, the records are contained in the -d
      '\<records\>' parameter.

3.  Review the possible success and failure code responses to your HTTP
    POST requests.

- For more information on the possible error codes you can encounter, so
  you can troubleshoot the errors, see [Success and failure code
  responses to your HTTP POST
  requests](#UUIDa68627de14c547d7f21e0e5f57c68408).

4.  Monitor the **Remote Files** table in the Settings \> Configurations
    \> Data Management \> Archived Data page.

- Once the HTTP requests are sent to the HTTP collector, the data begins
  to be displayed by the dataset name in the **Remote Files** table. It
  can take time for all the datasets and associated folders with the
  data to be displayed as this is dependent on several factors, such as
  the number of files, daily upload limit of the HTTP collector, and
  total daily upload capacity for sending files to the HTTP collector.
  In some cases, it can take several days to complete. When the dataset
  has a **Remote** status, the upload is complete.

Task 3. Import the dataset files to cold storage

1.  Select the datasets and associated folders with the data to upload
    in the **Remote Files** table.

- You can select the data to upload in two different ways:

  - To import all the folders associated with the dataset, select the
    dataset name in the **Remote Files** table.

  - To import specific folders from a dataset, click the dataset name in
    the **Remote Files** table, and then select the files you want to
    import.

2.  When you\'ve finished selecting the applicable datasets and folders
    to import, right-click, and select `Import`.

3.  Confirm the import in the dialog box that opens by clicking
    **Start Import**.

- The statuses of the datasets and folders imported in the
  **Remote Files** table will update to **In Progress**. The import can
  take time, even up to several days. For more information, see [Phase
  6](#Xf53beb9cabdaa58841f351a383d0c8dd78a62ce).

  When the import finishes, a notification is sent to your Notification
  Center indicating whether the import was successful or not. The
  statuses of the imported datasets and folders get updated, which is
  dependent on the dataset and folders imported.

  - **Import entire dataset**: If you import a dataset, or multiple
    datasets, with all its folders, after the import completes the
    status of the dataset updates to **Imported**, the dataset is
    disabled from the **Remote Files** table, and the dataset is now
    listed in the **Dataset Management** page with the name using the
    format `archive_<dataset name>`.

  - **Partial import of dataset**: If you import only some of the
    folders of a dataset, after the import completes the status of the
    dataset updates to **Partially Imported**, the folders imported have
    a status of **Imported**, and the rest of the folders not imported
    remain with a **Remote** status. The dataset is left enabled to
    allow you to import the rest of the data at a later time. Also, the
    dataset is now listed in the **Dataset Management** page with the
    name using the format `archive_<dataset name>`. If you choose to
    import the same dataset with the rest of the folders at a later
    time, the leftover files are added to the dataset that was created
    previously in the **Data Management** page. In addition, in the
    **Remote Files** table, the dataset is disabled and the status
    updates to **Imported**.

  - **Reimport dataset or folders in a dataset**: If you resend data to
    the HTTP collector that has already been imported previously, the
    statuses of the dataset and folders update to
    **Partially Imported**. If you choose to reimport, these files will
    be added (not replaced) to the existing dataset in the
    **Dataset Management** page. It\'s also possible to combine
    previously imported folders with new folders, so the statuses of the
    new folders update to **Remote** and the previously imported dataset
    and folders update to **Partially Imported**.

##### Remote Files table

The **Remote Files** table on the **Archived Data** page enables you to
keep track of your data that you\'re in the process of uploading to
import to cold storage and the data ready to import. As soon as any
files are received by the HTTP collector through the HTTP POST requests
sent, the file contents are displayed in the **Remote Files** table. The
data is ordered by the datasets, where for each dataset you can see the
aggregated number of folders/files, total folder size when calculated,
and the status of the files. When you select any dataset name, the
folders indicate the different dates of the data as sent in the HTTP
request header. For each folder, the following is listed: aggregated
number of folders/files, total folder size when calculated, and the
status of the files.

You can select different datasets and folders to import to cold storage.
It can take time for all the files to be sent to the HTTP collector and
then imported as this is dependent on several factors, such as the
numbers of files, daily upload limit of files sent to the HTTP
collector, and total daily upload capacity by the HTTP collector. In
some cases, it can take several days to complete. Use the statuses to
help you monitor your data.

You can pivot (right-click) any dataset or folder listed in the
**Remote Files** table to import, delete, and calculate the folder size.
You can calculate the folder size of a dataset or folder, when the
dataset or folder status are either **Remote** or
**Partially Imported**.

#### Building XQL archived data queries

> **Prerequisite**
>
> Archived cold storage, in addition to a Period-Based Retention - Cold
> Storage add-on license, requires compute units (CU) to run archived
> cold storage queries. Cortex XSIAM provides a free daily quota
> of compute units (CU) allocated according to your license size.
> Queries run without enough quota will fail. To expand your
> investigation capabilities, you can purchase additional CU by enabling
> the Compute Unit add-on. Ensure that you have enough CU to run your
> archived cold storage data. For more information on CU and running
> cold storage queries, see [Manage compute
> units](#UUID6b9a9271d5433372d393491a1b72cacf).
>
> For information on the CU add-on license, see
> [/document/preview/1159030#UUID-8b02916d-8f31-beb9-22bb-a3d4c1cfe0f4](/document/preview/1159030#UUID-8b02916d-8f31-beb9-22bb-a3d4c1cfe0f4).

Each data source that is imported to Cortex XSIAM is available as a cold
storage dataset and can be accessed using Cortex Query Language (XQL).
These datasets are a new type of archived dataset. After being imported
to cold storage, the datasets are renamed using the format
`archive_<dataset name>`, and can be queried as any other cold storage
dataset, with one exception that makes them unique. You can query these
datasets during the hot retention period. Typically, this isn\'t enabled
for cold storage datasets, as during the hot storage period, the cold
storage data isn\'t relevant. Yet, for this type of data, you can query
the archived data in cold storage during the hot storage period using
CU.

You can perform queries on archived cold storage data using the dataset
format:

    cold_dataset = archive_<dataset name>

#### Success and failure code responses to your HTTP POST requests

The following table provides the various success and failure code
responses to your HTTP POST requests, which can help you troubleshoot
any problems with your HTTP collector configuration.

+-----------------------+-----------------------------+-------------------------------------------------------------------------------+
| Success/failure       | Description                 | Output code displayed (if applicable)                                         |
| response code         |                             |                                                                               |
+=======================+=============================+===============================================================================+
| 200                   | Success code that indicates |     {    "ok": "true"}                                                        |
|                       | there are no errors and the |                                                                               |
|                       | request was successful. The |                                                                               |
|                       | `last_used` field timestamp |                                                                               |
|                       | is updated accordingly.     |                                                                               |
+-----------------------+-----------------------------+-------------------------------------------------------------------------------+
| 400                   | Error code that indicates   | - Partition date value missing or empty in HTTP request:                      |
|                       | that there is a problem     |                                                                               |
|                       | with any of the following:  | <!-- -->                                                                      |
|                       |                             |                                                                               |
|                       | - The partition date value  | - {    "error": "partition value is missing or empty"}                        |
|                       |   (`x-cortex-partition`) is |                                                                               |
|                       |   missing or empty in the   | <!-- -->                                                                      |
|                       |   HTTP request.             |                                                                               |
|                       |                             | - Partition date value isn\'t in the correct format of `YYYY-MM-DD`:          |
|                       | - The partition date value  |                                                                               |
|                       |   in the HTTP request       | <!-- -->                                                                      |
|                       |   (`x-cortex-partition`)    |                                                                               |
|                       |   isn\'t in the correct     | - {    "error": "partition value must be in the format YYYY-MM-DD"}           |
|                       |   format of `YYYY-MM-DD`.   |                                                                               |
|                       |                             | <!-- -->                                                                      |
|                       | - Dataset name value        |                                                                               |
|                       |   (x-cortex-source-dataset) | - Dataset name value is missing or empty in the HTTP request:                 |
|                       |   is missing or empty in    |                                                                               |
|                       |   the HTTP request.         | <!-- -->                                                                      |
|                       |                             |                                                                               |
|                       |                             | - {    "error": "sourceDataset value is missing or empty"}                    |
+-----------------------+-----------------------------+-------------------------------------------------------------------------------+
| 401                   | Unauthorized error code     |     {    "error": "Failed to validate authentication detail"}                 |
|                       | that indicates an incorrect |                                                                               |
|                       | authorization key for the   |                                                                               |
|                       | HTTP collector is being     |                                                                               |
|                       | used.                       |                                                                               |
+-----------------------+-----------------------------+-------------------------------------------------------------------------------+
| 403                   | Error code that indicates   | - Wrong header key in HTTP request:                                           |
|                       | one of the following:       |                                                                               |
|                       |                             | <!-- -->                                                                      |
|                       | - Wrong header key for the  |                                                                               |
|                       |   HTTP collector is in the  | - The Bulk Load configuration has been deleted and cannot be used             |
|                       |   HTTP request and cannot   |                                                                               |
|                       |   be used.                  | <!-- -->                                                                      |
|                       |                             |                                                                               |
|                       | - Partition date value      | - Partition date value not within the license retention period for hot and    |
|                       |   (x-cortex-partition) in   |   cold storage:                                                               |
|                       |   the HTTP request is not   |                                                                               |
|                       |   within the license        | <!-- -->                                                                      |
|                       |   retention period for hot  |                                                                               |
|                       |   and cold storage.         | - partition is less than license start date                                   |
+-----------------------+-----------------------------+-------------------------------------------------------------------------------+
| 413                   | Error code indicating the   |     Request entity too large as the size is more than 25 MB limit             |
|                       | request entity is too large |                                                                               |
|                       | as the request size is more |                                                                               |
|                       | than the 25 MB limit.       |                                                                               |
+-----------------------+-----------------------------+-------------------------------------------------------------------------------+
| 429                   | Error code indicating too   | - Daily ingestion file limit exceeded:                                        |
|                       | many requests as the        |                                                                               |
|                       | ingestion limits are        | <!-- -->                                                                      |
|                       | reached for one of the      |                                                                               |
|                       | following reasons:          | - exceeds daily limit of <number> files                                       |
|                       |                             |                                                                               |
|                       | - Daily ingestion limit of  | <!-- -->                                                                      |
|                       |   \<number\> files is       |                                                                               |
|                       |   exceeded.                 | - Daily ingestion file size limit exceeded:                                   |
|                       |                             |                                                                               |
|                       | - Daily file size ingestion | <!-- -->                                                                      |
|                       |   limit of \<number\> GB is |                                                                               |
|                       |   exceeded.                 | - Daily file size limit of <number> GB for HTTP requests sent is exceeded     |
+-----------------------+-----------------------------+-------------------------------------------------------------------------------+
| 500                   | Error code indicating an    |     Unexpected error while processing bulk load request                       |
|                       | unexpected internal error   |                                                                               |
|                       | while processing the bulk   |                                                                               |
|                       | load HTTP request.          |                                                                               |
+-----------------------+-----------------------------+-------------------------------------------------------------------------------+

### Parsing Rules

#### What are Parsing Rules?

> **Note**
>
> Only a user with Cortex Account Administrator or Instance
> Administrator permissions can access Parsing Rules.

Cortex XSIAM includes an editor for creating 3rd party Parsing Rules,
which enables you to:

- Remove unused data that is not required for analytics, hunting, or
  regulation.

- Reduce your data storage costs.

- Pre-process all incoming data for complex rule performance.

- Add tags to the ingested data as part of the ingestion flow.

- Easily identify and resolve Parsing Rules errors so you can
  troubleshoot them quickly.

- Test your Parsing Rules on actual logs and validate their outputs
  before implementation.

Parsing Rules contain the following built-in characteristics:

- Parsing Rules are bound to a specific vendor and product.

- Parsing Rules take raw log input, perform an arbitrary number of
  transitions and modifications to the data using Cortex Query Language
  (XQL), and return zero, one, or more rows that are eventually inserted
  into the Cortex XSIAM tenant.

- Parsing Rules can be grouped together by a no-match policy. If all the
  rules of a group did not produce an output for a specific log record,
  a no-match policy defines what to do, such as drop the log or keep the
  log in some default format.

- Upon ingestion, all fields are retained even fields with a null value.
  You can also use XQL to query parsing rules for null values.

#### Parsing Rules editor views

> **Note**
>
> Only a user with Cortex Account Administrator or Instance
> Administrator permissions can access Parsing Rules.

The Parsing Rules editor contains the following views:

- **User Defined** (default): Displays an editor for writing your own
  custom parsing rules that override or extend the default rules and a
  **List of Errors** section to help you troubleshoot errors in your
  Parsing Rules.

- **Default Rules**: Displays the parsing rules that are provided by
  default with Cortex XSIAM in read-only mode and a **List of Errors**
  section to view any errors in your Parsing Rules.Any Parsing Rules
  that are included with your installed Content Packs from the
  Marketplace are also listed under Installed Rules.

- **Both**: Side-by-side view of both the **Default Rules** and
  **User Defined** rules, so you can easily view the different rules on
  one screen. In addition, the **List of Errors** section helps you
  troubleshoot any errors in your Parsing Rules.

- **Simulate**: Enables you to test your Parsing Rules on actual logs
  and validate their outputs, which helps minimize your errors when
  creating Parsing Rules. The editor includes the following sections.

  - **User defined**: A list of the current **User defined** rules on
    the left side of the window.

  - **XQL Samples**: A table of the existing Cortex Query Language (XQL)
    raw data samples on the right side of the window, which contain
    sample logs listing the **Vendor**, **Product**, **Raw Log**, and
    **Sample Time**. For each **Vendor** and **Product**, up to 5
    different samples are available to choose from. From this list, you
    can select the logs used to simulate the rule.

  - **Logs Output**: Displays in a table format the following columns
    per dataset at the bottom of the window.

    - **Dataset**: Displays the applicable dataset name and a line
      number associated to this dataset in the **User defined** section.

    - **Vendor**: The vendor associated with this dataset.

    - **Product**: The product associated with this dataset.

    - **Logs Output**: Displays the output logs that are available based
      on your **User defined** rules and **XQL Samples** selected after
      simulating the results. When there is no output log to display,
      the text `Output logs is not available` with the corresponding
      error message is displayed. When there is no output due to a
      missing rule in the **User defined** section for the logs
      selected, the text
      **No output logs. You can change your parsing rules and try again**
      is displayed.

    - **Input Logs**: Displays the relevant input log with a right-click
      pivot to **Show diff** between the **Output Logs** and
      **Input Logs**.

#### Parsing Rules file structure and syntax

> **Note**
>
> Only a user with Cortex Account Administrator or Instance
> Administrator permissions can access Parsing Rules.

##### File structure

The Parsing Rules file consists of multiple sections of these three
types, which also represent the custom syntax specific to Parsing Rules.

- `INGEST`: This section is used to define the resulting dataset.

- `COLLECT` (Optional): This section defines a rule that enables data
  reduction and data manipulation at the Broker VM to help avoid sending
  unnecessary data to the Cortex XSIAM server and reduce traffic,
  storage, and computing costs. In addition, the `COLLECT` section is
  used to manipulate, alter, and enrich the data before it's passed to
  the Cortex XSIAM server. While this rule is optional to configure,
  once added this rule runs before the `INGEST` section.

- `CONST` (Optional): This section is used to define strings and numbers
  that can be reused multiple times within Cortex Query Language (XQL)
  statements in other `INGEST` sections by using `$constName`.

- `RULE` (Optional): Rules are part of the XQL syntax, which are tagged
  with a name, and can be reused in the code in the `INGEST` sections by
  using `[rule:ruleName]`.

- `EXTEND` (Optional): This section is used to chain your Parsing Rules
  logic to extend your existing default `RULE` sections, which are added
  by a Content Package you installed from the Marketplace. An `EXTEND`
  section runs immediately after the default `RULE` section that it
  extends and enables data manipulation without overriding or
  interfering with the existing vendor Parsing Rules.

The order of the sections is unimportant. The data of each section type
gets grouped together during the parsing stage. Before any action takes
place all `COLLECT`, `CONST`, `RULE`, `EXTEND`, and `INGEST` objects are
grouped together and collected to the same list.

##### Syntax

The syntax used in the Parsing Rules file is derived from XQL, but with
a few modifications. This subset of XQL is called XQL for Parsing
(XQLp).

> **Note**
>
> For more information on the XQL syntax, see Cortex XQL Language
> Reference.

The `COLLECT`, `CONST`, `INGEST`, `RULE`, and `EXTEND`syntax is derived
from XQL, but with the following modifications for XQLp:

- A statement never starts with a dataset or preset selection. The
  query\'s data source is meaningless. It is transparent to the user
  where the raw logs are coming from, fully handled by the system.

- Only the following XQL stages are permitted:
  [alter](#UUID29adfeb71cc64bb046509f7b4089e1b5),
  [fields](#UUIDa22fb161beb7d67c6d9bc8c52d66f3bd),
  [filter](#UUID75b17bafc8212d4207f11d67acd47c76), and
  [join](#UUID01b27dc92ccd24c18588d2388b5a0f90). In addition, a new
  `call` stage is supported, which is used to invoke another rule.

<!-- -->

- > **Note**

  - > An `inner` type of `join` stage is only supported in `CONST`,
    > `INGEST`, and `RULE` sections and is not supported in a `COLLECT`
    > section.

  - > You cannot `call` a `RULE` section that exists in
    > **Default Rules** from the **User Defined Rules** section.

<!-- -->

- Only the following XQL functions are permitted in all sections:
  [parse_timestamp](#UUID4334b9f5df4f4ae75af4dfba8b052a14),
  [parse_epoch](#UUID160588c69ee420b771b850663e8fb666), and
  [regexcapture](#UUID8faa08e9fc957f2287f3b9865b94acaa).

<!-- -->

- > **Note**

  > The [regexcapture](#UUID8faa08e9fc957f2287f3b9865b94acaa) function
  > is only supported in Parsing Rules and cannot be used in any other
  > XQL query.

<!-- -->

- No output stages are supported.

- A `Rule` object can only contain a single statement.

- A `join inner` query is restricted to using a lookup as a data source
  and is only supported in XQLp stages.

<!-- -->

- There is no default lookup, so all `join inner` queries must start
  with `dataset=<lookup> | ...`.

<!-- -->

- `CONST` reference (`$MY_CONST`) is supported.

- An `IN` condition can only take a sequence list, such as
  `device_name in (“device1”, “device2”, “device3”)` and not another XQL
  or XQLp `inner` queries.

- You can\'t create parsing rules for Next-Generation Firewall (NGFW)
  datasets that are in the format `panw_ngfw_<text>_raw`, and the
  Observability dataset called `panw_observability_raw`.

Comments in C programming language can be used anywhere throughout the
Parsing Rules file:

    // line comment
    /* inner comment */

> **Note**
>
> Every statement in the Parsing Rules file must end with a semicolon
> (`;`).

##### INGEST

> **Note**
>
> Only a user with Cortex Account Administrator or Instance
> Administrator permissions can access Parsing Rules.

An `INGEST` section is used to define the resulting dataset. The
`COLLECT`, `CONST`, and `RULE` sections are only add-ons, used to help
organize the `INGEST` sections, and are optional to configure. Yet, a
Parsing Rules file that contains no `INGEST` sections, generates no
Parsing Rules. Therefore, the `INGEST` section is mandatory to
configure.

`INGEST` syntax is derived from Cortex Query Language (XQL) with a few
modifications as explained in the [Parsing Rules
syntax](#UUIDb762677835f218360781ef52b29e56ea). In addition, `INGEST`
sections contain the following syntax add-ons:

- `INGEST` sections can have more than one XQLp statement, separated by
  a semicolon (`;`). Each statement creates a different Parsing Rule.

- The following XQL functions and stages are also supported in the
  `INGEST` section:

  - Functions: [arrayfilter](#UUID250e3212a3f12a2130c927aac1d3a5c2),
    [arraycreate](#UUIDf3b4133337e9333c15025fde93b891c6),
    [arraymerge](#UUID698fb386f4504b490b9a5422d876b8f2), and
    [object_create](#UUID53fd97b4d188d94e6beea31d1764356a).

  - Stages: [iploc](#UUID70093380d7b5138baf4093321f2750ce) and
    [arrayexpand](#UUIDf2011116be19e9cb3bad99ae05c73743).

- Another new stage is available called `drop`.

  - `drop` takes a condition similar to the XQL `filter` stage (same
    syntax), but drops every log entry that passes that condition. One
    can think of it as a negative filter, so `drop <condition>` is not
    equivalent to `filter not <condition>`.

  - `drop` can only appear last in a statement. No other XQLp rules can
    follow.

- `INGEST` sections take parameters, and not names as `RULE` sections
  use, where some are mandatory and others optional.

<!-- -->

- [ingest:vendor=<vendor>, product=<product>, target_dataset=<dataset>, no_hit=<keep\drop>, ingestnull=<true\false>]
      filter raw_log not contains "issue";

The parameter descriptions are explained in the following table:

+-----------------------------------+-----------------------------------+
| Parameter                         | Description                       |
+===================================+===================================+
| `vendor`                          | The vendor that the specified     |
|                                   | Parsing Rules apply to            |
|                                   | (mandatory).                      |
+-----------------------------------+-----------------------------------+
| `product`                         | The product that the specified    |
|                                   | Parsing Rules apply to            |
|                                   | (mandatory).                      |
+-----------------------------------+-----------------------------------+
| `target_dataset`                  | The name of the dataset to insert |
|                                   | every row with the results after  |
|                                   | applying any of the specified     |
|                                   | Parsing Rules (mandatory).        |
+-----------------------------------+-----------------------------------+
| `no_hit`                          | No-match strategy to use for the  |
|                                   | entire specified group of rules   |
|                                   | (optional). The default is        |
|                                   | `keep`.                           |
|                                   |                                   |
|                                   | - If `no_hit = drop`, then in a   |
|                                   |   scenario where none of the      |
|                                   |   rules in the group generates    |
|                                   |   output for a given log record,  |
|                                   |   that record is discarded.       |
|                                   |                                   |
|                                   | - If `no_hit = keep`, then in a   |
|                                   |   scenario where none of the      |
|                                   |   rules in the group generates    |
|                                   |   output for a given log record,  |
|                                   |   that record is kept in the      |
|                                   |   `_raw_log` field. This record   |
|                                   |   is inserted into the group\'s   |
|                                   |   dataset once, but every column  |
|                                   |   holds `NULL` except for         |
|                                   |   `_raw_log`, which holds the     |
|                                   |   original JSON log record.       |
+-----------------------------------+-----------------------------------+
| `ingestnull`                      | Defines whether null value fields |
|                                   | are ingested (optional). By       |
|                                   | default this is set to `true`, so |
|                                   | you only need to set this         |
|                                   | parameter when you want to        |
|                                   | overwrite the default definition. |
+-----------------------------------+-----------------------------------+

Each statement represents a different Parsing Rule in the same group as
depicted in the following example:

    [CONST]
    DEVICE_NAME = "ngfw"; 
    [rule:use_two_rules]
    filter severity = "medium" | call basic_rule | call use_xql_and_another_rule; 
    [rule:basic_rule]
    fields log_type, severity | filter log_type="eal" and severity="HIGH" and type="something"; 
    [rule:use_xql_and_another_rule]call multiline_statement | filter severity = "medium"; 
    [rule:multiline_statement]
    alter url = json_extract(_raw_log, "$.url")
    | join type = inner conflict_strategy = both (dataset=my_lookup) as inn url=inn.url 
    |filter severity = "medium"; 
    [ingest:vendor=panw, product=ngfw, target_dataset=panw_ngfw_ds, no_hit=drop]
    filter log_type="traffic" | alter url = json_extract(_raw_log, "$.url");
    call use_two_rules | join type = inner conflict_strategy = both (dataset=my_lookup) as inn severity=inn.severity | fields severity, log_type | drop device_name = $DEVICE_NAME;

This generates 1 group of 2 Parsing Rules for panw/ngfw, where all the
ingested data into `panw_ngfw_ds` dataset.

The following represents the syntax for the rules:

    Rule #1:
    filter log_type="traffic" | alter url = json_extract(_raw_log, "$.url"); 
    Rule #2:
    filter severity = "medium"
    | fields log_type, severity
    | filter log_type="eal" and severity="HIGH" and type="something"
    | alter url = json_extract(_raw_log, "$.url")
    | join type = inner conflict_strategy = both (dataset=my_lookup) as inn url=inn.url
    | filter severity = "medium"
    | filter severity = "medium"
    | join type = inner conflict_strategy = both (dataset=my_lookup) as inn severity=inn.severity
    | fields severity, log_type
    | drop device_name = $DEVICE_NAME

A few more points to keep in mind when writing `INGEST` sections:

- `INGEST` parameter names are not case-sensitive. Therefore,
  `vendor=PANW` and `vendor=panw` are the same.

- Since section order is unimportant, you do not have to declare a
  `RULE` or a `CONST` before using it in an `INGEST` section.

- You can have multiple `INGEST` sections with the same `vendor`,
  `product`, `dataset` , and `no_hit` values. Yet, this can lead to
  unexpected results. Consider the following example:

<!-- -->

- [ingest:vendor=panw, product=ngfw, tartget_dataset=panw_ngfw_ds, no_hit=keep]
      filter raw_log not contains "issue"; 
      [ingest:vendor=panw, product=ngfw, target_dataset=panw_ngfw_ds, no_hit=keep]
      filter device_type not contains "agent";

  Let `lw` be a log row. If `lw.raw_log` doesn\'t contain an `issue` and
  `lw.device_type` doesn\'t contain an `agent`, then `lw` is inserted
  twice into the `pan_ngfw_ds` dataset as every section is standalone.

  - To eliminate these kind of errors and misunderstandings, it is
    highly advised to group all rules having the same `vendor`,
    `product`, `dataset` , and `no_hit` values in a single `INGEST`
    section.

  - Logs that were discarded by a `drop` stage are considered ingested
    with a no-match policy. This means they are not kept even if
    `no_hit = keep`.

  - Keep in mind that all rules inside a group get evaluated
    independently. This is in contrast to firewall-like rules, which
    stop evaluating the first rule that is able to make a decision.
    Therefore, without proper filtering, it is possible to ingest the
    same log more than once.

<!-- -->

- You can override the default raw dataset in `INGEST` sections. For
  more information, see [Parsing Rules Raw
  Dataset](#UUIDb562a5de488db3ff0ec561dbaa21f50f).

- Cortex XSIAM supports configuring case sensitivity in Parsing Rules
  only within the `INGEST` section using the following configuration
  stage:

<!-- -->

- config case_sensitive = true | false

<!-- -->

- You can add a single tag or list of tags to the ingested data as part
  of the ingestion flow that you can easily query. You can add tags as
  part of the `INGEST` section or use both the `INGEST` and `RULE`
  sections.

<!-- -->

- > **Note**

  > You can\'t add tags to parsing rules using the
  > Next-Generation Firewall (NGFW) datasets that are in the format
  > `panw_ngfw_<text>_raw`, and the Observability dataset called
  > `panw_observability_raw`.

  The following are examples of each:

  - `INGEST` section:

  <!-- -->

  - Adding a single tag:

        [INGEST:vendor="MSFT", product="Azure AD Audit", target_dataset="msft_ad_audit_tagging", no_hit=drop, ingestnull = false ]
        tag add "New Event"

    Adding a list of tags:

        [INGEST:vendor="MSFT", product="Azure AD Audit", target_dataset="msft_ad_audit_tagging", no_hit=drop, ingestnull = false ]
        tag add "New Event1", "New Event2", "New Event3"

  <!-- -->

  - `INGEST` and `RULE` sections:

  <!-- -->

  - Adding a single tag:

        [INGEST:vendor="Check Point", product="Anti Malware", target_dataset="malware_test", no_hit= drop  , ingestnull = true ]
        alter xx = call new_tag_rule; 

        [RULE:new_tag_rule]
        tag add "test";

    Adding a list of tags:

        [INGEST:vendor="Check Point", product="Anti Malware", target_dataset="malware_test", no_hit= drop  , ingestnull = true ]
        alter xx = call new_tag_rule; 

        [RULE:new_tag_rule]
        tag add  "test1", "test2", "test3";

##### COLLECT

> **Note**
>
> Only a user with Cortex Account Administrator or Instance
> Administrator permissions can access Parsing Rules.

A `COLLECT` section defines a rule that enables data reduction and data
manipulation at the Broker VM to help avoid sending unnecessary data to
the Cortex XSIAM server and reduces traffic, storage, and computing
costs. In addition, the `COLLECT` section is used to manipulate, alter,
and enrich the data before it's passed to the Cortex XSIAM server. While
this rule is optional to configure, once added, this rule runs before
the `INGEST` section.

> **Note**
>
> The [CSV Collector applet](#UUID9549d7acf76fed0ee5998431177a1e9a) is
> not affected by the `COLLECT` rules applied to a Broker VM.

To avoid performance issues on the Broker VM, Cortex XSIAM does not
permit all Parsing Rules to run on the Broker VM by default, but only
the Parsing Rules that you designate.

The Broker VM is directly affected by the `[COLLECT]` rules you create,
so depending on the complexity of the rules more hardware resources on
the Broker VM may be required. As a result, ensure that your Broker VM
meets the following minimum hardware requirements to run `[COLLECT]`
rules:

- 8-core processor

- 8GB RAM

- 512GB disk

- Plan for a max of 10K eps (events per second) per core.

`COLLECT` syntax is derived from Cortex Query Language (XQL) with a few
modifications as explained in the [Parsing Rules
syntax](#UUIDb762677835f218360781ef52b29e56ea). In addition, `COLLECT`
rules contain the following syntax add-ons:

- `COLLECT` rules can have more than one XQLp statement, separated by a
  semicolon (`;`). Each statement creates a different data reduction and
  manipulation at the Broker VM for a different vendor and product.

- While the XQL stages [alter](#UUID29adfeb71cc64bb046509f7b4089e1b5)
  and [fields](#UUIDa22fb161beb7d67c6d9bc8c52d66f3bd) are permitted in
  `COLLECT` rules for various vendors and products, you should avoid
  using them for supported vendors that can be used for Analytics as
  these stages can disrupt the operation of the Analytics Engine. For a
  list of these vendors, see the
  [/document/preview/1003965#UUID-e4462bc5-900e-ff78-0214-3d287464452b](/document/preview/1003965#UUID-e4462bc5-900e-ff78-0214-3d287464452b)
  table specifically those vendors with Normalized Log Visibility.

- Another new stage is available called `drop`.

  - `drop` takes a condition similar to the XQL `filter` stage (same
    syntax), but drops every log entry that passes that condition. One
    can think of it as a negative filter, so `drop <condition>` is not
    equivalent to `filter not <condition>`.

  - `drop` can only appear last in a statement. No other XQLp syntax can
    follow.

- `COLLECT` sections take parameters, where some are mandatory and
  others optional.

<!-- -->

- [COLLECT:vendor=<vendor>, product=<product>, target_brokers = (<broker_ID1, brokerID2,...>), no_hit = <keep\drop>];

  Here\'s an example of how to define the `COLLECT` section with a
  single `broker_ID`:

      [COLLECT:vendor="PANW", product="NGFW_CEF", target_brokers=(BROKER_ID), no_hit=drop]

  Here\'s an example of how to define the `COLLECT` section with
  multiple `broker_ID`s:

      [COLLECT:vendor="PANW", product="NGFW_CEF", target_brokers=(BROKER_ID1, BROKER_ID2, BROKERID3), no_hit=drop]

The parameter descriptions are explained in the following table:

+-----------------------------------+--------------------------------------------------+
| Parameter                         | Description                                      |
+===================================+==================================================+
| `vendor`                          | The vendor that the specified `COLLECT` rule for |
|                                   | data reduction and data manipulation at the      |
|                                   | Broker VM applies to (mandatory).                |
+-----------------------------------+--------------------------------------------------+
| `product`                         | The product that the specified `COLLECT` rule    |
|                                   | for data reduction and data manipulation at the  |
|                                   | Broker VM applies to (mandatory).                |
+-----------------------------------+--------------------------------------------------+
| `target_brokers`                  | Specifies the list of Brokers to run the         |
|                                   | `COLLECT` rule for data reduction and data       |
|                                   | manipulation based on the vendor and product     |
|                                   | configured (mandatory). When `target_brokers=*`, |
|                                   | the `COLLECT` rule applies to all the data       |
|                                   | collected by the Broker VM applets.              |
|                                   |                                                  |
|                                   | > **Note**                                       |
|                                   | >                                                |
|                                   | > The [CSV Collector                             |
|                                   | > applet](#UUID9549d7acf76fed0ee5998431177a1e9a) |
|                                   | > is not affected by the `COLLECT` rules applied |
|                                   | > to a Broker VM.                                |
+-----------------------------------+--------------------------------------------------+
| `no_hit`                          | No-match strategy to use for the entire          |
|                                   | specified group of `COLLECT` rules (optional).   |
|                                   | The default is `keep`.                           |
|                                   |                                                  |
|                                   | - If `no_hit = drop`, then in a scenario where   |
|                                   |   none of the `COLLECT` rules in the group       |
|                                   |   generates output for a given event, that event |
|                                   |   is discarded.                                  |
|                                   |                                                  |
|                                   | - If `no_hit = keep`, then in a scenario where   |
|                                   |   none of the `COLLECT` rules in the group       |
|                                   |   generates output for a given event, that event |
|                                   |   is passed to the Cortex XSIAM server.          |
+-----------------------------------+--------------------------------------------------+

The following is an example of using a `COLLECT` rule to filter data for
a specific vendor and product that will run before the `INGEST` section.

    [COLLECT:vendor="Apache", product="ApacheServer", target_brokers = (bvm1, bvm2, bvm3), no_hit = drop]
    alter source_log = json_extract_scalar(_raw_log, "$.source") 
    | filter source_log = "WebApp-Logs"
    | fields source_log, _raw_log;
    [INGEST:vendor="Apache", product="ApacheServer", target_dataset = "dvwa_application_log"]
    alter log_timestamp = json_extract_scalar(_raw_log, "$.timestamp")
    | alter log_msg = json_extract_scalar(_raw_log, "$.msg")
    | alter log_remote_ip = json_extract_scalar(_raw_log, "$.Remote_IP")
    | alter scanned_ip = json_extract_scalar(_raw_log, "$.Scanned_IP")
    | fields log_msg ,log_remote_ip ,log_timestamp ,source_log ,scanned_ip , _raw_log;

A few more points to keep in mind when writing `COLLECT` rules:

- There are no `COLLECT` rules by default, so all collected events are
  forwarded by the Broker VM to the Cortex XSIAM server.

<!-- -->

- > **Tip**

  > To reduce the amount of data transmitted to Cortex XSIAM from the
  > broker, use filters to drop logs. Yet, be aware that once the logs
  > are modified using `alter` or `fields` stages, the Broker VM will
  > convert the original log into a JSON format, which could increase
  > the data size being sent from the broker to Cortex XSIAM.

<!-- -->

- When `COLLECT` rules are defined, the designated Broker VMs check
  every collected event versus each rule. When there is a match for a
  given product or vendor, the Broker VM checks if it meets the filter
  criteria.

  - If it meets the criteria, the event is passed to the Cortex XSIAM
    server.

  - If it doesn't meet the criteria, it depends on the `no_hit`
    parameter.

  <!-- -->

  - -If `no_hit=drop`, then this `COLLECT` rule will not pass the event.
    Yet, the event still goes through other rules on this Broker VM.

    -If `no_hit=keep`, the event is passed to the Cortex XSIAM server,
    and goes through other rules on this Broker VM.

- When the evaluated event, doesn't match any product or vendor for a
  defined `COLLECT` rule, the event is passed to the Cortex XSIAM
  server.

##### CONST

> **Note**
>
> Only a user with Cortex Account Administrator or Instance
> Administrator permissions can access Parsing Rules.

A `CONST` section is used to define strings and numbers that can be
reused multiple times within Cortex Query Language (XQL) statements in
other `INGEST` sections by using `$constName`. This can be helpful to
avoid writing the same value in multiple sections, similar to constants
in modern programming languages.

    [CONST]
    DEFAULT_DEVICE_NAME = "firewall3060";       // string
    FILE_REGEX = "c:\\users\\[a-zA-Z0-9.]*";    // complex string
    my_num = 3;                                 /* int */

An example of using a `CONST` inside XQL statements in other `INGEST`
sections using `$constName`:

> **Note**
>
> The dollar sign (`$`) must be adjacent to the `[CONST]` name, without
> any whitespace in between.

    ...
    | filter device_name = $DEFAULT_DEVICE_NAME
    | alter new_field = JSON_EXTRACT(field, $FILE_REGEX)
    | filter age < $MAX_TIMEOUT
    | join type=$DEFAULT_JOIN_TYPE conflict_strategy=$DEFAULT_JOIN_CONFLICT_STRATEGY (dataset=my_lookup) as inn url=inn.url
    ...

> **Important**
>
> Only quoted or integer terminal values are considered valid for
> `CONST` sections.

These will not compile:

    [CONST]
    WORD_CONST = abcde;                             //invalid
    func_val = regex_extract(_raw_log, "regex");    // not possible
    RECURSIVE_CONST = $WORD_CONST;                  // not terminal - not possible

`CONST` sections are meant to replace values. Other types, such as
column names, are not supported:

    ...
    | filter $DEVICE_NAME = "my_device"             // illegal
    ...

A few more points to keep in mind when writing `CONST` sections:

- `CONST` names are not case-sensitive. They can be written in any
  user-desired casing, such as UPPER_SNAKE, lower_snake, camelCase, and
  CamelCase. For example, `MY_CONST=My_Const=my_const`.

- `CONST` names must be unique inside a section, and across all sections
  of the file. You cannot have the same `CONST` name defined again in
  the same section, or in any other `CONST` sections in the file.

- Since section order is unimportant, you do not have to declare a
  `CONST` before using it. You can have the `CONST` section written
  below other sections that use those `CONST` sections.

- A `CONST` is an add-on to the Parsing Rule syntax and is optional to
  configure.

- `CONST` syntax is derived from XQL, but a few modifications as
  explained in the [Parsing Rules
  syntax](#UUIDb762677835f218360781ef52b29e56ea).

##### RULE

> **Note**
>
> Only a user with Cortex Account Administrator or Instance
> Administrator permissions can access Parsing Rules.

Rules are very similar to functions in modern programming languages.
They are essentially pieces of Cortex Query Language (XQL) syntax,
tagged with a name - alias, for easier code reuse and avoiding code
duplications. A `RULE` is an add-on to the Parsing Rule syntax and is
optional to configure.

`RULE` syntax is derived from XQL with a few modifications, as explained
in the [Parsing Rules syntax](#UUIDb762677835f218360781ef52b29e56ea).

> **Note**
>
> For more information on the XQL syntax, see [Get started with
> XQL](#UUIDd08e1a9c8397fe101964781e65ee8407).

A few more points to keep in mind when writing `RULE` sections.

- Rules are defined by `[rule:ruleName]` as depicted in the following
  example:

<!-- -->

- [rule:filter_issues]
      filter raw_log not contains "issue";

<!-- -->

- Rules are invoked by using a `call` keyword as depicted in the
  following example:

<!-- -->

- [rule:filter_issues]
      filter raw_log not contains "issue"; 
      [rule:use_another_rule]
      filter severity="LOW" | call filter_issues | fields - raw_log;

  This is equivalent to writing:

      [rule:use_another_rule]
      filter severity="LOW" | filter raw_log not contains "issue" | fields - raw_log;

<!-- -->

- Rule names are not case-sensitive. They can be written in any
  user-desired casing, such as UPPER_SNAKE, lower_snake, camelCase, and
  CamelCase). For example, `MY_RULE=My_Rule=my_rule`.

- Rule names must be unique across the entire file. This means you
  cannot have the same rule name defined more than once in the same
  file.

- Since section order is unimportant, you do not have to declare a
  `rule` before using it. You can have the `rule` definition section
  written below other sections that use this rule.

- You can add a single tag or list of tags to the ingested data as part
  of the ingestion flow that you can easily query. You can add tags
  using both the `INGEST` and `RULE` sections.

<!-- -->

- Adding a single tag:

      [INGEST:vendor="Check Point", product="Anti Malware", target_dataset="malware_test", no_hit= drop  , ingestnull = true ]
      alter xx = call new_tag_rule; 

      [RULE:new_tag_rule]
      tag add "test";

  Adding a list of tags:

      [INGEST:vendor="Check Point", product="Anti Malware", target_dataset="malware_test", no_hit= drop  , ingestnull = true ]
      alter xx = call new_tag_rule; 

      [RULE:new_tag_rule]
      tag add "test1", "test2", "test3";

  > **Note**

  > You can also add tags using only the `INGEST` section. For more
  > information, see [INGEST](#UUIDc8b3e88ba43497d5ca36909d1cdf2759).

##### EXTEND

> **Note**
>
> Only a user with Cortex Account Administrator or Instance
> Administrator permissions can access Parsing Rules.

An `EXTEND` section is used to chain your Parsing Rules logic to extend
your existing default `RULE` sections, which are added by a Content
Package you installed from
[Marketplace](#UUIDbd7bb72a06838e5b74c7b3b159596497). While optional to
configure, an `EXTEND` section runs immediately after the default `RULE`
section that it extends, and enables data manipulation without
overriding or interfering with the existing vendor Parsing Rules. For
more information on the `RULE` section in Parsing Rules, see
[RULE](#UUID9df0bc04f36b1ced79002e5a0f725d4a).

`EXTEND` syntax is derived from Cortex Query Language (XQL) with a few
modifications as explained in the [Parsing Rules file structure and
syntax](#UUIDb762677835f218360781ef52b29e56ea) section. You can have
multiple XQL statements, separated by a semicolon (;). Each statement
creates a different extension.

> **Note**
>
> For more information on the XQL syntax, see [Get started with
> XQL](#UUIDd08e1a9c8397fe101964781e65ee8407).

A few more points to keep in mind when writing `EXTEND` sections:

- You can only extend a default rule that is not overridden in the
  `RULE` sections.

- A rule can only be extended once.

- A `CONST` section that is defined in **Default Rules** cannot be used
  in the **User Defined Rules** when configuring an `EXTEND` section.

- An `EXTEND` section must specify the full header of the rule it is
  extending. When you extend a rule that was added by a Content Package
  installed from Marketplace, the `EXTEND` section uses the format
  `[EXTEND:<rule name> content_id = "<pack id>"]`, where the
  `content_id` comes from the Content Package that the extended rule
  belongs to.

<!-- -->

- You can see here the `EXTEND` section in **User Defined Rules** uses
  the full header of the `RULE` it's extending from **Default Rules**.

  Default Rules:

      [RULE:parse_ngfw_hipmatch content_id = "IronNet"]
      alter _time = time_generated
      | call extract_common_ngfw_fields
      | call extract_hipmatch_only_fields
      | call common_post_processing;

  User Defined Rules:

      [EXTEND:parse_ngfw_hipmatch content_id = "IronNet"]
      alter source = json_extract_scalar(source, "$.string")
      | filter __firewall_type = "firewall.hipmatch";

  When this rule is run, the default `RULE` section runs, and is
  immediately followed by the `EXTEND` section. This is equivalent to
  running one single `RULE` section as follows:

      [RULE:parse_ngfw_hipmatch content_id = "IronNet"]
      alter _time = time_generated
      | call extract_common_ngfw_fields
      | call extract_hipmatch_only_fields
      | call common_post_processing
      | alter source = json_extract_scalar(source, "$.string")
      | filter __firewall_type = "firewall.hipmatch";

#### Create Parsing Rules

> **Note**
>
> Only a user with Cortex Account Administrator or Instance
> Administrator permissions can access Parsing Rules.

Cortex XSIAM provides a number of default Parsing Rules that you can
easily override or extend as required using XQL and additional custom
syntax that is specific to creating Parsing Rules. Before creating your
own Parsing Rules, we recommend you review the following:

- [Parsing Rules editor views](#UUID9c200e879a2c03280847800db1a509dd)

- [Parsing Rules file structure and
  syntax](#UUIDb762677835f218360781ef52b29e56ea)

**How to create Parsing Rules**

1.  In Cortex XSIAM , select Settings \> Configurations \> Data
    Management \> Parsing Rules.

2.  Select the Parsing Rules editor view for writing your Parsing Rules.

- You can select one of the following views.

  - **User Defined**: Leave the default view open and write your Parsing
    Rules directly in the editor.

  - **Default Rules**: Select this view to understand which parsing
    rules are provided by default with Cortex XSIAM in read-only mode.

  - **Both**: Select this view to see the Parsing Rules editor as well
    as the default rules as you write your Parsing Rules.

  - **Simulate**: Select this view to test your Parsing Rules on actual
    logs and validate their outputs as you write your Parsing Rules.

3.  Write your Parsing Rules using XQL syntax and the syntax specific
    for Parsing Rules.

4.  (*Optional*) Test your Parsing Rules on actual logs and validate
    their outputs using the **Simulate** view.

- > **Note**

  > You need Cortex XSIAM administrator or Instance Administrator
  > permissions to access the **Simulate** view and perform these tests.

  a.  Select the **Simulate** view.

  b.  For the **User defined** rules that you want to test, select the
      logs from the **XQL Samples** listed that you want to use to
      simulate the rule. For each **Vendor** and **Product**, up to 5
      different samples are available to choose from.

  c.  **Simulate** the rules based on the logs selected.

  - You can also pivot (right-click) any of the logs that you've
    selected to **Simulate** the rules.

  d.  Review the results in the **Logs output** table to determine if
      your **User defined** rules are fine or need further changes.

  - The **Logs output** table displays the following columns per dataset
    at the bottom of the window.

    - **Dataset**: Displays the applicable dataset name and a line
      number associated with this dataset in the **User defined** rules
      section.

    - **Vendor**: The vendor associated with this dataset.

    - **Product**: The product associated with this dataset.

    - **Output Logs**: Displays the available output log. When there is
      no output log to display, the text `Output logs is not available`
      with the corresponding error message is displayed. When there is
      no output due to a missing rule in the **User defined** rules
      section for the logs selected, the text
      **No output logs. You can change your parsing rules and try again**
      is displayed.

    - **Input Logs**: Displays the relevant input log with a right-click
      pivot to **Show diff** between the **Output Logs** and
      **Input Logs**.

  e.  (*Optional*) Modify your **User defined** rules and repeat steps
      #2-4 until you are satisfied with the results.

5.  (*Optional*) Override the default [Parsing Rules raw
    dataset](#UUIDb562a5de488db3ff0ec561dbaa21f50f).

6.  Save your changes.

#### Troubleshooting Parsing rules errors

> **Note**
>
> Only a user with Cortex Account Administrator or Instance
> Administrator permissions can access Parsing Rules.

To help you easily identify and resolve parsing errors in Cortex XSIAM,
all parsing errors are saved to a separate dataset called
`parsing_rules_errors`. This dataset displays important information
about each error, including the **RAW_LOG**, log metadata, Parsing Rule
metadata, and error description, which you need to effectively
troubleshoot the problem. In addition, a **Parsing Rules Error**
notification is sent to the Notification Center whenever a new parsing
error is added to the dataset.

##### Types of Parsing Errors

There are different types of parsing errors:

- Compilation Errors: Unable to compile a rule for different reasons
  including invalid function parameters, such as invalid regex.

- Data Format Errors: A mismatch between the expected data type, such as
  CEF, LEEF, or JSON with the actual data, such as TEXT or CSV.

- Runtime Errors: Unable to apply a rule to the data, such as an attempt
  to add a String to a Number.

##### Parsing Errors Dataset

All parsing errors and Cortex Data Model (XDM) errors are saved to a
dataset called `parsing_rules_errors`. The following table describes the
fields that are available when running a query in XQL Search for the
`parsing_rules_errors` dataset in alphabetical order.

> **Note**
>
> Some errors can only be found after the applicable logs are collected
> in Cortex XSIAM.

+-------------------------------+-----------------------+-----------------------+
| Field                         | Description           | Source                |
+===============================+=======================+=======================+
| \_BROKER_DEVICE_ID            | Displays the ID of    | Log Metadata          |
|                               | the Broker VM         |                       |
|                               | associated to the log |                       |
|                               | that triggered this   |                       |
|                               | error.                |                       |
+-------------------------------+-----------------------+-----------------------+
| \_BROKER_DEVICE_IP            | Displays the IP       | Log Metadata          |
|                               | address of the Broker |                       |
|                               | VM associated to the  |                       |
|                               | log that triggered    |                       |
|                               | this error.           |                       |
+-------------------------------+-----------------------+-----------------------+
| \_BROKER_DEVICE_NAME          | Displays the device   | Log Metadata          |
|                               | name of the Broker VM |                       |
|                               | associated to the log |                       |
|                               | that triggered this   |                       |
|                               | error.                |                       |
+-------------------------------+-----------------------+-----------------------+
| \_COLLECTOR_HOSTNAME          | Displays the host     | Log Metadata          |
|                               | name of the data      |                       |
|                               | collector associated  |                       |
|                               | to the log that       |                       |
|                               | triggered this error. |                       |
+-------------------------------+-----------------------+-----------------------+
| \_COLLECTOR_ID                | Displays the ID of    | Log Metadata          |
|                               | the data collector    |                       |
|                               | associated to the log |                       |
|                               | that triggered this   |                       |
|                               | error.                |                       |
+-------------------------------+-----------------------+-----------------------+
| \_COLLECTOR_IP_ADDRESS        | Displays the IP       | Log Metadata          |
|                               | address of the data   |                       |
|                               | collector associated  |                       |
|                               | to the log that       |                       |
|                               | triggered this error. |                       |
+-------------------------------+-----------------------+-----------------------+
| \_COLLECTOR_NAME              | Displays the name of  | Log Metadata          |
|                               | the data collector    |                       |
|                               | associated to the log |                       |
|                               | that triggered this   |                       |
|                               | error.                |                       |
+-------------------------------+-----------------------+-----------------------+
| \_COLLECTOR_TYPE              | Displays the type of  | Log Metadata          |
|                               | data collector        |                       |
|                               | associated to the log |                       |
|                               | that triggered this   |                       |
|                               | error.                |                       |
+-------------------------------+-----------------------+-----------------------+
| CONTENT_ID                    | Displays the          | Parsing Rule          |
|                               | `package_id` of a     |                       |
|                               | content pack          |                       |
|                               | containing the        |                       |
|                               | default Parsing Rule  |                       |
|                               | for which this error  |                       |
|                               | was generated.        |                       |
+-------------------------------+-----------------------+-----------------------+
| CREATED_AT                    | Displays a timestamp  | Parsing Rule          |
|                               | for when the rule,    |                       |
|                               | which generated the   |                       |
|                               | error, was created.   |                       |
+-------------------------------+-----------------------+-----------------------+
| END_LINE                      | Displays the last     | Parsing Rule          |
|                               | line of the           |                       |
|                               | particular rule       |                       |
|                               | associated to this    |                       |
|                               | error.                |                       |
+-------------------------------+-----------------------+-----------------------+
| ERROR_CATEGORY                | Displays the category | N/A                   |
|                               | of the error, which   |                       |
|                               | can be one of the     |                       |
|                               | following:            |                       |
|                               |                       |                       |
|                               | - **Compile**:        |                       |
|                               |   Compilation error,  |                       |
|                               |   such as syntax      |                       |
|                               |   error, missing      |                       |
|                               |   argument, and       |                       |
|                               |   invalid regex.      |                       |
|                               |                       |                       |
|                               | - **Data format**:    |                       |
|                               |   Errors relating to  |                       |
|                               |   the data format,    |                       |
|                               |   such as received    |                       |
|                               |   LEEF when expected  |                       |
|                               |   CEF.                |                       |
|                               |                       |                       |
|                               | - **Runtime**: Error  |                       |
|                               |   at run time, such   |                       |
|                               |   as an attempt to    |                       |
|                               |   add a String to a   |                       |
|                               |   Number.             |                       |
+-------------------------------+-----------------------+-----------------------+
| ERROR_MESSAGE                 | Displays the error    | N/A                   |
|                               | message.              |                       |
+-------------------------------+-----------------------+-----------------------+
| \_FINAL_REPORTING_DEVICE_IP   | Displays the IP       | Log Metadata          |
|                               | address of the device |                       |
|                               | that the log was      |                       |
|                               | collected from that   |                       |
|                               | triggered this error. |                       |
+-------------------------------+-----------------------+-----------------------+
| \_FINAL_REPORTING_DEVICE_NAME | Displays the name of  | Log Metadata          |
|                               | the device that the   |                       |
|                               | log was collected     |                       |
|                               | from that triggered   |                       |
|                               | this error.           |                       |
+-------------------------------+-----------------------+-----------------------+
| \_ID                          | Displays the Rule ID  | Parsing Rule          |
|                               | that triggered this   |                       |
|                               | error.                |                       |
+-------------------------------+-----------------------+-----------------------+
| INGEST_NULL                   | Displays a boolean    | Parsing Rule          |
|                               | value of either       |                       |
|                               | **TRUE** or **FALSE** |                       |
|                               | to indicate whether   |                       |
|                               | null value fields are |                       |
|                               | configured to be      |                       |
|                               | ingested or not. By   |                       |
|                               | default, null fields  |                       |
|                               | are ingested.         |                       |
+-------------------------------+-----------------------+-----------------------+
| NO_HIT                        | Displays the no-match | Parsing Rule          |
|                               | strategy configured   |                       |
|                               | for the rule group    |                       |
|                               | that generated the    |                       |
|                               | parsing error.        |                       |
+-------------------------------+-----------------------+-----------------------+
| \_PRODUCT                     | Displays the defined  | Log Metadata or       |
|                               | **PRODUCT**           | Parsing Rule          |
|                               | associated to the log |                       |
|                               | (for data format      |                       |
|                               | errors) or rule (for  |                       |
|                               | compilation and       |                       |
|                               | runtime errors) that  |                       |
|                               | triggered this error. |                       |
+-------------------------------+-----------------------+-----------------------+
| RAW_LOG                       | Displays the raw log  | Raw log               |
|                               | for the Parsing Rule  |                       |
|                               | error or parsed log   |                       |
|                               | for the Data Model    |                       |
|                               | Rule error.           |                       |
+-------------------------------+-----------------------+-----------------------+
| \_REPORTING_DEVICE_IP         | Displays the IP       | Log Metadata          |
|                               | address of the device |                       |
|                               | that the log          |                       |
|                               | originated from that  |                       |
|                               | triggered this error. |                       |
+-------------------------------+-----------------------+-----------------------+
| \_REPORTING_DEVICE_NAME       | Displays the name of  | Log Metadata          |
|                               | the device that the   |                       |
|                               | log originated from   |                       |
|                               | that triggered this   |                       |
|                               | error.                |                       |
+-------------------------------+-----------------------+-----------------------+
| RULE_TYPE                     | Displays the type of  | Parsing Rule          |
|                               | rule that triggered   |                       |
|                               | this error.           |                       |
+-------------------------------+-----------------------+-----------------------+
| START_LINE                    | Displays the first    | Parsing Rule          |
|                               | line of the           |                       |
|                               | particular rule       |                       |
|                               | associated to this    |                       |
|                               | error.                |                       |
+-------------------------------+-----------------------+-----------------------+
| TARGET_DATASET                | Displays the Target   | Parsing Rule          |
|                               | dataset associated to |                       |
|                               | the rule that         |                       |
|                               | triggered this error. |                       |
+-------------------------------+-----------------------+-----------------------+
| \_TIME                        | Displays the          | Raw log               |
|                               | timestamp when the    |                       |
|                               | error was generated.  |                       |
+-------------------------------+-----------------------+-----------------------+
| \_VENDOR                      | Displays the defined  | Raw log or Parsing    |
|                               | **VENDOR** associated | Rule                  |
|                               | to the log (for data  |                       |
|                               | format errors) or     |                       |
|                               | rule (for compilation |                       |
|                               | and runtime errors)   |                       |
|                               | that triggered this   |                       |
|                               | error.                |                       |
+-------------------------------+-----------------------+-----------------------+
| XDRC_ID                       | Displays the ID of    | Log Metadata          |
|                               | the XDR Collector     |                       |
|                               | associated to the log |                       |
|                               | that triggered this   |                       |
|                               | error.                |                       |
+-------------------------------+-----------------------+-----------------------+
| XDRC_IP                       | Displays the IP       | Log Metadata          |
|                               | address of the XDR    |                       |
|                               | Collector associated  |                       |
|                               | to the log that       |                       |
|                               | triggered this error. |                       |
+-------------------------------+-----------------------+-----------------------+
| XDRC_NAME                     | Displays the name of  | Log Metadata          |
|                               | the XDR Collector     |                       |
|                               | associated to the log |                       |
|                               | that triggered this   |                       |
|                               | error.                |                       |
+-------------------------------+-----------------------+-----------------------+
| XQL_TEXT                      | Displays the specific | Parsing Rule          |
|                               | section of the rule   |                       |
|                               | related to the error  |                       |
|                               | generated.            |                       |
+-------------------------------+-----------------------+-----------------------+

#### Parsing Rules Raw Dataset

> **Note**
>
> Only a user with Cortex Account Administrator or Instance
> Administrator permissions can access Parsing Rules.

Each vendor and product has its own raw dataset that uses the format
`<vendor>_<product>_raw`. For example, for Palo Alto Networks
Next-Generation Firewall, the dataset is called `panw_ngfw_raw`. This
raw dataset by default keeps all raw logs, whether ingested or dropped
for other datasets.

You can override the default raw dataset, by creating an `INGEST`
section referring to that dataset.

The following syntax overrides the `panw_ngfw_raw` automatic Parsing
Rule:

    [ingest:vendor=panw, product=ngfw, target_dataset=panw_ngfw_raw]
    filter ... | alter ...;

### Data Model Rules

> **Note**
>
> Only a user with Cortex Account Administrator or Instance
> Administrator permissions can access Data Model Rules.

Learn more about Cortex Data Model (XDM) Rules.

#### What are Data Model Rules?

> **Note**
>
> Only a user with Cortex Account Administrator or Instance
> Administrator permissions can access Data Model Rules.

Cortex XSIAM enables you to map your logs into a single, unified data
model. This data model provides a consolidated schema, and a simpler way
to interact with your data, regardless of its source or dataset. To
familiarize yourself with the data model schema, see [Cortex XSIAM Data
Model
Schema](https://docs-cortex.paloaltonetworks.com/r/0aa6Nsr7VgBlIGGUI8TKHg/root).

You can map your data to the data model using Data Model Rules, either
by using the Default Rules that are automatically added when installing
Content Packages from the Marketplace, or by creating user-defined
rules. You create rules with the Data Model Rules editor, which enables
you to do the following:

- Map 3^rd^ party data to a consolidated schema with predefined data
  types.

- Enjoy auto-complete and mapping suggestions.

- Map multiple quarriable datasets to the data model.

Data Model Rules contain the following built-in characteristics:

- Each Data Model Rule is mapped between one dataset and the data model.

- A Data Model Rule takes rows from a dataset to use as an input,
  performs an arbitrary number of transitions and modifications on each
  column in the dataset using Cortex Query Language (XQL), and then
  returns the normalized rows with the corresponding data model's
  schema.

#### Data Model Rules editor views

> **Note**
>
> Only a user with Cortex Account Administrator or Instance
> Administrator permissions can access Data Model Rules.

The Data Model Rules editor contains the following views.

- **User Defined Rules** (default): Displays an editor for writing your
  own custom Data Model Rules that override the default rules. Once you
  edit a default Data Model mapping, you will no longer receive
  Marketplace updates.

- **Default Rules** (read-only): Displays the data model rules that are
  provided by default.

- **Both**: Side-by-side view of both the **Default Rules** and
  **User Defined Rules**, so that you can easily view both sets of rules
  on the same screen.

#### Data Model Rules file structure and syntax

> **Note**
>
> Only a user with Cortex Account Administrator or Instance
> Administrator permissions can access Data Model Rules.

##### File structure

The Data Model Rules file consists of multiple sections of the following
two types, which also represent the custom syntax specific to Data Model
Rules:

- [MODEL](#UUIDafdf6badf4a75dd79aa1d38d85bf277d): This section is used
  to define the mapping between a single dataset and the data model.

- (OPTIONAL) [RULE](#UUID2c00d999c7aba07e7fde65f0011208e7): Rules are
  part of the Cortex Query Language (XQL) syntax, which are tagged with
  a name, and can be reused in the code in the **MODEL** sections, or in
  other **RULE** sections (recursively), by using `[rule:ruleName]`.

The order of the sections is not significant.

##### Syntax

The syntax used in the Data Model Rules file is derived from XQL, with a
few modifications. This subset of XQL is called
*XQL for Data Modeling (XQLm)*.

> **Note**
>
> For more information on XQL syntax, see the XQL Language Reference
> Guide.

In the `MODEL` and `RULE` sections, the following modifications apply to
the XQLm syntax:

- Only the following XQL stages are permitted:
  [alter](#UUID29adfeb71cc64bb046509f7b4089e1b5) and
  [filter](#UUID75b17bafc8212d4207f11d67acd47c76). An additional `call`
  stage is supported, which is used to invoke another rule.

<!-- -->

- > **Note**

  > You cannot `call` a `RULE` section that exists in Default Rules from
  > the User Defined Rules section.

<!-- -->

- No output stages are supported.

- `XDM_ALIAS` cannot be used in rules. It is only supported in queries.
  For more information, see the
  [search](#UUID798f98339a9b4beb22e46a9612ba92bb) stage.

- Every model definition in the Data Model Rules file must end with a
  semicolon (`;`).

- Each XDM field used in the `MODEL` and `RULE` sections is constructed
  using dot notation using the following format:

<!-- -->

- xdm.[<context>].[<compound>].<field>

  For more information, see [Field
  structure](#UUID9e05948ad5126d212187da01584c3729).

##### MODEL

A **MODEL** section is used to define the mapping between a single
dataset and the data model. The **MODEL** section is mandatory per
dataset. A **RULE** section is optional, and is used to help organize
the **MODEL** sections.

**MODEL** syntax is derived from Cortex Query Language (XQL), with a few
modifications, as explained in [Data Model Rules file structure and
syntax](#UUIDba7c041896f11aed9f214bc9cd95cc6a). In addition, **MODEL**
sections contain the following syntax add-ons:

- You can have multiple **MODEL** sections.

- **MODEL** sections take parameters, and not names as **RULE** sections
  use, where some are mandatory and others are optional.

<!-- -->

- [MODEL: dataset=<dataset>, content_id=<content_id>]
      <build the XQL logic>;

The parameter descriptions are explained in the following table:

  -----------------------------------------------------------------------
  Parameter                           Description
  ----------------------------------- -----------------------------------
  dataset                             The name of the dataset that
                                      contains the source data to apply
                                      the mapping on (*mandatory*).

  content_id                          Identifier of the content as
                                      defined in the content package from
                                      the Marketplace. This parameter is
                                      relevant only for Default Rules and
                                      is not available in User Defined
                                      Rules (*optional*).
  -----------------------------------------------------------------------

    [MODEL: dataset=panw_ngfw_traffic]
    filter appid = "dns"
    | alter dns_helper = json_extract(event, "$.dns")
    | alter xdm.network.dns.opcode = to_integer(json_extract_scalar(dns_helper, "$.opcode"),
            xdm.network.dns.is_truncated = to_boolean(json_extract_scalar(dns_helper, "$.is_truncated")
    );

###### Points to keep in mind when writing MODEL sections

- **MODEL** parameter names are not case-sensitive.

- Cortex Data Model (XDM) System fields (`_time`, `_insert_time`,
  `_vendor`, `_product`) are mapped automatically from the dataset from
  the fields with the same names.

- As section order is not significant, you do not have to declare a
  `RULE` before using it in a `MODEL` section.

- Each field used in the `MODEL` and `RULE` sections is constructed
  using dot notation with a specific format. Each field must be part of
  the predefined field set of the data model\'s schema. However,
  temporary variables, which will not affect the modeling, may be used.
  For more information, see [Field
  structure](#UUID9e05948ad5126d212187da01584c3729).

- A `MODEL` section can invoke a rule using the `call` stage.

<!-- -->

- In this example, both the
  [RULE](#UUID2c00d999c7aba07e7fde65f0011208e7) and `MODEL` sections are
  provided, so you can see how the `call` stage invokes the rule.

      [RULE: common_ngfw_modeling]
      alter xdm.source.ipv4 = json_extract_scalar(actor, "$.client_ip")
      | alter xdm.network.ip_protocol = if(
          proto = 6, XDM_CONST.IP_PROTOCOL_TCP,
          proto = 11, XDM_CONST.IP_PROTOCOL_UDP,
          proto
      );

      [MODEL: dataset=panw_ngfw_traffic]
      filter appid = "dns"
      | call common_ngfw_modeling
      | alter dns_helper = json_extract(event, "$.dns")
      | alter xdm.network.dns.opcode = to_integer(json_extract_scalar(dns_helper, "$.opcode"),
              xdm.network.dns.is_truncated = to_boolean(json_extract_scalar(dns_helper, "$.is_truncated")
      );

<!-- -->

- You can use the `config case_sensitive` stage in the `MODEL` section
  to configure whether field values in the XDM are evaluated as case
  sensitive or case insensitive. The `config case_sensitive` stage must
  be added at the beginnning of the query. If you do not provide this
  stage in your query, the default behavior is `false`; case is not
  considered when evaluating field values.

<!-- -->

- > **Note**

  > The Settings \> Configurations \> XQL Configuration \> Case
  > Sensitivity (case_sensitive) setting can overwrite this
  > `case_sensitive` configuration for all fields in the application
  > except for BIOCs, which will remain case insensitive no matter what
  > this setting is set to. For more information on this setting, see
  > [XQL Configuration](#UUIDe99e556efcc7391bff6fddf7e17625cf).

<!-- -->

- Cortex XSIAM enables analytics to run on the following data:

  - All mapped network data to the network 5 tuple (source IP, source
    port, target IP, target port, IP protocol), automatically creating
    network stories for XDM network data.

  - All mapped authentication data, automatically creating
    authentication stories for XDM identity data when certain mandatory
    fields are mapped. For more information, see [How to map
    authentication story
    events?](#UUID78102dec163e28d4bfb33dbd9a7d20e6).

<!-- -->

- > **Note**

  > We recommend that you do not configure the same data source in both
  > [Marketplace](#UUIDbd7bb72a06838e5b74c7b3b159596497) and using a
  > Cortex XSIAM [data
  > collector](#UUIDd3cabef50e385d5ee3ce5edd94719d6a). Yet, if you do,
  > the following will happen:

  - > For network data, all relevant logs from the different data
    > sources are stitched to the same network story.

  - > For authentication data, all relevant logs from the different data
    > sources are stitched to the same authentication story as long as
    > the logs contain the network 5 tuple (source IP, source port,
    > target IP, target port, IP protocol). The rest of the logs,
    > without the network 5 tuple, create duplicate authentication
    > stories.

##### RULE

Rules are very similar to functions in modern programming languages.
They are essentially named pieces of Cortex Query Language (XQL) syntax,
and can be reused in the code in the `MODEL` sections, or in other
`RULE` sections (recursively), by using `[rule:ruleName]`. A `RULE` is
an optional data model syntax.

`RULE` syntax is derived from XQL with a few modifications, as explained
in the [Data Model Rules file structure and
syntax](#UUIDba7c041896f11aed9f214bc9cd95cc6a).

> **Note**
>
> For more information on the XQL syntax, see the XQL Language Reference
> Guide.

###### Points to keep in mind when writing RULE sections

- Rules are defined by `[rule:ruleName]` as shown in the following
  example:

<!-- -->

- [RULE: common_ngfw_modeling]
      alter xdm.source.ipv4 = json_extract_scalar(actor, "$.client_ip")
      | alter xdm.network.ip_protocol = if(
          proto = 6, XDM_CONST.IP_PROTOCOL_TCP,
          proto = 11, XDM_CONST.IP_PROTOCOL_UDP,
          proto
      );

<!-- -->

- Rules are invoked by using a `call` stage.

- Rule names are not case-sensitive.

- Rule names must be unique across the entire file.

- As section order is not significant, you do not have to declare a
  `rule` before using it. You can have the `rule` definition section
  written below other sections that use that specific rule.

- Each field used in the `MODEL` and `RULE` sections is constructed
  using dot notation with a specific format. However, temporary
  variables, which will not affect the modeling, can be used. For more
  information, see [Field
  structure](#UUID9e05948ad5126d212187da01584c3729).

##### Field structure

When creating Data Model Rules, each field used in the `MODEL` and
`RULE` sections is constructed using dot notation using the following
format:

    xdm.<context>.[<compound>].<field>

- `xdm.<context>.[<compound>].<field>`

<!-- -->

- xdm.source.host.device_id

<!-- -->

- `xdm.<context>.<field>`

<!-- -->

- xdm.source.ipv4

  -----------------------------------------------------------------------
  Part                                Description
  ----------------------------------- -----------------------------------
  `<context>`                         This is a composition of fields
                                      (`<field>`), either simple or
                                      `<compound>`, that are grouped
                                      together to form a logically
                                      coherent unit.

  `<compound>`                        This is a set of simple fields that
                                      are grouped together to form a
                                      meaningful group. For example,
                                      `subject` and `recipients` are part
                                      of the `<compound>` field called
                                      `email`.

  `<field>`                           This is a field that represents a
                                      primitive data type, such as a
                                      string or number or an array, or an
                                      IP address.
  -----------------------------------------------------------------------

> **Note**
>
> For more information on these data model fields, see [Cortex XSIAM
> Data Model
> Schema](https://docs-cortex.paloaltonetworks.com/r/0aa6Nsr7VgBlIGGUI8TKHg/root).

###### Using ENUM fields

For fields of the `ENUM` type, you can map values from a predefined list
of ENUMs. For example, the field `xdm.network.ip_protocol` is defined as
`Enum.IP_PROTOCOL`, so you can assign it values such as
`XDM_CONST.IP_PROTOCOL_TCP`. The full list can be found in the
automatically suggested values for the relevant fields.

This syntax is not mandatory, and you can map any `STRING` value, but we
recommend its use for consistency across all model mapping.

    [RULE: common_ngfw_modeling]
    alter xdm.source.ipv4 = json_extract_scalar(actor, "$.client_ip")
    | alter xdm.network.ip_protocol = if( 
        proto = 6, XDM_CONST.IP_PROTOCOL_TCP, 
        proto = 11, XDM_CONST.IP_PROTOCOL_UDP, 
        proto
    );

#### How to map authentication story events?

Cortex XSIAM enables analytics to run on all mapped authentication data,
which automatically creates authentication stories for Cortex Data Model
(XDM) identity data. As a result, you need to map authentication events
to the Cortex XSIAM XDM schema to build the authentication story. For a
complete list of these fields, see [XDM fields for mapping
authentication events](#UUID972b3c36452674eb82ca69ddfcf3eec1).

> **Scope Clarification**
>
> This Feature focuses on authentication events related to SSO (Single
> Sign-On) and SaaS (Software-as-a-Service) application authentications.
> It does not cover internal authentication mechanisms such as Kerberos,
> NTLM, or traditional domain logon events generated by on-premise
> infrastructure.
>
> **Prerequisite**
>
> Familiarize yourself with the Cortex Data model (XDM) schema for field
> definitions and naming conventions, see [Cortex XSIAM Data Model
> Schema](https://docs-cortex.paloaltonetworks.com/r/Cortex-XSIAM/Data-Model-Schema-Guide-for-Cortex-XSIAM-with-Cortex-Cloud/Introduction).

Mapping principals

When mapping your authentication data, follow these guidelines:

- **Prioritize conclusive events**: Focus on mapping events that clearly
  represent the final stage or result of an authentication process.

- **Exclude ambiguous or non-final steps from outcome decisions**:
  Intermediate or informational events should not be treated as
  indicators of success or failure.

- **Preserve context across the full authentication flow**: Include
  intermediate events to provide visibility into the process, while
  making it clear they are not final outcomes.

- **Normalize raw error and outcome data**: You must define explicit
  mapping logic between the raw event fields that contain outcome or
  error messages, such as `get_reason` and `debugdata_errorcode`, and
  the target XDM fields. This logic should normalize provider-specific
  strings or codes into a canonical format to support reliable detection
  and analytics across diverse sources.

Why follow the mapping principals?

- Prevents misclassification of failed sessions as successful.

- Avoids distorted behavioral baselines that can mask real attacks.

- Preserves full visibility of authentication flows without misleading
  analytics.

Third-party mapping examples

- [DUO- SSO - Data Model
  Mapping](https://github.com/demisto/content/blob/master/Packs/DuoAdminApi/ModelingRules/DuoModelingRule_2_0/DuoModelingRule_2_0.xif#L26)

- [Okta - SSO - Data Model
  Mapping](https://github.com/demisto/content/blob/master/Packs/Okta/ModelingRules/OktaModelingRules_2_0/OktaModelingRules_2_0.xif)

Mandatory XDM fields to map for authentication events

There are mandatory fields that you need map to the Cortex Data Model
(XDM) schema to build authentication stories based on the mapped
authentication data. For more detailed information on these fields, see
[XDM fields for mapping authentication
events](#UUID972b3c36452674eb82ca69ddfcf3eec1).

The following fields are mandatory to map::

- `xdm.source.port`

- `xdm.target.ipv4`

- `xdr.target.port`

- `xdm.network.ip_protocol`

- `xdm.source.ipv4`

- `xdm.event.type`

- `xdm.event.tags`

- `xdm.event.operation`

- `xdm.event.original_event_type`

- `xdm.auth.service`

- `xdm.event.outcome`

- `xdm.event.outcome_reason`

- `xdm.source.user.upn`

> **Important**
>
> To maximize the variety of issues that are retrieved based on the XDM
> authentication stories, we recommend that the following additional
> fields are populated: `xdm.target.resource_name`, `xdm.logon.type`,
> `xdm.source.user_agent`, and `xdm.source.host.device_category`. Should
> you decide to change the default XDM mappings, ensure that both the
> mandatory and recommended fields are populated and do not contain any
> empty values.

#### Create Data Model Rules

> **Note**
>
> Only a user with Cortex Account Administrator or Instance
> Administrator permissions can access Data Model Rules.

You can override rules or create your own rules using XQL and additional
custom syntax that is specific to defining Data Model Rules. Once you
edit a default data model mapping, you will no longer receive
Marketplace updates.

> **Prerequisite**

- > [Data Model Rules editor
  > views](#UUIDfb22b854c38f8e5e80f3caeb9716e283)

- > [Data Model Rules file structure and
  > syntax](#UUIDba7c041896f11aed9f214bc9cd95cc6a)

- > [How to map authentication story
  > events?](#UUID78102dec163e28d4bfb33dbd9a7d20e6)

**How to create Data Model Rules**

1.  In Cortex XSIAM, select Settings \> Configurations \> Data
    Management \> Data Model Rules.

2.  Select the Data Model editor view for writing your Data Model Rules.

- You can select one of the following views:

  - **User Defined Rules**: Leave the default view open and write your
    Data Model Rules directly in the editor.

  - **Both**: Select this view to see the Data Model Rules editor as
    well as the default rules as you write your Data Model Rules.

3.  Write your rules using XQL syntax and the syntax specific to Data
    Model Rules.

4.  (*Optional*) Use XQL Search to test your Data Model Rules and review
    logs.

- You can create queries on the data model. For more information, see
  [Create XQL query](#UUID5f5b967337c3cb75a413d47e8b461681).

#### Troubleshooting Data Model Rules

> **Note**
>
> Only a user with Cortex Account Administrator or Instance
> Administrator permissions can access Data Model Rules.

To help you easily identify and resolve errors related to invalid Cortex
Data Model (XDM) Rules, Cortex XSIAM provides the following:

- When an XDM query runs and one of the Data Model Rules is invalid, the
  invalid rule is automatically disabled and excluded from the query,
  and a warning is displayed.

- When a Data Model Rule is disabled, a message is added to your Cortex
  XSIAM console Notification Center. For more information about the Data
  Model Rules notifications, see [Data Model Rules
  notifications](#UUIDfa91650da76f133a651a04a3921b9134).

- The Data Model Rules editor displays an error icon and a message
  beside invalid Data Model Rules.

- An audit log is added to the Management Audit Log whenever a Data
  Model Rule becomes invalid, and when an invalid Data Model Rule
  becomes valid.

<!-- -->

- > **Tip**

  > To ensure you and your colleagues stay informed about Data Model
  > Rules activity, you can also [Configure notification
  > forwarding](#UUID3738ce324545c768e170afa247d5abc2) to forward your
  > Data Model Rules audit logs to an email distribution list or Syslog
  > server. For more information about the Data Model Rules audit logs,
  > see [Monitor Data Model Rules
  > activity](#UUID9a4d1ac10d9689653bcc1c8f04d6df7b).

<!-- -->

- When a rule is fixed, it is automatically enabled. User defined Data
  Model Rules are updated manually in the User Defined Rules editor.
  While default Data Model Rules are updated as part of a Marketplace
  package update, or a background change, such as an XQL content change.

- All Data Model Rules compilation errors are added to the
  `parsing_rules_errors` dataset.

##### Dataset for Data Model Rules Errors

All Data Model Rules compilation errors, such as syntax errors, missing
arguments, and invalid regex, are saved to a dataset called
`parsing_rules_errors`. This dataset also includes Parsing Rules errors.
The following table describes the fields that are applicable to
troubleshooting Data Model Rules errors when running a query in XQL
Search for the `parsing_rules_errors` dataset in alphabetical order.

> **Note**
>
> Since this dataset also contains Parsing Rules errors, some of the
> fields are irrelevant for Data Model Rules and aren\'t included in the
> table.

  -----------------------------------------------------------------------
  Field                               Description
  ----------------------------------- -----------------------------------
  CREATED_AT                          Displays the timestamp when the
                                      error was generated.

  ERROR_CATEGORY                      Displays the category of the error,
                                      which for Data Model Rules errors
                                      is always **Compile** for
                                      compilation errors.

  ERROR_MESSAGE                       Displays the error message.

  \_ID                                Displays the Rule ID that triggered
                                      this error.

  RULE_TYPE                           Displays the type of rule that
                                      triggered this error.

  TARGET_DATASET                      Displays the target dataset
                                      associated to the rule that
                                      triggered this error.

  \_TIME                              Displays the timestamp when the
                                      error was generated.

  XQL_TEXT                            Displays the specific section of
                                      the Data Model Rule related to the
                                      error generated.
  -----------------------------------------------------------------------

#### Using data enrichment

Cortex XSIAM automatically enriches your Cortex Data Model (XDM) data
with additional information and context. Some examples of the types of
data that are enriched include:

> **Note**
>
> For a complete list of auto-enriched fields, see the [Cortex Data
> Model Schema
> Guide](https://docs-cortex.paloaltonetworks.com/r/Cortex-XSIAM/Cortex-Data-Model-Schema-Guide/Introduction).

- IP addresses are enriched with geolocation information.

- User data is normalized.

- If DSS exists, it is also enriched.

These enrichments are important for cyber analytics, rule detection, and
investigations. Since these fields are enriched automatically by
default, they do not have to be mapped manually in Data Model Rules.
Note that enrichment is not performed when the input fields needed for
enrichment are not available.

Enriched data is calculated by the system upon ingestion, and is saved
for future queries. Keep in mind that some data may change over time,
such as IP addresses that may change geolocation.  Therefore, checking
the same IP address in external systems at a later time might return a
different geolocation result.

##### Overriding Data Enrichment

We do not recommend overriding enriched fields. However, if enriched
fields are not desired, they can be overridden by mapping data to fields
that are usually enriched.

    [MODEL: dataset=okta_sso_raw]
    | alter xdm.source.ip = actor->ip_address,
          xdm.source.location.country = actor->country,
          xdm.source.location.city = actor->geo.city;

When overriding enriched fields, ensure the following:

- The overridden data should be normalized. 

- All relevant enriched fields should be overridden (for example, all
  location fields), and empty values should be filled with "unknown" (or
  with NULL, if calculated enrichments are desired). These actions will
  prevent data mismatch and conflicts.

> **Important**
>
> When manually mapping ASN fields that are enriched, such as
> `xdm.source.asn.as_number`, with other ISP and domain fields that are
> not enriched, such as `xdm.source.asn.isp` and
> `xdm.source.asn.domain`, it\'s possible to receive incorrect XDM query
> results due to the misalignment between the overridden enrichement and
> system enrichment fields.

##### Limitations

- Geolocation limitations

  - Some values will be NULL if the log country doesn\'t match the
    country detected by an external geolocation tool.

  - There might be discrepancies when some data come from the log and
    other data from the enrichment. For example, log country data versus
    enrichment longitude data.

- Data enrichment is not performed for EDR events.

- This feature is not supported in cold storage.

##### Backward compatibility

Data ingested by versions prior to Cortex XSIAM version 1.3 will not be
enriched, because enrichment is calculated at the time of ingestion.

By default, enrichment is performed for NULL values only (non-NULL
values are not overridden). Therefore, some existing mapping rules may
need to be updated, in order to prevent mapping data to the enriched
fields. Contact Customer Support for assistance with converting custom
modeling rules and saved queries.

##### 

#### Data Model Rules notifications

To help you monitor effectively your Data Model Rules, Cortex XSIAM
sends notifications to your Cortex XSIAM console Notification Center.

Cortex XSIAM sends the following notification:

- **Invalid Data Model Rules**: Notifies when a Data Model Rule is
  invalid and will be excluded from `datamodel` queries.

To ensure you and your colleagues stay informed about Data Model Rules
activity, you can also [Configure notification
forwarding](#UUID3738ce324545c768e170afa247d5abc2) to forward your Data
Model Rules logs to an email distribution list or Syslog server. For
more information about the Data Model Rules audit logs, see [Monitor
Data Model Rules activity](#UUID9a4d1ac10d9689653bcc1c8f04d6df7b).

#### Monitor Data Model Rules activity

Cortex XSIAM logs entries for events related to the Data Model Rules
monitored activities. Cortex XSIAM stores the logs for 365 days. To view
the Data Model Rules audit logs, select Settings \> Management Audit
Logs.

To ensure you and your colleagues stay informed about Data Model Rules
activity, you can [Configure notification
forwarding](#UUID3738ce324545c768e170afa247d5abc2) to forward your Data
Model Rules audit logs to an email distribution list or Syslog server.

You can customize your view of the logs by adding or removing filters to
the **Management Audit Logs** table. You can also filter the page result
to narrow down your search. The following table describes the default
and optional fields that you can view in the Cortex XSIAM
**Management Audit Logs** table:

> **Note**
>
> Certain fields are exposed and hidden by default. An asterisk (\*) is
> beside every field that is exposed by default.

+-----------------------------------+-----------------------------------+
| Field                             | Description                       |
+===================================+===================================+
| Description\*                     | Log message that describes the    |
|                                   | action.                           |
+-----------------------------------+-----------------------------------+
| Email                             | Email of the user who performed   |
|                                   | the action.                       |
+-----------------------------------+-----------------------------------+
| Host Name\*                       | This field is not applicable for  |
|                                   | Data Model Rules logs.            |
+-----------------------------------+-----------------------------------+
| ID                                | Unique ID of the action.          |
+-----------------------------------+-----------------------------------+
| Reason                            | This field is not applicable for  |
|                                   | Data Model Rules logs.            |
+-----------------------------------+-----------------------------------+
| Result\*                          | The result of the action (        |
|                                   | `Success`, `Fail`, or `Partial`)  |
+-----------------------------------+-----------------------------------+
| Severity\*                        | Severity associated with the log: |
|                                   |                                   |
|                                   | - `Critical`                      |
|                                   |                                   |
|                                   | - `High`                          |
|                                   |                                   |
|                                   | - `Medium`                        |
|                                   |                                   |
|                                   | - `Low`                           |
|                                   |                                   |
|                                   | - `Informational`                 |
+-----------------------------------+-----------------------------------+
| Timestamp\*                       | Date and time when the action     |
|                                   | occurred.                         |
+-----------------------------------+-----------------------------------+
| Type\* and Sub-Type\*             | Additional classifications of     |
|                                   | Data Model Rules logs (Type and   |
|                                   | Sub-Type):                        |
|                                   |                                   |
|                                   | - **XDM Config**:                 |
|                                   |                                   |
|                                   |   - **Saving XDM mappings file**: |
|                                   |     Indicates whenever a Data     |
|                                   |     Model Rule is saved in the    |
|                                   |     editor, the specific changes  |
|                                   |     made to the Cortex Data Model |
|                                   |     (XDM) mappings. In addition,  |
|                                   |     indicates whenever the        |
|                                   |     changes weren\'t able to be   |
|                                   |     saved.                        |
|                                   |                                   |
|                                   |   - **Disabled**: Indicates the   |
|                                   |     Data Model Rule and           |
|                                   |     associated dataset that are   |
|                                   |     now disabled. This invalid    |
|                                   |     rule is excluded from the     |
|                                   |     query until the changes are   |
|                                   |     made to fix the problem.      |
|                                   |                                   |
|                                   |   - **Enabled**: Indicates the    |
|                                   |     Data Model Rule and           |
|                                   |     associated dataset that have  |
|                                   |     been updated and are now      |
|                                   |     enabled.                      |
+-----------------------------------+-----------------------------------+
| User Name\*                       | Name of the user who performed    |
|                                   | the action.                       |
+-----------------------------------+-----------------------------------+

### Manage Event Forwarding

> **Note**
>
> This feature requires an Event Forwarding add-on license. Only
> Administrators have access to this screen.

You can save your ingested, parsed data in an external location by
exporting your event logs to a temporary storage bucket on Google Cloud
Platform (GCP), from where you can download them for up to 7 days.

Use the **Event Forwarding** page to activate your Event Forwarding
licenses, to retrieve the path and credentials of your external storage
destination on GPC. Once this page is activated, Cortex XSIAM
automatically creates the GCP bucket.

> **Important**
>
> Since data is aggregated and compressed, it can take up to two hours
> until the data is available in the forwarding bucket.

#### Upload to a temporary GCP storage bucket

1.  Under Settings \> Configurations \> Data Management \> Event
    Forwarding, activate the licenses in the **Activation** section.

    - **Enable GB Event Forwarding** to export parsed logs to an
      external SIEM for storage. This enables you to keep data in your
      own storage in addition to the Cortex XSIAM data layer, for
      compliance requirements and machine learning purposes. The
      exported logs are raw data, without any stories. Cortex XSIAM
      exports all the data without filtering or configuration options.

    - **Enable Endpoints Event Forwarding** to export raw endpoint data
      for Cortex XSIAM Pro EP and Cloud Endpoints. The exported logs are
      raw data, without any stories. Cortex XSIAM exports a subset of
      the endpoint data without filtering or configuration options.

2.  **Save** your selection.

3.  Access GCP Cloud Storage using the Service Account.

- The **Destination** section displays the details of the GCP bucket
  created by Cortex XSIAM, where your data is stored for 7 days. The
  data is compressed and saved as a line-delimited JSON gzip file.

  a.  **Copy** the storage path displayed.

  b.  **Generate and download** the Service Account JSON WEB TOKEN,
      which contains the access key.

  - Save it in a secure location. If you need to regenerate the access
    token, **Replace and download** a new token. This action invalidates
    the previous token.

    The token provides access to all your data stored in this bucket. It
    must be saved in a safe place.

    Use the storage path and access key to manually retrieve your files
    or use an API for automated retrieval.

  c.  Using the storage path and the access key, retrieve your files
      manually or using an API.

      - [Authenticating as a service
        account](https://cloud.google.com/docs/authentication/production)

      - [Copying files and objects from
        GCP](https://cloud.google.com/storage/docs/gsutil/commands/cp)

4.  (Optional) Use the Pub/Sub subscription to ensure reliable data
    retrieval without any loss.

    a.  **Copy** the Pub/Sub subscription provided.

    b.  Configure your application or system to receive messages from
        the Pub/Sub subscription.

    - Whenever a new file is added to the GCS bucket, a message is sent
      to the Pub/Sub subscription. The object path of the file in the
      bucket has the prefix `internal/`.

    c.  Process the received message to initiate the download of the
        corresponding file.

#### Endpoints Event Forwarding - included/excluded fields by event type

> **Note**
>
> This feature requires an Event Forwarding add-on license. Only
> Administrators have access to this screen.

Endpoints Event Forwarding exports ingested, parsed endpoint data for
Cortex XDR pro EP and Cloud Endpoints. The exported logs are raw data,
without any stories. Cortex XSIAM exports the data without filtering or
configuration options. The tables below list the fields that are
included and excluded for:

- Types of events exported for the endpoints

- Common fields for all event types

##### Types of events exported for the endpoints

The table below lists the types of events exported for the endpoints and
the fields that are included and excluded:

  -------------------------------------------------------------------------------------------------------------------------------------
  Exported event type                  Included field                                Excluded field
  ------------------------------------ --------------------------------------------- --------------------------------------------------
  **Network**                          action_socket_type                            is_boot_replay

                                       action_remote_ip                              action_proxy

                                       action_remote_port                            action_network_app_ids

                                       action_local_ip                               action_network_rule_ids

                                       action_local_port                             action_network_dpi_fields

                                       action_network_connection_id                  action_network_is_loopback

                                       action_network_is_server                      action_upload

                                       action_network_creation_time                  action_download

                                       action_total_upload                           action_network_stats_seq

                                       action_total_download                         action_network_is_ipv6

                                       action_network_protocol                       

                                       action_network_stats_is_last                  

  **Process**                          uuid / \_id                                   action_process_causality_id

                                       action_process_os_pid                         action_process_is_causality_root

                                       action_process_instance_id                    action_process_is_replay

                                       action_process_image_md5                      action_process_yara_file_scan_result

                                       action_process_image_sha256                   action_process_wf_verdict

                                       action_process_image_path                     action_process_static_analysis_score

                                       action_process_image_name                     execution_actor_causality_id

                                       action_process_image_extension                action_process_ns_pid

                                       action_process_image_command_line             action_process_container_id

                                       action_process_signature_product              action_process_is_container_root

                                       action_process_signature_vendor               action_process_image_command_line_indices

                                       action_process_signature_is_embedded          action_process_is_special

                                       action_process_signature_status               action_process_ns_user_sid

                                       action_process_integrity_level                action_process_ns_user_real_sid

                                       action_process_username                       action_process_file_size

                                       action_process_user_sid                       action_process_file_create_time

                                       action_process_in_txn                         action_process_file_mod_time

                                       action_process_pe_load_info                   action_process_remote_session_ip

                                       action_process_peb                            action_process_file_info

                                       action_process_peb32                          action_process_device_info

                                       action_process_last_writer_actor              execution_actor_instance_id

                                       action_process_token                          action_process_user_real_sid

                                       action_process_privileges                     action_process_requested_parent_pid

                                       action_process_fds                            action_process_requested_parent_iid

                                       action_process_scheduled_task_name            

                                       action_process_termination_date               

                                       action_process_instance_execution_time        

                                       action_process_termination_code               

  **File**                             action_file_path                              action_file_wf_verdict

                                       action_file_name                              action_file_yara_file_scan_result

                                       action_file_previous_file_path                action_file_dir_query

                                       action_file_previous_file_name                action_file_previous_device_info

                                       action_file_md5                               action_file_device_info

                                       action_file_sha256                            action_file_reparse_path

                                       action_file_size                              action_file_reparse_count

                                       action_file_attributes                        action_file_dirty_reason

                                       action_file_create_time                       action_file_remote_ip

                                       action_file_mod_time                          action_file_remote_port

                                       action_file_access_time                       action_file_remote_file_ip

                                       action_file_type                              action_file_remote_file_host

                                       action_file_operation_flags                   action_file_sec_desc

                                       action_file_mode                              action_file_previous_file_extension

                                       action_file_owner                             action_file_extension

                                       action_file_owner_name                        action_file_archive_list

                                       action_file_group                             action_file_contents

                                       action_file_group_name                        

                                       action_file_device_type                       

                                       action_file_signature_product                 

                                       action_file_signature_vendor                  

                                       action_file_signature_is_embedded             

                                       action_file_signature_status                  

                                       action_file_pe_info                           

                                       action_file_prev_type                         

                                       action_file_last_writer_actor                 

                                       action_file_is_anonymous                      

  **Registry**                         action_registry_value_type                    

                                       action_registry_key_name                      

                                       action_registry_data                          

                                       action_registry_value_name                    

                                       action_registry_old_key_name                  

                                       action_registry_file_path                     

                                       action_registry_return_val                    

  **Injection**                        action_remote_process_thread_id               action_remote_process_causality_id

                                       action_remote_process_os_pid                  action_remote_process_is_causality_root

                                       action_remote_process_instance_id             action_remote_process_is_replay

                                       action_remote_process_image_md5               action_remote_process_image_extension

                                       action_remote_process_image_sha256            action_remote_process_image_command_line_indices

                                       action_remote_process_image_path              action_remote_process_is_special

                                       action_remote_process_image_name              action_remote_process_file_size

                                       action_remote_process_image_command_line      action_remote_process_file_create_time

                                       action_remote_process_signature_product       action_remote_process_file_mod_time

                                       action_remote_process_signature_vendor        action_remote_process_file_info

                                       action_remote_process_signature_is_embedded   

                                       action_remote_process_signature_status        

                                       action_remote_process_thread_start_address    

                                       action_remote_process_integrity_level         

                                       action_remote_process_username                

                                       action_remote_process_user_sid                

                                       address_mapping                               

  **Load Image**                       action_module_path                            action_module_is_replay

                                       action_module_md5                             action_module_yara_file_scan_result

                                       action_module_sha256                          action_module_file_size

                                       action_module_base_address                    action_module_file_create_time

                                       action_module_image_size                      action_module_file_mod_time

                                       action_module_signature_product               action_module_file_access_time

                                       action_module_signature_vendor                action_module_device_info

                                       action_module_signature_is_embedded           action_module_wf_verdict

                                       action_module_signature_status                

                                       action_module_file_info                       

                                       action_module_last_writer_actor               

                                       action_module_other_load_location             

                                       action_module_page_protection                 

                                       action_module_system_properties               

                                       action_module_code_integrity                  

                                       action_module_boot_code_integrity             

  **User Status Change **              action_user_status                            

                                       action_username                               

                                       action_user_status_sid                        

                                       action_user_session_id                        

                                       action_user_is_local_session                  

  **Host Status Change **              action_boot_time                              

                                       action_powered_off                            

  **Agent Status Change**                                                            action_boot_instance_cleanup_required

                                                                                     agent_status_component

  **Host Metadata Discovery/Change**   host_metadata_interface_map                   

                                       host_metadata_hostname                        

                                       host_metadata_domain                          
  -------------------------------------------------------------------------------------------------------------------------------------

##### Common fields for all event types

The table below lists the common fields for all event types and the
fields that are included and excluded.

  -------------------------------------------------------------------------------------------------
  Common fields for all   Included field                      Excluded field
  event types                                                 
  ----------------------- ----------------------------------- -------------------------------------
  **Agent**               agent_content_version               agent_install_type

                          agent_hostname                      event_utc_diff_minutes

                          agent_interface_map                 manifest_file_version

                          agent_os_sub_type                   source_message_id

                          agent_os_type                       zip_id

                          agent_version                       agent_request_time

                          agent_id                            server_request_time

                          agent_ip_addresses                  agent_id_hash

                          agent_ip_addresses_v6               agent_id_hash_bre

                                                              backtrace_identities

                                                              \_product

                                                              \_vendor

                                                              actor_fields

                                                              agent_is_vdi

  **Common**              event_version                       event_is_impersonated

                          event_type                          event_is_replay

                          event_sub_type                      event_impersonation_status

                          event_id                            event_is_simulated

                          event_timestamp                     event_user_presence

                          event_rpc_interface_uuid            agent_host_boot_time

                          event_rpc_func_opnum                agent_session_start_time

                                                              event_validity_enum

                                                              event_invalidity_field

                                                              event_rpc_inteface_version_major

                                                              event_rpc_inteface_version_minor

                                                              event_rpc_protocol

                                                              event_address_mapped

                                                              event_user_presence_status

  **Actor**               os_actor_local_ip                   actor_ns_user_sid

                          os_actor_local_port                 actor_process_auth_id

                          os_actor_primary_user_sid           actor_process_causality_id

                          os_actor_primary_username           actor_process_ns_pid

                          os_actor_process_command_line       actor_process_session_id

                          os_actor_process_image_md5          actor_process_signature_is_embedded

                          os_actor_process_image_name         actor_process_signature_product

                          os_actor_process_image_path         actor_process_signature_vendor

                          os_actor_process_image_sha256       actor_remote_host

                          os_actor_process_signature_status   actor_remote_pipe_name

                          os_actor_process_logon_id           actor_remote_port

                          os_actor_process_os_pid             actor_rpc_interface_version_major

                          os_actor_remote_ip                  actor_rpc_interface_version_minor

                          os_actor_process_instance_id        actor_rpc_protocol

                          os_actor_thread_thread_id           actor_type

                                                              actor_rpc_func_opnum

                                                              actor_rpc_interface_uuid

                                                              actor_process_device_info

                                                              actor_process_execution_time

                                                              actor_process_file_create_time

                                                              actor_process_file_mod_time

                                                              actor_process_file_size

                                                              actor_process_image_extension

                                                              actor_process_instance_id

                                                              actor_process_command_line_indices

                                                              actor_process_integrity_level

                                                              actor_process_is_special

                                                              actor_process_last_writer_actor

                                                              actor_process_instance_id

                                                              actor_thread_thread_id

                                                              actor_is_injected_thread

                                                              actor_causality_id

                                                              actor_effective_username

                                                              actor_effective_user_sid
  -------------------------------------------------------------------------------------------------

### Manage compute units

Cortex XSIAM uses compute units (CU) for these types of queries:

- API Queries: When running Cortex Query Language (XQL) queries on your
  data sources using APIs, each XQL query API consumes CU based on the
  timeframe, complexity, and number of API response results.

- Apps: The Notebooks instance consumes 1000 CU each day, and BigQuery
  queries consume CU based on the timeframe, complexity, and number of
  results. Apps are charged daily at 00:00 UTC.

- Cold Storage Queries: Cold Storage is a data retention offering for
  cheaper storage, usually for long-term compliance needs, with limited
  search options. You can perform queries on Cold Storage data using the
  following dataset formats:

  - For typical cold storage queries: `cold_dataset = <dataset name>`

  - For historical data imported into cold storage queries:
    `cold_dataset = archive_<dataset name>`.

  <!-- -->

  - > **Note**

    > For more information, see [Import historical data into cold
    > storage](#UUIDf5d1e9e1601b5f33cb0de6f6e14945e4).

<!-- -->

- These cold storage queries consume CU according to the following
  calculations:

  - Amount of data queried. 1CU for querying 35GB of data.

  - Timeframe, complexity, and the number of Cold Storage response
    results of each XQL Cold Storage query.

  When you query Cold Storage data, the rewarmed data is saved in a
  temporary hot storage cache that is available for subsequent queries
  on the same time range at no additional cost. The rewarmed data is
  available in the cache for 24 hours, and on each re-query, the cached
  data is extended for 24 hours, for up to 7 days.

  > **Note**

  > The CU consumption of cold storage queries is based on the number of
  > days in the query time frame. For example, when querying 1 hour of a
  > specific day, the CU of querying this entire day is consumed. When
  > querying 1 hour that extends past 2 days, such as from 23:50 to
  > 00:50 of the following day, the CU of querying these two days is
  > consumed.

#### Compute units usage

Cortex XSIAM provides a free daily quota of compute units (CU) allocated
according to your license size. Queries called without enough quota will
fail. To expand your investigation capabilities, you can purchase
additional CU by enabling the Compute Unit add-on.

The Compute Unit add-on provides an additional 1 compute unit per day
for a year, in addition to your free annual quota. For example, if you
have allocated 1,825 free annual CU, with the add-on, you will have a
total of 2,190 annual compute units. The Compute Unit add-on is
calculated on an annual basis, starting from the procurement of your
add-on license. The minimum purchase amount is 50 compute units.

You can configure the daily consumption limit for your compute units
according to your organizational needs and change it when needed. For
example, you can set a lower limit on a daily basis, and during an
incident investigation, you can change it to a higher limit that enables
you to consume more compute units.

Your unused compute unit balance cannot be transferred from one
licensing period to the next.

To gauge how many CU you require, Cortex XSIAM provides a 30-day free
trial period with 1/12 of your allocated annual CU quota to run XQL API
and Cold Storage queries. You can then track the cost of each XQL API
and Cold Storage query responses, and the **Compute Units Usage** page.
In addition, Cortex XSIAM sends a notification when the Compute Units
add-on has reached your daily threshold.

> **Note**
>
> To enable the add-on, select Settings \> Configurations \> Cortex
> XSIAM License \> Addons tile, and select the **Compute Unit** tile and
> **Enable**.

**How to manage your CU usage for your queries**

1.  Select Settings \> Configurations \> Data Management \> Compute Unit
    Usage.

2.  In **Annual Usage in Compute Units**, monitor the number of free
    compute units per license year, the number of purchased compute
    units per license year, and the ratio of used compute units to your
    yearly total compute units.

- If you have **Edit** permissions for Public APIs, you can customize
  the **Daily limit** to cater to your needs.

  - **Divide annual quota evenly**: Total annual compute units divided
    by 365.

  - **1% of annual quota**: 1% of the total annual compute units.

  - **No limit**

  - **Custom**: Configure a daily amount that is equal to or greater
    than your daily average calculated over a year (annual total/365).
    Use only integers.

  The default daily limit is the annual quota divided evenly.

  For Managed Security tenants, the values calculated are the total
  daily usage of parent and child tenants.

3.  In **Compute Units Usage** , view the compute unit usage over the
    past 30 days or over the past 12 months. For Managed Security
    tenants, make sure you select the tenant for which you want to
    display the information from the **MSSP Tenant Selection** drop-down
    menu.

    - **Compute Units Usage over the Last 30 Days**: Hover over each bar
      to view the total number of units used each day. The daily compute
      units are calculated at 00:00 UTC time. The red line represents
      your daily limit for that day. If you change the daily limit a few
      times on a specific day, the displayed limit is the last number
      you configured on that day. Select a bar to display in the
      **Compute Unit Usage** table the list of queries executed on the
      selected day.

    - **Compute Units Usage over the Last 12 Months**: Hover over each
      bar to view the total number of compute units used each month. The
      dotted gray line represents your average annual limit per month.
      You can use the 12-month display to plan how many compute units
      you need in the next licensing period.

4.  In the **Compute Units Usage** table, investigate all the queries
    that were executed on your tenant. For Managed Security tenants,
    make sure you select from the **MSSP Tenant Selection** drop-down
    menu the tenant for which you want to display the information. You
    can filter and sort according to the following fields.

    - **ID**: Unique identifier representing the executed XQL API query.

    - **Timestamp**

      - For XQL API: date and time of query execution.

      - For Notebooks and BQ queries: date and time the query is
        charged.

    - **Type**: Indicates the type of query performed.

    - **PAPI Key ID**: API Key ID used to execute XQL APIs.

    - **Query**: The query description.

    - **Compute Unit Usage**: Displays how many query units were used to
      execute the query.

    - **Tenant**: Appears only in a Managed Security tenant. Displays
      which tenant executed an API query or Cold Storage query.

5.  Investigate the XQL API or Cold Storage query results.

- In the **Compute Units Usage** table, locate an XQL API or Cold
  Storage query, right-click, and select **Show Results**.

  The query is displayed in the query ﬁeld of the Query Builder, where
  you can view the query results. For more information, see [How to
  build XQL queries](#UUID125805d7e53750e71a87cb4c4140fa73).

